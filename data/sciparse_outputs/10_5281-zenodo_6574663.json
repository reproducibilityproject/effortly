{"abstractText": "Copyright \u00a9 2022 I.-A. Kirca et al., released under a Creative Commons Attribution 4.0 International license. Correspondence should be addressed to Dani\u00ebl Hamerslag (d.c.hamerslag@gmail.com) The authors have declared that no competing interests exist. Code is available at https://github.com/DCHamerslag/FACT \u2013 DOI 10.5281/zenodo.6491095. \u2013 SWH swh:1:dir:25564a437957494e991b5205e262159e75d84d59;. Open peer review is available at https://openreview.net/forum?id=rKbgh3fXnRK.", "authors": [{"affiliations": [], "name": "Isa-Ali Kirca"}, {"affiliations": [], "name": "Dani\u00ebl Hamerslag"}, {"affiliations": [], "name": "Afra Baas"}, {"affiliations": [], "name": "Juno Prent"}, {"affiliations": [], "name": "Koustuv Sinha"}, {"affiliations": [], "name": "Sharath Chandra Raparthy"}], "id": "SP:e1f4777d3895107bd8ee0f8aeabd1d2a63b52cc3", "references": [{"authors": ["N. Mehrabi", "M. Naveed", "F. Morstatter", "A. Galstyan"], "title": "Exacerbating Algorithmic Bias through Fairness Attacks.", "venue": "arXiv preprint arXiv:2012.08723", "year": 2020}, {"authors": ["C. Dwork", "M. Hardt", "T. Pitassi", "O. Reingold"], "title": "and R", "venue": "Zemel. Fairness Through Awareness.", "year": 2012}, {"authors": ["M. Hardt", "E. Price"], "title": "and N", "venue": "Srebro. Equality of Opportunity in Supervised Learning.", "year": 2016}, {"authors": ["M.J. Kusner", "J.R. Loftus", "C. Russell"], "title": "and R", "venue": "Silva. Counterfactual Fairness.", "year": 2017}, {"authors": ["S. Verma", "J. Rubin"], "title": "Fairness Definitions Explained", "venue": "Gothenburg, Sweden,", "year": 2018}, {"authors": ["N. Mehrabi", "F. Morstatter", "N. Saxena", "K. Lerman"], "title": "and A", "venue": "Galstyan. A Survey on Bias and Fairness in Machine Learning.", "year": 2019}, {"authors": ["G. Duwe", "K. Kim"], "title": "Out With the Old and in With the New? An Empirical Comparison of Supervised Learning Algorithms to Predict Recidivism.", "venue": "Criminal Justice Policy Review", "year": 2017}, {"authors": ["Z. Ereiz"], "title": "Predicting Default Loans UsingMachine Learning (OptiML).", "venue": "27th Telecommunications Forum (TELFOR)", "year": 2019}, {"authors": ["D. Solans", "B. Biggio", "C. Castillo"], "title": "Poisoning Attacks on Algorithmic Fairness.", "venue": "CoRR abs/2004.07401", "year": 2020}, {"authors": ["J.A. Nelder", "R. Mead"], "title": "A Simplex Method for Function Minimization.", "venue": "The Computer Journal 7.4 (Jan", "year": 1965}, {"authors": ["B. Biggio", "B. Nelson"], "title": "and P", "venue": "Laskov. Poisoning Attacks against Support Vector Machines.", "year": 2013}, {"authors": ["S. Me"], "title": "and X", "venue": "Zhu. \u201cUsing Machine Teaching to Identify Optimal Training-Set Attacks on Machine Learners.\u201d In: AAAI.", "year": 2015}, {"authors": ["S. Me"], "title": "and X", "venue": "Zhu. \u201cThe Security of Latent Dirichlet Allocation.\u201d In: Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics. Ed. by G. Lebanon and S. V. N. Vishwanathan. Vol. 38. Proceedings of Machine Learning Research. San Diego, California, USA: PMLR, Sept.", "year": 2015}, {"authors": ["B. Li", "Y. Wang", "A. Singh"], "title": "and Y", "venue": "Vorobeychik. Data Poisoning Attacks on Factorization-Based Collaborative Filtering.", "year": 2016}, {"authors": ["P.W. Ko"], "title": "and P", "venue": "Liang. \u201cUnderstanding black-box predictions via influence functions.\u201d In: Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org.", "year": 2017}, {"authors": ["L. Mu\u00f1oz-Gonz\u00e1lez", "B. Biggio", "A. Demontis", "A. Paudice", "V. Wongrassamee", "E.C. Lupu"], "title": "and F", "venue": "Roli. Towards Poisoning of Deep Learning Algorithms with Back-gradient Optimization.", "year": 2017}, {"authors": ["C. Yang", "Q. Wu", "H. Li"], "title": "and Y", "venue": "Chen. Generative Poisoning Attack Method Against Neural Networks.", "year": 2017}, {"authors": ["M.B. Zafar", "I. Valera", "M.G. Rodriguez", "K.P. Gummadi"], "title": "Learning fair classifiers.", "year": 2015}, {"authors": ["P.W. Koh", "J. Steinhardt", "P. Liang"], "title": "Stronger data poisoning attacks break data sanitization defenses.", "year": 2018}, {"authors": ["D. Du"], "title": "and C", "venue": "Graff. UCI Machine Learning Repository.", "year": 2017}, {"authors": ["J. Larson", "S. Mattu", "L. Kirchner", "J. Angwin"], "title": "Compas analysis.\u201d In: GitHub, available at: https://github", "venue": "com/propublica/compas-analysis[Google Scholar]", "year": 2016}], "sections": [{"text": "R E S C I E N C E C Replication / ML Reproducibility Challenge 2021\n[\u00acRe] Reproducibility Study of \u2019Exacerbating Algorithmic"}, {"heading": "Bias through Fairness Attacks\u2019", "text": "Isa-Ali Kirca1,2, ID , Dani\u00ebl Hamerslag1,2, ID , Afra Baas1,2, ID , and Juno Prent1,2, ID 1Equal contributions \u2013 2Faculty of Science, University of Amsterdam, Science Park 904, 1098 XH Amsterdam, the Netherlands\nEdited by Koustuv Sinha,\nSharath Chandra Raparthy\nReviewed by Anonymous Reviewers\nReceived 04 February 2022\nPublished 23 May 2022\nDOI 10.5281/zenodo.6574663\n1 Reproducibility Summary"}, {"heading": "Scope of Reproducibility", "text": "The goal of this paper is to assess the reproducibility of experiments and results in the paper \u2019Exacerbating Algorithmic Bias through Fairness Attacks\u2019 by [1], from which the following claims are evaluated:\n\u2212 Claim 1: The anchoring attacks reduce the fairness of anMLmodel trained on the three data sets German Credit, COMPAS and Drug consumption.\n\u2212 Claim 2: The influence attack reduces the fairness of an ML model trained on the three data sets German Credit, COMPAS and Drug consumption."}, {"heading": "Methodology", "text": "We used the code the authors published alongside their paper as a resource to under\u2010 stand the methodology of their experiments, which was only briefly touched upon in the original paper. Our contribution is to extrapolate the original method using the pro\u2010 vided code and to use this to recreate the experiments, successfully obtaining similar results as the paper and supporting their claims."}, {"heading": "Results", "text": "Our results followed similar patterns as those of the authors, which backs up their claims regarding the attacks. However, our results did slightly deviate from their re\u2010 sults, meaning the original paper has some reproducibility issues in the context of our experimental setup."}, {"heading": "What was easy and what was difficult", "text": "It was difficult to understand the experiments from the paper. In our specific setting it was not possible to obtain similar results following only the methodology of their paper. Recreating the data sets required several assumptions. Reorganizing the code was a challenge in and of itself, owing to a lack of documentation within the original code.\nCopyright \u00a9 2022 I.-A. Kirca et al., released under a Creative Commons Attribution 4.0 International license. Correspondence should be addressed to Dani\u00ebl Hamerslag (d.c.hamerslag@gmail.com) The authors have declared that no competing interests exist. Code is available at https://github.com/DCHamerslag/FACT \u2013 DOI 10.5281/zenodo.6491095. \u2013 SWH swh:1:dir:25564a437957494e991b5205e262159e75d84d59;. Open peer review is available at https://openreview.net/forum?id=rKbgh3fXnRK.\nReScience C 8.2 (#19) \u2013 Kirca et al. 2022 1\nCommunication with original authors We had no direct contact with the authors. However, other research teams working on reproducing the samework provided us with a digital environment file supplied to them by the authors.\nReScience C 8.2 (#19) \u2013 Kirca et al. 2022 2\n2 Introduction\nRecent years have seen a rising interest in algorithmic fairness, which has led to differ\u2010 ent measures and definitions for characterizing fairness ([2, 3, 4, 5, 6]). Areas in which algorithmic fairness has become prevalent include predicting whether prisoners are likely to re\u2010offend upon release ([7]) or whether an individual is likely to default on a loan payment ([8]). In \u2019Exacerbating Algorithmic Bias through Fairness Attacks\u2019 by [1] it is claimed that ma\u2010 chine learning (ML) models are not only susceptible to various malicious adversarial attacks targeting their accuracy, but also to those targeting the fairness of ML models. Mehrabi argues that a model\u2019s fairness is as important as its accuracy and research into adversarial attacks specifically designed to attack fairness is thereforewarranted. To test the robustness of fairness methods intended to increase the fairness of an ML model, the researchers propose two novel data poising attacks on fairness, those being the an\u2010 choring attack and the influence attack. The anchoring attack has two variations; random and non\u2010random. The core concept is to place poisoned points near real data points of a data set, to skew the decision bound\u2010 ary of an ML model. These poisoned points are identical to the point they are placed close to, but with the opposite target label. The influence attack on fairness (IAF) aims to lower the fairness of an ML model by introducing fairness loss to the loss function. Maximizing for this loss functionmaximizes the covariance between the distance to the decision boundary and the sensitive features. This paper investigates the reproducibility of the research of [1]. Additionally, their claims regarding the two proposed fairness attacks the fairness of a targeted ML model will be tested, analyzed, and evaluated.\n3 Scope of reproducibility\nThemain contribution of [1] is presenting twonovel fairness attacks, called (randomand non\u2010random) anchoring attacks and influence attacks, and showing that these attacks more negatively impact the fairness scores of MLmodels than adversarial attacks on ac\u2010 curacy. To reproduce to work of the the original paper, the code and altered versions of three data sets, German Credit, Drug Consumption and COMPAS data sets accompany\u2010 ing the paper, which is publicly available on GitHub1, are utilized. Fairness is quantified using the metrics statistical parity difference (SPD) ([2]) and equality of opportunity dif\u2010 ference (EOD) ([3]), following the approach of [1]. The following are the main claims made within the original paper by [1]:\n\u2212 Claim 1: The anchoring attacks reduce the fairness of anMLmodel trained on the three data sets German Credit, COMPAS and Drug consumption.\n\u2212 Claim 2: The influence attack reduces the fairness of an ML model trained on the three data sets German Credit, COMPAS and Drug consumption.\n\u2212 Claim 3: Poisoning attacks designed to attack the accuracy of anMLmodel are not suitable as a fairness attack.\nClaim 3 will not be considered in this paper, as the original authors mention it only briefly. They only evaluated whether influence attacks on accuracy had any effect on a model\u2019s fairness, without evaluating any other form of accuracy attack. In order to obtain results that can reject or support this claim, one would have to consider other adversarial attacks on accuracy, which is beyond the scope of this paper. To demonstrate the effectiveness of their fairness attacks, the authors compare it to a fairness attack inspired by [9]. However, to thoroughly evaluate the effectiveness of the\n1https://github.com/Ninarehm/attack\nReScience C 8.2 (#19) \u2013 Kirca et al. 2022 3\nnovel attacks, one would have to compare against multiple other concurrent works on adversarial attacks on fairness. Since we were only allocated four weeks for this project, this is also beyond the scope of this paper. The focus of this paper will thus solely be on reproducing the novel attacks introduced by [1] and evaluating claims 1 and 2.\n4 Methodology\nThe authors\u2019 code, provided alongside the paper, includes a clear entry point as well as the data sets used for the discussed experiments. However, there were several is\u2010 sues with reproducing the experiments, such as a reliance on outdated Python libraries of which the new versions are not backwards\u2010compatible. This is likely a result of the code being a combination of the code of previous papers that [1] based their research on, which resulted in a lack of documentation. Furthermore, information about data pre\u2010processing is missing from the original paper, causing reproducibility issues. Only the attributes and the classification goal for each data set were clearly reported. Addi\u2010 tionally, the number of features we discovered in the data sets provided by the authors did not match the number of features described in their paper. These issues required us to make multiple assumptions as we aimed to recreate these modified data sets from the original raw versions. The exact nature of these assumptions is further detailed in section 4.2. A list of all made assumptions is found in the Appendix. As a result of this obscurity regarding both the originalmethod and the number of assumptions necessary to reconstruct the method, we decided not to re\u2010implement the code in its entirety, in\u2010 stead making adjustments and additions to the original code to reproduce the original implementation. This is discussed in the next section. To increase the scalability and maintainability of the code base, the intent was to em\u2010 ploy the PyTorch framework instead of the TensorFlow framework used by the origi\u2010 nal authors. However, there were no straighforward substitutions for some TensorFlow functions, such as tf.truncated_normal_initializer and tf.variable. This would necessi\u2010 tate a change to some of the code\u2019s fundamental structures. As our approach is centered aroundutilizing the code provided by the authors, which, due to its complexity, required a significant amount of time to understand, therewas a limited amount of time available for making such substantial modifications to the code.\n4.1 Model descriptions The model that the authors used to minimize the classification loss was not specified in the original paper. The authors\u2019 code, however, revealed that SciPy\u2019s fmin optimizer2 was utilized as a minimizer for the experiments, which minimizes the loss by applying the Nelder\u2010Mead algorithm ([10]). A data poisoning attack (DPA) has the goal of creating poisoned data set Dp using the original clean data setDc, such that the defender\u2019s test loss function L(\u03b8\u0302;Dtest) is maxi\u2010 mized. To do so, iterative gradient steps are taken on each of the features of the poisoned data points Dp. The poisoned points are then projected to the feasible set F\u03b2 to avoid being detected by the defender\u2019s anomaly detector. According to the paper, as well as the algorithms in Figure 11, the feasible set is obtained by applying anomaly detector B; Fb \u2190 B(Dc\u222aDp). However, the anomaly detector B is not described in detail. Observing the code led to the assumption that the feasible set is determined by simply projecting the data onto a slab in close proximity to the target, shielding the attacker from anomaly detection. This is not the first time that such gradient\u2010oriented poisoning of data was implemented, as it was first explored using SVMs ([11]), and in the following years extended to linear\n2https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin.html\nReScience C 8.2 (#19) \u2013 Kirca et al. 2022 4\nand logistic regression ([12]), topic modeling ([13]), collaborative filtering ([14]), and neu\u2010 ral networks ([15, 16, 17]). [15] called this the projected gradient ascent method since it calculates the gradient during training, but instead of changing themodel parameters to decrease the loss, it poisons the data to increase the loss. This attack on accuracy can be defined as the following optimization problem, where \u03f5 is a hyperparameter discussed in section 4.4.\nmax Dp Ladv\n( \u03b8\u0302;Dtest ) s.t. |Dp| = \u03f5 |Dc| with Dp \u2286 F\u03b2\nwhere \u03b8\u0302 = argminL (\u03b8;Dc \u222a Dp) . (1)\nInfluence Attack on Fairness (IAF) is a DPA inspired by the influence attack on accu\u2010 racy ([15]) and the work of [18], which introduced a loss function for fair classification involving a fairness constraint, called decision boundary covariance. Decision bound\u2010 ary covariance is the covariance between the sensitive feature z, which is gender in this case, and the signed distance from the feature vector to the decision boundary d\u03b8(x)).\nCov(z, d\u03b8(x)) \u2248 1\nN N\u2211 i=1 (zi \u2212 z)d\u03b8(xi) (2)\nIf class labels in the training set are correlatedwith one ormore sensitive attributes ziNi=1 (e.g. gender, race), the percentage of samples with a certain sensitive attribute having d\u03b8(xi) \u2265 0 may differ drastically from the percentage of users without this sensitive at\u2010 tribute value having d\u03b8(xi) \u2265 0. The intuition behind decision boundary covariance is that the sensitive attributes should not determine which side of the decision boundary a point is on, and thus which label it receives. The left side of Figure 1 shows an instance where the sensitive attribute (shape) and assigned label (color) have zero covariance, in\u2010 dicating that the sensitive attribute has no influence on classification. On the right, the covariance is either extremely positive or extremely negative, indicating that the sensi\u2010 tive attribute does correlate with the classification result. The goal of the adversary is to maximize the covariance between z and d\u03b8(xi), which will decrease the fairness of the classification. It is worth noting that this covariance can happen even if sensitive attributes aren\u2019t utilized to construct the decision boundary, because sensitive attributes can be correlated with one or more of the other features. IAF is a variant of the influence attack by [19] and [15] that includes demographic in\u2010 formation. This demographic information, specifically gender, is used to decide which group is advantaged and disadvantaged, calledDa andDd respectively, during sampling. Similar to the convention in [19], one positive and one negative instance are sampled uni\u2010 formly at random, after which |Dc| instances are created to act as poisoned points Dp. The poisoneddata points are inversely proportional to the class balance, such that (|D+c |) positive poisoned data points are sampled from Da and (|D+c |) negative poisoned data points are sampled from Dd, in which |D+c | and |D\u2212c | represent the number of positive and negative points in the clean data respectively.\nReScience C 8.2 (#19) \u2013 Kirca et al. 2022 5\nThe loss function of IAF, combines \u2113fairness with the loss function of the influence at\u2010 tack, \u2113acc as defined in Equation 3, with hyperparameter \u03bb controlling the impact of the fairness loss on the adversarial loss.\nLadv(\u03b8;Dtest) = \u2113acc + \u03bb\u2113fairness where lfairness = 1\nN N\u2211 i=1 (zi \u2212 z)d\u03b8(xi) (3)\nAlgorithm 1, as shown in Figure 11, details the implementation of this poisoning attack, using the aforementioned parameters. Anchoring Attack is another DPA and its objective is to target some points and cloud their labels with poisoned points with opposing labels, resulting in a skewed decision boundary. In contrast to IAF, the loss of the model is not used, meaning this attack can be used in combination with any model and loss function. A target point xtarget is sampled in one of twoways, as demonstrated in Figure 11. In the random anchoring attack (RAA), these anchor points are chosen uniformly at random for each demographic group, while in the non\u2010random instance (NRAA) they are picked based on their popularity, which is defined as the amount of similar data points in their vicinity. Next, poisoned points are created and are placed in close vicinity of xtarget, resulting in them having the same demographic as xtarget, but the opposite label. This will skew the decision boundary, causing more advantaged points to have a predictive outcome of +1 and more disadvantaged points to have a predictive outcome of \u20101, as depicted in Figure 2.\n4.2 Data sets The data sets listed below were used in both the original paper\u2019s experiment and our own. The data is split 80\u201020 between the training and test set and and gender has been chosen as the sensitive attribute for each data set. German Credit data set3 This data set is from the UCI ML repository ([20]). It contains credit profiles with 20 attributes for 1000 individuals. The classification goal is to predict whether an individual has a good or bad credit score. The pre\u2010processed German data provided with the paper has the same number of samples, but 58 attributes instead of 20. Based on data exploration we made the assumption that this is the result of one\u2010hot\u2010 encoding of categorical features. COMPAS data set4 This data set is provided by ProPublica ([21]). It consists of profiles with 52 attributes such as criminal history, jail time and demographics about 7214 defen\u2010 dants from Broward County. In this case the classification goal is to predict whether an individual will re\u2010offend within two years after being released5. The original paper only\n3https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data) 4https://www.propublica.org/datastore/dataset/compas\u2010recidivism\u2010risk\u2010score\u2010data\u2010and\u2010analysis 5Therefore, COMPAS\u2010scores\u2010two\u2010years.csv is the relevant data set\nReScience C 8.2 (#19) \u2013 Kirca et al. 2022 6\nlooked at the eight attributes specified in Table 1. The pre\u2010processed COMPAS data pro\u2010 vided with the paper has the same number of samples as the original but 16 attributes, again due to one\u2010hot\u2010encoding. Drug Consumption data set6 This data set is also from the UCI ML repository ([20]). It contains profiles of 1885 individuals, consisting of 32 attributes. The classification goal is to predict whether or not an individual has consumed cocaine at some point in their lifetime. Only the 13 attributes specified in Table 1 are used in the original experiments and our own. The pre\u2010processed drug data provided with the code had 1885 samples and 13 attributes, like the original.\nThe authors provide pre\u2010processed versions of the aforementioned data sets without a description of the pre\u2010processing methodology. As such, we made the decision to pre\u2010 process the raw data we obtained from the original sources and will further refer to these as the recreated data sets. Our pre\u2010processing procedure is based on data exploration of the data sets provided by the authors. To run the code, the sensitive feature index must be specified. Data exploration revealed that the sensitive feature indexes are 36, 4 and 12 respectively for German, COMPAS and Drug. For the sake of simplicity, the sensitive feature is always moved to index 0 in the recreated data sets. Furthermore, males were represented with 0 and females with 1, as this is how they were labeled in the code. After finding out that the number of attributes in the recreated data set did notmatch the number of attributes of the authors\u2019 data, it was discovered that one\u2010hot encoding was used for categorical features, which could explain the reason these data sets contained more attributes than indicated in their paper. Thereafter the data was standardized, with a mean of 0 and a standard deviation of 1. To seewhether the attribute valuesmatched the attribute values of the authors\u2019 data, all the attributes were compared and popped once they matched. This procedure revealed the index of the sensitive features as well. Finally, the data was shuffled as this is common practice.\n4.3 Extension Our contribution to the existing work is making the original paper more reproducible, by documenting how we reproduced the findings for their novel fairness attacks. This is done by providing the pre\u2010processing procedure of the data7, which was discussed in section 4.2. Furthermore, we organized the code by removing unnecessary code and adding somedocumentation. This paper also covers all the assumptionsmade and infor\u2010 mation obtained from the code that was used to reproduce the results, shown in section 5. This is accumulated into a more comprehensive model description in section 4.1 and experimental setup in section 4.4.\n4.4 Experimental setup and Computational requirements The hyperparameters for this experiment are \u03f5 and \u03bb. \u03f5 determines the size of the poi\u2010 soned data set as a fraction of the clean data and \u03bb controls the trade\u2010off between accu\u2010\n6https://archive.ics.uci.edu/ml/datasets/Drug+consumption+%28quantified%29 7https://github.com/DCHamerslag/FACT\nReScience C 8.2 (#19) \u2013 Kirca et al. 2022 7\nracy loss and fairness loss, in the loss function of IAF; Ladv = \u2113acc + \u03bb\u2113fairness. Statistical Parity Difference captures the difference in predictive outcome between dif\u2010 ferent (advantaged and disadvantaged) demographic groups. It is defined as:\nSPD = |p(Y\u0302 = +1|x \u2208 Da)\u2212 p(Y\u0302 = +1|x \u2208 Dd)| (4)\nEquality of Opportunity Difference captures the difference in the true positive rate be\u2010 tween different (advantaged and disadvantaged) demographic groups. It is defined as:\nEOD = |p(Y\u0302 = +1|x \u2208 Da, Y = +1)\u2212 p(Y\u0302 = +1|x \u2208 Dd, Y = +1)| (5)\nAs in the original paper, we evaluate the attacks by plotting accuracy and the aforemen\u2010 tioned SPD and EOD fairness criteria. The model becomes more unfair as SPD and EOD get closer to 1. Despite the fact that the authors do not indicate the seed used in their experiment or if they averaged numerous seeds, the code revealed a default seed for each attack setup. In our experiment, three runs were executed for each type of fair\u2010 ness attack and data set. The used seeds for each attack and data set combination were the default seed, the default seed plus 1 and the default seed plus 2. Each run examined \u03f5 values ranging from 0.0 to 1.0, with 0.1 increments. \u03bb was set to 1.0 for all runs with IAF, like in the original work. Because the original results are only presented as graphs, instead of numbers, we examine the difference between the original and reproduced plots to assess if the reproduced results are similar to the results in the original paper. It was not specified whether the average accuracy, max accuracy or the last iteration\u2019s accuracy was taken over multiple runs. We plotted the results for each instance \u2010 an example is given in Figure 6 in the Appendix \u2010 and observed that the last of the metrics, accuracy, SPD and EOD is most similar to the results in the original paper. Therefore, metrics of the last iteration are used in Section 5. Furthermore, the code is not optimised to utilize a GPU, so the experiments are executed on a MacBook Pro (2017) with a 3.3 GHz Dual\u2010Core Intel Core i5 processor and 16 GB memory. The training time was about five minutes for IFA with 30 to 200 iterations, less than oneminute for RAA with 29 iterations and less than twominutes for NRAAwith 29 iterations. However, the training time for NRAA on the COMPAS data set was about 90 minutes. See Table 3 in the Appendix for further specifics regarding run times.\n5 Results\n5.1 Results reproducing the original paper The results in Figure 3 display the last iteration\u2019s accuracy, SPD and EOD, obtained using the data providedwith the default seed. The influence attack andboth anchoring attacks are presented in the same plot. The reproduced results are similar to those presented by the authors, see Figure 5 in the Appendix. Because the SPD and EOD scores are relatively high for IAF, RAA and NRAA, the results support both claims 1 and 2 from Section 3.\n5.2 Results beyond original paper The results in Figure 4 display the last iteration\u2019s accuracy, SPD and EOD of the recreated data sets, with the default seed. Although the results differ from the results obtained when using the data provided by the authors, the SPD and EOD scores are relatively high for IAF, RAA andNRAA and therefore, these results also support claim 1 and 2 in section 3. Furthermore, the results for the last iteration\u2019s accuracy, SPD and EOD with different seeds for both the authors\u2019 data as well as the recreated data are shown in figures 7, 8, 9 and 10.\nReScience C 8.2 (#19) \u2013 Kirca et al. 2022 8\n6 Discussion\nUpon visual inspection, the results obtained using the authors\u2019 data sets, seen in Figure 3, are similar to those presented in their paper, with the graphs following similar pat\u2010 terns as those in the original paper. Small differencesmay be caused by our assumption that the default seed was used and not an average over various seeds. The results ob\u2010 tained from the recreated data sets, seen in Figure 4, do not appear very similar to those in the original paper. This could be the result of any of the assumptions that needed to be made to recreate the authors\u2019 altered data sets, such as the assumption that the\nReScience C 8.2 (#19) \u2013 Kirca et al. 2022 9\ndata had been shuffled. If any of our assumptions are incorrect, this could well explain the differences. They do, however, follow a similar pattern. It can thus be stated that claims 1 and 2 of the authors are supported by our experimental results. Future work could be to test the robustness of fairnessmethods using the novel fairness attacks. This was beyond the scope of the work done in [1], but would be a sensible next step to take, as they were designed for this purpose. Another way in which this work can be expanded upon is by thoroughly comparing these results to those of attacks on accuracy to test claim 3 as listed in Section 3. Also, these results can be compared with the results of other fairness attacks to better contextualize the performances of the novel attacks. Additionally, it can be of interest to test the fairness performance of the novel attacks on different data sets with sensitive attributes other than gender to see how well the attacks generalize.\n6.1 What was easy and what was difficult Once the digital environment was received from the authors, we were able to run the code with the provided data sets and obtain results similar to those given in the original paper, see Figure 5.1. However, the lack of documentation in the method regarding the type of model used, the data pre\u2010processing procedure, a lack of details regarding SVM and hinge loss make the original paper unnecessarily time\u2010consuming to reproduce. A significant amount of the information about the implementation, needed to reproduce the experiments from scratch, was provided by the code they released and their reference materials, such as [19] and [18].\n6.2 Communication with original authors There was no direct communication between us and the original authors. However, we communicated with other research teams working on reproducing the same work and they provided us with a digital environment file supplied by the authors that is not publicly available. Its content is listed in the Appendix.\n7 Conclusion\nIt can be concluded that the main claims of [1] regarding the effectiveness of their fair\u2010 ness attacks are correct. However, fully reproducing their results proved too difficult with our setup. The main obstacles we encountered were a lack of documentation re\u2010 garding their data pre\u2010processing and their used model. Future work would do well to focus on several areas, such as comparisons with other attacks or experimentation with different data sets."}, {"heading": "Appendix", "text": ""}, {"heading": "More details", "text": "List of used dependencies\n\u2022 Python 3.6\n\u2022 PIP 20.3.1\n\u2022 setuptools 19.2 (in most of the cases you have to downgrade)\n\u2022 Tensorflow 1.12.3\n\u2022 scikit\u2010learn 0.23.1\n\u2022 tensorboard 1.12.2\n\u2022 cvxpy 0.4.11 [cvxpy 1.0+ is not backwards compatible, therefore the downgrade of setuptools]\n\u2022 CVXcanon 0.1.1\n\u2022 scs 2.1.2\n\u2022 scipy 1.1.0\n\u2022 numpy 1.16.2\n\u2022 pandas 1.1.4\n\u2022 Matplotlib 3.3.3\n\u2022 tabulate 0.8.9\n\u2022 seaborn 0.11.0\n\u2022 tqdm 4.62.3\n\u2022 IPython 7.16.1\n\u2022 pillow 8.0.1"}, {"heading": "List of Assumptions Made", "text": "\u2022 The seed used by the authors is the default seed observed in the code.\n\u2022 Data was shuffled before use\n\u2022 Categorical features were one\u2010hot encoded except the sensitive feature.\n\u2022 Female is represented with the value 1 and male with the value 0.\n\u2022 Data was standardized with a mean of 0 and a standard deviation of 1\n\u2022 Results were based on the test error, SPD and EOD of the last iteration.\n\u2022 The feasible set is assumed to be decided by simply projecting the data to a sphere or slab within the vicinity of the target\nReScience C 8.2 (#19) \u2013 Kirca et al. 2022 12\nReScience C 8.2 (#19) \u2013 Kirca et al. 2022 13\nReScience C 8.2 (#19) \u2013 Kirca et al. 2022 14\nReScience C 8.2 (#19) \u2013 Kirca et al. 2022 15\nReScience C 8.2 (#19) \u2013 Kirca et al. 2022 16"}], "title": "[\u00acRe] Reproducibility Study of \u2019Exacerbating Algorithmic Bias through Fairness Attacks\u2019", "year": 2022}