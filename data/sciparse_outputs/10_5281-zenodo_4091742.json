{"abstractText": "DOI 10.5281/zenodo.4091742 AbstractThis article reports on the largely successful reproduction of the author\u2019s decade-old conference paper. The original 2010 paper demonstrated that character recognition performance could be improved on difficult problems of scene text recognition by leveraging font-specific correlations between character identity and width. The work relied on a sizeable array of languages, tools, and libraries. The computation proceeded in three major phases: data synthesis, training statistical models on artificial image data and actual text data, and finally fine-tuning and running a parserwith the trainedmodels applied to the test images. Using the learnedmodels stored from the original paper\u2019s experiments, the original parser code successfully reproduced the results exactly. The training code required minor changes to run with current host environments and libraries. Although the updated experimental results are not identical, they follow the same general trends, indicating a successfully repeated experiment.", "authors": [{"affiliations": [], "name": "Jerod Weinman"}, {"affiliations": [], "name": "Nicolas P. Rougier"}], "id": "SP:ce0577db5ebe74fe08f4fcbb7c42935ae092448c", "references": [{"authors": ["J.J. Weinman"], "title": "Typographical Features for Scene Text Recognition.", "venue": "Proc. IAPR Intl. Conf. on Pattern Recognition. Istanbul, Turkey,", "year": 2010}, {"authors": ["J. Weinman.Data"], "title": "Repository for Reproducible Research", "venue": "Tech. rep. Grinnell, Iowa: Grinnell College,", "year": 2014}, {"authors": ["J.J. Weinman"], "title": "Unified Detection and Recognition for Reading Text in Scene Images.", "venue": "PhD thesis. University of Massachusetts Amherst,", "year": 2008}, {"authors": ["J.J. Weinman", "E. Learned-Miller", "A.R. Hanson"], "title": "A Discriminative Semi-Markov Model for Robust Scene Text Recognition.", "venue": "Proc. IAPR Intl. Conf. on Pattern Recognition", "year": 2008}, {"authors": ["R.D. Peng", "S.P. Eckel"], "title": "Distributed Reproducible Research Using Cached Computations.", "venue": "Computing in Science Engineering", "year": 2009}, {"authors": ["E.P. Simoncelli", "W.T. Freeman"], "title": "The Steerable Pyramid: A Flexible Architecture for Multi-Scale Derivative Computation.", "venue": "Proc. IEEE Intl. Conf. on Image Processing. Vol", "year": 1995}, {"authors": ["A.L. Berger", "S.A. Della Pietra", "V.J. Della Pietra"], "title": "A Maximum Entropy Approach to Natural Language Processing.", "venue": "Computational Linguistics", "year": 1996}, {"authors": ["B. Krishnapuram", "L. Carin", "M.A. Figueiredo", "A.J. Hartemink"], "title": "Sparse Multinomial Logistic Regression: Fast Algorithms and Generalization Bounds.", "venue": "IEEE Trans. on Pattern Analysis and Machine Intelligence", "year": 2005}, {"authors": ["R.H. Byrd", "J. Nocedal", "R.B. Schnabel"], "title": "Representations of quasi-Newton matrices and their use in limited memory methods.", "venue": "In:Mathematical Programming", "year": 1994}, {"authors": ["J.J. Weinman", "A. Lidaka", "S. Aggarwal"], "title": "Large Scale Machine Learning.", "venue": "GPU Computing Gems. Ed. by W.-m", "year": 2010}], "sections": [{"text": "R E S C I E N C E C"}, {"heading": "Reproduction / Pattern Recognition", "text": "[Rp] Reproducing \u201dTypographical Features for Scene Text Recognition\u201d\nJerod Weinman1, ID 1Grinnell College, Grinnell, Iowa, USA\nEdited by Georgios Is. Detorakis ID\nReviewed by Nicolas P. Rougier ID\nReceived April 30, 2020\nPublished October 15, 2020\nDOI 10.5281/zenodo.4091742\nAbstractThis article reports on the largely successful reproduction of the author\u2019s decade-old conference paper. The original 2010 paper demonstrated that character recognition performance could be improved on difficult problems of scene text recognition by leveraging font-specific correlations between character identity and width. The work relied on a sizeable array of languages, tools, and libraries. The computation proceeded in three major phases: data synthesis, training statistical models on artificial image data and actual text data, and finally fine-tuning and running a parserwith the trainedmodels applied to the test images. Using the learnedmodels stored from the original paper\u2019s experiments, the original parser code successfully reproduced the results exactly. The training code required minor changes to run with current host environments and libraries. Although the updated experimental results are not identical, they follow the same general trends, indicating a successfully repeated experiment.\nA reproduction of J. J. Weinman. \u201cTypographical Features for Scene Text Recognition.\u201d In: Proc. IAPR Intl. Conf. on Pattern Recognition. Istanbul, Turkey, Aug. 2010, pp. 3987\u20133990. DOI: 10.1109/ICPR.2010.970.\n1 Introduction\nAs readers of this journal know, reproducibility of computational experiments presents an important but surmountable challenge. This article reports on a peer-reviewed, archived conference paper by the author originally published in August 2010 [1]; it demonstrated that character recognition performance could be improved on difficult problems of scene text recognitionby leveraging font-specific correlations between character identity and width. That paper constituted the author s\u0313 first work to be fully conceived, developed, submitted, and published as a full-time faculty member following doctoral training and the transition to a new institution two years before. Although some of the raw benchmark data used in the paper dates to several years prior (2003), most of the experimental data for training the system dates to 2009, with the final reported experiments dating to January 2010 for the submission that same month.\nIt is possible to reconstruct the arc of the work because of the author s\u0313 use of a homespun experimental data repository created in 2009 precisely so that the details of experiments could not only be retraced, but ideally reproduced if necessary [2]. The motivation for that data repository was precisely because of the author s\u0313 own earlier inability to accurately trace and ultimately reproduce some dissertation results [3] after transitioning institutions and computational environments. It therefore seems quite fitting that the decade-old paper tested here represents the \u201cfirst fruits\u201d of that system and the first whose long-term reproducibility has been assessed.\nAfter briefly reviewing the original work and its context, the remainder of the article gives some additional details on the nature of the author s\u0313 local archival system for ex-\nCopyright \u00a9 2020 J. Weinman, released under a Creative Commons Attribution 4.0 International license. Correspondence should be addressed to Jerod Weinman (jerod@acm.org) The authors have declared that no competing interests exist. Code is available at http://hdl.handle.net/11084/23333 \u2013 DOI 11084/23333. \u2013 SWH swh:1:dir:0c20dd08d449661bbf90f9a209c822b3aeb1edb7. Data is available at http://hdl.handle.net/11084/23336 \u2013 DOI 11084/23336. Open peer review is available at https://github.com/ReScience/submissions/issues/35.\nReScience C 6.1 (#) \u2013 Weinman 2020 1\nperiments and data (Section 3), then describesmore about the experimental work under examination including the details of the software and computations in Section 4. Section 5 presents the work involved in bringing the experimental system back up to speed. A variety of ablation tests in the experimental chain attempt to identify or eliminate possible sources of variation in the reproduced results; Section 6 concludes with updated experimental results that match precisely in some cases, while the trend of relative performance in other cases still align with the original results.\n2 Background Context\nIn an application ofmachine-learning for pattern recognition, the original paper demonstrated that character recognition performance could be improved on difficult problems of scene text recognitionby leveraging font-specific correlations between character identity andwidth [1]. For example, although thewidth of the character \u201ce\u201d varieswidely across fonts, knowing (or hypothesizing) that information for a specific font should strongly inform beliefs about the width of other characters in the same font.\nWhile a deep body of work on document analysis and recognition existed at the time, the task of recognizing words from photographs of everyday scenes was still burgeoning. The challenges include a small sample of text for recognition (i.e., a few characters rather than a whole page), out of plane distortions, uneven lighting, unusual fonts, and unconventional word or character spacings. Thus, most prior methods for text line detection, word segmentation, text/non-text dichotomization, and character segmentation generally did not apply to this task.\nThe computational model introduced in the 2010 paper reproduced here relaxed many of the standing assumptions by integrating the word and character segmentation tasks with recognition, opening recognition to words from outside a limited vocabulary while retaining a bias for known lexiconwords, and eliminating the strict reliance on text/nontext binarization by working with raw pixels. For all these benefits, the work required significant amounts of data to train a large parametric model (over 11M parameters) with GPU acceleration.\nOperationally, the recognition model itself uses an energy minimization scheme that can also be understood as a weighted finite state transducer. The discriminative semiMarkov model resembles the familiar hidden Markov model (HMM), but with two important differences. First, it is trained discriminatively, meaning that the data is always assumed to be observed, so the conditional likelihood is maximized (rather than the joint likelihood of both the latent state and the observed data, as with the HMM). This change relaxes independence assumptions among the observed data, allowing for more powerful features. Second, the semi-Markov nature of the model further relaxes independence assumptions of the predicted/hidden states, so that characters of arbitrary width become the object of interest, rather than individual pixel columns. The model is optimized (energy minimized) via a dynamic programming algorithm similar to the Viterbi algorithm for HMMs, but approximated with a beam search to speed results when word segmentation is integrated with the recognition task [4].\nThe contribution of the original work was in demonstrating that in addition to the common technique of learning and leveraging character bigrams (i.e., \u201cq\u201d should be followed by a \u201cu\u201d), width bigrams can be leveraged to improve recognition (i.e., a relatively wide \u201cn\u201d should be followed by another wide character). Section 4 below outlines all the code, data, and processes that went into crafting that work, but first Section 3 describes the mechanisms for preserving most aspects of the data and computations.\nReScience C 6.1 (#) \u2013 Weinman 2020 2\n3 Data Repository for Reproducible Research\nThe author s\u0313 data repository for understanding reproducible research closely resembles the distributed cached computations framework of Peng and Eckel [5]; additional details are in Weinman [2]. Computations are cached in immutable objects called collections, and subsequent collections may utilize the results of other (previous) collections through a large dependency graph. Each collection is housed within a namespace hierarchy that helps identify and understand its role; the descriptive name of each collection also features a timestamp. For example, collection eB in Figure 1 is named experiments/text/ngrams/bigrams/tied_nums_intracase_L1_validation-20090708075734.\nVersionnumbers of source controlled code are documentedwithin a collection for reproducibility (and in current practice, the source code and appropriate revision are checked out of the source repository as part of the collection build process). For simplicity, the repository resides in the file system,managed by a few scripts and adherence to the practice; nothing \u201cforces\u201d a collection to be write-only. Use of the native file system makes both the generating code and resulting data relatively transparent, highly accessible, and easily portable, all key qualities for a framework deeply entangled with everyday work.\nThe key properties of the data collections are (i) that they may only access experimental software through well-defined source code repositories (system software, programming\nReScience C 6.1 (#) \u2013 Weinman 2020 3\nenvironments, or external libraries have not been precisely tracked), and (ii) they may only access other experimental data through the data dependency graph. In particular, scripts running in collections may not directly access other collections\u02bc source code nor any part of the host file system except through the dependencies. The only source code \u201cnative\u201d to a collection is a small set of scripts that invoke the library code (a copy of which is typically \u201cchecked out\u201d or \u201ccloned\u201d into the collection) on the data of interest (a dependency from another collection), together with any parameters needed. A simple Makefile is required so that invoking an argument-free make command generates the data after checking out (and compiling, if necessary) any additional source code. In this way, the data collectionsmake it fairly easy to understand how a particular computation or result was achieved. Other utilities make it easy to copy collections (sans data) for alteration (e.g., parameter tweaking) in a way that traces the provenance (which assists bug tracking/recovery) and access the dependencies (a simple text file) in a variety of programming languages.\nThe collections are divided into three categories: raw, processed, and experiments. Raw collections house curated or manually annotated data that is not programmatically generated. Associated codemight be storedwith these collections, such as download scripts or interactive prompts for label acquisition. They may also have dependencies, as in the case when the collection stores the annotation of another raw data set. The processed collections are for relatively straightforward programmatic transformations of data, such as denoising, format conversion, feature extraction, etc. Finally, the experimental collections house the most interesting data.\nNotably, none of the restrictions are programmatically enforced; the relatively lightweight framework operates by convention. However, because it uses the file system, collections are easily transportable across host machines. For example, experimental computations have been deployed to remote high-performance computing clusters on which the author has copied a subset of the data repository, so that the experimental dependencies can be found.\n4 Review of the Experiments\nThe paper s\u0313 result required a deep pipeline of data and awide array of tools and programming environments. This section provides a somewhat archeological and forensic view of everything that needed to be run and briefly indicates the dependencies on external libraries.\n4.1 Experimental Structure Figure 1 illustrates the dependency graph of all data repository collections (cf. Section 3) utilized in the paper. This section sketches the basic purpose and tool dependencies of each collection, with further details in Table 3.\nRaw Collections \u2014 Raw collections rB and rA represent the benchmark data used for evaluation; rB contains the cropped images of street and storefront signs recognized by the system, while rA contains the ground truth character annotations (the dependency arising from the interactive script used to acquire the annotations). Although the entire data repository itself was not created until 2009, the sign data collections are artificially timestamped to their original dates of creation in 2003 and 2005, respectively. Raw collection rC contains synthetic images of individual characters rendered in over 1800 fonts, which forms the basis of training data used for learning the appearance-based module of the recognizer as well as the intra-font character width correlations. Finally, rD contains\nReScience C 6.1 (#) \u2013 Weinman 2020 4\nthe raw ASCII text of 85 English-language books downloaded from Project Gutenberg, used for training the bigram-based language module.\nProcessed Collections \u2014 The four isomorphic processing chains in the center of the graph represent rendered and processed variations of charactersmade to appear as theywould in photographed storefronts, rather than the clean glyphs of a font library (rC). The recognizer was trained to discriminate not only among characters and numbers (the dependency chain rooted at pF), but also among inter-word spaces (pV ) and intra-word gaps between characters (pB), so synthetic exemplars from each of these categories are rendered, in addition to horizontally stretched and scaled versions of the original characters (pC). These root collections render the text with occasional randomly-placed borders. Collection pD simply counts the bigram frequencies in the books of rD; chains pF and pC utilize these bigram statistics to synthesize plausible neighboring characters that appear alongside the target character in the image. Collection pS measures the width of each character in each font for later use in the width bigram module that represents the paper s\u0313 scientific and technical contribution.\nThe roots of these chains\u2014pB, pV, pF, pC, and pS\u2014all use MATLAB as the computational engine with a small dependency on the author s\u0313 support library (stored in a version control system) of MATLAB tools. The bigrams of pD were counted using a simple standard C program housed and compiled within the collection.\nDeeper in the data processing chains, additional image transformations are applied before extracting image features used by the recognizer. Collections pA, pU, pE, and pQ add random contrasts, brightness, and linear bias fields to the images, while pH, pT, pR, and pP downsample the images by \u00bc. These collections also use MATLAB with minimal assistance from a local support library. Collections pO, pK, pM, and pI binarize the resulting images in the same way the test data will be; run within MATLAB, the Niblack binarization routine from the author s\u0313 library uses a mex implementation for speed (the mex interface allows programs written in C to be compiled and used from within MATLAB). The sibling collections pG, pL, pJ, and pN apply a wavelet transform to the images that produces the representation used by the recognition module. The implementation of this transform\u2014the steerable pyramid [6], which localizes image edges, orientations, and scales\u2014is a mex implementation by its original creator (Simoncelli) dating to 1996; although updated versions now exist on GitHub, the version in the author s\u0313 local library (used in these experiments) dates to 28 March 2001.\nExperiment Collections \u2014 In practice, distinguishingprocessed fromexperiment collections is not always as clear as the nomenclature might indicate. For instance, collection eE is simply a ten-way partition of the books in rD, used for cross-validation experiments; it should probably be considered processed. Likewise, collection eH captures the intrafont character width bigrams statistics in a frequency table (and plots), making it analogous in role to pD (character bigram counts). Experimental collection eI fits the parameters of a regularized exponential probability model over these character width bigrams, using cross-validation to determine the amount of statistical regularization (which limits overfitting). Collection eG adds spaces as one of the categories for the width bigram model. Collection eB does the same thing, but for actual character identity bigrams of the usual sort used in language modeling (with ten-fold cross validation based on the partition in eE).\nThese \u201clearned model\u201d collections (eE, eI, and eG) effectively train what is variously called a maximum entropy (MaxEnt) classifier [7] or multinomial logistic regression [8] (in this case with no feature inputs; only class bias weights are learned). The MATLAB implementation of the classifier and the L-BFGS optimizer needed [9] to train it were created by the author (housed in the local, version-controlled support library), and have been publicly shared under the GPL since 2010.\nReScience C 6.1 (#) \u2013 Weinman 2020 5\nThe hub experimental collection eD trains the character recognition model, a discriminative MaxEnt classifier. The implementation is a combination of the MATLAB MaxEnt class and L-BFGS code above, as well as a CUDA-backed mex kernel to rapidly accelerate the computations needed for each stage of the batch gradient descent algorithm. CUDA is the language framework supporting general purpose computation on a GPU (graphics processing unit). The so-called cudamaxent software was published (GPLv3) and described in a publication by Weinman, Lidaka, and Aggarwal in 2010 [10].\nThe first result published in the paper is finally computed in collection eJ, where the test images\u02bc binarizations and steerable pyramid features are calculated the on the fly, put through the MaxEnt model, and finally parsed by a Java-based Viterbi (dynamic programming) algorithm. Intermediate collection eC fine tunes the relative weights of a module that selectively penalizes certain overlaps or gaps among the parse segments as scored within the Viterbi calculations. Collections eA and eF create the last two results in the paper, eA adding the character identity bigram score within the Viterbi parser, and eF adding the character width bigram score on top of that. With the addition of the width-basedmodel, the semi-Markov model s\u0313 dynamic programming table is larger and the experiment uses a different Java subclass within the same hierarchy.\n4.2 Code and Data Footprint In addition to highlighting the structure of inter-experimental data dependencies (cf. Section 4.1 and Figure 1), it is useful to comprehend the magnitude of code dependencies and the resulting data footprints.\nTable 1 enumerates the primary external code dependencies used in the experiments. These omit the obvious need for the built-in MATLAB tools and toolboxes, which included the ImageProcessing, Statistics (nowStatistics andMachine Learning), Optimization, and Parallel Computing Toolboxes. All of the other local software libraries listed in the table were managed through an institutional Subversion source control system. Revision numbers for each experiment were manually recorded by the author in the text manifest for each collection. Bug fixes and improvements to the software have of course been recorded since the original experiments were performed for the publication. Most of the author s\u0313 software\u2014L-BFGS, MaxEnt, and CUDAMaxEnt libraries\u2014was publicly distributed with those improvements on the author s\u0313 home page at around the time or shortly after the publication; all were published under the GPLv3 free software license. Simoncelli s\u0313 PyrTools was obtained under the MIT license. The author s\u0313 VIDI tool was not previously published.\nTable 3 also quantifies the foot print of the collections themselves. On average, each collection relies on approximately three scripts. For example, this might be the main, argument-free \u201crun\u201d script, which invokes a parameter-driven sub-script, which may\nReScience C 6.1 (#) \u2013 Weinman 2020 6\nitself be aided by some instance-level helper. Although there is substantial variation among individual collections, the total amount of stored data involved in producing the work is 127 GiB, even though the \u201craw\u201d data is only 320 MiB.\nMATLAB was \u201cchosen\u201d as the primary environment for hosting the experiments because the author had, in a sense, \u201cgrown up\u201d working in MATLAB a decade earlier in undergraduate image processing and computer vision courses. By the time these experiments were crafted, the author had developed quite a library of operational tools through his dissertation work [3]. Moreover, the model in the paper was an extension of that earlier dissertationwork, so several bits of that codewould be leveraged (although, as observed above, the version control throughout the dissertation work was not as strong or prevalent; RCS was used sparingly).\nJava became involved because a robust and stateful class/object capabilitywas necessary to support the parser efficiently with a broad standard library. At the time, the author had used Java for over a decade (since its 1.0 days) and happened to be far more familiar with it than alternatives such asC++. Moreover,MATLABhad just began to support native Java class integration with its environment.\n5 Reproducing the Work\nHow was the reproduction process initiated? The author includes (as a comment in the LATEX source for each paper) a reference to the data collection supporting any experimental result reported. Thus, any \u201cleaves\u201d of the dependency graph (as in Figure 1) are immediately available, and the entire chain of the work can be examined by following these dependencies. Of course, this strategy also requires one to preserve the source of the paper as well. In this case, the source for the original paper included explicit references to collections denoted eJ, eA, and eF (the results corresponding to using \u201cAppearance\u201d,\u201c+ Char. Bigram\u201d, and \u201c+ Char. and Width Bigrams\u201d in the original paper and as shown in Table 2 below). These collections thus constituted the entry points for reproducing the work.\nThe remainder of this section documents the reproduction experience on an updated computing platform, including minor changes to the code that were necessary.\n5.1 Compute Environments\nOriginal Environment \u2014 The host computer system onwhich the original experiments were run is still operational in the author s\u0313 research laboratory. However, like the legendary Ship of Theseus, enough of its hardware and software have been upgraded in the last decade to question whether it is truly the same.\nHardware TheCPUs are dual quad-core 64-bit Intel Xeons (E5520), withhyper-threading that present to the OS as a total of 16 compute cores. The system has 48GiB of host (CPU) memory. The GPU used for the original experiments was an NVIDIA Tesla C1060, featuring 240 compute cores and 4GiB of memory.\nSoftware At the time, the host operating systemwas Ubuntu 8.04 LTS, though precisely which version is not known. This distribution and version was intentionally chosen for the ease of installation, maintenance, stability, and most notably the duration of support. Operating system upgrades can be fraught with difficulty and the preference was to avoid them for as long as possible. None of the distribution package details were recorded and there was no regular practice of updating them. Hence, important details such as the precise C compiler used cannot be known (though it would have been whatever the default of the distribution was). In order to use CUDA and the NVIDIA GPU,\nReScience C 6.1 (#) \u2013 Weinman 2020 7\nboth the CUDA library and the NVIDIA driver were necessary. The versions installed by the author on the platform and used in the original experiments are not known. However, considering the timing of the experiments it was most likely CUDA 2.2 or 2.3. A Makefile path indicates MATLAB R2009a compiled the CUDAMaxEnt code; given that the system administrator at the time updated software infrequently, it is almost certain the same version was used for all the experiments in the original paper as well. A now deprecated nvmex tool was used to bridge the CUDA/MATLAB divide; the Makefile pointed to a user s\u0313 home directory where the file still exists. Although the version of Java is not known precisely, the the Ant build.xml file for VIDI indicates Java version 1.5 was to be used.\nReproduction Environment \u2014 Restoring the original machine to its state at the time of the original experiments is within the realm of possibility. The author retains all the original hardware (the GPU included), and archived versions of the OS and library software are available for download. However, rather than pursue that rather rewardless task, it seemed preferable to embark upon a replication study to verify the stability of standard tools (i.e., bash, C, Java, Matlab, and CUDA) .\nHardware An entirely different host was used. Its dual 14-core Intel Xeon (E5-2695) CPUs present as having 56 cores, with 512 GiB of available host memory. The system houses several GPUs, but experiments relied on only one Titan RTX GPU, with 23.6 GiB of RAM and 4,608 cores.\nSoftware The host operating system is Ubuntu 18.04.3 LTS. MATLAB is R2018a. CUDA 10.1 sits atop NVIDIA drivers with version 418.67. The default C compiler is gcc 7.4.0. The Java is OpenJDK 1.8.0_222.\n5.2 Software Preparation The Subversion server hosted by the author s\u0313 institution was taken down about one year before embarking upon reproducing this work. Although local copies of themost recent code exist, specific versions of the code could not be checked outwithout the Subversion server. Fortunately, the system administrator was able to quickly restore a server and source repositories to their previous working order. This proved pivotal to the precise reproduction of at least some experimental results.\nMATLAB and a license for it have been maintained on the system, including all the requisite toolboxes. However, a lack of ongoing licensing could have been an impediment; it seems unlikely Octave would have produced similar, let alone identical results.\nCompiling and running the C code required for calculating text bigrams was effortless, as it was all standard C. Checking out and building the Java files for the author s\u0313 VIDI library also worked perfectly (there were no external library dependencies). Whereas the author had already adapted the Steerable Pyramid toolbox to the Ubuntu ecosystem, copies were on hand that were compiled most recently in 2016 using MATLAB R2015a. However, these were easily recompiled and verified to produce identical results (cf. Section 6.3). In addition, the author s\u0313 C/mex code for an efficient 2D convolutional box filter compiled without trouble for these experiments with MATLAB R2018a.\nBy far the most challenging part of the process was in resurrecting the CUDA MaxEnt code and restoring it to a runnable state. One expects fairly significant differences to emerge over the evolution from version 2.x to 10.1 of the CUDA library. Surprisingly, to get the code compiled required only modest changes to the test harness and virtually none in the core computational code. The undergraduate students who primarily authored the code wrote a plethora of unit tests, accompanied by Matlab scripts that\nReScience C 6.1 (#) \u2013 Weinman 2020 8\ngenerate the underlying data used for regression tests against native Matlab routines. (This accounts for the very large number of files listed for the CUDA MaxEnt library in Table 1.) Without the nvmex script to unite the CUDA compilation and Matlab-object linking, some tinkering with the Makefile was required. The resulting strategy was to invoke nvcc (the NVIDIA compiler for CUDA code) to create the appropriate object files and then the MATLAB mex command could be used to link everything into a mex file for invocation from within the MATLAB environment. Before the build could complete, however, the latest version of CUnit, the requisite unit test framework, needed to be installed (easily accomplished with the apt-get command, which installed version 2.1-3). This version was clearly newer than the version that the software relied on, because compile errors indicated the API had changed. A quick review of the updated API indicated only a few additional (unused NULL) parameters needed to be inserted into a test suite array. In addition, the return type for the cudaMalloc function from the CUDA library may have evolved in way that now resulted in errors, so four of these calls needed to be changed. Although several deprecation warnings were given for the use of cudaThreadSynchronize(), the resulting code all compiled. (A newer replacement exists, but the correctness of the code does not seem to be affected.)\nIn the revision of the code documented as the one used in the paper, the CUDAMaxEnt testing breaks down into 7 test suites consisting of 185 tests having a grand total of 309 assertions. Using the Titan RTX, only one of these tests failed\u2014a cudaMalloc call, but the resulting kernel failed to produce any useful results in the experiments. Forging ahead, the last version of the repository was used instead (dating to just one year later, Feb 2011). It involved the same basic changes (enumerated above), passed the unit tests, and produced plausible, though slightly different results when experiments were re-run (cf. Figure 2 and Table 2). Without a more careful examination of the failure of the original code to run effectively, it is impossible to attribute the differences in results more specifically to the different CUDA library, the slightly different MaxEnt implementation, or the different hardware.\nAlthough somewhat tedious, recreating the 36 experimental collections (diagrammed in Figure 1) was a straightforward matter of manually invoking the author s\u0313 make-collection and copy-collection utility scripts for each. Then, the dependency files (DEPS) needed to be updated for each so that it would point to any newly recreated parent collection.\nSome detail workwas necessary to get a few collections that utilizeMATLAB scripts working with the current version of the software and the general data repository setup. Common to virtually every collection was the mechanism for initializing the MATLAB path to point to the appropriate Subversion repositories (since the system-wide version generally utilized was too recent). Two other small changes were necessary in a few places. The first updated the method for seeding the random number generator (though it is doubtful this had any useful effect, then or now), and the second updated the call for opening a thread pool to parallelize computations using the MATLAB Parallel Computing Toolbox. All in all, these were relatively small changes that required no intimate knowledge of the scripts, only an awareness of how the MATLAB API had evolved.\nIt was discovered that one substantive element was not achieved (as it should have been) through the collections\u0313 src/ folder, though it was properly archived in its data/ folder. The learned character classifier had been hand-modified to exclude the narrowest and widest \u201cspace\u201d characters. Because the file was named precog_no_extreme_spaces.mat, recreating the effect in the reproduced experiments was fairly straight forward (i.e., load weight matrices, set two entries to -Inf, store modified weight matrices and save the file), but the need to do so surely would not have been obvious to anyone beside the original author.\nAlthough recreating and rerunning the various configurations took a non-trivial amount\nReScience C 6.1 (#) \u2013 Weinman 2020 9\nof time (several days and nearly asmany hours of real compute time), it is still somewhat remarkable that this rather complex web of inter-dependent experiments could be recreated at all. Considering that precisely tracing computations and dependencies was onemajor design goal of the author s\u0313 data repository, it seems to have succeeded in that regard. The next question is whether it would enable a reproduction of the results.\n6 Results\nTo isolate possible sources of variation, computations were re-run in phases, from the bottom up.\n6.1 Parsing The earliest/easiest experiments are those lowest in the computational graph (Figure 1), which were re-computed by preserving the dependencies on the original collections, thus utilizing the original data. The parsing experiments were indeed re-produced exactly. This is remarkable for a few reasons. First, there are several inner optimizations usingMatlabs\u0313fminbndprocedure that is \u201cbased on golden section search andparabolic interpolation\u201d according to the documentation;1 the reproduced experiments produced a bitwise equivalent double-precision floating point value in all cases. Second, only the raw test images are provided to the collections; upon reproduction these images had to be fed through the mex-based box filter for binarization and steerable pyramid tools library for feature calculation. Because those results are not cached, it is unknown whether they are identical, but they must be close enough so that the Viterbi parse produced the same path (if perhaps not precisely the same score).\nOne difference in runtime behavior on the reproduced experiment is worth noting. Because the Matlab Parallel Compute toolbox automatically spawns a thread pool according to the local host configuration, four times as many threads (48) were used for accelerating the runtime of the reproduced experiments as compared to the original (16). Because the parses are entirely independent of one another (and the underlying Java and C/mex libraries are thread-safe), the results proved to be the same regardless of the parallel speedup.\n6.2 Statistical Model Training Next all of experimental results downstream of the pre-processed training data were re-run, without regenerating the training data itself. Unsurprisingly, the raw character bigram counts matched exactly. The weights of the learned character bigram model in eB (which rely on a Matlab-only MaxEnt model and the L-BFGS optimizer) match very\n1MathWorks. Find minimum of single-variable function on fixed interval. https://www.mathworks.com/help/ matlab/ref/fminbnd.html\nReScience C 6.1 (#) \u2013 Weinman 2020 10\nclosely. For weights with an absolute value mean/median/max of 0.9794/0.2233/7.3773, the absolute differences between original and new are 2.4e\u20137/1.7e\u20137/1.1e\u20136. The character width bigram weights in eI and eG match as well, being 3\u20134 orders of magnitude closer. (Results in eH and eE are identical, being mostly bookkeeping or simple pixel measurements.)\nThe biggest difference arises from retraining the character classifier in eD, which relied on the CUDA MaxEnt library. The experimental procedure for training the model incrementally relaxed a regularization penalty, designed to avoid over-fitting; a held-out validation set was used to identify the best performing model. The left panel of Figure 2 shows the training objective reducing with each step of gradient descent. As the minimization levels off and converges for each regularizing coefficient, the process then continues with amore relaxed penalty (signaled by a color shift). The optimization trace (not published in the paper, but extracted from a log file in the original 2010 collection) progresses as would be expected with rapid decreases as the penalty is eased, followed by gradual further improvements to themodel weights. When run on the same data, the updated version of the code produces the same general curve, but generally achieves superior minima with fewer iterations. However, the validation data show that while the results are indeed different, they are not as different as the training curves might show. Importantly, the same model regularization amount is chosen across all runs (whether with the original result, a re-run on the original data, or a re-run with newly generated data). Thus, the learned models\u2014which each have 11,315,642 parameters\u2014appear to have fairly similar characteristics.\nThe end results of these intermediate experiments appear in Table 2 under column \u201c+ Training\u201d, as in \u201cParser + Training\u201d because the parser experiments were re-run with the new models as well. Although the results are not precisely the same, the relative performance among the modules tested by the original question is the same\u2014as one would hope.\n6.3 Synthetic Data Generation The final test was to attempt reproducing all of the original synthetic training data, and then re-run themodel training and final parse experiments. Although theMATLAB code was unchanged but for the syntax of seeding the random number generator, it is not ev-\nReScience C 6.1 (#) \u2013 Weinman 2020 11\nident this was done correctly in the first place. Thus, the randomly sampled factors (neighboring characters, where synthetic sign borders appear\u2014if at all, brightness/contrast, and noise) are all distinct. However, the code operates without fail and the general visual effects are the same in the processing chains (i.e., pF\u2013pE\u2013pR). In sum, the data appears repeatable, if not exactly reproducible.\nAs an intermediate test of reproducibility, the cached image features used for training the character classifier were checked. The binarized images regenerated in pM from the original pR were identical (a result of the Niblack binarization algorithm, which uses thresholded local image statistics calculated with the recompiled mex box filter). The wavelet-transformed images (using the recompiled C/mex PyrTools) in pJ from the original pRwere bitwise identical inmost cases (median absolute difference of precisely zero), and the maximum absolute differences are 2\u03f5, where \u03f5 = 2\u221252, the granularity of a double-precision floating point number. Thus, the feature calculations were fully reproducible.\nThe complete reproduction results appear in Table 2 under column \u201c+Data\u201d, as in \u201cParser + Training + Data\u201d, meaning all data (i.e., collections p*) and intermediate or final results (i.e., collections e*) were recomputed from the previously stored raw primary data (collections r*). As expected from the differences in model training described above, the final results do differ from the original. However, the relative performance ranking is the same. The absolute performances are comparable, though it might be noted that with the relatively small (by today s\u0313 standards) test data set of N = 1144 characters, the gap between the standard character bigrammodel and the character plus width bigram model featured in the paper shrinks from six to three characters.\n7 Conclusions\nThe original 2010 work has been successfully repeated ten years later, if not reproduced exactly in all cases. Although many nodes in the experimental chain could be reproduced exactly, the most critical nodes involving the fitting of statistical models were only approximately reproduced. All of the straightforward data-processing tasks (i.e., image features, bigram counts) were identical; notably even the output of the statistical models (using previously learned parameters) applied to recalculated image features and the resulting predictions matched precisely.\nThe entire compute chain from raw data to three numbers has been stored in an experimental data repository spanning 36 collectionswhose 70 unique source code files having 3,500 lines of code generated 127GiB of data. This repository facilitated inspection and ease of reproduction, ensuring collections could be re-run against the correct versions of local (Subversion) source code repositories involving over 500 files and 50,000 lines of code. Java and standard C code required nomodifications, and only twoMatlab calls required updating. Surprisingly, the intervening decade brought very few changes to the CUDA library so that only minimal updates to the core machine learning code used in the paper were required. However, these failed to produce satisfactory results, so that a slightly newer (six months after the original publication) source version of the author s\u0313 library was used. This brought comparable, though not identical results (perhaps due to differing hardware, newer libraries, or the updated client software, if not all three).\nWhen a scientific work requires a long chain of data transformations that cannot be efficiently recomputed on the fly, caching the results and the method of computation is essential to bothunderstanding and reproducing thework. Althoughnot as easily portable or universal as modern-day containers such as Docker or Peng and Eckel s\u0313 Cacher add-on package for R [5], the transparency and simplicity of the author s\u0313 filesystem-based data repository [2] has proven invaluable for a small research lab with minimal external collaborations. The repository and its function has also scaled; a more recent work by the\nReScience C 6.1 (#) \u2013 Weinman 2020 12\nauthor [11] incorporated 170 collections in its results chain whose 513 unique source files (19,400 lines of code)\u2014not including library dependencies\u2014generated 857 GiB of data and span nearly six years. Although the provenance of the calculations and the intermediate data are easily traceable, the repository does not completely capture all the host software versions and configurations, as a tool like Docker might. This shortcoming limited the degree of reproducibility reported in this report. Thus, only time would tell whether these newer and ongoing computations will be truly reproducible. This article indicates that using long-lived tools along with well-tracked dependencies increases the chances of generating reproducible results."}], "title": "[Rp] Reproducing \u201dTypographical Features for Scene Text Recognition\u201d", "year": 2020}