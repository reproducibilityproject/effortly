{"abstractText": "Binarized Neural Networks are paving a way towards the deployment of deep neural networks with less memory and computation. In this report, we present a detailed study on the paper titled \"Latent Weights Do Not Exist: Rethinking Binarized Neural Network Optimization\" by [1] which proposes a new optimization method for training BNN called BOP. We first investigate the effect of using latent weights in BNN for analyzing prediction performance in terms of accuracy. Next, a comprehensive ablation study on hyperparameters is provided. Finally, we explore the usability of BNN in denoising autoencoders. Code for all our experiments are available at https://github.com/nancy-nayak/rethinking-bnn/", "authors": [{"affiliations": [], "name": "Nancy Nayak"}, {"affiliations": [], "name": "Vishnu Raj"}, {"affiliations": [], "name": "Sheetal Kalyani"}, {"affiliations": [], "name": "Koustuv Sinha"}], "id": "SP:48d5564355f0f08ca64be38e651edae767eda7e9", "references": [{"authors": ["K. Helwegen", "J. Widdicombe", "L. Geiger", "Z. Liu", "K.-T. Cheng", "R. Nusselder"], "title": "Latent Weights Do Not Exist: Rethinking Binarized Neural Network Optimization.", "year": 1906}, {"authors": ["M. Courbariaux", "Y. Bengio", "J.-P. David"], "title": "Binaryconnect: Training deep neural networks with binary weights during propagations.", "venue": "Advances in neural information processing systems", "year": 2015}, {"authors": ["M. Courbariaux", "I. Hubara", "D. Soudry", "R. El-Yaniv", "Y. Bengio"], "title": "Binarized neural networks: Training deep neural networks with weights and activations constrained to+ 1 or-1.", "year": 2016}, {"authors": ["G. Hinton"], "title": "Neural networks for machine learning.", "venue": "Coursera Video Lectures", "year": 2012}, {"authors": ["D. Kingmaand J. Ba. \u201cAdam"], "title": "Amethod for stochastic optimization", "venue": "arXiv 2014.\u201d In: arXiv preprint arXiv:1412.6980", "year": 2014}, {"authors": ["M. Rastegari", "V. Ordonez", "J. Redmon", "A. Farhadi"], "title": "XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks.", "year": 2016}, {"authors": ["B. Zhuang", "C. Shen", "M. Tan", "L. Liu", "I. Reid"], "title": "Structured Binary Neural Networks for Accurate Image Classification and Semantic Segmentation.", "year": 2019}, {"authors": ["K. Simonyan", "A. Zisserman"], "title": "Very deep convolutional networks for large-scale image recognition.", "year": 2014}, {"authors": ["J. Lei Ba", "J.R. Kiros", "G.E. Hinton"], "title": "Layer normalization.", "venue": "arXiv preprint arXiv:1607.06450", "year": 2016}], "sections": [{"text": "Edited by Koustuv Sinha ID\nReviewed by Anonymous Reviewers\nReceived 15 February 2020\nPublished 21 May 2020\nDOI 10.5281/zenodo.3818607\n1 Introduction\nIn the era of Artificial Intelligence, deep learning methods are finding themselves useful in a variety of tasks. This is not only limited to image or speech processing using supervised learning, game playing using reinforcement learning but to more complex tasks ranging from self driving car to smart personal assistants to autonomous drones helping with household work. The main challenge of using deep learning in computationally complex tasks is that it requires a big complex network which in turn requires huge memory to save its parameters and high computational power. These big networks are often trained with one or many power hungry GPUs. However, in order to enter a mass deployment of solutions, methods that can run low power devices are desirable. In order to satisfy the need of good performance of these networks but deployed with less resources in terms of memory and compute, [2] proposed Binaryconnect in which the learnt parameters are limited +1 or \u22121. Later [3] introduced binary neural networks (BNN) where activation also is a binarization function. The main problem of training BNN is that the gradient of binarization function is almost zero everywhere. This was mitigated by utilizing a special kind of function called straight through estimator (STE) [4] . To train these neural networks with binary weights, a popular choice is to use Adam optimizer [5]. During the training process, real weights are updated using one of the traditional optimizers during backpropagation and during the forward propagation, only a binarized version (sign) of these real weights (latent weights) are used for prediction and subsequent loss calculations. In this paper ([1]), the authors have developed a novel optimization method, Binary Optimizer (BOP), for training BNN. The main insight which underpins this development is the observation that latent weights are not necessary for gradient based optimization\nCopyright \u00a9 2020 N. Nayak, V. Raj and S. Kalyani, released under a Creative Commons Attribution 4.0 International license. Correspondence should be addressed to Nancy Nayak (ee17d408@smail.iitm.ac.in) The authors have declared that no competing interests exist. Code is available at https://github.com/nancy-nayak/rethinking-bnn. \u2013 SWH swh:1:dir:fcda11a6bae27b85c1e564dbd7510d8ba80e6f6c. Open peer review is available at https://openreview.net/forum?id=y53aaSM5o.\nReScience C 6.2 (#9) \u2013 Nayak, Raj and Kalyani 2020 1\nof BNNs. Instead of updating latent weights using one of the traditional optimizers, the authors proposed to use accumulated momentum to gradients in order to flip between the binary weights possible for each parameter. They empirically demonstrate the performance of BOP on CIFAR10 and Imagenet datasets. In this study, we explore the capabilities of the proposed optimizer for training BNNs through comprehensive ablation studies.\n1.1 Outline of this study In order to verify the claims presented in the paper and to assess the applicability of the proposed methods to tasks other than classification, we focus on the following questions in this report.\n1. Does the use of latent weights instead of binary weights in BNN result in better performance?\n2. The Binary Optimizer proposed in the paper has two hyper-parameters to tune. We provide a comprehensive ablation study of the impact of these two parameters in the context of both.\n3. Most of the prior work in binary neural networks used batch normalization at the output of each layer to stabilize training. We propose layer normalization as an alternative for batch normalization and show that it can have slightly better performance for a proper choice of hyperparameters.\n4. Most of the previous works in binary neural networks concentrate on classification tasks, possibly due to the reduced representative power of neural networks with binarized weights. We study the applicability of binary neural networks outside to classification tasks. We present initial results on using BNNs for denosing autoencoders.\n2 How do Latent weights perform with respect to binary weights?\nPrevious works claimed that binary weight vector is an extremely quantized i.e. binarized version of the latent weights. Many of the previous works consider binary weights as an approximation of real latent weights. According to approximation viewpoint, using latent weights instead of binary weights along with binary activations should result in better accuracy than BNN. Authors of this paper ([1]) claimed that using latent weights in BNN may not always result into higher accuracy. In this section, we study this claim with experiments conducted on multiple architectures with different datasets. We consider the task of classification of images as the case for this study. Three neural network architectures are considered:\n1. A fully connected dense neural network (referred as FullyCon here on).\n2. Convolutional Neural Network based on popular LeNet architecture (referred as LENET5 here on).\n3. Convolutional Neural Network based in VGGNet architecture which is also considered by the authors (referred as ConvNet).\nThe architecture of each of the three networks are shown in Table 1, Table 2 and Table 3 respectively. For convolutional layers, the filter sizes of (5, 5) and (3, 3) are used for LENET5 and ConvNet respectively. The \"Activation\" mentioned in Table 1, Table 2 and Table 3 are the activation applied to the output of the corresponding layer. For all of the three cases, there is no binarization activation applied to the input layer. We\nReScience C 6.2 (#9) \u2013 Nayak, Raj and Kalyani 2020 2\nprovide the results on training these architectures in MNIST and CIFAR10 datasets till the training accuracy saturates. The results for MNIST and CIFAR10 datasets are provided in Table 4) and Table 5 respectively. We trained the models by backpropagating losses due to binary weights during the training process. At the end of training, the accuracy of using binary weights and latent weights are reported. We can observe from the experiment that, in general, latent weights when applied to BNN does not perform better than binary weights, verifying authors\u2019 claim. Hence, the alternate view point of seeing binary weights as approximation of real latent weights [2, 3, 6, 7] is not necessarily true. Even though BNN is trained with latent weights, mostly (except MNIST) the BNN with latent weights achieves a lower accuracy than using binary weights.\n3 Ablation studies on the effect of hyperparameters\nIn this section, we present results of the ablation studies on the effect of hyper parameters \u03c4 and \u03b3. The paper [1] considers a binary convolutional architecture called BVGG net as given in [8] inspired from the architecture of ConvNet in [3] we discussed before. The authors have shown that the main action which matters for BOP is flipping weights. The question boils down to the following: based on a sequence of gradients, whether to flip a weight or not. BOP should pay attention to the consistency and the strength (absolute value) of the gradient signals. In BOP the consistent signal is selected by looking at exponential moving average of gradients.\nmt = (1\u2212 \u03b3)mt\u22121 + \u03b3gt = \u03b3 t\u2211\nr=0\n(1\u2212 \u03b3)(t\u2212r)gr (1)\nwhere gt is the gradient at time t, mt is exponential moving average and \u03b3 is the adaptivity rate. The weight flip is determined by comparing the moving average with a threshold called \u03c4 :\nwit = { \u2212wit\u22121 if |mit| \u2265 \u03c4 and sign(mit) = sign(mit\u22121) wit\u22121 otherwise\n(2)\nReScience C 6.2 (#9) \u2013 Nayak, Raj and Kalyani 2020 3\nThe two hyper parameter for BOP are \u03c4 and \u03b3. In this section we show the results of the experiments on BVGG net with BOP as optimization method and with CIFAR10 dataset. To use the dataset for training a binary network, in our work similar modifications are done to the dataset as [1]. We use batch normalization to normalize the activations with a minibatch size of 64. A non zero threshold \u03b3 avoids rapid flip of weights when the gradient reverses on a weight flip. One thing to note here is that high value of \u03c4 can result in never flipping weights even though there is pressure from a consistent gradient signal. As given in the paper, a higher adaptivity rate \u03b3 gives more adaptive moving average which implies that if a new gradient signal pressurizes a weight to flip, it take lesser time steps to do so. Keeping \u03c4 same (10\u22128) we see the effect of different \u03b3 in Fig (1a). From eq (1) it is clear that with the increase in the value of \u03b3, mt fluctuates more. A high \u03b3(= 0.01) gives very poor performance as high adaptivity rate makes weight to flip in less time steps if a gradient signal starts pressurizing it. Even though in the very beginning of the training, the accuracy trend is steep in case of \u03b3 = 0.01, within a short period of time the accuracy converges to 40%. A low \u03b3(= 10\u22126) provides stable training process but it takes long time to fully converge to final accuracy. From the plots we found for \u03c4 = 10\u22128, \u03b3 = 10\u22125 performs best and converges to training accuracy 92.15%. Fig (1b) gives training accuracy and loss for different threshold \u03c4 and a fixed \u03b3(= 0.0001). As explained in the paper, \u03c4 should be a non zero real number but not so big that the weight flipping is hindered. In our simulation result, networks with \u03c4 in the range of 10\u22128 perform nearly same.\nReScience C 6.2 (#9) \u2013 Nayak, Raj and Kalyani 2020 4\n4 Layer Normalization\nTill date in most of the works in the line of BNN, batch normalization is used between consecutive non-linear layers in order to stabilize the training. The aim of batch normalization is to normalize the inputs with the global mean and variance but calculating mean and variance of activations for the whole dataset is very costly. So batch normalization is done in small batches and the estimated mean and variances vary from one minibatch to other. The main challenge of using batch normalization is limitation in batch size. If the batch size is very small the variance of the estimates would be very high which\nReScience C 6.2 (#9) \u2013 Nayak, Raj and Kalyani 2020 5\nmakes it difficult to use batchnorm in online learning. Also in case of recurrent neural network, use of batchnorm is difficult as the layer statistics change at each timestep. In this section, we look into layer normalization as an alternative solution to avoid these problems [9].\nLayer normalization works by normalizing across the activation (features) passed between two consecutive layers instead of over a mini batch. In layer normalization, the statistics are calculated across each feature and are independent of other example. We propose to use layer normalization instead of batch normalization as an alternate way of stabilizing training. In Fig 2, we provide a comparison of training performances of layer normalization and batch normalization for \u03c4 = 10\u22128 and \u03b3 = 10\u22125. It is clear that layer normalization performs slightly better than batch normalization for this choice of hyper parameter. In Fig. 3, we provide a comprehensive ablation study for the effect of hyper parameters of BOP in conjuction with layer normalization. From Fig (3a), we can observe that for fixed \u03c4(= 10\u22128) the performance of BVGG net is best for \u03b3 = 10\u22125 and the explanation follows from the batch normalization ablation studies. Similarly, as shown in Fig (3b) for fixed \u03b3, \u03c4 in the range of 10\u22128 has nearly same performance. The above studies conclude that layer normalization can be used as a viable alternative to Batch Normalization in the case of training Binary Neural Networks. This can be especially useful in the cases where BNN has to be trained in an online fashion (as in the case of deep reinforcement learning) where acquiring a mini-batch of sufficient size to alleviate the affect of noise variance in estimation is either costly or undesirable.\nReScience C 6.2 (#9) \u2013 Nayak, Raj and Kalyani 2020 6\n5 Using BNNs Beyond Classification: Denoising binary Autoencoder\nMost of the works on BNN till date focus on classification tasks as a benchmark for evaluating the performance of BNNs. In this section we explore the usage of BNNs in denoising auto encoder (AE), called Binary AE, referred as BAE. The architecture we consider for this experiment is given in Table 6. Convolution represents 2D convolution layer for AE and quantized 2D convolution layer for BAE. Kernel size of convolution and deconvolution layers is (3, 3) and pool size of max pool layers is (2, 2) for both AE and BAE. In this AE architecture first three layers make up the encoder and last three the decoder part. At the first layer of the encoder and decoder, real valued inputs are used. STE-sign function is used as activation on current layer inputs for BAE except for the last layer. The last layer is a deconvolution layer with tanh activation. Mean square\nloss is minimized with BOP and a metric called peak signal to noise ratio (PSNR) is calculated to see the performance of the network during training and testing.\nPSNR = 10 \u2217 log10 ( max2\nmse\n) (3)\nwhere max is the maximum possible pixel value of the image and mse is the mean square error between original image and the image output from the network. For traditional AE, the architecture is exactly same except that each layer is trained on real weights with relu activation (instead of STE-sign in BAE) except for the last layer. As the architecture of AE is similar to binary AE except few changes, we do not repeat the architecture.\nIn order to use CIFAR10 dataset for training, we scale the (32, 32) images to (\u22121,+1) say imorg. The shape of the images are referred as imshape. Then a random normal N (0, 1) noise is added with a noise factor f = 0.1 and the results are clipped between\nReScience C 6.2 (#9) \u2013 Nayak, Raj and Kalyani 2020 7\n(\u22121,+1) again resulting in a noisy image referred as imnoi.\nimnoi = clip ((imorg + f \u2217W ), 0, 1) where W \u223c N (0, 1, imshape) (4)\nWe perform three sets of experiments, all using batch normalization to stabilize the training with a minibatch size of 64. The sets are as follows:\n1. Traditional AE with Adam optimizer (with learning rate lr = 10\u22123, \u03b21 = 0.99, \u03b22 = 0.999)\n2. BAE with Adam optimizer with same hyper parameters\n3. BAE with BOP optimizer (\u03c4 = 10\u22128, \u03b3 = 10\u22125)\nIn Fig 4 the training loss and training PSNR is shown for all three cases. From the graphs we see that, BAE with BOP seems to be better than BAE with Adam for this task and the training converges faster. The test PSNRs of all three cases are as follows:\n1. AE with Adam: 14.87 dB\n2. Binary AE with Adam: 9.10 dB\nReScience C 6.2 (#9) \u2013 Nayak, Raj and Kalyani 2020 8\n3. Binary AE with BOP: 9.30 dB\nHowever, the results are not impressive for both of the BAE cases. We plot 5 noisy samples of the CIFAR10 test dataset and the recovered images from the above three cases in Fig. 5. The reason behind degraded reconstructions for both of the BAE cases are: binary activations cannot pass enough information between the layers to develop a faithful reconstruction. Hence only a reduced finite set of information can be passed between intermediate layers of encoder and decoder. This could be the reason for not so good performance of BAE. We see a huge scope of improvement for BNN in case of BAE as well as other use cases.\n6 Concluding Remarks\nTo summarize our work, we started with studying the effect of using latent weights on BNN. From our observations, it can be concluded that for most of the cases latent weights on BNN do not perform better than binary weights. This conclusion aligns with the authors\u2019 claim that binary weight should not be considered as an approximation of latent weights. We also present an ablation study for the choices of two hyper parameters \u03c4 and \u03b3 for BOP optimizer. Our comprehensive experimental analysis shows the effect of each of these hyper parameters in the optimization procedure. Next, while most of the works only considered batch normalization, we introduce layer normalization as an alternative way of normalization and it shows impressive results. An ablation study on the effects of optimizer hyper parameters shows that layer normalization can provide improved results in some cases. Finally to explore the applicability of BNN in other use cases, we consider Binary AutoEncoders (BAE). The performance of BOP for BAE is then compared with BAE with Adam and AE with Adam. Though a very preliminary results with BAE are shown in our report, BOP shows better PSNR than Adam when used for training binary neural networks."}, {"heading": "Acknowledgement", "text": "We sincerely thank Code Ocean for the compute support they have provided for this work."}, {"heading": "1. K. Helwegen, J. Widdicombe, L. Geiger, Z. Liu, K.-T. Cheng, and R. Nusselder. \u201cLatent Weights Do Not Exist:", "text": "Rethinking Binarized Neural Network Optimization.\u201d In: arXiv preprint arXiv:1906.02107 (2019). 2. M. Courbariaux, Y. Bengio, and J.-P. David. \u201cBinaryconnect: Training deep neural networks with binary weights during propagations.\u201d In: Advances in neural information processing systems. 2015, pp. 3123\u20133131. 3. M. Courbariaux, I. Hubara, D. Soudry, R. El-Yaniv, and Y. Bengio. \u201cBinarized neural networks: Training deep neural networks with weights and activations constrained to+ 1 or-1.\u201d In: arXiv preprint arXiv:1602.02830 (2016). 4. G. Hinton. \u201cNeural networks for machine learning.\u201d In: Coursera Video Lectures (2012). 5. D. Kingmaand J. Ba. \u201cAdam: Amethod for stochastic optimization. arXiv 2014.\u201d In: arXiv preprint arXiv:1412.6980\n(2014). 6. M. Rastegari, V. Ordonez, J. Redmon, and A. Farhadi. \u201cXNOR-Net: ImageNet Classification Using Binary Convo-\nlutional Neural Networks.\u201d In: CoRR abs/1603.05279 (2016). arXiv: 1603.05279. 7. B. Zhuang, C. Shen, M. Tan, L. Liu, and I. Reid. \u201cStructured Binary Neural Networks for Accurate Image Classifi-\ncation and Semantic Segmentation.\u201d In: CVPR (2019). 8. K. Simonyan and A. Zisserman. \u201cVery deep convolutional networks for large-scale image recognition.\u201d In: arXiv\npreprint arXiv:1409.1556 (2014). 9. J. Lei Ba, J. R. Kiros, and G. E. Hinton. \u201cLayer normalization.\u201d In: arXiv preprint arXiv:1607.06450 (2016).\nReScience C 6.2 (#9) \u2013 Nayak, Raj and Kalyani 2020 9"}], "title": "[Re] A comprehensive study on binary optimizer and its applicability", "year": 2020}