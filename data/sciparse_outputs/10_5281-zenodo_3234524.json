{"abstractText": "Ecological resources are often managed on the basis of outputs derived frommodels of species\u2010environment relationships, variously referred to as: habitat suitability models, species distribution models, resource selection functions, or ecological niche models. Therefore, evaluating the predictive ability of such models is a necessary prerequisite for robust decision\u2010making. While such evaluations are ideally based on independent data collected purely for the purpose of model testing, in ecological studies fully inde\u2010 pendent data is often unavailable due to logistical or financial constraints. Therefore, statistical resampling methods are the most important tool we have for evaluating pre\u2010 dictive ability.", "authors": [{"affiliations": [], "name": "Thomas R. Etherington"}, {"affiliations": [], "name": "David J. Lieske"}, {"affiliations": [], "name": "Timoth\u00e9e Poisot"}, {"affiliations": [], "name": "Justin Kitzes"}, {"affiliations": [], "name": "Laura J. Graham"}], "id": "SP:c52c624bc96937125d8c40ab71b30e6ea563cb3d", "references": [{"authors": ["D.L. Verbyla", "J.A. Litvaitis"], "title": "Resampling methods for evaluating classification accuracy of wildlife habitat models.", "venue": "Environmental Management", "year": 1989}, {"authors": ["A.H. Fielding", "J.F. Bell"], "title": "A review of methods for the assessment of prediction errors in conservation presence/absence models.", "venue": "Environmental Conservation", "year": 1997}, {"authors": ["P. Diaconis", "B. Efron"], "title": "Computer-intensive methods in statistics.", "venue": "Scientific American", "year": 1983}, {"authors": ["T. Hastie", "R. Tibshirani", "J. Friedman"], "title": "The Elements of Statistical Learning: data mining, inference, and prediction", "year": 2009}, {"authors": ["B. Efron"], "title": "Estimating the error rate of a prediction rule: improvement on cross-validation.", "venue": "Journal of the American Statistical Association", "year": 1983}, {"authors": ["A.K. Jain", "R.C. Dubes", "C.C. Chen"], "title": "Boostrap techniques for error estimation.", "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "year": 1987}, {"authors": ["B. Efron", "R.J. Tibshirani"], "title": "An Introduction to the Bootstrap", "year": 1993}, {"authors": ["B. Efron", "R. Tibshirani"], "title": "Improvements on cross-validation: the .632+ bootstrap method.", "venue": "Journal of the American Statistical Association", "year": 1997}, {"authors": ["P.A. Lachenbruch", "M.R. Mickey"], "title": "Estimation of error rates in discriminant analysis.", "year": 1968}, {"authors": ["D.E. Capen", "J.W. Fenwick", "D.B. Inkley", "A.C. Boynton"], "title": "Multivariate models of songbird habitat in New England forests.", "year": 1986}], "sections": [{"text": "R E S C I E N C E C Replication / Ecology\n[Re] Resampling methods for evaluating classification accuracy of wildlife habitat models\nThomas R. Etherington1, ID and David J. Lieske2 1Manaaki Whenua \u2013 Landcare Research, PO Box 69040, Lincoln 7608, New Zealand \u2013 2Department of Geography and Environment, Mount Allison University, 144 Main Street, Sackville, New Brunswick, E4L 1A7, Canada\nEdited by Timoth\u00e9e Poisot ID\nReviewed by Justin Kitzes ID\nLaura J. Graham ID\nReceived 31 August 2018\nPublished 28 May 2019\nDOI 10.5281/zenodo.3234524\n1 Introduction\nEcological resources are often managed on the basis of outputs derived frommodels of species\u2010environment relationships, variously referred to as: habitat suitability models, species distribution models, resource selection functions, or ecological niche models. Therefore, evaluating the predictive ability of such models is a necessary prerequisite for robust decision\u2010making. While such evaluations are ideally based on independent data collected purely for the purpose of model testing, in ecological studies fully inde\u2010 pendent data is often unavailable due to logistical or financial constraints. Therefore, statistical resampling methods are the most important tool we have for evaluating pre\u2010 dictive ability.\nTo briefly summarise, resampling methods repeatedly resample the full dataset to cre\u2010 ate training and testing data subsets that are independent of one another. The model is then iteratively fit using each training data subset and prediction error estimated using the independent testing data subset. An overall estimate of prediction error is then cal\u2010 culated as the average prediction error across all resampled data subsets. While resam\u2010 pling methods are not a replacement for independent data, they can be used to conduct an internal evaluation that penalises for optimism from overfitting1.\nIn their seminal paper on model evaluation Fielding and Bell2 state with reference to resampling methods \u201cThe ecological literature seems to have paid little attention to how the partitioning method can influence the error rates. Verbyla and Litvaitis briefly reviewed a range of partitioning methods in their assessment of resampling methods for evaluating classification accuracy.\u201d The work of Verbyla and Litvaitis1 remains the only comparison of resampling methods for evaluating species\u2010environment relation\u2010 shipmodels. Therefore, given the importance of this work, we endeavoured to replicate the study using an open computational approach.\n2 Prediction error\nWhendeveloping a species\u2010environment relationshipmodel, wewill usually have adataset d that contains observations or measurements that form a species response variable\nCopyright \u00a9 2019 T.R. Etherington and D.J. Lieske, released under a Creative Commons Attribution 4.0 International license. Correspondence should be addressed to Thomas R. Etherington (etheringtont@landcareresearch.co.nz) The authors have declared that no competing interests exists. Code is available at https://github.com/tretherington/ReScience-submission/tree/etherington-lieske \u2013 DOI 10.5281/zenodo.3229408. Open peer review is available at https://github.com/ReScience/ReScience-submission/pull/52.\nReScience C 5.1 (#4) \u2013 Etherington and Lieske 2019 1\ny and a set of one or more environmental explanatory variables x. Using a species\u2010 environment relationship model function f trained on the dataset f (d) a prediction of the response variable y\u0302 can be created from the environmental explanatory variables for each sample i:\ny\u0302i = f (d)(xi) (1)\nWe can then define the prediction errorErr for each sample i as the absolute difference between the observed response variable yi and predicted response variable y\u0302i:\nErri = |yi \u2212 y\u0302i| (2)\nThis definition ofErr is equivalent to the binarymisclassification error rate used by Ver\u2010 byla and Litvaitis1, as when yi = y\u0302 then Erri = 0, and when yi \u0338= y\u0302 then Erri = 1. But by expressing Err in these terms generalises the approach to situations in which the species\u2010environment relationships are measured or modelled on a continuous rather than binary scale, which has become more common practice since the original compu\u2010 tational experiment was conducted.\n3 Resampling methods\nDuring the replication process it became apparent that the terminology for resampling methods has developed over time, and has been used somewhat inconsistently. There\u2010 fore, we begin by naming and formally defining each of the resampling methods we have used based on descriptions within the references.\n3.1 Resubstitution Given a dataset d of size n the resubstitution method calculates the mean prediction error across all samples i from a modelling function trained on the entire dataset f (d).\nErrR = 1\nn n\u2211 i=1 |yi \u2212 f (d)(xi)| (3)\nThe value ErrR is called the resubstitution (or apparent) error rate and is likely to pro\u2010 vide an optimistic estimate of prediction error, as the same data is used to train and test the model.\n3.2 Hold-out cross-validation Hold\u2010out (or: split\u2010sample, randomised, Monte Carlo) cross\u2010validation randomly parti\u2010 tions the dataset into training and testing subsets. Verbyla and Litvaitis1 referred to this approach as simply \u201ccross\u2010validation\u201d but we have chosen to use the more specific term of hold\u2010out cross\u2010validation to clarify which of the many types of cross\u2010validation we are referring to. The proportion p of data \u2018held\u2010out\u2019 from the dataset d forms a testing dataset t of sample size \u266ft, with the remaining data forming training dataset {d\u2212t}. The model is fitted using the training dataset f ({d\u2212t}), and the prediction error is estimated as the mean prediction error for all i in t across a number ofH repetitions.\nErrHp = 1\nH H\u2211 t=1 1 \u266ft \u2211 i\u2208t |yi \u2212 f ({d\u2212t})(xi)| (4)\nIn general this method can be considered an improvement over resubstitution as the data used to train the model is separated from the data used to test the model.\nReScience C 5.1 (#4) \u2013 Etherington and Lieske 2019 2\n3.3 K-fold cross-validation\nVerbyla and Litvaitis1 describe a resamplingmethod called ten\u2010fold cross\u2010validation and another method called n\u2010fold cross\u2010validation or the jackknife. Both these methods are variations ofK\u2010fold cross\u2010validation. TheK\u2010fold cross\u2010validationmethod begins by ran\u2010 domly partitioning the dataset into k equally sized sets. Then the prediction for each sample i is calculated from a model fitted to the set {d\u2212 k : i \u2208 k}, which is the dataset d excluding the set k where k includes i.\nErrK = 1\nn n\u2211 i=1 |yi \u2212 f ({d\u2212k:i\u2208k})(xi)| (5)\nWhen k = n then we produce a special form of K\u2010fold cross\u2010validation called the jack\u2010 knife (or: leave\u2010one\u2010out, n\u2010fold) cross\u2010validation.\n3.4 Bootstrap cross-validation\nBootstrapping3 is based upon a set B of bootstrap sample datasets b, for which b is of size n and is generated by randomly sampling with replacement from the full dataset d. The model is iteratively fitted to each b and prediction errors calculated for all samples i in the test set {d\u2212 b} that consists of the dataset d with all the i in the bootstrap sample b removed. This results in around 0.632 of the dataset d occurring at least once in the training set b, and the remaining 0.368 of d occurring in testing set4. The estimated prediction error rate is then the mean prediction error across all bootstrap samples5,6.\nErrB =\nB\u2211 b=1 \u2211 i\u2208{d\u2212b} |yi \u2212 f (b)(xi)|\nB\u2211 b=1 \u266f{d\u2212 b} (6)\nIt is worth noting in the context of a replication study that the equation used to calculate ErrB was later changed, as this caused some confusion during our replication. The second equation7works through each sample i, and calculates themeanprediction error for a set Ci of size \u266fCi that is equal to the all bootstrap samples that do not contain i, Ci = {b \u2208 B : i /\u2208 b}.\nErrB = 1\nn n\u2211 i=1 1 \u266fCi \u2211 b\u2208Ci |yi \u2212 f (b)(xi)| (7)\nThis second definition was later termed the \u2018leave\u2010one\u2010out bootstrap\u2019 where it was also noted by Efron8 that these \u201ctwo definitions agree as B \u2192 \u221e and produced nearly the same results in our simulations\u201d which were based onB = 50. Although the later defin\u2010 tion of ErrB has become more commonly used4, as the two methods produce nearly identical results, we have used the original definition (Equation 6) in our replication as it is simpler to compute, and was the version used by Verbyla and Litvaitis1 that we are trying to replicate.\n4 Computational experiment replication\nVerbyla and Litvaitis1 based their computational experiment on applying linear discrim\u2010 inant analysismodels to a randomdataset. Their premise was that by creating a random dataset the predictive ability of a model should be no better than chance, and hence as the true prediction error was known exactly each resampling method could be assessed for bias and precision.\nReScience C 5.1 (#4) \u2013 Etherington and Lieske 2019 3\n4.1 Random datasets Each of 1000 computational experiments began by creating a random dataset. The re\u2010 sponse variable consisted of 30 observations that were randomly assigned a presence = 1 or absence = 0 value. These 30 observations were then matched with 10 explana\u2010 tory variables. Verbyla and Litvaitis1 state that the \u201cten predictor variables were gener\u2010 ated with univariate normal distributions and equal variances\u201d but neither the mean nor variance used was reported. Therefore, we assumed a standard normal distribution of \u00b5 = 0 and \u03c3 = 1. For each random dataset a linear discriminant analysis model was fitted and prediction error calculated using each resampling method.\n4.2 Resubstitution The resubstitution method is the simplest approach and the most consistently reported in the literature, therefore we applied the methodology exactly as described.\n4.3 K-fold cross-validation) We conducted K\u2010fold cross\u2010validation with K = 10 and K = n as in Verbyla and Lit\u2010 vaitis1 \u2013 remembering that K = n is equivalent to the jackknife method. We also in\u2010 cluded K = 3 as this represents a situation with a similar proportion of the dataset forming training and testing sets as with the bootstrap. This was done to assess if the proportion of data within training and testing sets was affecting comparisons of the re\u2010 sampling methods.\n4.4 Bootstrap cross-validation The bootstrap method was applied with bootstrap samples B = 200 as specified in the example code provided by Verbyla and Litvaitis1.\n4.5 Hold-out cross-validation\nWe included three cases of hold\u2010out cross\u2010validation aswhileVerbyla andLitvaitis1 stated that for this method the \u201cthe estimate of model classification accuracy will not be very precise\u201d no experiments or citations were provided to support this claim. In addition we felt there was some inconsistency in the two citations given with reference to this method. While Verbyla and Litvaitis1 state \u201conly one estimate of accuracy is made\u201d which matches the citation9 using hold\u2010out cross\u2010validation with H = 1, a second ci\u2010 tation10 referred to hold\u2010out cross\u2010validation with H = 10. Given this uncertainty we explored three different versions of hold\u2010out cross\u2010validation. We used p = 0.368 twice to match the proportion of test data in the bootstrap approach, and withH = 1 for one method, andH = 200 in the secondmethod for consistency with the bootstrap. We also usedH = 200 with p = 0.200 to examine sensitivity to the test data proportion.\n5 Results\nThe results for each resampling method were presented by Verbyla and Litvaitis1 as a \u201csmoothed frequency distribution\u201d but the smoothingmethodwas not reported. As their results appeared to be normally distributed, to mimic the original results to aid compar\u2010 isonsweproducedour smootheddistributionsusing a one\u2010dimensionalGaussian kernel density estimator with a bandwidth of one (Figure 1).\nLooking at the distribution of Err across the 1000 computational experiments, the re\u2010 substitutionmethod produced clearly biased estimates of prediction error. All the other\nReScience C 5.1 (#4) \u2013 Etherington and Lieske 2019 4\nmethods produced unbiased estimates, but there was notable variation in the precision of those estimates, with ErrB=200 (\u00b5 = 0.497, \u03c3 = 0.069) producing the most precise estimates.\n6 Conclusion\nWhile a lack of method description means our implementation will be different to that of Verbyla and Litvaitis1, we would conclude that our results are sufficiently similar to have replicated their computational experiments.\nOur findings confirm that resubstitution is a biased estimate of prediction error, and that bootstrap cross\u2010validation produces the most precise unbiased estimate. We also found that hold\u2010out cross\u2010validation produced unbiased but highly variable estimates of Err as the method is clearly sensitive to the choice of parameters. We found little difference between any of theK\u2010fold cross\u2010validation methods.\nGiven the findings from our replication, we would support Verbyla and Litvaitis1 in ad\u2010 vocating the use of the bootstrap, as it produced the most precise estimate, and unlike other resampling methods it does not require a arbitrary choice of dataset partitions or splits that could confound inter\u2010study comparisons of model evaluations.\nWe conclude that while not a substitute for truly independent data, resampling meth\u2010 ods should be considered an important part of species\u2010environment relationship model evaluation, and would encourage the use of the bootstrap cross\u2010validation method in particular.\nReScience C 5.1 (#4) \u2013 Etherington and Lieske 2019 5\n7 Acknowledgements\nThis research was funded by internal investment by Manaaki Whenua \u2013 Landcare Re\u2010 search."}], "title": "[Re] Resampling methods for evaluating classification accuracy of wildlife habitat models", "year": 2019}