{"abstractText": "Reinforcement learning algorithms generally belong to one of two categories: model-based techniques, which attempt to overcome the issue of a lack of prior knowledge by enabling the agent to construct a representation of its environment, and model-free techniques, which learn a direct mapping from states to actions. Model-free approaches are typically less practical because running a simulation is very time consuming or expensive, and model-based approaches tend to achieve lower asymptotic performance due to the error in model approximation. To design an effective model-based algorithm, Janner, M. et al [1]. studies the role of model usage in policy optimization and introduces a practical algorithm called model-based policy optimization (MBPO), which makes limited use of a predictive model to achieve pronounced improvements in performance comparing to other model-based approaches. The authors of the paper [1] first formulate and analyze a general implementation for MBPO with monotonic improvement at each step which uses a predictive model to optimize policy and utilizes the policy to collect data and train the model. Previous study shows it is difficult to justify model usage due to pessimistic bounds on model error and this paper finds a way to modify the pessimistic bounds which solves the problem. Based on the analysis, the authors propose a simple model-based reinforcement learning algorithm of using short model-generated rollouts branched from real data to improve model effectiveness. The experiments show this algorithm is faster than other state-ofthe-art model-based methods such as STEVE [2], and matches the performance of the best model-free methods like SAC [3]. In this reproducibility report, we study in detail the MBPO algorithm described in the paper (detailed in Section 3). Our work mainly focuses on the replication of their algorithm and the re-implementation of their predictive model but in a PyTorch version (detailed in Section 4). Lastly, we describe our experiments and provide insightful analysis of our results (detailed in Section 5). We provide the source code1 for generating the results and setting up the experiments.", "authors": [{"affiliations": [], "name": "Yuting Liu"}, {"affiliations": [], "name": "Jiahao Xu"}, {"affiliations": [], "name": "Yiming Pan"}, {"affiliations": [], "name": "Koustuv Sinha"}], "id": "SP:9b441276faaaa66c4f981b7aa91d66ddc3ea6501", "references": [{"authors": ["M. Janner", "J. Fu", "M. Zhang", "S. Levine"], "title": "When to trust your model: Model-based policy optimization.", "venue": "Advances in Neural Information Processing Systems", "year": 2019}, {"authors": ["J. Buckman", "D. Hafner", "G. Tucker", "E. Brevdo", "andH. Lee"], "title": "Sample-Efficient Reinforcement Learningwith Stochastic Ensemble Value Expansion.", "venue": "Advances in Neural Information Processing Systems", "year": 2018}, {"authors": ["T. Haarnoja", "A. Zhou", "P. Abbeel", "S. Levine"], "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.", "venue": "Proceedings of the 35th International Conference on Machine Learning", "year": 2018}, {"authors": ["E. Todorov", "T. Erez", "Y. Tassa"], "title": "Mujoco: A physics engine for model-based control.", "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems", "year": 2012}, {"authors": ["K. Chua", "R. Calandra", "R. McAllister", "S. Levine"], "title": "Deep Reinforcement Learning in a Handful of Trials using Probabilistic DynamicsModels.", "venue": "In:Advances in Neural Information Processing Systems", "year": 2018}, {"authors": ["A. Nagabandi", "G. Kahn", "R.S. Fearing", "S. Levine"], "title": "Neural network dynamics for model-based deep reinforcement learning with model-free fine-tuning.", "venue": "IEEE International Conference on Robotics and Automation (ICRA). IEEE", "year": 2018}, {"authors": ["T. Haarnoja", "H. Tang", "P. Abbeel", "S. Levine"], "title": "Reinforcement learning with deep energy-based policies.", "venue": "Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org", "year": 2017}, {"authors": ["Y. Luo", "H. Xu", "Y. Li", "Y. Tian", "T. Darrell", "T. Ma"], "title": "Algorithmic Framework for Model-based Deep Reinforcement Learning with Theoretical Guarantees.", "venue": "In: International Conference on Learning Representations", "year": 2019}, {"authors": ["V. Feinberg", "A. Wan", "I. Stoica", "M.I. Jordan", "J. Gonzalez", "S. Levine"], "title": "Model-Based Value Estimation for Efficient Model-Free Reinforcement Learning.", "year": 2018}], "sections": [{"text": "R E S C I E N C E C Replication / NeurIPS 2019 Reproducibility Challenge\n[Re] When to Trust Your Model: Model-Based Policy"}, {"heading": "Optimization", "text": "Yuting Liu1, Jiahao Xu1,, and Yiming Pan1, 1Brown University, Providence, Rhode Island, USA\nEdited by Koustuv Sinha ID\nReviewed by Anonymous Reviewers\nReceived 15 February 2020\nPublished 21 May 2020\nDOI 10.5281/zenodo.3818611\n1 Introduction\nReinforcement learning algorithms generally belong to one of two categories: model-based techniques, which attempt to overcome the issue of a lack of prior knowledge by enabling the agent to construct a representation of its environment, and model-free techniques, which learn a direct mapping from states to actions. Model-free approaches are typically less practical because running a simulation is very time consuming or expensive, and model-based approaches tend to achieve lower asymptotic performance due to the error in model approximation. To design an effective model-based algorithm, Janner, M. et al [1]. studies the role of model usage in policy optimization and introduces a practical algorithm called model-based policy optimization (MBPO), which makes limited use of a predictive model to achieve pronounced improvements in performance comparing to other model-based approaches. The authors of the paper [1] first formulate and analyze a general implementation for MBPO with monotonic improvement at each step which uses a predictive model to optimize policy and utilizes the policy to collect data and train the model. Previous study shows it is difficult to justify model usage due to pessimistic bounds on model error and this paper finds a way to modify the pessimistic bounds which solves the problem. Based on the analysis, the authors propose a simple model-based reinforcement learning algorithm of using short model-generated rollouts branched from real data to improve model effectiveness. The experiments show this algorithm is faster than other state-ofthe-art model-based methods such as STEVE [2], and matches the performance of the best model-free methods like SAC [3]. In this reproducibility report, we study in detail the MBPO algorithm described in the paper (detailed in Section 3). Our work mainly focuses on the replication of their algorithm and the re-implementation of their predictive model but in a PyTorch version (detailed in Section 4). Lastly, we describe our experiments and provide insightful analysis of our results (detailed in Section 5). We provide the source code1 for generating the results and setting up the experiments.\n2 Target Questions\nIn order to assess the reproducibility of the paper and validate its conclusions, the main questions we will focus on are:\n1https://github.com/jxu43/replication-mbpo\nCopyright \u00a9 2020 Y. Liu, J. Xu and Y. Pan, released under a Creative Commons Attribution 4.0 International license. Correspondence should be addressed to Yuting Liu (yuting_liu@brown.edu) The authors have declared that no competing interests exist. Code is available at https://github.com/jxu43/replication-mbpo. \u2013 SWH swh:1:dir:d89e39582172e356c1f4fdd5b5cbbde21a70bf27. Open peer review is available at https://openreview.net/forum?id=rkezvT9f6r.\nReScience C 6.2 (#7) \u2013 Liu, Xu and Pan 2020 1\n\u2022 Can we validate the performance of MBPO on two different benchmarks [4] (Hopper and Walker2d)?\n\u2022 Compared to different baselines, can MBPO achieve a pronounced better performance than these model-based and model-free algorithms?\n\u2022 How the proposed MBPO algorithm can be further improved?\n3 Methodology\n3.1 Overview This section introduces the details of the paper\u2019s theory and approaches. The main contribution of the paper is to introduce the MBPO algorithm which resolves the problem that accompanies during monotonic model-based improvement when the model error \u03f5m is high. Specifically, instead of propagating the predictive model for long trajectories, the authors suggest a method of disentangling the task horizon and the model propagation horizon [5]. The method chooses the random states observed previously during interaction with the environment and propagates short trajectories forward. The authors propose that this approach would effectively reduce the effect of compounding errors, and make policy learning substantially faster than most model-based ones while retaining the quality of the policy. To accomplish this goal, the main implementation is divided into three parts: a ensemble of probabilistic neural network, a policy optimizer, and the MBPO algorithm. The implementation of MBPO of the original paper is described in Algorithm 1.\nAlgorithm 1 Model-Based Policy Optimization with Deep Reinforcement Learning 1: Initialize policy \u03c0\u03d5, predictive model p\u03b8, environment dataset Denv, model dataset\nDmodel 2: for N epochs do 3: Train model p\u03b8 on Denv via maximum likelihood 4: for E steps do 5: Take action in environment according to \u03c0\u03d5; add to Denv 6: for M model rollouts do 7: Sample st uniformly from Denv 8: Perform k-step model rollout starting from st using \u03c0\u03d5; add to Dmodel 9: for G gradient updates do 10: Update policy parameters on model data: \u03d5\u2190 \u03d5\u2212 \u03bb\u03c0\u2206\u0302J\u03c0(\u03c0,Dmodel)\n3.2 Predictive Model The authors use the ensemble of probabilistic neural networks from the PETS [6]. It is a bootstrap ensemble of dynamics models. Each member of the ensemble has an output that parametrizes a Gaussian distribution. To generate a prediction from the ensemble, the authors randomly choose a model, so that different dynamics models can sample different transitions along a single model rollout. Predictive models are preferable because they combine the sample efficiency of a model-based approach with the asymptotic performance of function approximators [7].\n3.3 Soft Actor-Critic for Policy Optimization The authors choose Haarnoja\u2019s soft-actor critic algorithm (SAC), which is an off-policy model free deep RL algorithm, as the policy optimization algorithm. The most critical\nReScience C 6.2 (#7) \u2013 Liu, Xu and Pan 2020 2\nfeature of this algorithm is entropy regularization, which is used to maximize expected reward while also maximize entropy. SAC concurrently learns a policy \u03c0\u03b8, two Q-functions Q\u03d51, Q\u03d52 and a value function V\u03c8. It considers the entropy augmented objective:\nJ(\u03c0) = E\u03c0 [\u2211 t r(st, at)\u2212 \u03b1log(\u03c0(at|st)) ]\nSince the objective function consists of both a reward term and an entropy term, SAC encourages the policy network to explore and not assign a very high probability to any one of the actions. It controls the trade-off between exploration and exploitation by using a temperature parameter to determine the relative importance of the entropy term against the reward. SAC provides us a substantial improvement in exploration and robustness as maximum entropy policies could improve exploration by exploring diverse behaviors and reduces estimation errors [8]. Furthermore, as SAC is an off-policy method, it can reuse data collected for another task and will not suffer from poor sample complexity. Generally, the idea is that learned models may be incorporated into model-free methods for improvements in data efficiency.\n3.4 Model-Based Policy Optimization The authors state a way of optimization which interpolates model-based and model-free updates. It chooses to rely less on the model and instead more on real data collected from the true dynamics when the model is inaccurate. To achieve that, performing rollout is a desirable approach. In order to allow for dynamic adjustment between model-based and model-free rollouts, the authors introduce the notion of a branched rollout, in which they begin a rollout from a state under the previous state distribution of the real environment dataset and run k steps according to \u03c0 under the predictive model p\u03b8. Under the kbranched rollout method, using model error under the updated policy \u03f5m\u2032 , the equation of rollouts can be bounded into the following equation:\n\u03b7[\u03c0] \u2265 \u03b7branch[\u03c0]\u2212 2rmax[ \u03b3k+1\u03f5\u03c0 (1\u2212 \u03b3)2 + \u03b3k\u03f5\u03c0 (1\u2212 \u03b3) + k 1\u2212 \u03b3 (\u03f5m\u2032)]\nThe bound above motivates the authors\u2019 MBPO algorithm, which makes limited use of truncated, but nonzero-length, model rollouts.\nFigure 1 above is the diagram of the paper\u2019s algorithm. For each epoch, the authors train the predictive model from the samples of environment dataset Denv, and during\nReScience C 6.2 (#7) \u2013 Liu, Xu and Pan 2020 3\neach step in the epoch, they suggest to interact with the environments with the current policy from SAC, and add the state-action pairs to Denv. Then there will be M number of rollout. Each rollout is consist of choosing a random state from Denv at each time step, and applying the policy generated from SAC algorithm to perform the rollout. After that, they use the generated data to optimize the policy using SAC as the policy optimization algorithm. Using the approach above, the authors claim to achieve the results that are more effective than the best model-free algorithms, and surpass some recent model-based methods such as PETS.\n4 Reproducibility\nThis section will introduce the details of our re-implementation. The main contribution of the original paper is the MBPO algorithm, and we fully re-implemented the MBPO based on both the algorithm and hyperparameters of the original paper and the public codes of the original paper. We followed the codes here because some details are missing in the description of the algorithm in the original paper, which will be discussed extensively in the following parts. For the predictive model, the authors of the original paper uses the existing implementation of PETS by TensorFlow, however, we also re-implemented this part with PyTorch. We experimented with both PETS and our reproduced predictive model by PyTorch. For the policy, the original paper leverage the existing implementation of Softlearning by TensorFlow, while we chose to use another existing implementation of SAC by PyTorch.\n4.1 Predictive Model The authors use an existing bootstrap ensemble model PETS as the predictive model and each single model is a multi-layer fully connected neural network which whose outputs parametrize a Gaussian distribution. In order to generate a prediction from the ensemble model, the algorithm will select a single model randomly from the top n models with minimum loss. We re-implement the predictive model by PyTorch with the same structure, however due to the difference between PyTorch and TensorFlow, we have to make some changes to accomplish the same functionalities like how to generate an ensemble model. We experiment with both the PETS and our re-implemented predictive model. The results will be discussed in the next section.\n4.2 Soft Actor-Critic for policy optimization The authors used the existing reinforcement learning framework \"Softlearning\" which includes the official implementation of the Soft Actor-Critic algorithm. This toolbox trains maximum entropy policies in continuous domains and utilizes the TensorFlow modules for their models. Instead of using their entire framework, we use a PyTorch implementation of SAC to provide policy optimization [9]. We experiment with the efficiency and performance of the PyTorch implementation and the results are convincing. Since we are able to reproduce results by using the PyTorch SAC and it is much easier to integrate it with the rest piece of our MBPO, we decide to use the PyTorch implementation.\n4.3 Model-Based Policy Optimization(MBPO) with Deep Reinforcement Learning The MBPO algorithm is the main contribution of the original paper, thus we put more efforts into fully re-implementing the MBPO algorithm and experimenting with different environments. Generally we followed the same procedure as the algorithm in the original paper. The Algorithm 1 above is the pseudocode for the algorithm described in the original paper, and Algorithm 2 is our detailed algorithm to re-implement the MBPO algorithm, which shows more details about the parameters and algorithm itself.\nReScience C 6.2 (#7) \u2013 Liu, Xu and Pan 2020 4\nAlgorithm 2 Re-implemented Model-Based Policy Optimization with Deep Reinforcement Learning 1: Initialize policy \u03c0\u03d5, predictive model p\u03b8, environment dataset Denv, model dataset\nDmodel 2: for T steps do 3: Take action in environment according to initial policy \u03c0\u03d5; add to Denv 4: for N epochs do 5: for E steps do 6: Train model p\u03b8 on Denv per F steps 7: Take action in environment according to \u03c0\u03d5; add to Denv 8: for M model rollouts do 9: Sample st uniformly from Denv\n10: Perform k-step model rollout starting from st using \u03c0\u03d5; add to Dmodel 11: for G gradient updates do 12: Update policy parameters on model data and environment data with a\nratio of (1\u2212 r) : r: \u03d5\u2190 \u03d5\u2212 \u03bb\u03c0\u2206\u0302J\u03c0(\u03c0, [Dmodel, Denv])\nFirstly, there are T steps sampling in real environment with initial policy \u03c0\u03d5 before the following training process. As the policy has not been trained at this time, the current actions will be totally random choices, which means that these sampling steps will be exploration steps. These steps are important for the following rollouts because those rollouts will start from the samples uniformly randomly chosen from the environment dataset. Secondly, MBPO may train the predictive model once per F steps rather than training the model only once per epoch. It is designed to train the model more frequently so that the rollouts from the predictive model will be more accurate. There will be M rollouts in each step and there will be E steps per epoch, thus when E is set to be a large number, training the predictive model only once per epoch may be not sufficient for a complex environment. For Hopper and Walker2d, the predictive model will be trained once per 250 steps. Then the policy will not be updated only by the data sampled by the predictive model. A small portion of data sampled from the real environment will also be used to update the policy. It helps to update the policy more accurately especially when the predictive model has not been trained well at the beginning. As described in the appendix of the original paper, the rollout length k may be changed dynamically along with the growth of epochs with the equation k = min(max(x+ e\u2212ab\u2212a (y \u2212 x), x), y), where e is the current epoch number, a and b are the minimum epoch and maximum epoch respectively, and x and y are the minimum rollout length and maximum rollout length respectively.\n4.4 Reproducibility Costs Given that there was no mention in the paper about computational resources required, we describe the computational cost for running the experiments. All of our experiments are conducted on Google Cloud Platform. We use NVIDIA Tesla P100 GPU for each experiment, and we replicate three baselines to make comparisons, which are SAC, SLBO [10], and STEVE. We implement two versions of predictive models, one in TensorFlow, another in Pytorch. They both take seven hours to run 150k steps. SAC takes about one hour for training 150k steps which is much faster than other baselines since it is a model-free algorithm and no need to train model. SLBO takes around eight hours while STEVE takes 20 hours to train 150k steps. In conclusion, model-based methods usually take much longer than model-free methods and they need much more computational costs. Our reproducibility needs relatively large costs in terms of time and computing resources.\nReScience C 6.2 (#7) \u2013 Liu, Xu and Pan 2020 5\n5 Results and Analysis\nThe Figure shows that our re-implemented MBPO can achieve comparable performance on both two benchmarks. Our method on Hopper can even attain higher rewards after convergence and it converges a little bit faster than the original one. Better reward is possibly related to the different version of policy optimizer SAC we used in our implementation. However, our results show greater fluctuation. The reason might be that we don\u2019t have enough trials to take average so we have occasional outliers.\n5.3 Baseline Evaluation In our comparison, we also aim to understand how well the reproduction of MBPO compares to the state-of-the-art model-based (STEVE, SLBO) and model-free (SAC) methods. We use the standard full-length version of the two tasks Hopper and Walker2d to conduct the experiments following the paper.\nFigure 3 shows the training curves of re-implemented MBPO and three baselines on different benchmarks. The blue solid line and shaded area are the mean and standard deviation of our MBPO experiments. The yellow, green and pink curves represent the fitted results of our one trial of the baselines. The dashed lines with different colors represents the average return on convergence of different algorithms. We may observe that MBPO does outperform on both two experiments. Specifically, MBPO learns substantially faster, an order of magnitude faster on Hopper, than prior methods, while attaining comparable or better final performance. For example, MBPO converges after 150k steps on Walker2d while model-free SAC converges after 650k steps with the approximately same average return. Also, MBPO converges at a higher average reward than modelbased SLBO method. Thus, we may validate that the proposed MBPO algorithm has asymptotic performance rivaling the best model-free algorithms and learns substantially\nReScience C 6.2 (#7) \u2013 Liu, Xu and Pan 2020 7\nfaster than prior model-free or model-based methods.\n5.4 Predictive Model Effects on Performance Seeking for improvements on the average rewards, we ran our MBPO using two different versions of predictive model which are the ensemble model from PETS in TensorFlow and our own ensemble model of 7 probabilistic neural networks in PyTorch. Figure 4 shows the results of the experiments.\nGenerally, the results with our re-implemented predictive model cannot compete with the results using the existing PETS model, especially on Walker2d. That is probably due to the difference between TensorFlow and PyTorch, and we do not have enough resources to tune our predictive model.\n5.5 Exploration on Hyper Parameters The algorithm described in the paper omits some details. Additionally, there are some differences between the algorithms pseudocode described in the paper and the real implementation by the authors. Specifically, the authors described the training update frequency as 1 per 1000 steps but implemented as 1 per 250 steps. Moreover, the pseudocode did not include any data sampled from the real environment during the policy training process. We decided to implement both algorithms and compare the results. Figure 5 shows the difference between these two settings, and demonstrates that the described algorithm in the paper will not generate the desired results as the paper shows. Due to the limitation of time and GCP credits, we cannot tune the hyperparameters thoroughly. The following is an example of our attempt to tune the rollout size M. The default rollout size M is 400 and we experimented on a rollout size 4000 to see whether a larger rollout size will lead to faster convergence. Figure 6 shows that the performance with rollout size 400 is quite similar to the performance with rollout size 4000.\n6 Conclusion\nIn summary, despite some inconsistencies between the pseudocode and real code as we discussed above, we have successfully reproduced the main results presented by Janner, M. et al., which confirms the supremacy of MBPO algorithm over other baseline models in variety of RL problems, e.g. STEVE and SAC. Due to limit of time, fewer repeats of our re-implemented model have been run. This could be the reason for the overall higher standard deviation in our results. However, the intense fluctuation of average return in our model comparing to the authors\u2018 results should demand a further research. Overall, we should applaud for the authors for providing such practical algorithm which not only\nReScience C 6.2 (#7) \u2013 Liu, Xu and Pan 2020 8\nguarantees the monotonic improvement at each step, but also prevents the error from accumulating by dynamically controlling the rollout length."}, {"heading": "Acknowledgement", "text": "To achieve this reproducibility challenge, we consulted some related materials other than paper itself, including the different versions of SAC implementations, the review of the paper. All the materials we consulted are included in the reference. We want to thank all the staffs in Brown University CS2951-F, especially our professor Michael Littman, and our mentor TA Evan Cater for their help on clarifications and suggestions. Thank Google Cloud Platform for providing computational resources.\nReScience C 6.2 (#7) \u2013 Liu, Xu and Pan 2020 9"}], "title": "[Re] When to Trust Your Model: Model-Based Policy Optimization", "year": 2020}