{"abstractText": "This report reproduces the experiments and validates the results of the ECCV 2020 paper \u201dSolving Phase Retrieval with a Learned Reference\u201d by Hyder et al. [1]. The authors con\u2010 sider the task of recovering an unknown signal from its Fourier magnitudes, where the measurements are obtained after a reference image is added onto the signal. In order to solve this task a novel, iterative phase retrieval algorithm, presented as an unrolled network, that can train a such reference on a small amount of data is proposed. It is shown that the learned reference generalizes well to unseen data distributions and is robust to spatial data augmentation like shifting and rotation.", "authors": [{"affiliations": [], "name": "Nick Rucks"}, {"affiliations": [], "name": "Tobias Uelwer"}, {"affiliations": [], "name": "Stefan Harmeling"}, {"affiliations": [], "name": "Koustuv Sinha"}, {"affiliations": [], "name": "Sharath Chandra Raparthy"}], "id": "SP:6d23b58dc38c2a224f959e14a5bc87bfb7e5e524", "references": [{"authors": ["R. Hyder", "Z. Cai", "M.S. Asif"], "title": "Solving Phase Retrieval with a Learned Reference", "year": 2020}, {"authors": ["R.P. Millane"], "title": "Phase retrieval in crystallography and optics.", "venue": "J. Opt. Soc. Am. A 7.3 (Mar", "year": 1990}, {"authors": ["G. Zheng", "R. Horstmeyer", "C. Yang"], "title": "Wide-field, high-resolution Fourier ptychographic microscopy (vol 7, pg 739, 2013).", "venue": "Nature Photonics", "year": 2015}, {"authors": ["J.R. Fienup"], "title": "Phase Retrieval for Image Reconstruction.", "venue": "Imaging and Applied Optics", "year": 2019}, {"authors": ["E.J. Cand\u00e8s", "Y.C. Eldar", "T. Strohmer", "V. Voroninski"], "title": "Phase Retrieval viaMatrix Completion.", "venue": "SIAMReview", "year": 2015}, {"authors": ["D.A. Barmherzig", "J. Sun", "E.J. Cand\u00e8s", "T.J. Lane", "P. Li"], "title": "Holographic Phase Retrieval and Optimal Reference Design.", "venue": "CoRR abs/1901.06453", "year": 2019}, {"authors": ["A. Paszke", "S. Gross", "S. Chintala", "G. Chanan", "E. Yang", "Z. DeVito", "Z. Lin", "A. Desmaison", "L. Antiga", "A. Lerer"], "title": "Automatic differentiation in pytorch.", "year": 2017}, {"authors": ["Y. Lecun", "L. Bottou", "Y. Bengio", "P. Haffner"], "title": "Gradient-based learning applied to document recognition.", "venue": "Proceedings of the IEEE", "year": 1998}, {"authors": ["G. Cohen", "S. Afshar", "J. Tapson", "A. Van Schaik"], "title": "EMNIST: Extending MNIST to handwritten letters.", "venue": "International Joint Conference on Neural Networks (IJCNN)", "year": 2017}, {"authors": ["H. Xiao", "K. Rasul", "R. Vollgraf"], "title": "Fashion-MNIST: a Novel Image Dataset for BenchmarkingMachine Learning Algorithms.", "year": 2017}, {"authors": ["A. Krizhevsky", "G. Hinton"], "title": "Learning multiple layers of features from tiny images.", "year": 2009}, {"authors": ["Y. Netzer", "T. Wang", "A. Coates", "A. Bissacco", "B. Wu", "A.Y. Ng"], "title": "Reading Digits in Natural Images with Unsupervised Feature Learning.", "venue": "NIPSWorkshop on Deep Learning and Unsupervised Feature Learning", "year": 2011}, {"authors": ["Z. Liu", "P. Luo", "X. Wang", "X. Tang"], "title": "Deep Learning Face Attributes in the Wild.", "venue": "Proceedings of International Conference on Computer Vision (ICCV)", "year": 2015}, {"authors": ["S. Van der Walt", "J.L. Sch\u00f6nberger", "J. Nunez-Iglesias", "F. Boulogne", "J.D. Warner", "N. Yager", "E. Gouillart", "T. Yu"], "title": "scikit-image: image processing in Python.", "venue": "PeerJ", "year": 2014}, {"authors": ["C.R. Harris", "K.J. Millman", "S.J. van der Walt", "R. Gommers", "P. Virtanen", "D. Cournapeau", "E. Wieser", "J. Taylor", "S. Berg", "N.J. Smith"], "title": "Array programming with NumPy.", "year": 2020}, {"authors": ["J.R. Fienup"], "title": "Phase retrieval algorithms: a comparison.", "venue": "In: Appl. Opt", "year": 1982}, {"authors": ["R. Gerchberg"], "title": "A practical algorithm for the determination of phase from image and diffraction plane pictures.", "venue": "Optik", "year": 1972}], "sections": [{"text": "R E S C I E N C E C Replication / ML Reproducibility Challenge 2021\n[Re] Solving Phase Retrieval With a Learned Reference\nNick Rucks1, ID , Tobias Uelwer1, ID , and Stefan Harmeling1, ID 1Heinrich Heine University, D\u00fcsseldorf, Germany\nEdited by Koustuv Sinha,\nSharath Chandra Raparthy\nReviewed by Anonymous Reviewers\nReceived 04 February 2022\nPublished 23 May 2022\nDOI 10.5281/zenodo.6574695"}, {"heading": "Reproducibility Summary", "text": ""}, {"heading": "Scope of Reproducibility", "text": "This report reproduces the experiments and validates the results of the ECCV 2020 paper \u201dSolving Phase Retrieval with a Learned Reference\u201d by Hyder et al. [1]. The authors con\u2010 sider the task of recovering an unknown signal from its Fourier magnitudes, where the measurements are obtained after a reference image is added onto the signal. In order to solve this task a novel, iterative phase retrieval algorithm, presented as an unrolled network, that can train a such reference on a small amount of data is proposed. It is shown that the learned reference generalizes well to unseen data distributions and is robust to spatial data augmentation like shifting and rotation."}, {"heading": "Methodology", "text": "We use the provided original code to reproduce the experiments from Hyder et al. [1] that validate the proposed claims. Nevertheless, we refactor the code base to accelerate the performance and we extent it to carry out experiments where no code is available. We perform a hyperparameter search to investigate the influence and optimal values of the learning rates in both the training and retrieval process. Additionally, we do an ablation study to evaluate the necessary parts of the proposed algorithm. For our exper\u2010 iments we use a single NVIDIA TESLA P100 GPUwith 16GB RAM and approximately 100 computational hours for all experiments together."}, {"heading": "Results", "text": "In general, we are able to reproduce the results of Hyder et al. [1]. Because of the hy\u2010 perparameter search, we are certain that the results are not cherry\u2010picked and mostly reproducible using the authors\u2019 implementation of the algorithm. With our additional experiments, we further strengthen the validity of the proposedmethod and help future researchers and practitioners by providing additional information on the learning rates in the training and retrieval process."}, {"heading": "What Was Easy", "text": "The authors provide an implementation of their algorithm that is executable in our en\u2010 vironment after exchanging deprecated functions. The considered datasets are open\nCopyright \u00a9 2022 N. Rucks, T. Uelwer and S. Harmeling, released under a Creative Commons Attribution 4.0 International license. Correspondence should be addressed to Nick Rucks (nick.rucks@hhu.de) The authors have declared that no competing interests exist. Code is available at https://github.com/tuelwer/machine-learning-reproducibility-challenge-2021. \u2013 SWH swh:1:dir:0a7ba3d1b8f4d4e2ee09a62ddfbe2e8a124d6b1e. Open peer review is available at https://openreview.net/forum?id=rlWzUnM72RF.\nReScience C 8.2 (#35) \u2013 Rucks, Uelwer and Harmeling 2022 1\naccess, hence easy to use. Furthermore, the computational cost is fairly low such that we could run extensive experiments and even compare different hyperparameter set\u2010 tings."}, {"heading": "What Was Difficult", "text": "We spend some effort to understand the authors\u2019 implementation, as it is marginally documented and the used computational tricks are not explained in detail. Moreover, it contains some redundant code which slows down computation. Beyond refactoring, we had to extent the implementation to be able to run our experiments. The lack of information about the learning rates slowed down the reproduction of the results, as we first had to investigate the influences on the training and retrieval process before we could adjust the parameters effectively."}, {"heading": "Communication With Original Authors", "text": "Wewere in contact with the authors via mail and we would like to thank the authors for helping us. Especially, we thank Rakib Hyder who kindly answered all our questions regarding implementation details and hyperparameters and Salman Asif who was open for our implementation suggestions and provided useful feedback for this report.\n1 Introduction\nMany optical detection devices can onlymeasure the Fouriermagnitude of a signal (e.g., the intensity of light) but not its Fourier phase. This systematic loss of information is known as the phase problemand often arises in X\u2010ray crystallography [2], microscopy [3], astronomical imaging [4] and coherent diffraction imaging [5]. The goal of phase re\u2010 trieval algorithms is to efficiently recover the phase of a signal from its phaseless mag\u2010 nitude measurements. A special problem instance is Fourier phase retrieval, where am\u2010 plitudes of a Fourier transformed signal are measured and the task is to recover the original real or complex valued signal. In general, there is no unique mapping from the magnitude to the target signal, thus there exist various approaches to solve it. Mainly inspired by solving holographic phase retrieval using a reference signal by Barmherzig et al. [6], the authors apply a similar approach to Fourier phase retrieval. Therefore, they assume a setting where the target signal x and the reference signal u are additive and overlapping, i.e.,\ny = |F (x) + F (u)|+ \u03b7, (1)\nwhere F is the n\u2010dimensional Fourier transformation and \u03b7 is the measurement noise. For this particular setting, Hyder et al. [1] propose a novel, data\u2010driven retrieval algo\u2010 rithm as an unrolled network with a fixed number of layers. It is capable to learn a reference signal u and subsequently solve the phase retrieval problem utilizing u to re\u2010 cover the target signal x solely from the measurements y.\n2 Scope of Reproducibility\nIn this paper we reproduce themost important experiments using themethod proposed by Hyder et al. [1]. We examine, refactor and extend the original code which we incor\u2010 porate into our scripts to run our experiments.\nReScience C 8.2 (#35) \u2013 Rucks, Uelwer and Harmeling 2022 2\n2.1 Addressed Claims From the Original Paper We validate in this paper the following claims from Hyder et al. [1]:\n\u2022 The presented iterative algorithm is able to learn a reference signal and can utilize it in Fourier phase retrieval to improve the recovery of the target signal. Moreover, it requires only a small amount of training data to learn a reference.\n\u2022 The learned reference is (i) robust to data augmentation in spatial space, (ii) it generalizes well to unseen data distribution and (iii) it is better than other types of references, e.g., random references.\n2.2 Our Contribution Our contributions in this report are:\n1. We redo the experiments onphase retrievalwith a learned referencewith all datasets and report all used parameters.\n2. We reproduce the generalization studywith a subset of the data and report all used parameters.\n3. We validate the robustness claims with our experiments and use furthermore an additional dataset.\n4. We reproduce the experiments on the benefits of a learned references and also extend them with further types of references and new images.\n5. We validate and extend the comparison with some baseline phase retrieval algo\u2010 rithms.\n6. We perform an extensive hyperparameter search to analyze the influence of the learning rates on the reconstruction. We show that the performance of the algo\u2010 rithm can be improved by tuning the learning rates.\n7. We investigate on the necessity of a reference and on the amount of oversampling in the training and recovery process.\n3 Methodology\nMainly, we use the Algorithm 1 and 2 from [1] which are implemented in PyTorch [7] to validate the proposed claims and we mostly follow the restrictions and approaches described in the paper.\n3.1 Model Description In order to reconstruct the target signal x\u2217 given a reference signal u andmeasurements y = |F (x\u2217) + F (u)|, Hyder et al. [1] propose to minimize the loss function\nLx(x; y, u) = \u2225y \u2212 |F (x) + F (u)|\u222522 (2)\nusing a gradient descent algorithm\nxk+1 = xk \u2212 \u03b1\u2207xLx(xk; y, u), (3)\nwhere \u03b1 > 0 is the learning rate and xk is the reconstruction of the k\u2010th iteration (with x0 being properly initialized). The authors interpret the K iterations as an unrolled network withK layers, such that each layer of the network represents a single gradient\nReScience C 8.2 (#35) \u2013 Rucks, Uelwer and Harmeling 2022 3\ndescent update step. So, the input to the network is y and u and the output can bewritten as a function xK(y, u). The reference signal u is learned from a training dataset of images x1, \u2026, xN and cor\u2010 responding measurements (magnitudes) y1, \u2026, yN for a given reference u, which could be written as\nyi = |F (xi) + F (u)|. (4)\nSince for the training images and their magnitudes are known, a good reference image u can by learned by minimizing the least\u2010squares error\nLu(u;x1, . . . , xn, y1, . . . , yn) = N\u2211 i=1 \u2225xi \u2212 xK(yi, u)\u222522 (5)\nbetween signals from the training dataset x1, . . . , xN and their corresponding recon\u2010 structions xK(y1, u), . . . , xK(yN , u) using the unrolled network, Eq. (3). This loss is minimized by gradient descent\nuj+1 = uj \u2212 \u03b2\u2207uLu(uj ;x1, . . . , xn, y1, . . . , yn), (6)\nwhere \u03b2 > 0 is the learning rate for the reference and uj is the reference in the j\u2010th iteration (with u0 being properly initialized). The gradient \u2207uLu can be calculated via backpropagation. The update rule Eq. (6) is applied for fixed number of iterations J .\n3.2 Datasets Throughout our experiments, we use the same datasets as in the original work [1], i.e., MNIST [8], EMNIST [9], FMNIST [10], CIFAR\u201010 [11], SVHN [12], CelebA [13] and also 6 additional standard benchmark images 1. Three of these images were also used in the original work [1] and three are new. Mainly, we access the data via provided code by the authors. For training a reference, we use always 32 images from the training datasets and we test on the same amount of data as proposed by Hyder et al. [1]: We use 10000 test images fromMNIST, FMNIST and CIFAR\u201010, 24800 for EMNIST, 26032 from SVHN and 1000 from CelebA, if not mentioned otherwise. Furthermore, our preprocessing pipeline is similar to the original work [1]: All used images are converted to greyscale, have intensity values in range [0, 1] and we reshape images fromMNIST, EMNIST, FMNIST, CIFAR\u201010, SVHN to 32\u00d732, images from CelebA to 200\u00d7 200 and the standard benchmark images to 512\u00d7 512.\n3.3 Hyperparameter According to [1], we restrict the intensity values of the reference signal u to be within the interval [0, 1] throughout all experiments. Furthermore, we oversample four times in spatial domain by padding the input image with a black border, as this makes the problem more well\u2010behaved. Additionally, our unrolled network always consists of 50 layers and we consider a noise free setting for training and retrieval. However, we pro\u2010 vide detailed parameter configurations for all our experiments in the results section of the respective experiment.\n3.4 Experimental Setup To run the original code, we replaced deprecated functions from the algorithm and im\u2010 ported MNIST and CelebA manually. We use PyTorch 1.5.0 [7], scikit\u2010image 0.18.1 [14]\n1https://homepages.cae.wisc.edu/~ece533/images/ (Accessed on June 25, 2021)\nReScience C 8.2 (#35) \u2013 Rucks, Uelwer and Harmeling 2022 4\nand NumPy 1.21.0 [15] as environment and conduct our experiments in Jupyter note\u2010 books. To compare our results with the original ones, we mainly focus on the peak\u2010 signal\u2010noise\u2010ration (PSNR) over the test images. The used code is available on GitHub 2.\n3.5 Computational Requirements The original implementation requires a GPU with CUDA. Therefore, we use a single NVIDIA TESLA P100 GPU with 16 GBmemory for our experiments. Overall, we used ap\u2010 proximately 100 GPU hours but it is possible to verify the proposed claims within about 3 GPU hours, if all parameters are known. Moreover, by finding and removing unused code we are able to decrease the runtime of the algorithm by 15 to 30 times, depending on the shape of the image. For example, retrieving 26032 images with shape 32 \u00d7 32 takes approximately 9 seconds instead of 180 seconds.\n4 Results\n4.1 Reconstruction Using Learned References In our first experiment we reproduce the mean PSNR values on MNIST, EMNIST, FM\u2010 NIST, SVHN, CIFAR\u201010 andCelebA that are reported in Fig. 2 of [1], see our Tab. 1. Weuse the provided pre\u2010trained references and additionally self\u2010trained references and com\u2010 pare the mean peak\u2010signal\u2010noise\u2010ratio (PSNR) values as the performance criterion. For matching results we tune both \u03b2 (the learning rate for the reference u) 3 and \u03b1 (the learn\u2010 ing rate for the recovery) in the training and reconstruction process. We explain these hyperparameters more detailed in Sec. 4.6. However, in reconstruction we keep \u03b2 = 1 fixed and provide the \u03b1 values used in the retrieval process additional to the results also shown in Tab. 1. By adjusting the learning rate \u03b1 in the recovery process, we are able to reproduce all reported mean PSNR values within a deviation of 1% using the provided references and also our self\u2010trained references. For MNIST, EMNIST and FMNIST we train for 5 epochs with \u03b1 = 1 and \u03b2 = 1, for CelebA we need to train for at least 15 epochs with the same learning rates. To reproduce the reportedmeanPSNR forCIFAR\u201010we set\u03b1 = 1.3during training and train for 5 epochs. For SVHN we need to set \u03b1 = 1.3 and \u03b2 = 10 while we train for 10 epochs to receive the reported mean PSNR values.\n2https://anonymous.4open.science/r/Machine_Learning_Reproducibility_Challenge_Spring_2021-3910/ 3Note: \u03b2 is called lr_u in the implementation provided by the authors.\nReScience C 8.2 (#35) \u2013 Rucks, Uelwer and Harmeling 2022 5\nEvaluated on Trained on MNIST FMNIST CIFAR\u201010\nMNIST 59.76\u00b1 13.27 45.77\u00b1 15.31 32.07\u00b1 9.26 FMNIST 49.44\u00b1 18.11 49.07\u00b1 15.16 28.58\u00b1 11.65 CIFAR\u201010 52.04\u00b1 14.26 49.63\u00b1 15.20 37.20\u00b1 9.89\npadding to be the cause for this, as it has less impact on the dark\u2010edged MNIST images. However, since Hyder et al. [1] also show a decreased mean PSNR for shifting in Fig. 4 of their paper, we can validate their results.\n4.4 On the Benefit of a Learned Reference With this experiments we evaluate the advantages of a learned reference against (i) a constant, (ii) a randomly sampled and (iii) a handcrafted reference. We consider the six standard benchmark images. As referencesweuse our self\u2010trainedCelebA andCIFAR\u201010 references, whichwe resize to 512\u00d7512 by upscaling. The parameters are fixed in recon\u2010 struction to \u03b1 = 1.92 (for best mean PSNR in recovery). Fig. 1 shows our experimental reconstructions of the benchmark images together with the achieved PSNR values. First, we can show that the reported results fromHyder et al. [1] are reproducible, as we receive similar reconstruction results with our self\u2010trained CelebA reference. Addition\u2010 ally, we repeat the experiment with our self\u2010trained CIFAR\u201010 reference but only obtain reconstruction results between the result using a random and the CelebA reference. To generate our random references we follow the description in [1], i.e., we draw from a uniform distribution with range [0, 1]. Additionally, our random reference is drawn with shape 30 \u00d7 30 and resized to 512 \u00d7 512, because this setup performs best. Finally, we report the results of the best performing reference from 100 randomly sampled refer\u2010 ences also in Fig. 1. We observe that our experimental results are similar to the original reconstructions results. To show the advantage against a flat reference, we consider different flat references (all entries set to the same value), where we obtain comparable results for different flat ref\u2010 erences. Similar to the observation of the authors, the recovery results are frequently worse than results obtained with a random reference. We observe minor improvement of some decibel in mean PSNR if we assemble squares or lines manually to common figures like crosses, without any relation to the content of the pictures. However, the reconstructed images are still less noisy if we use a random reference as shown in Fig. 1. Overall, we can validate the reported results from [1], in particular the learned reference performs best against all other evaluated types.\n4.5 Comparison With Baseline Algorithms In this section, we validate the reported results of the hybrid\u2010input\u2010output algorithm (HIO) [16] and extend the experimental evaluation by including twomore baseline phase retrieval algorithms: Fienup\u2019s input\u2010output (IO) and the Gerchberg\u2010Saxton (GS) algo\u2010 rithm [17]. We re\u2010implement all three algorithms from scratch using NumPy [15]. We oversample the test images four times in spatial domain and run the algorithms for 100 iterations on each image with a step size of \u03b2 = 0.8 for input\u2010output [16] and HIO [16]. Also, the reconstructions are clipped to intensity values in range [0, 1]. For each image, we select the best PSNR from the cropped reconstruction and the cropped, flipped and shifted one. Tab. 5 shows the results on the different datasets. Overall, we can validate the claim byHyder et al. [1], even though ourHIO [16] implementation performs slightly better than the one reported in the original work.\nReScience C 8.2 (#35) \u2013 Rucks, Uelwer and Harmeling 2022 7\nReScience C 8.2 (#35) \u2013 Rucks, Uelwer and Harmeling 2022 8\n0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25 2.50 Learning rate in reconstruction only.\n0\n20\n40\n60\n80\nMNIST EMNIST FMNIST CIFAR-10 SVHN CelebA\n12.02\n(a) No oversampling\n15.18\n(b) 2\u00d7 oversampling\n30.06\n(c) 4\u00d7 oversampling\n30.71\n(d) 8\u00d7 oversampling\nIn conclusion, we can verify that the unrolled network proposed by Hyder et al. [1] is capable of learning a valuable reference that can be utilized to recover a signal from its Fourier magnitude measurement. We trained our references from scratch and we demonstrated that they are similar enough to the original ones. Moreover, we encoun\u2010 tered nomajor contradiction in our experiments if we use new data, references or gener\u2010 ative methods. However, an extensive hyperparameter search was necessary to match the reported results. Also, the hyperparameter search reveals that one should focus on tuning the learning rate \u03b1 during reconstruction as it yields to performance improve\u2010 ments across all datasets. Our ablation study shows that oversampling during training can be omitted to save computational resources. Nonetheless, by providing an official implementation of their algorithm the authors en\u2010 abled future researchers to utilize their method. Furthermore, we are grateful to the authors for kindly answering all of our questions regarding the implementation and providing feedback on our results.\nReScience C 8.2 (#35) \u2013 Rucks, Uelwer and Harmeling 2022 10"}], "title": "[Re] Solving Phase Retrieval With a Learned Reference", "year": 2022}