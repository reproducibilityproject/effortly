{"abstractText": "In this work, we study the reproducibility of the paper Counterfactual Generative Networks (CGN) by Sauer and Geiger to verify their main claims, which state that (i) their pro\u2010 posed model can reliably generate high\u2010quality counterfactual images by disentangling the shape, texture and background of the image into independentmechanisms, (ii) each independent mechanism has to be considered, and jointly optimizing all of them end\u2010 to\u2010end is needed for high\u2010quality images, and (iii) despite being synthetic, these coun\u2010 terfactual images can improve out\u2010of\u2010distribution performance of classifiers by making them invariant to spurious signals.", "authors": [{"affiliations": [], "name": "Piyush Bagad"}, {"affiliations": [], "name": "Paul Hilders"}, {"affiliations": [], "name": "Jesse Maas"}, {"affiliations": [], "name": "Danilo de Goede"}, {"affiliations": [], "name": "Koustuv Sinha"}, {"affiliations": [], "name": "Sharath Chandra Raparthy"}], "id": "SP:56ace25348d933089180eee43ff1b366c26b2018", "references": [{"authors": ["A. Saue"], "title": "and A", "venue": "Geiger. \u201cCounterfactual Generative Networks.\u201d In: International Conference on Learning Representations (ICLR).", "year": 2021}, {"authors": ["M.A. Alcorn", "Q. Li", "Z. Gong", "C. Wang", "L. Mai", "W.-S. Ku"], "title": "and A", "venue": "Nguyen. Strike (with) a Pose: Neural Networks Are Easily Fooled by Strange Poses of Familiar Objects.", "year": 2019}, {"authors": ["Y. Ming", "H. Yin"], "title": "and Y", "venue": "Li. On the Impact of Spurious Correlation for Out-of-distribution Detection.", "year": 2021}, {"authors": ["A. Rosenfeld", "R. Zemel"], "title": "and J", "venue": "K. Tsotsos. The Elephant in the Room.", "year": 2018}, {"authors": ["R. Geirhos", "J.-H. Jacobsen", "C. Michaelis", "R. Zemel", "W. Brendel", "M. Bethge", "F.A. Wichmann"], "title": "Shortcut learning in deep neural networks.", "venue": "Nature Machine Intelligence", "year": 2020}, {"authors": ["D. Kaushik", "E. Hovy", "Z.C. Lipton"], "title": "Learning the difference that makes a difference with counterfactuallyaugmented data.", "year": 1909}, {"authors": ["M.L. Olson", "R. Khanna", "L. Neal", "F. Li", "W.-K. Wong"], "title": "Counterfactual state explanations for reinforcement learning agents via generative deep learning.", "venue": "Artificial Intelligence", "year": 2021}, {"authors": ["J. Pearl"], "title": "Causality", "venue": "Cambridge university press,", "year": 2009}, {"authors": ["B. Sch\u00f6lkopf"], "title": "Causality for machine learning.", "venue": "arXiv preprint arXiv:1911.10500", "year": 2019}, {"authors": ["A. Brock", "J. Donahue", "K. Simonyan"], "title": "Large scale GAN training for high fidelity natural image synthesis.", "year": 2018}, {"authors": ["X. Qin", "Z. Zhang", "C. Huang", "M. Dehghan", "O.R. Zaiane", "M. Jagersand"], "title": "U2-Net: Going deeper with nested U-structure for salient object detection.", "venue": "Pattern Recognition", "year": 2020}, {"authors": ["A. Sauer", "A. Geiger"], "title": "Counterfactual Generative Networks GitHub", "venue": "https://github.com/autonomousvision/ counterfactual_generative_networks.", "year": 2021}, {"authors": ["N. Inkawhich"], "title": "DCGAN Tutorial", "venue": "https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html.", "year": 2021}, {"authors": ["I. Figotin"], "title": "ImageNet 1000 (Mini)", "venue": "https://www.kaggle.com/ifigotin/imagenetmini-1000.", "year": 2019}, {"authors": ["M. Arjovsky", "L. Bottou", "I. Gulrajani"], "title": "and D", "venue": "Lopez-Paz. Invariant Risk Minimization.", "year": 2020}, {"authors": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li"], "title": "and L", "venue": "Fei-Fei. \u201cImagenet: A large-scale hierarchical image database.\u201d In: 2009 IEEE conference on computer vision and pattern recognition. Ieee.", "year": 2009}, {"authors": ["R. Geirhos", "P. Rubisch", "C. Michaelis", "M. Bethge", "F.A. Wichmann", "W. Brendel"], "title": "ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness.", "year": 2018}, {"authors": ["K. Xiao", "L. Engstrom", "A. Ilyas", "A. Madry"], "title": "Noise or Signal: The Role of Image Backgrounds in Object Recognition.", "venue": "ArXiv preprint arXiv:2006.09994", "year": 2020}, {"authors": ["J. Gildenbla"], "title": "and contributors", "venue": "PyTorch library for CAM methods. https://github.com/jacobgil/pytorch-gradcam.", "year": 2021}, {"authors": ["R.R. Selvaraju", "M. Cogswell", "A. Das", "R. Vedantam", "D. Parikh", "D. Batra"], "title": "Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization.", "venue": "In: International Journal of Computer Vision", "year": 2019}, {"authors": ["K. Xiao", "L. Engstrom"], "title": "A", "venue": "Ilyas, andA.Madry.Noise or Signal: TheRole of ImageBackgrounds inObject Recognition.", "year": 2020}, {"authors": ["D. Hendrycks", "K. Zhao", "S. Basart", "J. Steinhardt", "D. Song"], "title": "Natural Adversarial Examples.", "year": 2021}, {"authors": ["H. Wang", "S. Ge", "Z. Lipton"], "title": "and E", "venue": "P. Xing. \u201cLearning Robust Global Representations by Penalizing Local Predictive Power.\u201d In: Advances in Neural Information Processing Systems.", "year": 2019}, {"authors": ["R. Geirhos", "P. Rubisch", "C. Michaelis", "M. Bethge", "F.A. Wichmann"], "title": "and W", "venue": "Brendel. \u201cImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness.\u201d In: International Conference on Learning Representations.", "year": 2019}, {"authors": ["A.B. Arrieta", "N. D\u0131\u0301az-Rodr\u0131\u01f5uez", "J. Del Ser", "A. Bennetot", "S. Tabik", "A. Barbado", "S. Garc\u0131\u00e1", "S. Gil-L\u00f3pez", "D. Molina", "R. Benjamins"], "title": "Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI.", "venue": "Information Fusion", "year": 2020}, {"authors": ["S. Verma", "J. Dickerson", "K. Hines"], "title": "Counterfactual explanations for machine learning: A review.", "venue": "arXiv preprint arXiv:2010.10596", "year": 2020}, {"authors": ["O. Lang", "Y. Gandelsman", "M. Yarom", "Y. Wald", "G. Elidan", "A. Hassidim", "W.T. Freeman", "P. Isola", "A. Globerson", "M. Irani"], "title": "Explaining in Style: Training a GAN to explain a classifier in StyleSpace.", "year": 2021}, {"authors": ["A.J. DeGrave", "J.D. Janizek", "S.-I. Lee"], "title": "AI for radiographic COVID-19 detection selects shortcuts over signal.", "venue": "Nature Machine Intelligence", "year": 2021}, {"authors": ["R. Guidotti", "A. Monreale", "S. Ruggieri", "F. Turini", "D. Pedreschi"], "title": "and F", "venue": "Giannotti. A Survey Of Methods For Explaining Black Box Models.", "year": 2018}], "sections": [{"text": "R E S C I E N C E C Replication / ML Reproducibility Challenge 2021\n[Re] Reproducibility Study of \u201cCounterfactual Generative Networks\u201d Piyush Bagad1, ID , Paul Hilders1, ID , Jesse Maas1, ID , and Danilo de Goede1, ID 1University of Amsterdam, Amsterdam, The Netherlands \u2013 1Equal contribution\nEdited by Koustuv Sinha,\nSharath Chandra Raparthy\nReviewed by Anonymous Reviewers\nReceived 04 February 2022\nPublished 23 May 2022\nDOI 10.5281/zenodo.6574635"}, {"heading": "Reproducibility Summary", "text": ""}, {"heading": "Scope of Reproducibility", "text": "In this work, we study the reproducibility of the paper Counterfactual Generative Networks (CGN) by Sauer and Geiger to verify their main claims, which state that (i) their pro\u2010 posed model can reliably generate high\u2010quality counterfactual images by disentangling the shape, texture and background of the image into independentmechanisms, (ii) each independent mechanism has to be considered, and jointly optimizing all of them end\u2010 to\u2010end is needed for high\u2010quality images, and (iii) despite being synthetic, these coun\u2010 terfactual images can improve out\u2010of\u2010distribution performance of classifiers by making them invariant to spurious signals."}, {"heading": "Methodology", "text": "The authors of the paper provide the implementation of CGN training in PyTorch. How\u2010 ever, they did not provide code for all experiments. Consequently, we re\u2010implement the code for most experiments, and run each experiment on 1080 Ti GPUs. Our repro\u2010 ducibility study comes at a total computational cost of 112 GPU hours."}, {"heading": "Results", "text": "We find that the main claims of the paper of (i) generating high\u2010quality counterfactuals, (ii) utilizing appropriate inductive biases, and (iii) using them to instil invariance in clas\u2010 sifiers, do largely hold. However, we found certain experiments that were not directly reproducible due to either inconsistency between the paper and code, or incomplete specification of the necessary hyperparameters. Further, we were unable to reproduce a subset of experiments on a large\u2010scale dataset due to resource constraints, for which we compensate by performing those on a smaller version of the same dataset with our results supporting the general performance trend."}, {"heading": "What was easy", "text": "The original paper provides an extensive appendix with implementation details and hy\u2010 perparameters. Beyond that, the original code implementation was publicly accessible and well structured. As such, getting started with the experiments proved to be quite\nCopyright \u00a9 2022 P. Bagad et al., released under a Creative Commons Attribution 4.0 International license. Correspondence should be addressed to Piyush Bagad (piyush.bagad@student.uva.nl) The authors have declared that no competing interests exist. Code is available at https://github.com/danilodegoede/fact-team3/. \u2013 SWH swh:1:dir:410075522df668dfae4742564f10b62de0cb8dc6. Open peer review is available at https://openreview.net/forum?id=HNlzT3G720t.\nReScience C 8.2 (#5) \u2013 Bagad et al. 2022 1\nstraightforward. The implementation included configuration files, download scripts for the pretrained weights and datasets, and clear instructions on how to get started with the framework."}, {"heading": "What was difficult", "text": "Some of the experiments required severe modifications to the provided code. Addition\u2010 ally, some details required for the implementation are not specified in the paper or in\u2010 consistent with the specifications in the code. Lastly, in evaluating out\u2010of\u2010distribution robustness, getting the baseline model to work and obtaining numbers similar to those reported in the respective papers was challenging, partly due to baseline model incon\u2010 sistencies within the literature.\nCommunication with original authors We have reached out to the original authors to get clarifications regarding the setup of some of the experiments, but unfortunately, we received a late response and only a subset of our questions was answered.\n1 INTRODUCTION\nDespite the considerable popularity of deep learningmodels within the field of artificial intelligence, recent literature suggests that these networks have a tendency of learning simple correlations that perform well on a benchmark dataset, instead of more com\u2010 plex relations that generalize better [2, 3, 4]. This phenomenon, which is referred to as shortcut learning by Geirhos et al. [5], makes these models more sensitive to input perturbation and unseen input contexts.\nIn order to enhance the robustness and interpretability of classifiers, Sauer and Geiger [1] introduce the idea of a Counterfactual Generative Network (CGN). Using appro\u2010 priate inductive biases to disentangle separate components within the input images, such as object shape, object texture, and background, this model is capable of augment\u2010 ing training data with generated counterfactual images. The authors claim that, using this model, they were able to improve out\u2010of\u2010distribution (OOD) robustness with only a marginal performance decrease for the original classification task.\nIn this work, we aim to reproduce their findings, verify their claims, and perform additional experiments to provide further evidence to support their claims. In summary, this work makes the following contributions:\n\u2022 We reproduce the main experiments conducted by Sauer and Geiger [1] to identify which parts of the experimental results supporting their claims can be reproduced, and at what cost in terms of resources (e.g., computational cost, development ef\u2010 fort, and communication with the authors).\n\u2022 We improve the performance consistency of the CGN during training.\n\u2022 We extend upon the work of Sauer and Geiger by empirically analyzing the deci\u2010 sions made by classifiers based on their proposed model. Based on this analysis, we propose a method to quantify the robustness of such classifiers against spuri\u2010 ous correlations.\n1.1 Scope of Reproducibility Distinguishing between spurious and causal correlation is an active topic in causality research [6, 7]. One central principle in causal inference is the assumption of indepen\u2010 dent mechanisms (IMs), which states that a causal generative process is composed of\nReScience C 8.2 (#5) \u2013 Bagad et al. 2022 2\nautonomous modules that do not influence each other [8, 1, 9]. The CGN introduced in the original paper exploits this idea to decompose the image generation process into three IMs, each controlling one factor of variation (FoV), namely the shape, texture, and background. Using this, the authors take a step towards more robust and interpretable classifiers that explicitly expose the causal structure of the classification task. In this re\u2010 producibility study, our main goal is to verify the following claims of the original paper:\n\u2022 High\u2010Quality Counterfactuals (HQC): By exploiting proper inductive biases, the CGN is able to reliably learn the independent mechanisms, which allow for the generation of high\u2010quality counterfactual images by disentangling the shape, tex\u2010 ture and background of the image.\n\u2022 Inductive Bias Requirements (IBR): Each independent mechanism has to be con\u2010 sidered, and jointly optimizing all of them end\u2010to\u2010end is needed for high\u2010quality images.\n\u2022 Out\u2010of\u2010DistributionRobustness (ODR): Despite being synthetic, the counterfactual images can improve out\u2010of\u2010distributionperformance of classifiers bymaking them invariant to spurious signals.\nThe remainder of this work is structured as follows. In Section 2, we introduce the model proposed in the original paper to provide the reader with the required back\u2010 ground knowledge. Section 3 then summarizes our approach to reproduce the original paper. Subsequently, Section 4 presents the replicated results and compares them to the original paper. Section 5 concludes this work by discussing our experience with reproducing the research by Sauer and Geiger [1].\n2 COUNTERFACTUAL GENERATIVE NETWORK\nThe counterfactual generative network is a manifestation of a structural causal model (SCM) for the task of image classification [1]. It decomposes the image generation pro\u2010 cess into four IMswhose losses are jointly optimized in an end\u2010to\u2010endmatter. Anoverview of the CGN architecture is shown in Appendix A.\nShape mechanism: The shape mechanism fshape captures the shape as a binary mask m, where 1 corresponds to the object and 0 to the background. For this purpose, it first samples a pre\u2010mask m\u0303 with exaggerated features from a fine\u2010tuned BigGAN [10], and extracts the binarymaskusing a pretrainedU2\u2010Net [11]. The shape lossLshape comprises (1) the pixelwise binary entropy of the mask, and (2) the mask loss:\nLmask(m) = Ep(u,y)\n[ max ( 0, \u03c4 \u2212 1\nN N\u2211 i=1 mi\n) +max ( 0, 1\nN N\u2211 i=1 mi \u2212 \u03c4\n)] . (1)\nThe pixelwise binary entropy forces the output to be close to either 0 or 1, whereas the mask loss discourages trivial solutions that are outside the interval defined by \u03c4 .\nTexture mechanism: The texture mechanism ftext generates the texture of the object. For MNIST, Sauer and Geiger use an additional layer that divides its input into patches and randomly rearranges them. In contrast, for ImageNet, they sample patches from the regions where the mask values are the highest and concatenate them into a patch grid pg. This mechanism is optimized by minimizing the perceptual loss between the foreground f and the patch grid pg. As such, the background gradually transforms into object texture during training.\nReScience C 8.2 (#5) \u2013 Bagad et al. 2022 3\nBackground mechanism: The background mechanism fbg models the background b of the image. It removes the object from the output of the BigGAN backbone and in\u2010 paints it using U2\u2010Net by minimizing the predicted saliency. Because there is no need for a globally coherent background in the MNIST setting, the MNIST variant of the CGN includes a second texture mechanism rather than a dedicated background mechanism.\nComposer: The composerC combines the output of the aforementionedmechanisms into a single composite image\nxgen = C(m,f , b) = m\u2299 f + (1\u2212m)\u2299 b, (2)\nwherem is the mask, f is the foreground, b is the background, and \u2299 is the Hadamard product. To optimize this mechanism, Sauer and Geiger use an external conditional GAN (cGAN) that generates pseudo\u2010ground\u2010truth images xgt from the same noise u and label y that is fed into the aforementionedmechanisms of the CGN. Using this, theymin\u2010 imize the reconstruction loss Lrec between the composite image xgen and the pseudo\u2010 ground\u2010truth image xgt.\nDuring training, each independent mechanism learns a class\u2010conditional distribu\u2010 tion over shapes, textures, or backgrounds. It can then generate counterfactual images by randomizing the noise u and label input y for each mechanism. A more detailed explanation regarding the purpose of these counterfactual images and the connection with explainable artificial intelligence (XAI) can be found in Appendix B.\nIn order to encode invariance to spurious correlations, Sauer and Geiger train classi\u2010 fiers on generated counterfactual data that retain the label from the shape with random\u2010 ized texture and backgrounds. For MNISTs, they use a standard CNN feature extractor followed by a single classification head. For ImageNet on the other hand, they use a CNN backbone with three classifier heads: shape, texture, and background; each in\u2010 variant to all but one factor of variation. The final prediction is obtained by averaging the individual head predictions.\n3 METHODOLOGY\nThe original implementation of the CGN is publicly available [12], but most of the exper\u2010 iments conducted in the original paper to support their claims are not. Consequently, we use the authors\u2019 code for the implementation of the CGN, and re\u2010implement the ex\u2010 periments and relevant evaluation metrics based on the descriptions provided in the paper. Furthermore, we both improve and extend upon the work of Sauer and Geiger by providing additional experiments and results. Because a description of the GANused in the original paper was not provided, we use a DCGAN [13].\n3.1 Datasets The experiments conducted in the original paper involve two tasks, namely generating counterfactual examples and training a classifier to be invariant to spurious correlations. We follow the paper and reproduce their evaluations on multiple datasets for each task. For both tasks, we present the relevant datasets and their main purpose in Table 1. Due to resource constraints, running all experiments on full ImageNet (IN\u20101k) is infeasible. As a compromise, we use ImageNet\u2010mini (IN\u2010Mini) [14], a small\u2010scale variant of Ima\u2010 geNet. Although this dataset contains fewer samples, we found it to be sufficient to reproduce the main findings of the original paper and verify their claims. Moreover, this dataset includes the same classes as IN\u20101k and hence does not induce any decrease in difficulty of the classification task.\nReScience C 8.2 (#5) \u2013 Bagad et al. 2022 4\nTo provide further evidence to support claim ODR, we conduct additional experi\u2010 ments to visually explain thedecisionsmadeby the invariant classifiers based ongradient\u2010 based localization. For this purpose, we use a PyTorch implementation of GradCAM [19, 20], a class activation map method that weighs the 2D activations by the average gradi\u2010 ent [20]. This method allows us to visualize the salient features on which the invariant classifiers base their predictions.\n3.4 Computational requirements We perform all experiments on a cluster whose nodes are equipped with Nvidia GeForce GTX 1080 Ti GPUs. Due to constraints in resources, we run most experiments once. As such, our experiments are indicative and not conclusive. Our reproducibility study comes at a total computational cost of 112 GPU hours (see Appendix D for more details).\n4 EXPERIMENTAL RESULTS\n4.1 Reproducibility study Evaluating counterfactual samples To verify claimHQC,wequalitatively evaluate coun\u2010 terfactual (CF) samples generated using CGNmodels on each dataset. For all our repro\u2010 ducibility experiments, we use the pretrained weights for CGN to generate CFs. We\n1This variant ofMNIST is generated by the authors themselves and can be generated using their repository.\nReScience C 8.2 (#5) \u2013 Bagad et al. 2022 5\nfound inconsistencies while training the CGN from scratch and refer the reader to Sec\u2010 tion 4.2.1 for a deeper investigation. For bothMNIST and ImageNet, our results indicate that the quality of the generated CFs matches with the quality of those reported in the original paper, as shown in Figure 1 and Figure 2 respectively. For ImageNet, although we can easily recognize the FoVs in the generated CFs, they are highly unrealistic.\nEvaluating loss ablation Weattempt to reproduce the loss ablation study to verify claim IBR. The authors claim that a CGN can be trained from scratch within 12 hours on a GTX 1080Ti GPU. However, when running the experiments as described by the authors, the estimated training time exceeded 200 hours. Upon further inspection, we found an alter\u2010 native configuration file containing the hyperparameters the authors used to train the CGN that was inconsistent with the default hyperparameters. Using these alternative hyperparameters, we managed to decrease the training time to approximately 20 hours. While the inception score magnitude directly depends on the number of generated im\u2010 ages used for the calculation, the original paper did not specify the exact number of images used during the experiment. We empirically found that using 2000 images pro\u2010 vides inception scores that resemble those reported in the original paper.\nThe results in Table 2 indicate that the inception scores follow a similar trend as re\u2010 ported by the authors (marked as x ). However, when disabling the texture loss, we found \u00b5mask to be 0.4, whereas the original paper reported a value of 0.9. This is a crucial difference, because the value of 0.9 of the original paper indicates a mask col\u2010 lapse, which the authors use to support claim IBR. Nonetheless, wewere able to support this claim by performing an additional qualitative experiment. Specifically, if we look at some samples as shown in Appendix E, it is clear that the generated texture still in\u2010 cludes some background. This indicates that the IMs for texture and background are no longer disentangled, which shows that the texture loss is indeed necessary.\nOnMNIST variants, we identify an inconsistency in the experimental setup stated in the paper and code. The paper seems to suggest using a combination of original and CF dataset, but the code only uses CF data. As reported in Table 3, we experiment with both and observe similar results for C\u2010MNIST and DC\u2010MNIST. Surprisingly, for CGN, adding original data hurts the performance for W\u2010MNIST (62.9 vs. 81.4). Apart from that, the majority of our results arewithin 5% variation from those reported in the paper (marked as x ), which supports the broader claim of better generalization even in the presence of spurious correlations (e.g., texture in case of colored MNIST).\nTo evaluate the invariance in classifier heads on IN\u2010mini, we first reproduce the ex\u2010 periment regarding shape bias from the original paper. The shape bias is defined as the fraction of test samples for which the predicted label matches the shape label of the input image [17]. In this case, we evaluate labels with predictions from each head. As reported in Table 4, our results are smaller in comparison to the IN\u20101k results reported in the original paper. Nonetheless, the overall trend does support claim ODR. Addition\u2010 ally, we replicate the experiment regarding the evaluation of background robustness. The paper uses the notion of BG\u2010gap that measures classifiers\u2019 reliance on background signal [21]. Our results, shown in Table 5, again slightly deviate from the original paper but the trend supports claimODR. Note that, although IN\u2010mini was used for the training set instead of IN\u20101k , the evaluation has been performed using the same datasets as in the paper.\nTo evaluate the effect of using more counterfactual datapoints or generating more counterfactual images per sampled noise, Sauer and Geiger performed an MNIST Ab\u2010 lation Study in the original paper. Our reproduction for this experiment, along with\nReScience C 8.2 (#5) \u2013 Bagad et al. 2022 7\na more detailed description regarding the experiment and results, can be found in Ap\u2010 pendix G.\n4.2 Results beyond original paper\nImproving CGN training onMNISTs \u2014While training the CGN on theMNIST, we encountered an issue that was not mentioned in the original paper. During the training process, we observed that the digit masks had a tendency of collapsing to an erroneous state, from where the digits would no longer improve during training. For this reason, it was not possible for us to reproduce the CGN training on the MNIST data using the default con\u2010 figuration. Therefore, we have proposed a solution that makes the CGN training on the MNIST datasets more consistent. Details regarding our solution can be found in Ap\u2010 pendix C.\nExplainability analysis for invariant classifiers \u2014While the reproduced experiments for the original paper provide some support for claim ODR, these results primarily show the effect of using counterfactuals on test accuracy performance. However, it is not directly clear from these quantitative experiments if the performance increase is actually due to the fact that the use of counterfactuals ensures that the classifier focuses on the right correlations (e.g., shape) and not spurious ones (e.g., background). To further verify the validity of claim ODR, we provide two additional analyses that combine qualitative and quantitative measures to evaluate the behaviour of the counterfactual classifiers.\nWhat does the latent feature space look like? First, we visualize learnt classifier fea\u2010 tures using t\u2010SNE for a subset of the test set of original and counterfactual (CF) data for C\u2010MNIST. Figure 3(a) shows that a classifier trained on CF data is indeed invariant to spurious correlations (e.g. digit color). Figure 3(b) shows that a classifier trained on CF data is also better at representing OOD samples (e.g. counterfactuals). Interestingly, the latter figure also shows that the CF\u2010trained classifier tends to group the clusters for 4\u20107\u20109 and 3\u20105\u20108 close to each other, which was not the case for the classifier trained on origi\u2010 nal data. These digits are also close in shape in reality, which suggests that the model is rightly focusing on the shape while ignoring texture. The results for other MNIST variants are consistent with this finding.\nWhat features does the model focus on? Second, we perform an experiment to visu\u2010 alize a spatial heatmap of areas that the model focuses on to make a prediction. Based on claims ODR and IBR, we would expect the different heads to operate separately from one another, while being completely invariant to the other FoVs. In order to generate the spatial heatmaps we use GradCAM. Some qualitative samples are shown in Figure 4. In addition to the qualitative analyses, using GradCAM provides the opportunity to for\u2010 mulate another quantitative measure to validate claims ODR and IBR. This quantitative\nReScience C 8.2 (#5) \u2013 Bagad et al. 2022 8\nanalysis aims to measure if CF\u2010trained models focus on shape more than those trained on original data. To this end, we compute themean Intersection of Union (IoU) between GradCAM heatmaps and binarized digit masks on the test set. We note that a classifier trained on CF data is consistently outperforms the classifier on original data.\nWhile the quantitative results using the IoUmetric cannot be performed on the Ima\u2010 geNet data, due to the lack of ground truth binary object maps, it is possible to evaluate the qualitative performance of the IMs using GradCAM. As shown in Appendix H, the individual classifier heads tend to focus on meaningful aspects.\nOOD generalization for invariant classifiers \u2014 In order to provide further evidence for the claim ODR, we test the model performance on alternative ImageNet datasets, which are designed to evaluate out\u2010of\u2010distribution robustness. Specifically, we evaluate the per\u2010 formance on ImageNet\u2010A (natural adversarial examples) [22], ImageNet\u2010Sketch [23] and Stylized\u2010ImageNet [24], and compare with a ResNet\u201050 baseline that is pretrained on IN\u2010 1k. Surprisingly, we find that the finetuned CGN\u2010based ensemble performs worse on all specified OOD\u2010benchmarks, compared to the pretrained ResNet\u201050 baseline as shown in Table 6.\nThroughout this work, we have conducted several experiments to reproduce the main results from the research by Sauer and Geiger [1]. The results of our reproducibility study provide support for their claims, as we were largely able to reproduce the original results. Specifically, our results showed that the test accuracy for the MNIST classifiers greatly improved when using generated counterfactual datasets. Then, we were able to use the ImageNet\u2010mini dataset to achieve similar performance trends compared to the original paper in terms of shape versus texture bias evaluation, and the background robustness evaluation. However, based on the qualitative analyses for claim HQC, it is clear that the quality of the generated counterfactual images could still be improved. Specifically, we have observed some distinct failure cases regarding the quality of gen\u2010 erated counterfactual images, which are described in Appendix I.\nInterestingly, while the loss ablation study provided similar results to what the au\u2010 thors reported in the original paper, we did obtain different results for the experimental run without texture loss. As the authors used this study to provide evidence for claim\nReScience C 8.2 (#5) \u2013 Bagad et al. 2022 9\nIBR, this difference is quite significant. Nonetheless, qualitative analysis of the images that were generated without texture loss revealed that the quality of the generated im\u2010 ages indeed reduced when the texture loss was omitted. Although this does provide support for claim IBR, it also shows that the IS and \u00b5mask metrics used by the authors in the loss ablation study may not be sufficient to support their claims. Since the loss ablation study is therefore not conclusive, further research is required to investigate if the inductive biases introduced by the authors are indeed \u2018appropriate\u2019. The results from our additional experiments provide further evidence that counterfactual images generated with the proposed CGN architecture can be used to train classifiers that are more robust against spurious signals. Using GradCAM, we were able to visualize this behaviour and formulate a quantitative performance metric.\nOverall, the experiments from the original paperwere largely reproducible, and their main claims seem reasonably substantiated but could benefit from additional evidence in future research. The code implementation of our reproducibility study is publicly available 1.\nLimitations Unfortunately, we did encounter some difficulties during the reproduc\u2010 tion process. First, since our model was trained on IN\u2010mini, we were not able to repro\u2010 duce the exact same results as the original paper. However, despite the slightly deviat\u2010 ing results, the overall trends in the results seem to correspond well with the original results. Second, as some experimental setup information wasmissing from the original paper, we had to rely on the default parameter configuration files that were provided in the original code implementation, even though we can not be completely certain that these parameters were used for the original experiments.\n5.1 Reflection: What was easy, and what was difficult? The original paper provides an extensive appendix with implementation details and hy\u2010 perparameters. Beyond that, the original code implementation was publicly accessible and well structured. As such, getting started with the experiments proved to be quite straightforward. The implementation included configuration files, download scripts for the pretrained weights and datasets, and clear instructions on how to get started with the framework.\nNonetheless, reproducing the original results turned out to be far from trivial as the setup of some of the experiments required severe modifications to the provided code. Additionally, some details required for the implementation are not specified in the pa\u2010 per or inconsistent with the specifications in the code (e.g., the GAN as mentioned in Section 3). Lastly, in evaluating robustness to OOD, getting the baseline model to work and obtaining numbers similar to those reported in the respective papers was challeng\u2010 ing, partly due to baseline model inconsistencies within the literature.\n5.2 Communication with original authors We have reached out to the original authors to get clarifications regarding the setup of some of the experiments. For example, we asked the authors if they could share pre\u2010 trained weights from the classifiers that were trained on full ImageNet, and which type of GAN architecture was used for the MNIST experiments. Unfortunately, we received a late response and only a subset of our questions was answered, and as a result we were not able to fully verify whether our design choices were consistent with those of the original paper.\n1https://github.com/danilodegoede/fact-team3/\nReScience C 8.2 (#5) \u2013 Bagad et al. 2022 10"}, {"heading": "Appendix", "text": "A Counterfactual Generative Network Architecture\nIn Figure 5, we provide an overview of the architecture of the CGN as provided in the paper. It illustrates how the CGN is split into four mechanism: the shape mechanism, the texture mechanism, the background mechanism, and the composer. Each mecha\u2010 nism takes a noise vector u and a label y as input. To generate a counterfactual image, we sample u and then sample a separate y for each mechanism Sauer and Geiger [1].\nB Counterfactual images and explainability in artificial intelligence\nOne of the primary contributions of the work by Sauer and Geiger [1] is the proposed method to create high\u2010quality \u2018counterfactual\u2019 images, which can be used to make a classifier more robust to spurious signals. As the concept of counterfactual explanations is closely related to the idea of explainable artificial intelligence (XAI) but is never ex\u2010 plicitly mentioned in the paper, we first want to place the article in a broader context to achieve a deeper understanding of how the considered work relates to other develop\u2010 ments within this field of research [25].\nBased on the reviewbyVerma, Dickerson, andHines [26], approaches for explainabil\u2010 ity in machine learning can be roughly divided into one of two categories: (i) methods that use inherently interpretable and transparent models, and (ii) methods that gener\u2010 ate post\u2010hoc explanations for opaque models.\nThe idea of counterfactual explanations belongs to the example\u2010based approaches within the category of post\u2010hoc explanations, that seek to offer explanations by either providing datapoints that receive the same prediction label as the observed datapoint, or by providing datapoints whose prediction label is different from the observed datapoint.\nConsider the example where a classifier is trained to distinguish images from polar bears and American black bears. Given an image that has been classified by the model as a black bear, we could attempt to provide a post\u2010hoc explanation for the model\u2019s pre\u2010 diction using a visual counterfactual explanation (i.e., a modified version of the input image that would be classified as a polar bear instead). These explanations can, for example, be generated using techniques such as StylEx [27]. A reasonable visual coun\u2010 terfactual explanation could consist of the input image, modified such that the fur of the black bear is now colored white. However, as most images of polar bears have a snow\u2010background, andmost images of American black bears likely do not, it is possible that the suggested visual counterfactual explanation still contains a black bear, but now on a snowy background.\nReScience C 8.2 (#5) \u2013 Bagad et al. 2022 12\nIn this case, one could argue that the background\u2010explanation that is captured by the model is a spurious signal. That is, the classifier \u2018falsely\u2019 makes predictions on the back\u2010 ground, even though the background, in reality, does not affect the actual object itself. Although this spurious signal might seem innocent within the context of this example, other spurious signals can play a role in a variety of high stake deep learning applica\u2010 tions, such as AI in medical\u2010imaging [28] and networks trained for military purposes [29]. While counterfactual explanations are thus capable of revealing such spurious sig\u2010 nals, the proposed method using counterfactual images by Sauer and Geiger provides an approach to mitigate this effect.\nC Improved CGN Training for MNIST\nWhile training the CGN on theMNIST, we encountered an issue that was not mentioned in the original paper. During the training process, we observed that while some digits were captured almost perfectly by the model, other digit masks seemed to collapse to a state where there was a black circular shape in the center of the image with a surround\u2010 ing white border (see Figure 6). When using the generated counterfactual datasets from these imperfect models to train a classifier, we then observed that the number of \u2018cor\u2010 rect\u2019 (i.e., non\u2010collapsed) images correlated strongly with the classifier performance.\nAny attempt to remedy this issueusing adjustedhyperparameter configurations proved to be ineffective, because thehyperparameter names in theprovideddefault configuration\u2010 files did not directly correspond to the descriptions given in the original paper. This observation inspired a solution where we add an extra loss term to the training objec\u2010 tive, which penalizes mask\u2010pixels at the borders of the image. Specifically, if we define the edge region E as the set of pixels that are within s pixels from the edge, the edge loss function can be defined as the sum of all pixel values mi within the specified edge region:\nLedge(m) = Ep(u,y)\n[ 1\nN N\u2211 i=1 mi \u00b7 [i \u2208 E ]\n] , (3)\nwhere N denotes the number of pixels in mask m, and [\u00b7] denotes the Iverson bracket. As the original MNIST images in the training and test datasets often contain almost no pixels at the borders, this loss function returns values close to 0 for all ground truth MNIST images. During our experiments, we used a border size of 3 pixels, as this con\u2010 figuration seems to perform well to mitigate the mask\u2010collapse issue, while still giving loss values close to 0 for the original MNIST images. By using this extra loss function, the training process becamemuchmore consistent and lead to an average classifier test accuracy of 89.8% for the Colored MNIST dataset, which is close to what was reported in the original paper.\nReScience C 8.2 (#5) \u2013 Bagad et al. 2022 13\nIn Figure 10, we show that our modified training formulation improves the quality of generated images. In particular, we notice that incorporating Ledge in the mask loss, on average, noticeably decreases the number of non\u2010broken images.\nD Computational Cost Taxonomy\nReScience C 8.2 (#5) \u2013 Bagad et al. 2022 14\nF GAN-based Baseline for MNISTs\nWe follow the ConvNet\u2010based architecture for the generator inspired by PyTorchDCGAN tutorial and retain the linear discriminator as is used by Sauer and Geiger [1]. We only use binary cross entropy loss for adversarial training of both G and D. All necessary hyperparameters are same as for the CGN training. These alongwith pretrainedweights can be found in our code repository.\nG Reproduced MNIST Ablation Study\nFigure 10 shows our reproduced results for the MNIST ablation study. Our results show that usingmore counterfactual datapoints generally improves the test accuracy, although this was not the case for the Colored MNIST dataset, where the test accuracy decreased when using 106 counterfactual datapoints instead of 105. However, the difference in performance is only minor. The differences in CF ratios do not seem to have a signif\u2010 icant effect on the test accuracies. These results seem to support the claim from the original paper that using more counterfactual images always increases the test domain results for MNIST datasets, although there only seems to be a significant performance increase when using 105 datapoints instead of 104. Using evenmore datapoints does not seem to provide a significant increase in performance.\nH GradCAM samples on ImageNet-mini\nA classifier trained jointly on original and CF data is expected to have encoded invari\u2010 ances for certain attributes and distinctiveness for others. Recall that the proposed clas\u2010 sifier architecture for ImageNet is an ensemble with three heads for shape, texture and\nReScience C 8.2 (#5) \u2013 Bagad et al. 2022 15\nbackground. We pose the question: What spatial aspects of an image does each head focus on and what prediction does it lead to? We answer this qualitatively by analyzing GradCAM heatmaps for outputs of each of the heads as well as the averaged ensemble output. In general, the individual heads tend to focus on meaningful aspects, as shown in Figure 11, background head focuses on background. Further, for original images, we observe that a correct prediction often relies on shape (e.g., puck in Figure 11a) or texture (e.g., goldfinch). In some cases, it correctly relies on background (e.g., castle). For counterfactuals, surprisingly, in most cases we found that the label predicted from shape, although correct, is dominated by incorrect label from background and texture. This may be a symptom of either insufficient counterfactual training data or the use of IN\u2010mini instead of IN\u20101k. We further note that texture often drives the label decision for counterfactuals.\nReScience C 8.2 (#5) \u2013 Bagad et al. 2022 16\nI Some failure modes in CGN-generated samples\nSince generation of high\u2010quality counterfactuals is one of the main claims of the paper, we perform a deeper qualitative analysis to observe if there exist typical failure modes. Based on anecdotal evidence, we note the following observations.\nTexture\u2010background entanglement for small objects For cases with small objects on a uniformbackground, such as the bird kite in sky, shown in Figure 12(a), or skiing on snow, shown in Figure 12(b), we see consistent entanglement between texture and background.\nObjects with complex texture We observe that objects with complicated texture, such as crossword puzzle, shown in Figure 12(c), result in poorly recovered texture by the CGN.\nComplex scenes As one would expect, the CGN approach does not generalize to com\u2010 plex scenes since it assumes a simplistic causal structure. We show an example of this in Figure 12(d).\nReScience C 8.2 (#5) \u2013 Bagad et al. 2022 17"}], "title": "[Re] Reproducibility Study of \u201cCounterfactual Generative Networks\u201d", "year": 2022}