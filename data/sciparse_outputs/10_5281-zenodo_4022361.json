{"abstractText": "Neuronal activity in mammalian central nervous systems is often oscillatory. Intensity, frequency, and other characteristics of the oscillations are closely associated with behavior and arousal state. As this association suggests a functional role of oscillations, they have been studied extensively. One of the most conspicuous cases of behaviordependent oscillatory neuronal activity occurs in hippocampus, a brain region pivotal for navigation and memory formation: prominent rhythms exist in the theta (4-12 Hz) and gamma (30-90 Hz) bands. Interestingly, the oscillations are coupled in a nonlinear fashion: the magnitude of gamma oscillations varies cyclically at theta frequency [2] (Fig. 1). Such phase-amplitude coupling (PAC), also termed \u02bcnesting\u02bc of oscillations, has been postulated to be important for memory formation and retrieval [3, 4]. A large body of research has shown that both theta and gamma rhythms depend on the neuromodulator acetylcholine, but before our study [1] it had not been clear whether and how their PAC would change with cholinergic signalling. As a first step towards shedding light on this, we performed experiments on mice with extracellular multielectrode arrays implanted into area CA1 of hippocampus. Each extracellular electrode picked up local field potentials (LFPs) \u2013 tiny, local, time-dependent changes in the electrical potential relative to a distant reference point (Fig. 1B). Themice were behaviorally scored and the LFPs recorded for periods of ca. 90 min, including administration of a blocker of muscarinic cholinergic receptors 30 min into the experiments.", "authors": [{"affiliations": [], "name": "Harald Hentschke"}, {"affiliations": [], "name": "Olivia Guest"}, {"affiliations": [], "name": "Tom Donoghue"}], "id": "SP:f7ebec7ceb4fd14a6d2827615481d6241b6c9d59", "references": [{"authors": ["H. Hentschke", "M. Perkins", "R. Pearce", "M. Banks"], "title": "Muscarinic blockade weakens interaction of gamma with theta rhythms in mouse hippocampus.", "venue": "European Journal of Neuroscience", "year": 2007}, {"authors": ["I. Soltesz", "M. Desch\u00eanes"], "title": "Low- and high-frequency membrane potential oscillations during theta activity in CA1 and CA3 pyramidal neurons of the rat hippocampus under ketamine-xylazine anesthesia.", "venue": "Journal of Neurophysiology", "year": 1993}, {"authors": ["J. Lisman andM. Idiart"], "title": "Storage of 7+- 2 short termmemories in oscillatory subcycles.", "venue": "Science", "year": 1995}, {"authors": ["A. Hyafil", "A.-L. Giraud", "L. Fontolan", "B. Gutkin"], "title": "Neural Cross-Frequency Coupling: Connecting Architectures, Mechanisms, and Functions.", "venue": "en. In: Trends in Neurosciences", "year": 2015}, {"authors": ["M. Perouansky", "H. Hentschke", "M. Perkins", "R. Pearce"], "title": "Amnesic concentrations of the nonimmobilizer 1,2dichlorohexafluorocyclobutane (F6, 2N) and isoflurane alter hippocampal theta oscillations in vivo.", "venue": "Anesthesiology", "year": 2007}, {"authors": ["H. Hentschke", "C. Benkwitz", "M. Banks", "M. Perkins", "G. Homanics", "R. Pearce"], "title": "Altered GABA A,slow inhibition and network oscillations in mice lacking the GABA A receptor beta3 subunit.", "venue": "Journal of Neurophysiology", "year": 2009}, {"authors": ["S. Balakrishnan", "R. Pearce"], "title": "Midazolam and atropine alter theta oscillations in the hippocampal CA1 region by modulating both the somatic and distal dendritic dipoles.", "venue": "Hippocampus DOI: 10.1002/hipo.22307", "year": 2014}, {"authors": ["N. Barnes"], "title": "Publish your computer code: it is good enough.", "venue": "Nature", "year": 2010}, {"authors": ["G. Wilson", "J. Bryan", "K. Cranston", "J. Kitzes", "L. Nederbragt", "T.K. Teal"], "title": "Good enough practices in scientific computing.", "venue": "en. In: PLOS Computational Biology", "year": 2017}], "sections": [{"text": "R E S C I E N C E C Replication / Neuroscience\n[Re] Hippocampal Phase-Amplitude Coupling unearthed again\nHarald Hentschke1, ID 1Freelancing Data Scientist, Berlin, Germany\nEdited by Olivia Guest ID\nReviewed by Tom Donoghue ID\nReceived 21 April 2020\nPublished 10 September 2020\nDOI 10.5281/zenodo.4022361\n1 Abstract\nThis article describes an attempt to reproduce key findings of a study of murine hippocampal oscillations published in 2007 [1]. Being a submission to the \u02bcTen Years Reproducibility Challenge ,\u0313 it focuses on the process of reviving the Matlab code underlying the analyses, particularly the neuronal signal processing toolbox created for the study and its follow-ups.\n2 Introduction\n2.1 Neuroscientific background Neuronal activity in mammalian central nervous systems is often oscillatory. Intensity, frequency, and other characteristics of the oscillations are closely associated with behavior and arousal state. As this association suggests a functional role of oscillations, they have been studied extensively. One of the most conspicuous cases of behaviordependent oscillatory neuronal activity occurs in hippocampus, a brain region pivotal for navigation and memory formation: prominent rhythms exist in the theta (4-12 Hz) and gamma (30-90 Hz) bands. Interestingly, the oscillations are coupled in a nonlinear fashion: the magnitude of gamma oscillations varies cyclically at theta frequency [2] (Fig. 1). Such phase-amplitude coupling (PAC), also termed \u02bcnesting\u02bc of oscillations, has been postulated to be important for memory formation and retrieval [3, 4]. A large body of research has shown that both theta and gamma rhythms depend on the neuromodulator acetylcholine, but before our study [1] it had not been clear whether and how their PAC would change with cholinergic signalling. As a first step towards shedding light on this, we performed experiments on mice with extracellular multielectrode arrays implanted into area CA1 of hippocampus. Each extracellular electrode picked up local field potentials (LFPs) \u2013 tiny, local, time-dependent changes in the electrical potential relative to a distant reference point (Fig. 1B). Themice were behaviorally scored and the LFPs recorded for periods of ca. 90 min, including administration of a blocker of muscarinic cholinergic receptors 30 min into the experiments.\nCopyright \u00a9 2020 H. Hentschke, released under a Creative Commons Attribution 4.0 International license. Correspondence should be addressed to Harald Hentschke (hhntschk@gmail.com) The authors have declared that no competing interests exist. Code is available at https://github.com/hhentschke/ReScience_10yrReproChallenge/tree/v.1.0.0 \u2013 DOI 10.5281/zenodo.4021389.. Open peer review is available at https://github.com/ReScience/submissions/issues/30.\nReScience C 6.3 (#3) \u2013 Hentschke 2020 1\nysis strategy had been devised by Matthew I. Banks (also UW Madison), who had also implemented a first version in a spreadsheet-cum-script language (Origin). Due to limitations inherent in the language it was decided to implement and extend the analysis in a more versatile language. Matlab was the natural way to go, as it was (and still is) the standard programming environment in many electrophysiology laboratories, and I had used it extensively before. In our original study [1], Matlab versions 6.5 - 7.4 (R13 - R2007a) were used. For later projects, the code has been continuously adapted toMatlab versions up to 7.9 (R2009b), after which it lay dormant. Various Matlab toolboxes were required: most importantly, the Signal Processing Toolbox; depending on the exact postprocessing routines, the Curve Fitting and Statistics Toolboxes were also required. A pivotal issue for the project, and in general for any data-focused undertaking, was the import of raw data. The time series data existed in a proprietary format, the Axon Binary Format (ABF), in which each data point is stored as a 16-bit integer. A function for efficiently importing data of this format intoMatlab did not exist. Although the software package provided by the vendor (former Axon Instruments, now Molecular Devices) allowed an export of the data into text format, this was not considered practical (tedium involved in manual conversion, inflation of data size). Hence, I wrote a routine for importing raw data in this format into Matlab and submitted it to the MathWorks Central File Exchange [5]. Initially considered a spin-off of minor importance, this routine and its upgrade [6] found a widespread distribution in the neuroscience community, and also sparked implementations in other languages [7, 8]. An ironic twist is that the more fail-safe way of implementing a data importing routine for the windows operating system, namely code employingMolecular Devices\u02bc dynamic link library (DLL), would have made sense at the project s\u0313 start in 2003, but has since the introduction of 64-bit versions of Matlab been rendered impractical by the limitation of this DLL to 32 bit (confirmed via an exchange with Molecular Devices Customer Support in December 2019). Readers further interested in this topic may want to consult the \u201dUnofficial Guide to the ABF File Format\u201d by Scott Harden [9]. As all code was written in pure Matlab, and data import had been solved within Matlab, there were no extraneous dependency issues. Although reproducibility of the study results was not an imminent concern to me back then, reusability of the code had definitely been an important design aspect early on. Series of experiments with a similar setup were in the pipeline, and given the limited term of my stay it was clear that the code would also be used by my colleagues [10, 11, 12]. Therefore, Iwrote amanualwith illustrations, with the intent of providing sufficient information for lab members familiar with the scientific background to find their way through the code. I also commented the code prolifically, especially the scripts calling the toolbox functions. The source code has never been properly archived or published, except for the data importing routine (until the reproducibility challenge camealong) for two reasons: first, I considered it too specialized and tailor-made to be of much use for other groups. Second, making the toolbox open access would have required making it more user-friendly and extending the documentation substantially. Absent an academic reward system for such efforts not only for myself but also the hosting lab, I considered the necessary investment of time inappropriate. I developed the code without any kind of version control worth the term, and left it \u02bcas is\u02bc on my then machine in the laboratory. The toolbox s\u0313 core code was in one dedicated directory, but auxiliary code was scattered throughout my Matlab code base, which of course also contained a great deal of unrelated code. I never saw the necessity of properly packaging the code: it would not be published, and almost all auxiliary functions employed by the toolbox were quite general and therefore also part of other (and future) programming projects. So, my justification for this practice was to avoid redundancy and the usual problems associated with maintaining multiple copies of code. Towards the end of my stay in the laboratory, I simply mirrored my Matlab directory on other users\u02bc machines to get the toolbox to run there. After I left, I developed it further and adapted it to the needs of the subsequent projects, which went on for quite a while.\nReScience C 6.3 (#3) \u2013 Hentschke 2020 3\nHardware was an issue, albeit an uncritical one. The analyses were run on 32-bit Windows PCs with 4 GB of RAM and not-too-bad Intel Pentium processors available at the time (more precise specs are not known anymore). The electrophysiological signals were sampled at ca. 1000 Hz from up to 16 channels and typically lasted for 30 minutes; the resulting individual raw data files occupied less than 100 Mb on disk. Their size in RAM, after conversion from 16-bit integers to the default 64-bit floating type in Matlab, posed no critical challenge. However, the code also implemented the creation of surrogate time series. Dealing with these, as well as multiple filtered copies of the data in RAM in parallel required some thought-through strategies and juggling with computer resources, and was taxing for the CPU. Yet, even full-blown analyses of the whole data set were accomplished overnight.\n3 Retrieval of the software\nRetrieval of the source code was easy; in fact, most of it did not have to be retrieved. As code files are ridiculously small even by the standards of themid-2000s, I generally leave them on the hard disks of the computers in my hands. The hard part was figuring out the organization of the code files. There were the toolbox folder, still in myMatlab code base, and the folder containing code files for generating the publication figures. Localizing both required just a fewminutes. I was also aware that several auxiliary functions, including the data-importing function, were outside the toolbox folder but inside my Matlab code base. However, I forgot that back then I placed scripts defining analysis parameters for each individual data file in yet a different location: namely, in the root folder for the raw data, as well as the subfolders therein. Only after I tried to run the first analysis step did I realize this.\n4 Execution\nReproduction was performed on a PC with Windows 7 and Matlab R2020a with the Signal Processing, Statistics and Machine Learning, and Parallel Computing Toolboxes installed. The PC had 32 GB of RAM, ample space on both hard disk and solid state drive, an Intel Core i7-4770K CPU with four physical cores, and an nvidida GeForce GTX1060 graphics card. Reproducingdata analysis including selectedfigure panels of the original paper required several processing steps. I started with the first step \u2013 processing the raw data \u2013 by trying to locate the scripts which defined the analysis parameters for each individual file to be processed. After finding only one, I read the manual, and realized that both the majority of the data and data-specific scripts were missing, contrary to my first cursory impression when entering the challenge. Thanks to the efforts of a former colleague (acknowledgements) I was able to retrieve six complete sets of data and data-specific scripts from three of five experimental animals used in the original study. With these available, I finally remembered how the toolbox was supposed to work. Before actually trying to run it, I assembled all code files I could identify at this stage as belonging to the project into a directory structure such that submitting the code to a repository would be possible. Next, I ran a dependency analysis on themain function of the toolbox and could identify a number of additional code files that weremissing. After copying these to the project directory, I excludedmy general Matlab code base from the path. Then, I tried in earnest to start number crunching. It took me about three hours to get the analyses to run on three experiments (two data files each). The adjustments required were, in order of severity:\n1. Retrieval of functions that were still missing from my general Matlab code base (about two dozen of files).\nReScience C 6.3 (#3) \u2013 Hentschke 2020 4\n2. \u02bcDowngrading\u02bc auxiliary functions which had been in my general code base and together with it had evolved over time to the point of being incompatible with the toolbox.\n3. Updating code to be compatible with the recent version of Matlab.\nI was shocked to learn how many of the required functions I had to retrieve from my \u02bc_legacy\u02bc folder. Had I not exerted caution in getting rid of code and instead deleted them, I would have spent hours on end to recapitulate what they were supposed to do, and possibly given up. A pleasant confirmation of expectations was the little effort I had to spend adapting the code to themost recent Matlab version. Essentially, all I had to do was write a wrapper for function diag, which did not accept cell arrays as inputs anymore. Surprisingly, graphics, including one very simple GUI, worked without hiccups; thanks to a major revision of graphics in R2014b, it also produced more aesthetically pleasing figures. Moreover, with little effort, it was possible to run number crunching in an embarassingly parallel way using parfeval. This, as well as moving the data to a solid state drive, cut down processing time from a baseline of 152 min to a very agreeable 66 min (three experimental subjects, two recordings each, 100 instances of surrogate data generation each). Although exact numbers are impossible to come by, my estimate is that the combination of recent software and more modern (certainly not recent) hardware sped up the computations by a factor of at least eight. Yet more speedup would have required fundamental re-coding of the toolbox. Thenext taskwas aggregating the data from thedifferent experiments, and (re-)producing selected figure panels of the original study. Numerous scripts for various visualizations of the results existed; as these were not strictly documented and kept up to date, a trialand-error period of identifying key pieces of code responsible for producing the plots\nReScience C 6.3 (#3) \u2013 Hentschke 2020 5\nensued. Apart from this, and on top of adjustments similar to those required for the toolbox, I expected two additional problems here. First, the original code made use of the Curve Fitting Toolbox, which was not at my disposition now. Second, code producing \u02bcChristmas Tree\u02bc plots \u2013 adorned horizontal bar plots often featuring a coniferous shape (Fig. 3) \u2013 featured low-level graphics commands. Due to substantial changes of Matlab graphics in the interim, I knew that this code would have to be modified. Both challenges proved surmountable. For the simple fits of 1D data, I used function fitnlm, and refurbished the affected code accordingly. The function producing the horizontal bar plots could also be updated without major pains. Following these fixes, key findings of the original study could be reproduced, albeit only with said partial data (Fig. 3). All other figure panels featuring above-mentioned horizontal bar plots could be reproduced with the same data base by changing the name of a target variable in a script and adapting the curve-fitting code (data not shown).\nReScience C 6.3 (#3) \u2013 Hentschke 2020 6\n5 Conclusion and personal evaluation\nDuring my last years in academia, I focused increasingly on professional programming, and eventually left academia in 2019 to work as a Data Scientist. Having along the way picked up at least a modicum of formal software education, my opinion on my original source code is mixed. On the positive side, I commented the code heavily, particularly the scripts which other users of the code were to use. Later on, I also spent a considerable amount of time on the documentation. A crude form of loggingwas implemented. Moreover, I had a knack for making code run fast, and consider some aspects of the implementation valid even today. However, a few of my past code design choices strike me as odd now, to say the least. First and foremost, this is above-mentioned liberal dispersal of code files across my entire code base, and the placement of scripts defining parameters in data directories. The cascades of scripts required for running analyses with the toolbox required immersion into the code, and could have been replaced by a more user-friendly graphical user interface. For post-processing, there had been a proliferation of poorly documented and partly redundant helper or plotting functions. Over time, 13 (thirteen) variants of a \u02bccombine_X -\u0313function had accumulated, the job of which was to assemble the results of the computations by the toolbox. Moreover, I would have compartmentalized the toolbox code much more. Finally, for a toolbox of this scope, more time should have been devoted to the documentation. All things considered, I believe that resurrecting the code would have been very arduous for anyone else, even a Matlab expert, due to a dearth of documentation. Yet, I view these deficiencies in context. As a scientist-cum-programmer, one also has the science to do, not to mention actually making sense of the analysis results and writing papers about them. As mentioned before, spending a large amount of time on the documentation would have been hard to justify. Moreover, like many of my former colleagues, I was a self-taught programmer essentially without formal computer science education, so most of my design choices were not based on traded wisdom, but selfaccrued, patchy knowledge. My former self was certainly not alone in this position, as academia in general seems to be a fertile ground for \u02bcorganically home-grown\u02bc code that may or may not be \u02bcgood enough\u02bc to publish [13]. It is heartening to see that programming scientists are increasingly given a hand in producing reproducible and efficient code [14].\n6 Acknowledgements\nI owe profound thanks to Claudia Holt, Section of Experimental Anesthesiology, University Hospital of T\u00fcbingen, who unearthed the data for this study. Many thanks also go to Matthew I. Banks for discussions of the manuscript, and to Robert A. Pearce for consenting to the publication of the code."}], "title": "[Re] Hippocampal Phase-Amplitude Coupling unearthed again", "year": 2020}