{"abstractText": "The studied paper proposes a novel output layer for graph neural networks (the graph edit network \u2010 GEN). The objective of this reproduction is to assess the possibility of its re\u2010implementation in the Python programming language and the adherence of the provided code to the methodology, described in the source material. Additionally, we rigorously evaluate the functions used to create the synthetic data sets, on which the models are evaluated. Finally, we also pay attention to the claim that the proposed ar\u2010 chitecture scales well to larger graphs.", "authors": [{"affiliations": [], "name": "Vid Stropnik"}, {"affiliations": [], "name": "Maru\u0161a Ora\u017eem"}, {"affiliations": [], "name": "Koustuv Sinha"}, {"affiliations": [], "name": "Sharath Chandra Raparthy"}], "id": "SP:5b09a16a9ce195f446b1e0a265a2eb96da6a34e2", "references": [{"authors": ["A. Sanfeliu", "K.-S. Fu"], "title": "A distance measure between attributed relational graphs for pattern recognition.", "venue": "IEEE transactions on systems, man, and cybernetics", "year": 1983}, {"authors": ["S. Bougleux", "L. Brun", "V. Carletti", "P. Foggia", "B. Ga\u00fcz\u00e8re", "M. Vento"], "title": "Graph edit distance as a quadratic assignment problem.", "venue": "Pattern Recognition Letters", "year": 2017}, {"authors": ["K. Zhang", "D. Shasha"], "title": "Simple fast algorithms for the editing distance between trees and related problems.", "venue": "SIAM journal on computing", "year": 1989}, {"authors": ["E. Hajiramezanali", "A. Hasanzadeh", "N. Duffield", "K.R. Narayanan", "M. Zhou", "X. Qian"], "title": "Variational graph recurrent neural networks.", "year": 1908}, {"authors": ["T.N. Kipf", "M. Welling. \u201cVariational graph auto-encoders.\u201d In"], "title": "arXiv preprint arXiv: 1611", "venue": "07308", "year": 2016}, {"authors": ["M. Gardner"], "title": "The Fantastic Combinations of John Conway\u2019s New Solitaire Game\u2019Life.", "venue": "In: Sc. Am", "year": 1970}, {"authors": ["J. Leskovec", "J. Kleinberg", "C. Faloutsos"], "title": "Graphs over time: densification laws, shrinking diameters and possible explanations.", "venue": "Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining", "year": 2005}, {"authors": ["B. Paa\u00dfen", "B. Mokbel", "B. Hammer"], "title": "A Toolbox for Adaptive Sequence Dissimilarity Measures for Intelligent Tutoring Systems.", "venue": "Proceedings of the 8th International Conference on Educational DataMining (EDM 2015) (Madrid, Spain). Ed. by S. et. al. International Educational Datamining Society,", "year": 2015}, {"authors": ["A.P. Erd\u0151s"], "title": "R\u00e9nyi. \u201cOn random graphs.", "venue": "Publicationes Mathematicae (Debre)", "year": 1959}, {"authors": ["M.E. Newman"], "title": "The structure and function of complex networks.", "venue": "SIAM review", "year": 2003}, {"authors": ["B. Paa\u00dfen", "C. G\u00f6pfert", "B. Hammer"], "title": "Time series prediction for graphs in kernel and dissimilarity spaces.", "venue": "Neural Processing Letters", "year": 2018}], "sections": [{"text": "R E S C I E N C E C"}, {"heading": "Replication / ML Reproducibility Challenge 2021", "text": "[Re] Graph Edit Networks\nVid Stropnik1 and Maru\u0161a Ora\u017eem1 1Faculty of Computer and Information Science, University of Ljubljana, Ljubljana, Slovenia\nEdited by Koustuv Sinha,\nSharath Chandra Raparthy\nReceived 04 February 2022\nPublished 23 May 2022\nDOI 10.5281/zenodo.6574701"}, {"heading": "Reproducibility Summary", "text": ""}, {"heading": "Scope of Reproducibility", "text": "The studied paper proposes a novel output layer for graph neural networks (the graph edit network \u2010 GEN). The objective of this reproduction is to assess the possibility of its re\u2010implementation in the Python programming language and the adherence of the provided code to the methodology, described in the source material. Additionally, we rigorously evaluate the functions used to create the synthetic data sets, on which the models are evaluated. Finally, we also pay attention to the claim that the proposed ar\u2010 chitecture scales well to larger graphs."}, {"heading": "Methodology", "text": "Formost of ourwork, wewere able to use the code, provided in the supplementary repos\u2010 itory. We also offer our own variations of the experimental setup, with an alternative method of risk estimation. A portion of the report is also devoted to a more exhaustive description of the included data generating functions, otherwise not offered original paper."}, {"heading": "Results", "text": "We were able to reproduce GEN\u2019s out\u2010performance of a chosen baseline and its perfect scores on synthetic data sets. We also confirm the author\u2019s claims of the sub\u2010quadratic scaling of GEN\u2019s forward passes and deduce that they reported the scaling of back\u2010passes too favourably. We conclude our work with scepticism of the chosen experiments\u2019 suit\u2010 ability to evaluate the model\u2019s performance and discuss our findings."}, {"heading": "What was easy", "text": "All the provided code has extensive documentationwhichmade the paper\u2019s experiments easy to reproduce. The entire code base is readable, modular and adheres to established practices on code readability. The authors also provide some unit tests for all of their models and have pre\u2010implemented several useful diagnostic measures.\nCopyright \u00a9 2022 V. Stropnik and M. Ora\u017eem, released under a Creative Commons Attribution 4.0 International license. Correspondence should be addressed to Vid Stropnik (vs2658@student.uni-lj.si) The authors have declared that no competing interests exist. Code is available at https://github.com/MarusaOrazem/reproducibility_challenge \u2013 DOI 10.5281/zenodo.6505384. \u2013 SWH swh:1:dir:a48bde44f1ef0b6f060687ea7b8b164a97b0931e. Open peer review is available at https://openreview.net/forum?id=H5lOnzXhAY.\nReScience C 8.2 (#38) \u2013 Stropnik and Ora\u017eem 2022 1"}, {"heading": "What was difficult", "text": "Running some of the provided code on a consumer\u2010grade laptop (as reported in the original work) was prohibitively expensive. The lack of transparency about the code base\u2019s runtimes made our work here much more difficult. Another time\u2010consuming task was the debugging of a section of author\u2010provided code. We\u2019ve helped the authors identify the problem, which has now been resolved.\nCommunication with original authors The authors were prompt with their responses, welcomed our efforts in reproducing their work and made themselves available for any questions. Upon our request, they happily provided additional implementations, not originally available in their reposi\u2010 tory, and offered their counter\u2010arguments to some methodological concerns that we expressed to them.\n1 Introduction\nThe studied paper proposes a novel output layer for graph neural networks (GNNs), the graph edit network (GEN). This layer yields a sequence of graph edits \u03b4 . Particularly, the graph edit schema considered in the work is the one initially proposed in [1], describing notions of node insertions (insx), deletions (delx) and replacements (repli,x), as well as edge insertions (einsi,j) and deletions (edeli,j). Note that the subscripts x in node edits refer to the attributes of the edited node (in repli,x, the additional subscript i denotes the to\u2010be replaced attributes), and i, j in the edge edits refer to the indices of nodes between which the edited edge can be found. These finite sequences of edits, also referred to as edit scripts \u03b4\u0304t = [\u03b41t , \u03b42t , . . . , \u03b4nt ], are general enough to describe any graph\u2010to\u2010graph transformation and are not only very interpretable for humans, but also computationally efficient. Both of these properties establish GENs as a useful tool for work in the domain of graph time series prediction. More particularly, GENs perform time series prediction under the Markovian assump\u2010 tion, which states that knowing the graphGt and themapping function\u03c8t, derived from the edit script \u03b4\u0304t, is sufficient for predicting the graph found in the next step of the time series as\nGt+1 = \u03c8t(Gt); \u03c8t := \u03b4 1 t \u25e6 \u03b42t \u25e6 \u00b7 \u00b7 \u00b7 \u25e6 \u03b4nt ; \u2200\u03b4it \u2208 \u03b4\u0304t,\nwhere the subscript t denotes the time\u2010dependant index in the time series.\n2 Scope of Reproducibility\nThe authors of the reproduced work formally prove theorems, stating that finding a mapping \u03c8 between pairs of time\u2010adjacent graphs is sufficient for constructing train\u2010 ing data for GENs. They propose that their GNN architecture be trained to reproduce specific teaching signals for this function \u03c8, which may be derived from any gathered training time series of graphs. This is done by first finding reference pair mappings \u03c8t : Gt \u2192 \u03b4\u0304t(Gt) \u2261 Gt \u2192 Gt+1 from the training series via graph edit distance approxi\u2010 mators1, and then computing teaching signals via an algorithm, provided in the paper\u2019s supplementary material. The authors empirically underpin this corollary by showing that the GEN performs well in a series of graph time\u2010series prediction tests. They define several data generating\n1Approximation is used due to the NP\u2010hard nature of the graph edit distance in general, as shown in [2]. In practice, exploiting domain knowledge may also lead to sensible mappings \u03c8t. As an example of domain knowledge exploitation, the authors cite [3].\nReScience C 8.2 (#38) \u2013 Stropnik and Ora\u017eem 2022 2\nprocesses (DGPs), from which the GEN attempts to learn the user\u2010defined functions \u03c8, which remain hidden to the algorithm. The tests can be roughly split into three classes, which have corresponding experiments in section 4. The explicit conclusions of the experimental subsection of the original paper are that the GENoutperforms the selected baselines in all of the observed tasks. In our work, we compare the GEN to one of the baselines \u2010 themodified version of Varia\u2010 tional graph autoencoders (VGAE). As in the original work, we observe amodification of the method, suggested by [4], where the method attempts to directly infer the the graph in the next step of the time series. In the other experiments, we interpret claims about GEN\u2019s performance on different datasets directly. Since the graphs, generated by the author\u2010defined DGPs, are of a completely synthetic nature and very limited in scale, the authors also attempt to establish that GENs scale well to real\u2010world networks. In their experimentation, they only pay attention to the scaling efficiency of the architecture and not to the quality of the predictions themselves. From the described conclusions, we identify the following claims, made in the experi\u2010 mental section of the paper, that we will be exploring:\nClaim (i): GENs, trained with either hinge or crossentropy loss, outperform the mod\u2010 ified VGAE on all three dynamical graph system DGPs.\nClaim (ii): GENs, trainedwith user defined losses, achieve a perfect accuracy score on both dynamical tree DGPs.\nClaim (iii): The runtime of forward passes of a GEN, trained on the social network dataset (with orwithout edgefiltering), scales sub\u2010quadratically as thenum\u2010 ber of nodes in a graph increases.\nClaim (iv): The runtime of backward passes of a GEN, trained on the social network dataset with edge filtering, scales approximately linearly, as the number of nodes in a graph increases.\nAn additional contribution of our work is the thorough study and description of the synthetic datasets, used to evaluate the GENs performance. We pay special attention to this part of the paper, as they were not exhaustively described in the original work. This examination helps us shed light on the performance of the GEN in the discussion section and evaluate the suitability of the used exprimental approachs. It also provides a more in\u2010depth descriptive resource to other researchers in the field, that might find these DGPs useful for their own work.\n3 Methodology\nThroughout our reproduction attempt, we have made great use of the code, provided in a supplementary repository to the original paper [5]. To replicate the author\u2019s ex\u2010 perimental environment, we try to make the same assumptions and hyperparameter choices than those provided either in the original paper, or the documentation of the supplementary repository. A fork of this repository with our changes and additions is available at [6].\n3.1 Model descriptions In the first class of experiments, we train 2 GEN models, one using the adapted cross\u2010 entropy loss (GEN\u2010XE) and the other using the adapted hinge loss (GEN), described in the paper. Both models are parametrized by their input, output and hidden dimension\u2010 alities, as well as their used nonlinearities. Given the short edit scripts expected in these scenarios, no edge filtering is used in these models.\nReScience C 8.2 (#38) \u2013 Stropnik and Ora\u017eem 2022 3\nWe also train the Variational Graph autoencoder model, as described in [7]. Apart from its input, output and hidden dimensionalities, it is also parametrized by the size of its encoding space, the regularization strength \u03b2 and a scaling factor for the noise on the last layer node features \u03c3. It also takes a hyperparametric definition of the used nonlin\u2010 earity. The GENmodels used in the experiments, governed by the Peano addition and Boolean formulae DGPs, are similar to those in the Dynamical graph systems class. The models here, however, use an author\u2010defined loss function, with respect to a custom teaching protocol, with only a single predictive step between graphs. Similarly to before, no edge filtering is used. In the experiments on the social network dataset, we train two variations of the GEN model. The first sets up two binary classifiers for each node to decide whether to con\u2010 sider changing outgoing/incoming edges or not. This approach is denoted in the results as flexible edge filtering. The secondmodel limits the number of permitted edge editswith a fixed upper bound \u2010 this is denoted as fixed edge filtering. The models use a simplified single\u2010step teaching protocol, over which its loss function is defined. In the protocol, all edits, except for node insertion, are processed as expected. For insertion, however, the protocol lets a given node n insert a neighbor n\u2032 when there is at least one edge (n, n\u2019) found in Gt+1, where n\u2032 is not a node found in Gt. The authors acknowledge potential shortcomings of this method, but cite the desire of using a single\u2010step protocol as the reason for choosing it.\n3.2 Data The paper contains three classes of experiments. The first two use user created DGPs, whereas the last one works with an external, well established social network. We de\u2010 scribe the dataset and DGPs in accordance to the class of experiments they correspond to, in the following subsections.\nDynamical graph systems \u2014 The Dynamical graph systems class of DGPs governs the train and test set generation in Experiments 4.1.1, 4.2.1 and 4.2.2. The class contains three discrete processes, provided in the supplementary repository in the form of scripts for the python programming language. During training/testing time, the time series gen\u2010 erator function is called, always returning a sequence of graphs based on DGP\u2010specific function arguments. The Edit Cycles DGP always yields one of three author specified cyclical time series, the outputs only differing in length and the starting time index. The edit script \u03b4\u0304t between two graphs is always of cardinality |\u03b4\u0304| \u2264 2 and all possible generated graphs consist of between two and four nodes. The cyclical series that the DGP yields are visualized in Figure 1. The Degree Rules data generating function generates a series of a determined length using the edit rules, described in Algorithm 1. The generator function accepts param\u2010 eters, corresponding to the series length and the number of nodes in the initial graph G0. G0\u2019s adjacency matrix is then randomly initialized. Consequentially, given a fixed time series length, the returned series is fully dependant on the random initialization of G0, as the rules are deterministic. In the examples in section 4.1, as per the author\u2019s\nReScience C 8.2 (#38) \u2013 Stropnik and Ora\u017eem 2022 4\nsource code, the randomization fromNumPy\u2019s random.rand is used, and all series\u2019 ini\u2010 tial graphs G0 start with exactly 8 nodes. We comment on this choice of randomization and provide our alternative in Section 4.2. The third and final DGP in this class is inspired by Conway\u2019s Game of Life [8]. Similarly to Degree Rules, it takes an input graph and applies a graph\u2010to\u2010graph mapping function. This one is specified by Algorithm 2 and is used to create a time\u2010series of a specified length. This function is also deterministic. In the resulting graphs, the nodes consid\u2010 ered alive in the Game of Life rule set are denoted with the feature value xn = 1. In contrast to degree rules, Game of Life graphs retain their number of nodes throughout evolution, as the graph will always denote theD \u00d7D grid with the neighborhood struc\u2010 turemodeling a nodes\u2019 8\u2010neighborhood, and only the nodes\u2019 alive/dead state will change. In each time series, a number of random Game of Life oscilators (randomly chosen be\u2010 tween 5 candidates) is chosen and made alive. Afterwards, each still dead cell will be made alive with a probability Pr(repl0,1(n)) = p. In the experiments in section 4.1, we report results using the parameters p = 0.1, D = 10, and always placing a single oscillator on the grid at initialization.\nTree dynamical systems \u2014 The Tree dynamical systems class of DGPs governs the train and test set generation in Experiments 4.1.2 and 4.2.3. It contains two distinct processes. They are distinguished from the DGP class in the previous section because they both generate strictly tree\u2010structured graphs, with no loops. Furthermore, they both include more complex node attribute encodings in the form of one\u2010hot vectors. The initial graph in a series, generated by the Boolean Formulae generator function, corresponds to a random Boolean formula. The time series following such a G0 rep\u2010 resents gradual simplifications of the formula, ending with a logic graph that can not be simplified any longer. An example evolution is given in Figure 2 for the formula (x \u2228 (y \u2227 \u00acy)) \u2228 x. The initial trees are generated via a stochastic regular tree grammar with a Pr(\u2227) = Pr(\u2228) = 0.3 and Pr(x) = Pr(\u00acx) = Pr(y) = Pr(\u00acy) = 0.1. The generator functions also offer a hyperparametric maximal number of applied rules p, where the authors use p = 3 in the original experiments.\nAlgorithm 1 The Gt \u2192 Gt+1 mapping for the Degree rules DGP. The function shareN returns true if the nodes share at least one neighbor. Input: Graph Gt, containing nodes n. 1: for each component C \u2208 Gt do 2: for each n \u2208 C do 3: d\u2190 degree(n) 4: if d \u2265 3 then del(n) 5: else if \u2203n\u2032 \u2208 C : shareN(n, n\u2032) then 6: for each n\u2032 \u2208 C : shareN(n, n\u2032) do 7: eins(n, n\u2032) 8: else ins1(n\u2217), eins(n, n\u2217)\nReScience C 8.2 (#38) \u2013 Stropnik and Ora\u017eem 2022 5\nAlgorithm 2 The Gt \u2192 Gt+1 mapping for the Game of Life DGP. The AliveDegree function returns the number of neighboring nodes n\u2032 with the attribute xn\u2032 = 1. Input: Graph Gt, containing nodes n. 1: for each n \u2208 Gt do 2: d\u2190 AliveDegree(n) 3: if (xn == 1) and (d < 2 or 4 \u2264 d) then 4: repl1,0(n) 5: else if (xn == 0) and (d == 3) then 6: repl0,1(n)\nThePeano additionDGPmodels Peano\u2019s recursive definition of addition. The operations are encoded similarly as in the Boolean formulae DGP, where both the operands and the arguments are represented as nodes in the dynamical tree graph.The initial graph gen\u2010 erator function receives an argument, specifying the maximal number n of additions. The authors use n = 3 in their experiments. Peano\u2019s addition rules simplify into four edit rules, the edit scripts of which are all upper bound as |\u03b4\u0304| \u2264 3. The node attributes appearing in the set are the 10 digit values, the summation operation +(m,n) = m+ n and the successor operation succ(m) = m+ 1. The author\u2010reported numbers of possible graphs, appearing in the time series, resultant from the five described DGPs, is tabulated in Table 1. Note, however, that not all of these graphs can be sampled as the initial graphs G0 in a given series and that the mappings \u03c8 : Gt \u2192 Gt+1 are deterministic in all DGPs. Hence, the actual number of unique pairs (Gt, Gt+1) is much lower.\nReal-world social network \u2014 For the final class of experiments, the arXiv HEP\u2010Th citation network data set, first described in [9], is used. It describes a graph, parsed from the e\u2010print arXiv and covers all mutual citations within a set of 27,700 papers. In it, a pa\u2010 per x, that cites paper y is connected with it with an outgoing edge. From this network, the authors parse sub\u2010graphs with a rolling window approach \u2010 considering only papers published within \u03c4 months of a given time point between January 1993 to April 2003. The number of nodes naturally grows with \u03c4, so the result is a collection of graphs with different orders of node\u2010count magnitude. In the presented experiment, these 1554 dis\u2010 covered sub\u2010graphs of node count NG \u2208 [100, 2786] are assumed as undirected.\n3.3 Hyperparameters For all the GNN\u2010based models in the first two classes of experiments, the authors use two hidden layers with 64 neurons each. As far as the architecture specification is con\u2010 cerned, the GENs use summation as the aggregation function and concatenation as the merge function. All networks are trained with the Adam optimizer using the learning rate of 10\u22123. The weight decay is set to 10\u22125 in the graph dynamical systems class of experiments and to 10\u22123 in the dynamical tree class of experiments. The results for the VGAE model are reported using \u03b2 = 10\u22123, \u03b3 = 10\u22123. The dimen\u2010 sionality of its embedding space is always equal to the size of the last hidden layer, so 64. As per the provided code by the authors, all models use the sigmoid nonlinearity\nReScience C 8.2 (#38) \u2013 Stropnik and Ora\u017eem 2022 6\nin the experiment on the Game of Life dataset, whereas we employ ReLU for all other experiments on synthetic data. In the experiments reported in section 4.1, both the training and the testing time series are sampled independently from their corresponding DGP, without special assertions of training and testing set discrepancy. All models train on 30,000 series, whereas the testing results are reported for 10 samples. We comment on the authors\u2019 methods of risk estimation and provide alternatives for these parameters in section 4.2. For the experiments on the social network dataset, a 3\u2010hidden layer architecture with the tanh nonlinearity, and PyTorch\u2019s default learning rate and weight decay are used.\n3.4 Experimental setup and code In our experiments, we use the metrics of precision and recall to evaluate the perfor\u2010 mance on insertion and deletion tasks. The experiments done on Tree dynamical systems use the notion of accuracy, which is an indicator function, defined at the value 1 when the nodes in the two input graphs match in all their features, and their adjacency matri\u2010 ces are identical. The reported accuracy is the average value of these indicator functions across all graph pairs in all time\u2010series in the test set. The experiments in section 4.1 were run in a loop across an entire class of DGPs, with 5 repetitions being ran for each consideredmodel. In the training phase, a time serieswas independently generated on each epoch using its corresponding generator function. As per the original paper, the considered stopping criterion was a rolling 10\u2010epoch average stop loss. Upon finishing training, the model was evaluated on time series, generated by the same generator functions as during training. We recognized this method of risk estimation as potentially problematic, given that there is no special care taken to ensure the discrepancy of the tranining and testing sets. It is for this reason that we change the used approach in some experiments, reported in section 4.2. In them, we sample our test set of graphs GTest0 ahead of time, and ensure that at each sampled training time series, the function \u03c8 : GTest0 \u2192 GTest1 , \u2200 GTest0 \u2208 T remains hidden from the algorithm.\n3.5 Computational requirements All experimentation was done on a desktop machine, running Windows 11, powered by an AMD Ryzen 7 2700X processor and 32 GB of RAM. The code was evaluated locally, in an environment, based on Python 3.8. The code base provided by the authors is depen\u2010 dant on the NumPy, PyTorch, PyTorch Geometric, Edist [10] and MatPlotLib packages. One repetition of running all three considered models on all three Graph dynamical systems (together) takes 90 minutes on average, with the VGAE taking the bulk of time to train, as the hinge\u2010loss GEN usually hits the stop loss threshold and stops training earlier. A single repetition of the experiment on the Peano addition DGP takes approxi\u2010 mately 15 minutes, whereas one over the Boolean formulae experiment takes 1 minute. On average, 60 minutes required to compute a full pass over all 12 months on the So\u2010 cial network experiment, for both edit schemas together. Working only with the largest graphs, i.e. \u03c4 = 12, takes 8 minutes on average.\n4 Experiments\nOur results confirm the authors\u2019 findings from claims (i) \u2010 (iii) when considering the results of the strict reproduction. We find that the scaling of the backward passes from claim (iv) is not linear, but remains sub\u2010quadratic. However, we show that these results are achieved by an architecture that is not able to optimize its loss function successfully. Our additional experiments in Subsection 4.2 show that the experimental results are stable for different choices of the initial graphG0. The results also stand formore robust\nReScience C 8.2 (#38) \u2013 Stropnik and Ora\u017eem 2022 7\nmethod of risk estimation. From these additional experiments, we derive important insights about the testing scenarios, presented in Section 5.\n4.1 Experiments reproducing original paper\nPrecision/Recall on Dynamical Graph System DGPs \u2014 In this task, we aimed to reproduce the results, stated in Claim (i) in Section 2. For almost all the metrics, we were able to repro\u2010 duce the values originally reported in the paper, with the difference \u03b4 := (our results\u2212 reported results) within a standard deviation of 0. The only major discrepancy we no\u2010 ticedwas an increase inmeandeletionprecision and insertion recall for theVGAEmodel in the edit cycle task, when comparing to the results, reported in the original paper. However, both GEN models still outperformed the VGAE, which supports Claim (i).\nAccuracy on Tree dynamical system DGPs \u2014 In this task, we address Claim (ii) from Section 2. In the original paper, the authors reported a 100% accuracy for both Tree dynamical system scenarios. While our results returned an accuracy of 0.98 \u00b1 0.02 in the Boolean Formulae task (and a perfect score for Peano addition), we can conclude that these re\u2010 sults are convincing enough to support Claim (ii).\nScaling of GENs on bigger graphs \u2014 This experiment addresss claims (iii) and (iv) from Sec\u2010 tion 2. In the original paper, the authors claim thatGENswere able to scale sub\u2010quadratically in their forward passes and approximately linearly in their backward passes, whenusing appropriate edge filtering approaches. Figure 3 shows scatter plots of the runtime\u2010graph scale dependency on a log\u2010log scale. Notice, that the runtime duration of the backward passes with constant edge filtering is very unstable, when compared to other scenar\u2010 ios. This is likely due to a higher difference in the fraction of considered edges, when compared to the flexible filtering approach. The scaling coefficients of the fitted linear models are further tabulated in Table 2. These results support Claim (iii) in that the for\u2010 ward passes scale sub\u2010quadratically. However, the lower of the two average coefficients for computing the gradient (the flexible approach) is still substantially larger than one. This indicates an exponential, albeit sub\u2010quadratic scaling of the backward passes. We conclude that these results do not support Claim (iv).\n4.2 Experiments beyond original paper\nEstablished methods of random graph generation \u2014 It is a common practice in the social net\u2010 work analysis (SNA) community to, when initializing random graphs, use specific meth\u2010\nReScience C 8.2 (#38) \u2013 Stropnik and Ora\u017eem 2022 8\nods of graph generation. Namely, if we want to make general statements about SNA methods, inferred fromexperiments on randomgraphs, these should be similar to those that tend to appear in nature. At the very least, it is considered a good practice to use established randomization methods, to more easily compare to results in other publica\u2010 tions. In this experiment, we repeat the methods from experiment 4.1.1 on the Degree rules DGP. However, instead of randomly initializing the adjecency matrix, we use two established methods of random graph generation: the Erd\u0151s\u2013R\u00e9nyi model [11] and the Configuration graph model [12]. In our experiment, graphs G0 were always initialized with 36 nodes in both models. We set the edge creation probability in the Erd\u0151s\u2010Renyi model p = 0.5 and the degree sequence of the Configuration model follows a random power\u2010law sequence with the exponent \u03b3 = 3. Allmetrics on thesenewly generated randomgraphs remained in the 0.05\u2010neighborhood of the originally reported results. We conclude that the performed experiments are ro\u2010 bust to different methods of random graph generation, and that the change in graph generation does not disprove Claim (i).\nAlternative methods of risk estimation - Dynamical graph systems \u2014 As established above, no special care is taken to ensure the discrepancy between the training and testing set of time\u2010series in the original results. In this experiment, we re\u2010run experiments 4.1.1 and 4.1.2 with our changed method of risk assessment, described in Section 3.4. We also raise the cardinality of testing set to 100, attempting to achieve stable results. We analyze our results by comparing several repetitions of the new experiment with the reported values. As a diagnostic tool, we employed the automatic plotting of \u03b4\u2010boxplots. An exam\u2010 ple of such a plot \u2010 describing the testing scenario where the discrepancy between the reported results and our experiments was the largest, is provided in 4. Notice that, while our change in the experimental setup did contribute to slightly worse metric scores, these changes are still minimal (\u03b4 \u2208 [\u22120.1, 0.05] for all observed testing scenarios). Con\u2010 sequentially, we conclude that the experimental results are robust for ourmethod of risk estimation. Other diagnostic boxplots are available in the supplementary repository [6]. The insights of Figure 4 should not be interpreted as solely positive, as we discuss in Section 5.\nAlternative methods of risk estimation - Dynamical tree systems \u2014 For the Peano addition and Boolean Formulae DGPs, we attempted to employ a similar sampling restriction for training series generation, as described above. During sampling, however, we noticed that our described methodology failed to sample a sufficient amount of training exam\u2010 ples. Our troubleshooting lead us towards the realization, that these DGPs were very\nReScience C 8.2 (#38) \u2013 Stropnik and Ora\u017eem 2022 9\nprone to generating trees that could not be simplified any further, which meant that no mapping pairs (G0, G1) could be generated from such a sample. Our diagnostic results in Figure 5 show the overwhelming majority of samples being part of this group, which casts doubt on the claims,made in 1. We evaluated the empirical probability of a unique, simplifyable tree G0, being sampled from a DGP. Our results show that the Boolean ad\u2010 dition DGP sampled such a treewith probability PrBoolean = 0.13\u00b10.003, while the Peano addition DGP performed at PrPeano = 0.26\u00b10.002. These results are derived over 300,000 DGP samples with uniformly distributed hyperparametric values of maximal permited operations p \u2208 [3, 5], with 3 repetitions. In an attempt to evaluate the performance of the GEN on this family of data, we loosen our restrictions, set in Subsection 3.4. Instead, we run 5 repetitions of training, with holdout estimation (|Test set| = 100) on the time series, generated by the unique graphs G0, described in the previous paragraph. In this setup, the results were not perfect, but remained in the \u00b10.05 standard\u2010deviation\u2010neighborhood of the reported results.\nPerformance on the social network dataset \u2014 The authors use the social network data set only to evaluate the scaling capabili\u2010 ties of the GEN, but do not offer any in\u2010 formation on themodel\u2019s performance on the set.2 Since the model\u2019s scaling may be dependant on specific model param\u2010 eters (specifically, the used user\u2010defined loss function), we examine if the model is capable of training using gradient de\u2010 scent in this experiment. We visualize the loss curves of training the model over 1554 iterations (one pass of each available graph in Figure 6, with all hyper parameters similar to the origi\u2010 nal experiment, and using the Adam op\u2010 timizer. We see that the model, imple\u2010 mented in the scenario, does not optimize its loss function successfully.\n5 Discussion\nOur experimental results conclusively show that most of the claims in the original work hold. It is imperative, however, to discuss the choice of DGPs on which the model was evaluated to achieve these results. Consider, for example, the Game of Life DGP, used for evaluating the precision and recall of the test set. While at first glance, a perfect re\u2010 sult on a relatively involved system might be impressive, we must recall that node/edge edits and insertions should never appear in an edit script \u03b4\u0304 between two Game of life graphs, as the only changes in the systems correspond to replication edits. Consequen\u2010 tially, the system in the scenario is only asked to output without any addition or inser\u2010 tion edits. Since experiment 4.2.1 showed that this output does not always appear, this casts a doubt over the model\u2019s expressive power. Another example of a somewhat poor test setup is the Edit Cycles DGP, in which the network will always test on transitions \u03c8, to which it was already introduce during training, given that the series are cyclical and Markovian. Adding to this, it is very likely that, due to the nature of the problem\n2This concern was also raised to the authors during the paper\u2019s submis\u2010 sion and review process by AnonReviewer4. See the section Weak points in: https://openreview.net/forum?id=dlEJsyHGeaL&noteId=Sg922s85khx\nReScience C 8.2 (#38) \u2013 Stropnik and Ora\u017eem 2022 10\nthey describe, the mappings \u03c8, inferred from the Peano addition and Degree formulae DGPs, are often seen during training. We support this claim with our description of the sampling problems we encountered in Experiment 4.2.2. Our experimental results on the arXiv citation network show that the network\u2019s runtimes are subquadratically dependant on the number of nodes in the given graph. This par\u2010 tially corroborates the authors\u2019 claims. However, we note that these results are achieved by an architecture, that is not able to optimize its loss function correctly. Given that the loss cumulative loss increases with \u03c4 (as one would expect), we hypothesize that this performance is not a result of a simple syntactical error in the author\u2010defined loss func\u2010 tion. While this additional insight does not disprove Claim (iii), we note that a different, better performing loss function, might. We propose that th weakneseses we higlighted here be considered in future work, We believe that a more in\u2010depth and practical experimental evaluation of an otherwise ele\u2010 gant and interpretable solution could greatly benefit the machine learning community in the years to come.\n5.1 What was easy All the provided code has extensive and clear documentationwhichmade the paper\u2019s ex\u2010 periments easy to reproduce. The entire code base is readable, verymodular, adheres to established practices on code readability, and goes hand\u2010in\u2010handwith the nomenclature of the paper. While the presented implementations do require intermediate familiarity with common PyTorch constructs, the authors do admirable work in explaining every\u2010 thing else as\u2010they\u2010go, almost always without using unnecessary dependencies or need\u2010 lessly referencing the reader elsewhere. The authors also provide amoderate amount of clearly written unit tests for all of their models and have already pre\u2010implemented sev\u2010 eral diagnostic measures, such as execution runtime logging, repetition handling and plotting of training curves, which made our work a lot easier.\n5.2 What was difficult Even with the extensive supplementary material, we believe that it would have been very difficult to reproduce the exact implementation of GEN and the presented DGPs by reading the paper alone, as we\u2019ve discovered many important details from the supple\u2010 mentary documentation. In the paper, the authors state that all experimentswere run on a consumer\u2010grade laptop. While this may be the case, running some of the provided code is prohibitively time consuming to run on such a machine. For example, we were not able to finish a single pairwise distance calculation in a day\u2019s worth of computing time (and have thus not reported on the results of that method here) on the kernel\u2010based baseline from [13]. The lack of transparency about the code base\u2019s runtimes made our work here much more difficult. The original paper also uses a direct implementation of the [4] as a baseline for exper\u2010 iments, relating to Claim (i). This model was not provided in the repository at the be\u2010 ginning of our work. The authors later provided us with the implementation, which encountered runtime errors. Even though the model now works, the trouble\u2010shooting of this part of the code was especially time\u2010consuming. The author\u2019s repository also lacks a hierarchical structure of related items. While the purpose of every file is clearly explained, our reproduction would have been easier with some reorganization.\n5.3 Communication with original authors We contacted Mr. Paa\u00dfen, along with his colleagues to inform them about our efforts to reproduce their work inmid\u2010January. He was prompt with his responses, welcomed our\nReScience C 8.2 (#38) \u2013 Stropnik and Ora\u017eem 2022 11\nwork and made himself available for any questions. Upon our request, he forwarded the code with which the authors evaluated a baseline, reported in the paper, but not available in the repository. Upon our discovering of its aforementioned problems, he was prompt to offer solutions and sent us an adapted file in a couple of days. He let us know that the authors plan to update the repository with this working file shortly, which we see as an aditional benefit of our effort reproducing this article. When asked about their method of risk estimation, the author argued that the combi\u2010 natorial explosion of possible starting states makes it unlikely that GEN just memorizes the training data without generalization. For the case of the Edit Cycles dataset, where this obviously has to happen, since there is no underlying ground\u2010truth function \u03c8, he offered the insight that generalization was not the main aim of the inclusion of this dataset. Rather, it was intended to test the expresiveness of the edits, as memoriztation alone does not suffice to solve the task of mapping Gt \u2192 Gt+1. Summing up, we greatly appreciate the authors\u2019 responses and their general attitude towards their work being reproduced as a part of this challenge."}], "title": "[Re] Graph Edit Networks", "year": 2022}