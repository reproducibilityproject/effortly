{"abstractText": "Koustuv Sinha1,2, ID , Jesse Dodge6, Sasha Luccioni5, ID , Jessica Zosa Forde4, ID , Sharath Chandra Raparthy3, ID , Joelle Pineau1,2,8, ID , and Robert Stojnic7,8, ID 1School of Computer Science, McGill University, Montreal, Canada \u2013 2Mila Quebec AI Institute, Montreal, Canada \u2013 3Universit\u00e9 de Montr\u00e9al, Canada \u2013 4Brown University, USA \u2013 5Allen Institute for AI, USA \u2013 6HuggingFace, USA \u2013 7PapersWithCode, USA \u2013 8Meta AI, USA and Canada", "authors": [{"affiliations": [], "name": "Koustuv Sinha"}, {"affiliations": [], "name": "Jesse Dodge"}, {"affiliations": [], "name": "Sasha Luccioni"}, {"affiliations": [], "name": "Jessica Zosa Forde"}, {"affiliations": [], "name": "Sharath Chandra Raparthy"}, {"affiliations": [], "name": "Joelle Pineau"}, {"affiliations": [], "name": "Robert Stojnic"}], "id": "SP:a79866a718c3ca7e20e70d7205e1f88e002f4d11", "references": [], "sections": [{"text": "R E S C I E N C E C"}, {"heading": "Editorial / ML Reproducibility Challenge 2021", "text": ""}, {"heading": "ML Reproducibility Challenge 2021", "text": "Koustuv Sinha1,2, ID , Jesse Dodge6, Sasha Luccioni5, ID , Jessica Zosa Forde4, ID , Sharath Chandra Raparthy3, ID , Joelle Pineau1,2,8, ID , and Robert Stojnic7,8, ID 1School of Computer Science, McGill University, Montreal, Canada \u2013 2Mila - Quebec AI Institute, Montreal, Canada \u2013 3Universit\u00e9 de Montr\u00e9al, Canada \u2013 4Brown University, USA \u2013 5Allen Institute for AI, USA \u2013 6HuggingFace, USA \u2013 7PapersWithCode, USA \u2013 8Meta AI, USA and Canada\nEdited by Nicolas P. Rougier\nReviewed by Anonymous Reviewers\nReceived 19 May 2022\nPublished 23 May 2022\nDOI 10.5281/zenodo.6574723\n1 Introduction\nThe importance of reproducibility in science cannot be overstated. It is one of the key mechanisms in place to enforce the high standards of scientific discoveries, and a key in\u2010 gredient for an impactful scientific discovery, allowing future practitioners to build on the shoulders of priorwork. Reproducible science also promotes open and accessible re\u2010 search, allowing the scientific community to quickly integrate new findings and convert ideas to practice more seamlessly. In the spirit of promoting a culture of reproducible science in the Machine Learning community, we have hosted the fifth iteration of the ML Reproducibility Challenge in 2021. Following the trend of inclusivity and breadth, this iteration involves a challenge to reproduce papers published in nine top confer\u2010 ences in Machine Learning, including NeurIPS, ICML, ICLR, CVPR, ICCV, ACL, EMNLP, AAAI and IJCAI. An important objective of this challenge is to contribute toward im\u2010 proving the understanding of the central claims of the papers published in these top conferences, by inviting participants to run reproducibility study on them. In this spe\u2010 cial issue of ReScience C Journal, we are proud to present the peer\u2010reviewed accepted papers of the 2021 ML Reproducibility Challenge.\n2 Challenge\nThe goal of the challenge was to reproduce the central claims of papers published in top Machine Learning conferences of the year. Participants were invited to work on either all claims, or partial claims, depending on the complexity of the project. Participants were also free to reuse authors\u2019 code when available, while being encouraged to explore beyond simply running the code provided to verify reproducibility. Unlike the previous challenge, in this iteration we removed the \u201cClaim paper\u201d step. This step, which was previously used, involved participants pre\u2010registering which paper they wanted to re\u2010 produce, in order to encourage early commitment, narrow down the claims they wish to explore in the paper, and covering a larger number of papers. However, we received feedback that this step was not useful for participants, which was also reflected by the low percentage of the number of reproducibility reports submitted vs papers claimed. Removing this step also reduced the complexity of participating in the challenge. As in the last iteration, participants were free to claim multiple papers, and multiple teams could claim the same paper. In this iteration, we observed a jump of reproducibil\u2010 ity report submissions to 102, compared to 82 from last year (Figure 1). Reproducibility\nCopyright \u00a9 2022 K. Sinha et al., released under a Creative Commons Attribution 4.0 International license. Correspondence should be addressed to Koustuv Sinha (koustuv.sinha@mail.mcgill.ca) The authors have declared that no competing interests exist.\nReScience C 8.2 (#48) \u2013 Sinha et al. 2022 1\nreports were spread across all nine conferences, with most papers chosen from ICML 2021, and the least fromACL 2021. Amajority of the participantswere students using the challenge as a part of their machine learning courses from various institutions around the world, including but not limited to: KTH (Royal Institute of Technology Stockholm, Sweden), Queen\u2019s University (Ontario, Canada), Indian Institute of Technology (Gand\u2010 hinagar, India), University of Amsterdam (Netherlands), University of Southern Cal\u2010 ifornia, (USA), Indian Institute of Technology (Guwahati, India), Tsinghua University (China), University of Ljubljana (Slovenia), University of Michigan (USA), University of Waterloo (Ontario, Canada), Istanbul Technical University (Turkey), and EPFL (Switzer\u2010 land).\nAfter in\u2010depth peer review, in this special issue we present the top 47 1 accepted reports, selected from 102 submissions, thus driving up the acceptance rate from 28% last year to 47% this year. This increase is largely due to significant improvements in the quality of the reports & their methodology, which is encouraging to see.\n3 Best Paper Awards\nStarting this year, we are presenting best paper awards to a few select reports to high\u2010 light the excellent quality all\u2010round of their reproducibility work. The selection criteria consisted of votes from the Area Chairs, based on the reproducibility motivation, exper\u2010 imental depth, results beyond the original paper, ablation studies, and discussion/rec\u2010 ommendations. We believe the community will appreciate the strong reproducibility efforts in each of these papers, which will improve the understanding of the original publications, and inspire authors to promote better science in their own work.\n3.1 Best Paper Award \u2022 Piyush Bagad, Jesse Maas, Paul Hilders, Danilo de Goede; Reproducibility Study of \u201cCounterfactual Generative Networks\u201d\n3.2 Outstanding Paper Award \u2022 Matija Ter\u0161ek, Domen Vre\u0161, Ma\u0161a Kljun; Study of \u201cCounterfactual Generative Network\u201d\n\u2022 Ian Hardy; [RE] An Implementation of Fair Robust Learning 1We accepted 48 reports, but one team did not submit their camera ready version till the time of the publi\u2010\ncation of this editorial.\nReScience C 8.2 (#48) \u2013 Sinha et al. 2022 2\n\u2022 Guilly Kolkman, Maks kulicki, Jan Athmer, Alex Labro; Strategic classification made practical: reproduction\n\u2022 Andrea Lombardo, Matteo Tafuro, Tin Had\u017ei Veljkovi\u0107, Lasse Becker\u2010Czarnetzki; On the reproducibility of \u201dExacerbating Algorithmic Bias through Fairness Attacks\u201d\n4 Platforms\nThis challenge is conducted with the support of PapersWithCode2 and OpenReview3. PapersWithCode is an open, collaborative platform to discover latest trending machine learning research papers with their codebases, which enables rapid re\u2010usability and re\u2010 producibility of published works. PapersWithCode enabled the challenge organizers to reach a wide audience of students and researchers who participated in the competition. As was the case last year, OpenReview provided crucial logistic support by providing an unique platform to claim and submit reproducibility reports. After submission, all reports went through a thorough peer review process consisting of hundreds of review\u2010 ers from the Machine Learning community, and OpenReview provided an easy\u2010to\u2010use platform for managing reviews and administrative processes. Finally, we used a public Github repository4 to perform the final editorial process of converting accepted papers into ReScience format, and thereby publish 48 high quality reports in this special issue.\n5 Conclusion\nReproducibility of central claims of papers published in Machine Learning conferences has been a center of considerable attention over the past several years. In recent years, conferences such as NeurIPS, ICLR, AAAI, ICML, EMNLP have routinely included re\u2010 producibility workshops and challenges to cultivate the culture of reproducible science in the community. Several conferences have also introduced code submission policies and Reproducibility Checklists to further advance the cause and build momentum of reproducible science. We hope our continued endeavour of hosting annual challenges and publishing high\u2010quality peer\u2010reviewed reproducibility reports will contribute more information about existing published papers, and help strengthen their core contribu\u2010 tions in the process, while also promoting open, accessible and soundmachine learning research.\n6 Acknowledgement\nWe thank the board and program committee of NeurIPS, ICML, ICLR, ACL, EMNLP, CVPR, ICCV, AAAI and IJCAI for partnering with us in this massive initiative and sup\u2010 porting the challenge. We thank the OpenReview team (in particular Andrew McCal\u2010 lum, Parag Pachpute, Melisa Bok, Celeste Martinez Gomez, Pam Mandler and Mohit Uniyal) for their constant support in hosting and building the customized portal used in our challenge. We thank Ana Lucic, Maurits Bleeker and Samarth Bhargav for their feedback on using the Reproducibility Challenge in their course at University of Amster\u2010 dam. We thank Robert Stojnic, Ross Taylor and Elvis Saravia from PapersWithCode for hosting and supporting the challenge along with its logistics. We thank the ReScience board (in particular Nicolas Rougier, Konrad Hinsen, Olivia Guest and Beno\u00eet Girard) for presenting the accepted reports in their esteemed journal. Finally, we thank all of\n2https://paperswithcode.com 3https://openreview.net/group?id=ML_Reproducibility_Challenge/2021/Fall/ 4https://github.com/ReScience/MLRC\nReScience C 8.2 (#48) \u2013 Sinha et al. 2022 3\nour participants who dedicated time and effort to verify results that were not their own, to help strengthen our understanding of the concepts presented in the papers.\n7 Reviewers\nOur reviewers need a special section dedicated to thank them for their tireless efforts in screening and providing valuable feedback to the Area Chairs (Jesse Dodge, Sasha Luccioni, Jessica Zosa Forde, Sharath Chandra Raparthy and Koustuv Sinha) to select the best papers. We were fortunate enough to attract a large pool of reviewers, who spent their precious time to critically review the reports. We would like to specifically acknowledge our Emergency reviewers who responded to our call for help to review some additional reports at the last minute. Starting this iteration, we also introduce Outstanding Reviewer Award to select reviewers for their high quality and timely re\u2010 views for the challenge. The selection criteria involved votes from the Area Chairs after careful review of the reviews posted in the challenge. We thank the reviewers for their exceptional effort and hope they will continue to support us in future iterations.\n7.1 Outstanding Reviewers\nAlex Gu\nCagri Coltekin\nDavid Rau\nDivyat Mahajan\nFrederik Paul Nolte\nKanika Madan\nKaran Shah\nLeo M Lahti\nMaxime Wabartha\nMaxwell D Collins\nOlga Isupova\nOlivier Delalleau\nPascal Lamblin\nPrithvijit Chattopadhyay\nSamuel Albanie\nSunnie S. Y. Kim\nTobias Uelwer\nVarun Sundar\n7.2 All Reviewers\nAbhinav Agarwalla\nAkshay Ravindra Kulkarni\nAli H\u00fcrriyeto\u011flu\nAndreas Ruttor\nAnimesh Gupta\nAnis Zahedifard\nBrent M. Berry\nChao Qin\nDavid Arbour\nDavid Krueger\nDong Gong\nFan Feng\nFelix Gimeno\nFurkan K\u0131nl\u0131\nGabriel Synnaeve\nGang Wang\nGeorgios Leontidis\nHanna Suominen\nHao He\nHarsha Kokel\nHeng Fang\nJiangwen Sun\nJie Fu\nJishnu Jaykumar P\nKaushy Kularatnam\nKiana Alikhademi\nLabiba Kanij Rupty\nLi Erran Li\nLluis Castrejon\nMahima Agumbe Suresh\nMahzad Khoshlessan\nMaja Schneider\nMani A\nMarija Stanojevic\nMartin Klissarov\nMatthew Kyle Schlegel\nMatthew Ryan Krause\nMayur Arvind\nMelanie F. Pradier\nMike Chrzanowski\nMinjia Zhang\nMonjoy Saha\nNadia Tahiri\nNan Rosemary Ke\nNikolaos Vasiloglou\nOtasowie Owolafe\nOwen Lockwood\nPablo Robles\u2010Granda\nPatrick Philipp\nPaul Tylkin\nPrateek Garg\nReScience C 8.2 (#48) \u2013 Sinha et al. 2022 4\nPraveen Narayanan\nQingzhi Hu\nRaj Ghugare\nRamesh Ragala\nRazvan Pascanu\nSamiran Das\nSeohyun Kim\nShiju SS\nSimon Kornblith\nStefan Magureanu\nUjjwal Verma\nVenkatadheeraj Pichapati\nVibha Belavadi\nWei Han\nWenbin Zhang\nWenhao Yu\nXavier Bouthillier\nXavier Sumba\nXiang Zhang\nXin Lu\nXinggang Wang\nXingrui Yu\nYuntian Deng\nZahra Atashgahi\nZhourong Chen\nReScience C 8.2 (#48) \u2013 Sinha et al. 2022 5"}], "title": "ML Reproducibility Challenge 2021", "year": 2022}