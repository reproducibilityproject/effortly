{"abstractText": "DOI 10.5281/zenodo.4242943 Abstract \u2014We partially replicated the model described by Rafferty et al. to optimize automated teaching via POMDP planning. Teaching is formulated as a partially observable Markov decision process (POMDP) in which the teacher operates and plans actions based on the belief that reflects the learner s\u0313 state. The automated teacher employs a cognitive learnermodel that defines how the learner s\u0313 knowledge state changes.Two concept learning tasks are used to evaluate the approach: (i) a simple letter arithmetic task with the goal of finding the correct mapping between a set of letters and numbers, and (ii) a number game, where a target number concept needs to be learned. Three learner models were postulated: a memoryless model that stochastically chooses a matching concept based on the current action, a discrete model with memory that additionally matches concepts with previously seen actions and a continuous model with a probability distribution over all concepts that eliminates inconsistent concepts based on the actions. We implemented all models and both tasks, and ran simulations following the same protocol as in the original paper. We were able to replicate the results for the first task with comparable results except for one case. In the second task, our results differ more significantly. While the POMDP policies outperform the random baselines overall, a clear advantage over the policy based on maximum information gain cannot be seen. We open source our implementation in Python and extend the description of the learner models with explicit formulas for the belief update, as well as an extended description of the planning algorithm, hoping that this will help other researchers to extend this work.", "authors": [{"affiliations": [], "name": "Lukas Br\u00fcckner"}, {"affiliations": [], "name": "Aur\u00e9lien Nioche"}, {"affiliations": [], "name": "Koustuv Sinha"}, {"affiliations": [], "name": "Emmanuel Bengio"}, {"affiliations": [], "name": "Amy Zhang"}], "id": "SP:bfd0080500fbbea86828531b6f189dac3ed012a3", "references": [{"authors": ["A.N. Rafferty", "E. Brunskill", "T.L. Griffiths", "P. Shafto"], "title": "Faster teaching via pomdp planning.", "venue": "Cognitive science", "year": 2016}, {"authors": ["S. Ross", "B. Chaib-draa", "J. Pineau"], "title": "Bayesian reinforcement learning in continuous POMDPs with application to robot navigation.", "venue": "IEEE International Conference on Robotics and Automation", "year": 2008}, {"authors": ["J. Feldman"], "title": "The simplicity principle in human concept learning.", "year": 2003}, {"authors": ["F. Restle"], "title": "The selection of strategies in cue learning.", "venue": "Psychological Review", "year": 1962}, {"authors": ["J.B. Tenenbaum"], "title": "Rules and similarity in concept learning.", "venue": "Advances in neural information processing systems", "year": 2000}], "sections": [{"text": "Edited by Koustuv Sinha ID\nReviewed by Emmanuel Bengio ID\nAmy Zhang ID\nReceived 28 June 2020\nPublished 03 November 2020\nDOI 10.5281/zenodo.4242943\n1 Introduction\nTeaching students in an automated fashion is a challenging task. Human teachers are able to adjust the teaching process to the students depending on their current situation (e.g., a teacher will act differently if they think that the student did not fully understand a concept yet, or if they alreadymastered it). This level of understanding of the student s\u0313 knowledge and a consequential ability to adjust the teaching activities is not straightforward in automated teaching applications. One method for increasing the teaching effectiveness in automated teaching applications was proposed by Rafferty et al. [1]1. They model teaching as a partially observable Markov decision process (POMDP), considering the selection of the next teaching activity as a planning problem. The automated teacher employs a cognitive learner model that defines how the student s\u0313 knowledge state is expected to behave and that defines\n1Note that there is a shorter version with the same method described which only contains the first task of this longer paper. We always refer to the longer and later paper in this replication.\nCopyright \u00a9 2020 L. Br\u00fcckner and A. Nioche, released under a Creative Commons Attribution 4.0 International license. Correspondence should be addressed to Lukas Br\u00fcckner (lukas.brueckner.89@gmail.com) The authors have declared that no competing interests exist. Code is available at https://github.com/luksurious/faster-teaching. \u2013 SWH swh:1:dir:3335796ebdc27256ec8cb381f8baf60ace85fe9d. Open peer review is available at https://github.com/ReScience/submissions/issues/44.\nReScience C 6.3 (#7) \u2013 Br\u00fcckner and Nioche 2020 1\nhow the internal belief update is calculated. As the title of the paper suggests, the goal of the teacher is to teach the student quickly. The teaching activities have a time cost associated and the goal is to minimize the total time until a concept is learned. The teacher has three types of teaching activities available: showing an example (i.e., teaching new content), asking a quiz (i.e., assessing the knowledge of the student), and a question with feedback action (i.e., a question is asked and the questions\u0313 correct answer is then revealed). Three learner models were postulated: a memoryless model that stochastically chooses a matching concept based on the current action, a discrete model with memory that additionally matches concepts with previously seen actions and a continuousmodel with a probability distribution over all concepts that eliminates inconsistent concepts based on the actions. The combination of learnermodel and teaching action defines the belief update computation. During the planning phase, a set of sample actions are evaluated by a tree search algorithm with limited horizon, where the belief is simulated according to the learner model. The teacher selects the action with the lowest cost according to the action sequence costs leading the student the closest to the desired knowledge. The algorithm is evaluated on two concept learning tasks: (i) a simple letter arithmetic task with the goal of finding the correct mapping between a set of letters and numbers, and (ii) a number game, in which students need to learn the target number concept (e.g., is the rule used for generating numbers \u02bcodd numbers ,\u0313 \u02bcnumbers between 15-25 ,\u0313 ...?). We implemented the algorithms and evaluated our implementation through the same simulations as in the original work. We received comparable results for the first task with one deviation. However, the different models achieve a very similar performance if paired with a sophisticated learner and are not better than a baseline based on maximum information gain. In the second, more complex task, the POMDP policies outperformed the random baselines but have no clear advantage over the policy based on maximum information gain. Further, no single learner model was clearly better than the others. In addition, we report explicit failure rates of the policies when paired with a particular learner model. This showed that the simulated memoryless learner often fails to learn the concept, and that the continuous policy tends to overestimate the learner abilities and does not discover mismatches between the belief and the state. Our implementation is open and canbe foundat https://github.com/luksurious/faster-teaching.\n2 Methods\n2.1 Teaching as a POMDP\nGeneral framework \u2014 A partially observable Markov decision process (POMDP) extends a Markov decision process (MDP) such that the agent does not directly observe the state of the environment and instead receives (partial) observations of the state. Similar to an MDP, the state space S describes the state of the environment, the action spaceA is the set of possible actions the agent can take, the reward functionR(s, a) = r describes the outcome for the agent after taking an action a \u2208 A in state s \u2208 S, and the transition model T (s\u2032|s, a) gives the conditional probability of the environment transitioning from state s to state s\u2032 \u2208 S after the agent has taken action a \u2208 A. \u03b3 \u2208 [0, 1] is a discount factor that describes how important future rewards are in comparison to immediate rewards when calculating total rewards. In a POMDP, as the states are not directly available to the agent, the set of possible observations of the environment are denoted z \u2208 Z and the conditional observation model O(z|s\u2032, a) assigns a probability of receiving the observation z after taking action a causing a transition to s\u2032. To track the state of the environment, the agent maintains a probability distribution over the state space S called the belief b. b(s) is the probability the agent assigns to the state s matching the environment s\u0313 state. Through a series of observations, the agent can update this belief to infer the state of the environment. The\nReScience C 6.3 (#7) \u2013 Br\u00fcckner and Nioche 2020 2\ngoal of the agent is to find an action sequence that maximizes the expected discounted future rewards E [ \u2211\u221e t=0 \u03b3\ntrt] where t denotes the time step of the interaction and rt is the reward at that step. Figure 1 describes the interaction in a POMDP. Taking an action a0 \u2208 A causes a transition of the environment s\u0313 state from s0 to s1 with probability T (s1|s0, a0). Then, the agent receives the observation z1withprobabilityO(z1|s1, a0) and a reward r1 = R(s1, a0). This enables the agent to update its belief from b0 to b1 as described in the next section.\nBelief update \u2014We denote the operation of updating the belief from b to b\u2032 as \u03c4(b, a, z). A generic belief update can be described with Bayesian statistics. For discrete states, the probability of a single state can then be updated using the formula:\nb\u2032(s\u2032) = \u03b7 \u00b7O(z|s\u2032, a) \u2211 s\u2208S T (s\u2032|s, a) \u00b7 b(s) (1)\nwhere \u03b7 is the normalization term 1/ \u2211\ns\u2032 b \u2032(s\u2032) to assert its sum is 1. This formula must\nbe applied to every state s \u2208 S to obtain the updated belief b\u2032. The complexity of a full belief update is then of order O(|S|2).\nApplication to automated teaching \u2014 This formulation can be applied to automated teaching as follows: the student represents the environment and the knowledge of the student is modeled as the state s \u2208 S that is hidden from the teaching program. The automated teacher is the agent operating in this environment by choosing actions a \u2208 A to teach the topic. The student responds to actions with answers z \u2208 Z that represent the observations for the automated teacher. The automated teacher maintains a belief b as a probability distribution of the knowledge of the student. In general, its goal is to change the state of the student such that the student has mastered the topic according to the reward function according to the pedagogical objective(s). The original formulation by Rafferty et al. simplifies the reward function to only depend on the action R(a). If the student s\u0313 learning behavior is known, the teacher can plan and choose actions in such away that the learning of the student is optimized. Such amodel would thus define the transition and observation functions needed to apply the POMDP framework. Rafferty et al. consider that an action a is the choice of a specific type of teaching activity t \u2208 T (type) and a specific teaching item i \u2208 I that describes some content or exam-\nReScience C 6.3 (#7) \u2013 Br\u00fcckner and Nioche 2020 3\nple of the topic. This decomposition allows simplifying the formulation when different teaching methods are available. See section 2.2 for more details.\nPlanning optimal actions \u2014 Finding a policy that optimizes the pedagogical objective(s) is done via online planning, i.e., during execution. Offline planning, i.e., precomputing best actions for every possible belief state, might become computationally expensive and, thus, not tractable for longer trials or teaching taskswith a large state space. Hence, Rafferty et al. employ a forward tree search algorithm with a finite horizon similar to Ross et al. [2]. In a nutshell, the process is as follows: The forward search starts from the current belief state b. A set of actions A\u0303 \u2282 A is sampled to lower the computational cost. Their values q are calculated by simulating an interaction until horizon d (depth) and the action a\u2217 \u2208 A with the highest value is selected by the automated teacher. Thus, we compute the best action a\u2217d(b) for a fixed horizon as follows:\na\u2217d(b) = argmax a\u2208A\u0303 Qd(b, a) (2)\nwith b the current belief state, d \u2208 Z the search depth, A\u0303 \u2282 A the sampled actions, and Qd(b, a) the action-value function for action a under the belief b calculated until d. This follows the common notation in reinforcement learning where action-value functions are used to denote the expected reward when performing a particular action in a given state. Since actions are composed of items i \u2208 I and teaching activity types t \u2208 T , the action sampling is decomposed as follows: n teaching items I\u0303 \u2282 I are sampled and the cartesian product with all teaching activity types T is constructed to create A\u0303 = I\u0303 \u00d7 T . The number of sampled actions is then equal to the product of sampled items and number of teaching types |A\u0303| = n \u00b7 |T |. This prevents ending up with actions of only a few types, which is very probable in the case of |I| \u226b |T |. A single action a might cause different state changes according to T (s\u2032|s, a), and thus, different observations z. For each action-observation pair, a new belief b\u2032 = \u03c4(b, a, z) is computed. With this new belief, the process starts anew: new actions are sampled and evaluated. The forward search expands tree-like and grows exponentially in breadth. To keep the computations tractable, the search is limited by a fixed depth and a reduced sample size. If the set of possible observations following an action depends on the type of teaching activity associated with that action, we can simplify the calculation to only consider those observations that are possible for that teaching activity Zia \u2282 Z. To calculate the value of an action under the current belief, the future expected values of all possible observations for that action need to be considered and added to the value of the action itself. We compute Qd(b, a), the expected value for using the action a \u2208 A in the belief b with search depth d as:\nQd(b, a) =\n{ R(a) + \u2211 z\u2208Zia\nPr(z|b, a) \u00b7 \u03b3 \u00b7 Vd\u22121(b\u2032 = \u03c4(b, a, z)) if d > 0 V\u0302 (b) if d = 0\n(3)\nwith R(a) the reward for action a, Pr(z|b, a) the probability of receiving z for action a under the belief b, \u03b3 \u2208 [0, 1] the discount factor, Vd(b\u2032) the value function for the new belief, and V\u0302 (b) an estimation function of the value of the belief bwhen the search depth is reached. The value of the belief Vd(b) is defined as the maximum of the action-valuesQd(b, a) for search depth d over the sampled actions A\u0303:\nVd(b) = max a\u2208A\u0303 Qd(b, a) (4)\nReScience C 6.3 (#7) \u2013 Br\u00fcckner and Nioche 2020 4\nFurther, the probability of receiving anobservation for an actionunder a beliefPr(z|b, a) is simply the sum of the observation model O(z|s, a) for every state, weighted by the belief probability of that state b(s).\nPr(z|b, a) = \u2211 s\u2208S b(s) \u00b7O(z|s, a) (5)\nFinally, when the depth d is reached, the value of the belief state at the leaf node is estimated by V\u0302 (b). The specific estimation function thus needs to be defined by the task implementation of the method (see section 2.2). The estimated leaf value is propagated back up and discounted according to \u03b3. The value of an action under a belief at a certain depth Qd(b, a) is thus the sum of the immediate reward of the action R(a) and the sum over all valid observations for that action Zia of the discounted back-propagated belief state values \u03b3 \u00b7 Vd\u22121(b\u2032), weighted by the observation probability Pr(z|b, a). According to the values of a set of actions, the best action a\u2217d(b) can be chosen according to Equation 2.\nAlgorithm 1: Recursive forward search planning algorithm Input: b current belief n no. of samples per level d horizon Output: v\u2217 best value of next action a\u2217 best action Function ForwardSearch(b, n, d): I\u0303 \u2190 sample n items uniformly from I A\u0303\u2190 I\u0303 \u00d7 T v\u2217 \u2190\u221e for a \u2208 A\u0303 do\nq \u2190 R(a) for z \u2208 Zia do\nb\u2032 \u2190 \u03c4(b, a, z) if d = 1 then\nv \u2190 V\u0302 (b\u2032) else\nv \u2190 v\u2217 of ForwardSearch(b\u2032, n, d\u2212 1) end q \u2190 q + Pr(z|b, a) \u00b7 \u03b3 \u00b7 v\nend if q > v\u2217 then\nv\u2217 \u2190 q a\u2217 \u2190 a\nend end return v\u2217, a\u2217\nThis algorithm is described in algorithm 1. It is implemented as a recursive algorithm that returns themaximumexpected reward of the next action and the best action according to this reward. In intermediate calls, the best value is used to calculate the value of the immediate action while at the top level the best action is used to define the next action to take. For simplicity, we describe it with a single best action although in practice it is useful to maintain a list of actions with the highest known reward and choose the\nReScience C 6.3 (#7) \u2013 Br\u00fcckner and Nioche 2020 5\nnext action from that list. This is to prevent biasing the method toward some internal structure of the actions.\n2.2 Application as \u201cfaster teaching\u201d to concept learning The authors apply this general formulation to concept learning tasks. Category or concept learning is the process of learning categories from examples [3]. In such tasks, we denote the set of concepts as hypotheses H.The task description has to be complemented with (i) a prior distribution p0 over these concepts, (ii) the items that can be used to teach the task i \u2208 I, (iii) the teaching activities t \u2208 T , and (iv) the possible responses Z. The reward function is defined such that the teacher focuses on reducing the timeneeded for learning the task, hence the name \u201cfaster teaching\u201d. Each teaching action is associated with a cost (negative reward) that is modeled after the time it takes the student to complete the action. Thus,minimizing the costs results in the quickest learning. Weuse the cost of an action C(a) and a minimization objective instead of maximizing rewards (if R(a) = \u2212C(a), then argmaxa R(a) = argmina\u2212R(a)). Teaching is terminated once the student has learned the concept. This is assumed if in an assessment phase the learner answers all questions correctly. Such an assessment is performed at regular intervals to check for termination, but not used for updating the beliefs of the teacher.\nTeaching activity types \u2014 In the context of concept learning, Rafferty et al. define three possible types of teaching activity T : (1) showing an example, (2) asking a quiz, and (3) asking a question and subsequently giving feedback about the student s\u0313 response.\n(1) Example In an example, the teacher presents an item with the correct result or concept. No response by the student is expected. Thus, Zexample = {\u2205}. The observation model is trivial: O(z|s, a) = 1 if z = \u2205 and 0 otherwise. However, as the example can provide new insight for the learner, the transitionmodel assumes a state change for such cases. This change depends on the learner model and is described in the next section.\n(2) Quiz In a quiz, the teacher presents an item and the student has to respond with an answer. This allows the teacher to refine their belief about the student s\u0313 true state, but it does not give new information to the student. The set of observations for this activity type contains the full set of observations valid for the concept task, Zquiz = Z. The definition of the observation model O(z|s, a) depends on the learnermodel and is described in the next section. The transitionmodel T (s\u2032|s, a) is trivial: as no new information is presented, the student is not expected to change their state. The transition probability is set to 1 if both states are the same, 0 otherwise.\nT (s\u2032|s, a) =\n{ 1 s = s\u2032\n0 s \u0338= s\u2032 (6)\nAs a result, \u2211\ns T (s \u2032|s, a)b(s) of the belief update in Equation 1 collapses to just b(s\u2032).\n(3) Feedback In a question with feedback type of teaching activity, the previous types are combined. First, the learner is asked a question to which they respond according to their knowledge. Then, the teacher reveals whether the response was correct, and gives the correct answer in case of an incorrect answer. Here, both the observationmodel and the transition model are used in a non-trivial way. As in the quiz, the set of observations for this teaching activity contains the full set of observations valid for the concept task, Zfeedback = Z, and the observation model O(z|s, a) depends on the learner model. And\nReScience C 6.3 (#7) \u2013 Br\u00fcckner and Nioche 2020 6\nas in the example, the transition model depends on the learner model, as the feedback can provide new information to the learner. Note that when modeling the update as in the Bayesian belief formula (1), the update has to be split into two separate steps: the response of the learner is always related to the state before taking the action, while the feedback might trigger a state change without an additional observation. So first, the belief update according to the response z has to be calculated in the same way as in the quiz activity. Second, if the response was incorrect, a state change is expected based on the true answer, as in the example activity type.\nTo reflect these similarities between the feedback type and the other activity types, we will use refinement activities to refer to the quiz activity and the question part of the feedback activity (as they both allow the teacher to refine the belief), and evidence activities to refer to the example activity and the feedback part of the feedback activity (as they both allow the learner to improve its knowledge). We define the generic belief update formulas for refinement and evidence activities as:\nb\u2032(s\u2032) = { \u03b7 \u00b7O(z|s\u2032, a) \u00b7 b(s\u2032) for refinement activities \u03b7 \u00b7 \u2211\ns\u2208S T (s \u2032|s, a) \u00b7 b(s) for evidence activities\n(7)\n\u03b7 refers to the normalization term and depends on the belief formula. Finally, we denote byHa the set of concepts (hypotheses) that are consistent with action a, andHz|a the set of concepts which imply that z is a correct answer to action a.\n2.3 Learner models The learner model defines the details of the transition and observation functions. Rafferty et al. postulate three learnermodels: (1) a discretememorylessmodel, (2) a discrete model with memory, and (3) a continuous model with a dynamic probability distribution over the concept space. All models are extended to include noise to accommodate human error during the learning.\nNoise Two types of noises are added to all models: a production noise \u03f5p for cases where the student responds inconsistently to their knowledge, and a transition noise \u03f5t for cases where the student ignores new evidence and does not transition to a new consistent state. For the production noise, it is assumed that the student respondswith a random answer drawn uniformly from all possible answers. The transition noise affects the state transitions, and themodels need to incorporate this behavior in their updating rules to prevent diverging beliefs and states.\n(1) Memoryless model This model is close to the learning model of Restle et al. [4] and assumes that no explicit memory of previous actions is kept while storing a specific concept hypothesis that is currently believed to be true by the learner (as opposed to considering multiple possible plausible concepts). Thus, the learner state only depends on the current knowledge and the immediate teaching activity. The state space S is isomorphic to the space of possible conceptsH, such that any s \u2208 S corresponds to one specific concept hypothesis denoted hs. For refinement activities, the observation model needs to be defined. As the learner holds a single true concept, the response corresponds to the result under this single concept. Taking noise into account, the observation model is defined as follows:\nO(z|s, a) = { (1\u2212 \u03f5p) + \u03f5p|Zia | if z is consistent for a under s \u03f5p\n|Zia | otherwise\n(8)\nwith \u03f5p|Zia | the probability of choosing a random response in case of a production error.\nReScience C 6.3 (#7) \u2013 Br\u00fcckner and Nioche 2020 7\nFor evidence activities, the learner is assumed to transition to a concept consistent with the new evidence if not already in a consistent state. The probability of each consistent concept is proportional its prior probability. With noise, the transitionmodel is defined as follows:\nT (s\u2032|s, a) =  1 if hs\u2032 \u2208 Ha and s\u2032 = s, (1\u2212 \u03f5t) \u00b7 p0(hs\u2032)\u2211 s\u2032\u2032|hs\u2032\u2032\u2208Ha p0(hs\u2032\u2032) if hs\u2032 \u2208 Ha and s\u2032 \u0338= s,\n\u03f5t if hs\u2032 /\u2208 Ha and s\u2032 = s, 0 otherwise\n(9)\nwith p0 the prior probability over the possible concepts, and p0(hs\u2032 )\u2211 s\u2032\u2032|h\ns\u2032\u2032\u2208Ha p0(hs\u2032\u2032 )\nthe prob-\nability of going from an inconsistent state to a particular consistent state based on the relative prior among the consistent states. The sum over states in Equation 7 for evidence actions can be decomposed to accommodate this structure. For states consistent with the information provided by the action, it is composed of two parts: the likelihood of transitioning from an inconsistent state to this consistent state, and the likelihood of already being in this state and, thus, not transitioning. The belief update is consequently:\nb(s\u2032) =\n{ \u03b7 \u00b7 [ (1\u2212 \u03f5t) \u00b7 p0(hs\u2032 )\u2211 s|hs\u2208Ha p0(hs) \u00b7 \u2211 s|hs /\u2208Ha b(s) ] + b(s\u2032) if hs\u2032 \u2208 Ha\n\u03b7 \u00b7 \u03f5t \u00b7 b(s\u2032) otherwise (10)\nNote, that in case of feedback activities, the evidence update only has to be performed for incorrect answers, as the learner is assumed to not transition for correct responses.\n(2) Discrete model with memory This model extends the memoryless model so that a history of the past m actions is kept. Transitions then have to be consistent with the current action plus the memory actions, denoted AM . The memory only needs to store actions that contain information, i.e., quiz activities are ignored. As for thememoryless model, it is assumed that the student only holds a single concept as true and responds accordingly to it. If the memory would not be perfect, it must be considered part of the student s\u0313 state. In this case, the number of possible memory states SM is calculated based on the set of teaching items I and the set of valid memory teaching activities Tmem as |SM | =\u2211m\nk=0 |I \u00d7 Tmem|k. Taken together, the total state space would increase by the memory states and become |S| = |H| \u00b7 |SM | resulting in a possibly huge state space. For example, in the letter arithmetic task with 6 letter-number pairs, 15 teaching items, 2 valid memory teaching activities, and a memory size of 2, the total number of memory states would grow from |H| = 720 to |H| \u00b7 (1 + 30 + 302) = 670.302. However, the model assumes a \u02bbflawless\u02bc memory, i.e., even if an action is ignored for updating the state due to the transition noise, it is kept in memory. As a consequence, at every time step, only the single memory state based on the action history is possible and needs to be considered, reducing the state space to |H| as before. The transition and observation models are formulated analogously to the memoryless case. The only difference is that the set of consistent concepts with the current action Ha has to be consistent also with the actions in memory AM .\n(3) Continuousmodel Thismodel assumes that the studentmaintains a probability distribution over all possible solutions as formulated in [5]. No explicit history of previous actions is stored but the state contains implicit information about the history. In this case, every state is a probability distribution over all elements of H, making the state space infinitely large. Thus, approximations are needed to make this model feasible.\nReScience C 6.3 (#7) \u2013 Br\u00fcckner and Nioche 2020 8\nIn this model, a state consistent with an action-observation pair is one in which the probabilities of all inconsistent concepts with the action is zero ps(h /\u2208 Ha) = 0. Responses to teaching actions are probabilistic and correspond to the combined probability the student places on the concepts with z as a correct answer.\nO(z|s, a) = \u2211\nh\u2208Hz|a\nps(h) (11)\nwhere ps(h) is the probability of the hypothesis h in the state s. The transition model is defined such that for new evidence, the learner transitions deterministically to the closest consistent derivation of the current state s. This derived state s\u2217a for the current action a is a copy of the current state in which the probabilities of all inconsistent hypotheses are set to 0 while the other probabilities are retained and normalized.\nT (s\u2032|s, a) =  1 if s is consistent with a and s\u2032 = s 1 if s is inconsistent with a and s\u2032 = s\u2217a 0 otherwise\n(12)\nWith diverse evidence, the probability distribution converges to the true concept. From the perspective of the teacher, it is intractable to hold a belief over the infinite set of states. Below, we review the particle filter implementation that is employed instead.\nParticle filter A particle filter is used to approximate the infinite set of possible states via a limited number of weighted particles [6]. In this particle filter, each particle p \u2208 P represents one possible state sp (i.e., a probability distribution over concepts), and the weight wp indicates the probability the teacher assigns to this particle. The sum of the weightsmust always be 1 as in a probability distribution. The particlesP can be thought of as corresponding to b and the weights to b(s) in the discrete case. As the belief state is approximated, the belief update \u03c4(b, a, z) is now a function of the existing particles as described below. In the beginning, the particles are initialized with two particles sharing equal weight: one particle corresponding to the prior distribution p0, and one particle with a uniform distribution over the concepts. If the prior distribution is uniform, only one particle is created. After each action, the particles are updated, and new particles are created based on the observation and transitionmodels, and theirweights are recalculated. This corresponds to the belief updates in the discrete models. For refinement activities, theweights of the particles are updatedbased on the observation model as follows:\nw\u2032p = \u03b7 \u00b7 wp \u00b7O(z|s, a) (13)\nO(z|s, a) = (1\u2212 \u03f5p) \u00b7 \u2211\nh\u2208Hz|a\npsp(h) + \u03f5p |Zia |\n(14)\nwith \u03f5p|Zia | being the probability of observing the response z due to a production error. \u03b7 now refers to the renormalization factor after processing all particles, 1/ \u2211 p\u2208P wp. This corresponds to the regular belief update Equation 7. For actions using an evidence activity, every particle is replaced by two new particles, one assuming the learner did not transition, and one assuming the learner transitioned. The non-transitioned particle is simply a copy of the previous particle, with the weight adjusted by \u03f5t. The transitioned particle is based on the old particle and updated to reflect the new evidence. For this, the probability for all inconsistent states is set to zero psp(h /\u2208 Ha)\u2190 0. The weight of both particles is thus updated according to:\nReScience C 6.3 (#7) \u2013 Br\u00fcckner and Nioche 2020 9\nw\u2032p = { \u03b7 \u00b7 (1\u2212 \u03f5t) \u00b7 wp for transitioned particles \u03b7 \u00b7 \u03f5t \u00b7 wp for non-transitioned particles\n(15)\nSince this update doubles the number of particles, there is a limit imposed to prevent uncontrolled growth on the number of particles. If the total number of particles exceeds some predefined maximum, the particles with the smallest weight are removed so that the limit is satisfied, and the weights are normalized again. The weight updates and renormalizations are performed after both refinement and evidence activities separately (relevant in the case of feedback activities). Further, in both cases, there is a check for particle depletion (i.e., no particle is probable). This happens if the sum2 of all particle weights is below some threshold. In this case, all particles are eliminated, and two new particles are initialized with equal weight. One particle is created according to the prior distribution and the other particle according to the state a learner without noise would have ended up in following the history of previously seen actions. In this model, the update based on new evidence has to be done also for correct responses to feedback activities, as the response is sampled from the state of the learner and does not represent a certain answer.\nPlanning \u2014 For every learner model, a corresponding policy is constructed that follows the forward tree search planning algorithm described in section 2.1.4. To estimate the cost of the leaf nodes in such concept learning tasks, the authors take the probability of failing the assessment phase (given by the belief) and multiply it by a scaling of the minimum future costs:\nV\u0302 (b) = (1\u2212 pb(htrue)) \u00b7 \u03b1 \u00b7min a C(a) (16)\nwith pb(htrue), the probability assigned to the true concept in the current belief, and \u03b1 a scale parameter. For the memoryless and the model with memory, pb(htrue) is simply the belief probability of the true concept. For the continuous model, it is the combined probability of the true concept of all particles, weighted by the corresponding particle weight.\npb(htrue | b) =\n{ b(s = htrue) for the discrete models\u2211\nsp|p\u2208P wsp \u00b7 psp(htrue) for the continuous model (17)\n2.4 Comparison with original paper We tried to honor the originalmethod description asmuch aswe could. While certain elementswere not clear to us, we discussed open questionswith thefirst author of the original paper, and verified certain assumptions with their implementation. Specifically, we verified that one should compute the belief update without the explicit Bayesian equation (1), and treat the belief update as a two-step process for the refinement and evidence parts of the actions, which is especially relevant for the feedback activity. We deduced the planning algorithm 1 from the referenced papers and validated it with the original implementation, and verified that the memory is not part of the state in the discrete model with memory, and that the memory ignores quizzes. As such, we did not have to take design decisions as clarifications were provided by the original authors. Still, there might be differences in implementation details as we focused our validation on the conceptual algorithms and model equations.\n2Note that it is always the sum and not the maximum of the weights as wrongly stated in one passage in the original supplementary material (as confirmed from the original implementation).\nReScience C 6.3 (#7) \u2013 Br\u00fcckner and Nioche 2020 10\n3 Experiments\nWe applied our implementation to both concept learning task from the original paper: the Letter Arithmetic task and the Number Game. As we were interested in the method and its possible applications, we performed simulations only and no experiments with humans.\n3.1 Baselines We compare the results to the baselines introduced in the original paper: a random policy, a random policy with only quiz and example actions (quiz-example only/QE only) and policy with planning according to maximum information gain (MIG). The random policy selects any of the a \u2208 A actions randomly, however, making sure that no item i \u2208 I is sampled twice in the same teaching phase. The quiz-example only policy ignores the feedback activity. It was introduced since in the first original experiment with the letter arithmetic task, the planning algorithms did not use any feedback actions, resulting in amuch higher average cost for the random policy compared to the POMDP planners. The maximum information gain policy performs a single planning step with the continuous learner model. For every possible action, it simulates the belief change according to the continuousmodel and compares the Shannon entropy of the current and the new belief state.\nentropy(b) = \u2211\nsp|p\u2208P\nwsp \u00b7 ( \u2212 \u2211 h\u2208H psp(h) \u00b7 ln ( psp(h) )) (18)\nThe planner selects the action which produces the highest information gain for the learner, i.e., the one that reduces the entropy the most. As with the continuous policy, the particle filter approximation is employed, and thus, the entropy of the belief state is calculated as the weighted sum over the entropy of the particles. This policy provides only example activities to teach a concept as refinement activities do not provide new information to reduce the entropy. As such, it cannot understand errors during learning and revise its belief.\n3.2 Task 1: Letter Arithmetic The letter arithmetic task is composed of addition equations with two letters, such asA+ B = 3. In the original experiments, the mapping length is set to 6, which corresponds to the letters A-F being mapped to the numbers 0-5 (we confirmed that there was a typo in one passage noting a larger range of numbers 0-6). Some sample actions are thus: B + F = 5 (example), A + C =? (quiz) and followed by correct or answer = 2 for feedback actions. The number of possible teaching items I can be reduced by only considering the possible combinations of the letters, i.e., ignoring order. For the case with 6 letters, this gives |I| = 15 and a maximum |I \u00d7 T | = 45 actions to evaluate. The set of valid responses Z are the numbers 1 \u2212 9. During the assessment phase, the learner is queried to provide the correct mapping for each letter. The estimation of the belief value of the leaf nodes V\u0302 (b) follows Equation 16 with \u03b1 = 10. We use the original values for the cost function C(a) and noise parameters, as shown in tables 1 and 2, that were fitted from control experiments with users in the original study. The concept spaceH is composed of all permutations of the 6 letters mapped to 0-5, so |H| = 720. This is also the state space for the memoryless and discrete policy, as well as the particle size in the continuous policy. All concepts are assumed to be equally likely, so a uniform prior p0 over the concepts is used.\nReScience C 6.3 (#7) \u2013 Br\u00fcckner and Nioche 2020 11\n3.3 Task 2: Number Game The second task is called Number Game [5]. The goal in this task is to infer a specific number concept for the number range 1 \u2212 100. Such a concept can be either mathematicalHmath, like odd, even, multiples of 5, etc., or a range based conceptHrange, like 10-20 or 64-83. The student is presented a number and is either told if the number belongs to the target concept (example and feedback activities) or is asked to provide this categorization (quiz and question activities). The set of teaching items I is the range of numbers 1 \u2212 100, resulting in |I \u00d7 T | = 300 possible actions. The set of possible responses Z only contains two values: \u02bbinside\u02bc and \u02bboutside\u02bc the concept. During the assessment phase, the learner is presented with 10 items and has to give 10 correct answers to displaymastery of the concept and terminate the teaching. Of these items, 5 are sampled from within the concept and 5 from outside the concept. For V\u0302 (b) to estimate the belief value of leaf nodes, Equation 16 with \u03b1 = 10 is used as before. Again, we use the original values for the cost function C(a) and noise parameters, as shown in tables 3 and 4, that were fitted from control experiments with users in the original paper. The exact concept space is more difficult to recover. Tenenbaum et al. [5] originally described 5,083 possible concepts. Rafferty et al. reported to use 6,412 concepts, but the exact definition is not given. We received the original unlabelled concept values from the original author, from which we reverse engineered 6,354 of the 6,412 concepts (the remaining 58 concepts were the same as other concepts, and as there were no labels, we considered them duplicates). In addition to the original 5,083 concepts, the set of less probable mathematical concepts Hmath+ is added, e.g., \u201cmultiples of 4 minus 1\u201d or larger multiples such as \u201cmultiples of 25\u201d. Details of the concept space can be found in Table 5. The definition of the prior p0 is following a hierarchical model as described in [5]. The mathematical concepts Hmath and Hmath+ are assigned \u03bb of the probability and each class in turn is assigned half of this prior. As both classes are a lot smaller than the range concepts, they are assigned the majority of the prior if \u03bb is not small. Inside each class, the prior is shared uniformly: p0(h \u2208 Hmath) = 0.5 \u00b7 \u03bb/|Hmath| = \u03bb/168 and p0(h \u2208 Hmath+) = 0.5 \u00b7 \u03bb/|Hmath+| = \u03bb/5048. The remainder (1\u2212\u03bb) is shared among the number range conceptsHrange proportional to an Erlang distribution according to their interval size: p0(h \u2208 Hrange) \u221d (|h|/\u03c32) \u00b7 e\u2212|h|/\u03c3. This should capture the intuition that medium sized ranges are more likely than very big or very small ranges. For the free parameters \u03bb, \u03c3, we use the same values as originally described in [5]: \u03bb = 1/2, \u03c3 = 10. That means, all mathematical concepts are assigned a prior probability of 1/168 \u2248 5.95 \u00b7 10\u22123, the less probable mathematical concepts a prior of 1/5048 \u2248 1.98 \u00b7 10\u22124, and e.g. the range 1-100 is assigned the prior\u2248 3 \u00b7 10\u22127 while the range 64-83 a prior of \u2248 1.67 \u00b7 10\u22124.\nReScience C 6.3 (#7) \u2013 Br\u00fcckner and Nioche 2020 12\nAs the less probable mathematical concepts Hmath+ were reverse engineered, the corresponding prior was similarly deduced to best match the associated prior given by the original authors. Still, these priors slightly differ 5-10% from our reported values. On the one hand, this is because our space is smaller by 58 items but also because they might have used a slightly different \u03bb (for instance, a value around 0.55519 instead of 1/2 could explain this difference but it does not change the results significantly). Finally, the two random policies\u02bc sampling strategy is modified such that with 50% probability an item inside the concept, and otherwise outside the concept is sampled. This was introduced after the original authors noticed that a completely random policy was too frustrating for the human learners.\n3.4 Simulation We validated our implementation by running the same simulations as in the original paper. Every planning policy was tested with every learner model simulation. Hence, for every simulated learning model, the three baselines and the three planning policies with the learning models were tested, leading to a total of 18 pairings. Those pairs were run 50 times each with a different but controlled seed set for the random number generators. In all trials of the same pair, the task instance was the same. The horizon depth and samples per level were the same as in the original paper. In the letter arithmetic task, all models used a horizon of d = 2. The memoryless model sampled 7 items in the first level and 6 in the second level. The discrete model with memory sampled 8 items in both levels, and the continuousmodel sampled respectively 4 and 3 items. Teaching phases consist of three actions followed by an assessment phase. After a maximum of 40 teaching phases (i.e., 120 total actions) and a failed assessment phase, the learning was terminated without success. In the number game, the memoryless model and the discrete model used a horizon of d = 2, with 6 and 8 items sampled, and 6 items sampled in both levels respectively. The continuous model had a horizon of d = 3, with 6, 6 and 8 items sampled. Teaching phases lasted 5 actions. The maximum number of teaching phases was again 40 (i.e., 200 total actions). In both tasks, the memory model was using a memory size of 2, and the continuous model s\u0313 maximum particle size was set to 16 with the particle depletion threshold of 0.005, as in the original experiments.\nReScience C 6.3 (#7) \u2013 Br\u00fcckner and Nioche 2020 13\nPrecomputing actions \u2014 The authors note that for computational reasons, they precomputed the first actions and cached them to be used in all trials. During this precomputation, 10 items were sampled per level. The original paper reports only this number for the first task, but we used the same number for the second task. In the letter arithmetic task, 9 actions were precomputed, while in the number game, 20 actions were precomputed. Also, the same horizon as the target policy is used in every step. Through precomputing actions, possibly better starting points would be found. Naturally, during precomputation, the true responses of the learners are not known. If an action with a non-empty response set is returned as the best action, a separate branch for every possible response has to be followed. This ensures that during teaching, the corresponding path according to the learner s\u0313 response is available. This leads to a possibly very large number of branches.\nEvaluation metric \u2014 To compare our replication, we use the same metric as in the original study to evaluate the planning policies for each simulated learner, the median time to mastery. This refers to the hypothetical time the simulated learner spends with the automated teacher based on the time cost of each action type. For example, in the letter arithmetic task, if a learner is shown three examples and one quiz, the expected time to mastery is 27.6. Note that assessment phases are not taken into consideration in the time calculation. If mastery is not achieved, the total time spent until this point is used. As each teaching phase consists of 3 actions and the least costly activity is the quiz, the theoreticalminimum time is 19.8 (although it is impossible to learn only via quizzes), and themaximum time would be 1440. In a setting using only examples, the minimum time would be 21.0, while for maximum runs, averaging over the action costs, the time until termination is expected to be 1024. For the number game, the action costs are much smaller, while the teaching phases are longer. The theoretical minimum time is 12.0 using example activities. The averaged maximum in this task is then \u2248 666.67. These numbers allow us to get an intuition of how well a learner is doing in respect to the bounds of the problem.\n3.5 Results\nLetter Arithmetic \u2014 Our simulation results are shown in Figure 2 (a). The overall results of the median time to mastery are very similar to the original paper with some notable differences. Most notably, the result for the memoryless learner with the memoryless policy differs. It is lowest for the memoryless learner with 323.5s while the original authors reported a time around 810s. The results for the other learners match the original data closely, the random policies differ slightly. The results for the three planner and the maximum information gain policies paired with the discrete and continuous learners all have a median of 42.0s (two rounds of example activities). Only the memoryless policy for the discrete learner deviates from this value, although it is still contained in its confidence interval. When comparing the time to mastery for the two random policies, note that the QE only policy might achieve a lower number because the most expensive action is not used. This can have a negative effect on the learning because with the random policy, there is a 2/3 chance of seeing evidence, while with QE only, there is only a 1/2 chance. This might explain the higher failure rate for thememoryless learner with the QE only policy. The failure rate of the different simulations, i.e., howmany times a learning session was terminated after 40 rounds of teaching phases before the learner achieved mastery of the concept, are reported in Table 6. It shows that the memoryless learner consistently failed to learn the concept, ranging from 18%-96% of the cases. It is highest with the continuous policy at 96% and second highest for the QE only policy at 68%. Themedian\nReScience C 6.3 (#7) \u2013 Br\u00fcckner and Nioche 2020 14\ntime tomastery is very similar in both cases though since the continuous policy samples only the cheapest activity type (quiz) after somepoint. The discretememory learner only fails with the continuous policy in 26% of the cases, while the continuous learner never fails to learn. Although the information is not available in the original paper, we assume that the failure rates are similar since the median time to mastery is very close. Table 6 shows computation times for the differentmodels across all learners. The online planning computation times are well below 3 seconds which was put forward as the threshold in the original paper. Increasing the sample sizes did not produce significantly different results. Figure 3 shows the teaching activity types planned for eachmodel. Both thememoryless model and the discretememorymodel plannedmainly example activities. Interestingly, both sampled a quiz type after eight and nine example types. Afterwards, the discrete memory model employed more quizzes than the memoryless model. The continuous model started with example activities only but gradually used more quiz type actions which were the only type used after action step 12. No model planned feedback activities. We provide data tables of the results in the supplementary section 8.\nReScience C 6.3 (#7) \u2013 Br\u00fcckner and Nioche 2020 15\nNumber game \u2014 Our results are shown in Figure 4 (a). The overall structure of the results appear similar to the original results but there are major differences in our data. We consider differences major if the values differ at least 50% and the relative performance compared to the other policies is different. For the simulated memoryless learner (top chart), our results for the target multiples of 7 are slightly higher and not clearly better than the random policies while their relative performance matches the previously reported results. For the target multiples of 4 minus 1, our results with the memoryless planner and the discrete memory planner differ significantly. Our simulations resulted in a median time to mastery of 480s for both which was reported as 260s and 140s in the previous work. Hence in our results, no planner exhibits a high performance while previously, the discrete planners resulted in low teaching times. In the target Range of 64-83, we find the maximum information gain policy performs comparable to the continuous policy at 108s while it performed significantly worse than the other planners at 350s in the original results. For the simulated discrete memory learner (middle row), with the target multiples of 4 minus 1, our continuous planner resulted in a significantly worse performance than all other policies which performed remarkable in the previous paper. For target Range of 64-83, our maximum information gain policy achieves a very high performance with 12s, while in the original results, it was significantly worse than all the other policies with 230s.\nReScience C 6.3 (#7) \u2013 Br\u00fcckner and Nioche 2020 16\nReScience C 6.3 (#7) \u2013 Br\u00fcckner and Nioche 2020 17\nFor the simulated continuous learner, we find it to perform significantly better with all targets andwith all policies, achieving the best results among the different learners, and not allowing to distinguishbetween theplanners\u02bc performance. In the originalwork, the continuous learner was outperformed in many cases by the discrete memory learner and even sometimes the memoryless learner. For the target multiples of 4 minus 1, our policy based on the discrete memory policy performed equally well to the continuous policy with 26s, while it was previously reported with a performance of 180s, much higher than the other policies. In general, we note that our results do not follow a clear line in comparison to the original data. The best policy for a learner-policy pair is often different in our simulations even if the differences are not large. Table 7 shows the failure rates for the second experiment. We note similar results as in the first experiment: The memoryless learner fails to learn in some cases, especially for the target multiples of 4 minus 1. In fact, the failure rates are very similar across all planners. In that task, also the discrete memory learner fails to learn with the random, maximum information gain and continuous policies. However, the failure rates are lower than in the first experiment. The continuous learner never fails to learn. The computation times are reported in table 7 in the last column. They are significantly higher than in the first experiment and close or larger than the threshold of 3s. In particular, the continuous policy exhibited amean computation time ofmore than 20s since it was planning three horizons into the future. The higher computation times make sense considering that the state space is around 7 times larger than in the letter arithmetic task3. Note though that we invested less effort into making the computations efficient for the number game. Finally, Figure 5 shows the sampled teaching types for the different models, combined for all three tasks. Recall that in the number game, the cheapest action type was the example action. We notice that it is strongly dominated by example actions in all planners. The discrete memory and the continuous model sometimes planned feedback actions and quiz actions. Again, detailed results and statistics are provided in the supplementary section 8.\n3.6 Implementation We implemented the model in Python 3, using Numpy to performmany calculations in vectorized form. Simulations are run in parallel to reduce the needed execution time. Reproducibility is achieved by using fixed seeds for the random number generators in Python and Numpy. The code is publicly available on GitHub at https://github.com/ luksurious/faster-teaching/. All possible execution modes are configurable via commandline arguments which also allows a manual learning mode for diagnosis.\n3The precomputation times were sometimes significantly higher for the number game, depending on the number of quizzes it sampled in the first 20 steps. This resulted in some cases in over 5,000 paths to be precomputed and a runtime of more than 10 hours. In most cases though, the number of evaluated paths was below 10 and done in few minutes.\nReScience C 6.3 (#7) \u2013 Br\u00fcckner and Nioche 2020 18\nAll simulations were executed on Ubuntu 18.04 with Python 3.7.5 on an Intel(R) Xeon(R) CPU E5-1650 v4 @ 3.60GHz and 32GB of RAM. To improve performance, the belief update is always performed separately for refinement and evidence activities as described before. Further, the planning algorithm is slightly improved to stop the iteration over possible observations if the current cost of the action is already higher than the currently known best value. To handle edge cases where a belief contains zero probability for all concepts (usually due to inconsistent responses), we reset the belief in the discrete models to the initial belief, similar to the particle depletion case in the particle filter implementation. The biggest difference to the original implementation is that we did not employ the limit on the computation times to 3s since we did not perform a user study.\n4 Discussion\n4.1 Results Wewere able to partially replicate the results reported by Rafferty et al. for the simulated learners. Indeed, the overall picture is similar but some results are different that lead to a different evaluation. We encountered two main differences in our replication. First, in the letter arithmetic task, the memoryless policy performed significantly better with the simulated memoryless learner. Intuitively this makes sense because it is matching the cognitive model between the planner and learner. Second, in the number game, the maximum information gain policy achieved comparable performance to the POMDP policies where it was previously reported to be greatly outperformed by them. Certainly, the randompolicieswere not good teachers. However, the policy based on the maximum information gain achieved comparable results to the other POMDP policies, while mostly failing in the original paper. Since it also employs the continuous belief model inside to trace the state of the learner, it would be interesting to see whether reducing the horizon of the other models leads to similar performance (with better run-\nReScience C 6.3 (#7) \u2013 Br\u00fcckner and Nioche 2020 19\ntimes) and if the maximum information gain policy can be further improved by increasing the horizon. Our results show that for a strong learner, the planning model is not very important as all achieve high performance. For a weaker learner, our simulations show that a policy with weaker learning assumptions fits best. The letter arithmetic does not allow to draw conclusions as all planning policies perform equally well. In the number game, the results are more differentiated but it is still not possible to draw clear conclusions about the suitability of a policy and whether they are superior to the maximum information gain policy. Even though the continuous policy was the only one to use a horizon of 3 in the number game which resulted in much higher computation times, its results for mismatching simulated learners were worse than in the original paper. This might be due to our implementation not stopping calculations after 3 seconds. If that is one reason it might imply that an improved search might actually be counter-productive in these cases.\n4.2 Learner models Our results provide extra information regarding the failure rates to evaluate the policies. The mastery time alone can be misleading as it might mask failure rates with cheap actions as it is the case with the continuous policy in the first task. From this analysis we found that the simulated memoryless learner often fails to learn the concept and as such it seems not a very plausible model for human concept learning. We deduce from the original paper that the human learners were able to mostly learn the concepts correctly in contrary. On the other side, the continuous policy most often led to failures in the learners, indicating that it is not well suited for different types of learners. This can also be seen from the types of actions sampled. In the letter arithmetic task, it converged to choose only quiz activities. This fails to give new evidence and correct mistakes, and resulted in the high failure rate for the memoryless learner. This is in line with the analysis by Rafferty et al. that the continuous policy overestimates the learning capabilities and might not discover a divergence between the belief and the true state. The two discrete models (memoryless model and model with memory) could be improved by assigning higher transition probabilities to states that are closer to the current state. For example, in the letter arithmetic task, the distance can be measured by the number of necessary pairwise changes to move from one state to another. We argue that this also better reflects human learning. Further, the discrete model with memory assumes a perfect memory. It seems plausible that this is not always the case for human learners and might be too strong of an assumption. However, if different memory state possibilities were integrated into the model, the state space would increase significantly, reducing its tractability. Nevertheless, it could be a reason for the high transition noise determined for this learner model in the letter arithmetic task.\n4.3 Framework One key to implementing the models efficiently is to not use the explicit Bayesian belief update equation but instead treat the belief update separately for inferring the learner s\u0313 previous state and for calculating transitions based on new evidence, mainly in the case of feedback activities. The formulation as a POMDP could be improved by taking this into account or employing different action types that fit the structure better. Defining the teaching goal and associated rewards or costs is a critical part in this framework. Focusing on shortest time and associating the average time to process an action type nearly eliminated one of the teaching types (feedback type), presumably due to the high costs associated. It was shown that the continuous model converges to the cheapest action type and the differences in costs between the first and second task resulted\nReScience C 6.3 (#7) \u2013 Br\u00fcckner and Nioche 2020 20\nin a very different behavior of the model. As the main planning decision, as stated in the original paper, can be seen as deciding between teaching new content (examples) versus inferring the learner s\u0313 state (quizzes), defining the cost function in the current way does not seem to facilitate this decision well enough. The estimation of leaf nodes requires more evaluation. The dimension \u03b1 in the formula needs to be scaled appropriately to the state space and corresponding changes in success probability from teaching actions. Otherwise, the forward search could lead to mainly choosing the cheapest action if the success probability of the correct concept is too small in comparison. It would be interesting to see if the estimation of leaf nodes correspond to the true values of the states in the experiments. Having a separate assessment phasewhose purpose is solely to determine if the teaching is finished, and not using the responses to tune the belief, falls outside of the POMDP formulation and appears counter-intuitive. This results in a few cases where the teacher is not able to determine the wrong hypothesis of the learner when the belief diverges from the state, and no quiz actions are planned to validate the belief. It would be interesting to see if integrating the assessment as a separate action type into the planning algorithm would solve this issue.\n5 Conclusion\nFormulating teaching as a POMDP is a useful approach that allows to use sophisticated planning algorithmswith cognitive learnermodels. While computational challenges remain for employing the proposedmethod in real-time settings, investigating challenges and trade-offs for real-world teaching problems (e.g., second language learning) are still necessary to fully understand the applicability of the formulation. Through this replication, we hope to facilitate research in this direction as currently employed heuristics in tutoring systems lack adaptability for which model-based systems are promising candidates.\n6 Acknowledgements\nWe would like to thank Anna Rafferty (the first author of the original paper) for answering our numerous questions and sharing her implementation with us.\n7 Author contributions\nAN and LB designed the replication. LB implemented the model, performed the subsequent analysis, and wrote the paper. AN supervised the process."}], "title": "[Re] Faster Teaching via POMDP Planning", "year": 2020}