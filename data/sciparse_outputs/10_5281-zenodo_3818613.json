{"abstractText": "The paper Unsupervised Scalable Representation Learning for Multivariate Time Series by [1] presents an unsupervised approach to learning representations for time series that can be used for subsequent classification. An encoder architecture with dilated causal convolutional blocks is trained to minimize a triplet loss. The triplet loss is based on the idea that a subseries, xpos, of a time series, xref , should be closer to xref in representation space than the representation of a randomly sampled time series from the dataset, xneg. This type of loss was first introduced by [2]. To evaluate the strength of the learned representations, an SVM classifier is trained using the labels corresponding to these representations. The evaluation is performed on 159 datasets, together constituting themain benchmarking datasets for time series. The first group of datasets, the UCR Archive [3], contains univariate, short (between 15 and 2844 time steps) series; the second group, theUEAdataset [4], containsmultivariate time series of up to 1345 dimensions and varying sample lengths. The last dataset, the Individual Household Electric Power Consumption (IHEPC) dataset from the UCI Machine Learning Repository [5], is a single seven-dimensional time series with more than two million time steps. The authors\u02bc intention with using this variety of datasets is to show the universality of their method, which is one of their main claims. We have re-implemented the authors\u02bc method from scratch as best as we could by using only the paper as instruction. It should be noted that a code repository for the article was made public by the authors, but this was not employed for the purpose of investigating the replicability of the work starting from scratch. This means that we adhere to the Replication track of the Reproducibility challenge for NeurIPS 2019. Our implementation, as well as the authors ,\u0313 was made using the Pytorch [6] library and can be found at https://github.com/lilfelix/reproducibility_NeurIPS19. The main motivation to do a full replication study, avoiding the use of readily provided code, is to better be able to detect if there are parts of the implementation that are crucial for the results, but not presented as such in the original paper. This risk exists generally in machine learning [7], and has in particular been brought to light for deep learning (e.g. [8], [9]), due to the latter s\u0313 typically vast number of hyperparameters to tune.", "authors": [{"affiliations": [], "name": "Felix Liljefors"}, {"affiliations": [], "name": "Moein Sorkhei"}, {"affiliations": [], "name": "Sofia Broom\u00e9"}, {"affiliations": [], "name": "Koustuv Sinha"}], "id": "SP:dbb928d5a013f7898b585c787f48d43a529e78ac", "references": [{"authors": ["J. Franceschi", "A. Dieuleveut", "M. Jaggi"], "title": "Unsupervised Scalable Representation Learning for Multivariate Time Series.", "venue": "CoRR abs/1901.10738", "year": 2019}, {"authors": ["F. Schroff", "D. Kalenichenko", "J. Philbin"], "title": "Facenet: A unified embedding for face recognition and clustering.", "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition", "year": 2015}, {"authors": ["H.A. Dau"], "title": "The UCR Time Series Classification Archive", "venue": "https://www.cs.ucr.edu/~eamonn/time_series_ data_2018/", "year": 2018}, {"authors": ["A. Bagnall", "H.A. Dau", "J. Lines", "M. Flynn", "J. Large", "A. Bostrom", "P. Southam", "E. Keogh"], "title": "The UEA multivariate time series classification archive, 2018.", "year": 2018}, {"authors": ["A. Paszke", "S. Gross", "S. Chintala", "G. Chanan", "E. Yang", "Z. DeVito", "Z. Lin", "A. Desmaison", "L. Antiga", "A. Lerer"], "title": "Automatic Differentiation in PyTorch.", "venue": "NIPS Autodiff Workshop", "year": 2017}, {"authors": ["Z.C. Lipton", "J. Steinhardt"], "title": "Troubling Trends in Machine Learning Scholarship.", "venue": "Queue 17.1 (Feb", "year": 2019}, {"authors": ["P. Henderson", "R. Islam", "P. Bachman", "J. Pineau", "D. Precup", "D. Meger"], "title": "Deep Reinforcement Learning that Matters. cite arxiv:1709.06560Comment: Accepted to the Thirthy-Second", "venue": "AAAI Conference On Artificial Intelligence (AAAI),", "year": 2017}, {"authors": ["G. Melis", "C. Dyer", "P. Blunsom"], "title": "On the State of the Art of Evaluation in Neural Language Models.", "year": 2017}, {"authors": ["F. Pedregosa"], "title": "Scikit-learn: Machine Learning in Python.", "venue": "Journal of Machine Learning Research", "year": 2011}, {"authors": ["P. Malhotra", "V. TV", "L. Vig", "P. Agarwal", "G. Shroff"], "title": "TimeNet: Pre-trained deep recurrent neural network for time series classification.", "year": 2017}, {"authors": ["L. Wu", "I.E.-H. Yen", "J. Yi", "F. Xu", "Q. Lei", "andM.Witbrock"], "title": "RandomWarping Series: A RandomFeaturesMethod for Time-Series Embedding.", "venue": "In: International Conference on Artificial Intelligence and Statistics", "year": 2018}, {"authors": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "title": "DeepResidual Learning for ImageRecognition.", "year": 2015}, {"authors": ["Q. Lei", "J. Yi", "R. Vaculin", "L. Wu", "I.S. Dhillon"], "title": "Similarity preserving representation learning for time series analysis.", "year": 2017}, {"authors": ["J. Lines", "S. Taylor", "A. Bagnall"], "title": "HIVE-COTE: The Hierarchical Vote Collective of Transformation-Based Ensembles for Time Series Classification.", "venue": "IEEE 16th International Conference on Data Mining (ICDM). Dec", "year": 2016}, {"authors": ["A. Bostrom", "A. Bagnall"], "title": "Binary Shapelet Transform for Multiclass Time Series Classification.", "venue": "Big Data Analytics and Knowledge Discovery. Ed. by S. Madria and T. Hara. Cham: Springer International Publishing,", "year": 2015}, {"authors": ["P. Sch\u00e4fer"], "title": "The BOSS is Concerned with Time Series Classification in the Presence of Noise.", "venue": "Data Min. Knowl. Discov", "year": 2015}, {"authors": ["J. Lines", "A. Bagnall"], "title": "Time series classification with ensembles of elastic distance measures.", "venue": "Data Mining and Knowledge Discovery", "year": 2015}, {"authors": ["H.I. Fawaz", "G. Forestier", "J. Weber", "L. Idoumghar", "P.-A. Muller"], "title": "Deep learning for time series classification: a review.", "venue": "Data Mining and Knowledge Discovery", "year": 2019}], "sections": [{"text": "R E S C I E N C E C Replication / NeurIPS 2019 Reproducibility Challenge\n[Re] Unsupervised Scalable Representation Learning for"}, {"heading": "Multivariate Time Series", "text": ""}, {"heading": "Felix Liljefors1,\u2020, ID , Moein Sorkhei1,\u2020, ID , and Sofia Broom\u00e91,\u2020, ID", "text": "1KTH Royal Institute of Technology, Stockholm, Sweden \u2013 \u2020Equal contribution\nEdited by Koustuv Sinha ID\nReviewed by Anonymous Reviewers\nReceived 15 February 2020\nPublished 21 May 2020\nDOI 10.5281/zenodo.3818613\n1 Introduction\nThe paper Unsupervised Scalable Representation Learning for Multivariate Time Series by [1] presents an unsupervised approach to learning representations for time series that can be used for subsequent classification. An encoder architecture with dilated causal convolutional blocks is trained to minimize a triplet loss. The triplet loss is based on the idea that a subseries, xpos, of a time series, xref , should be closer to xref in representation space than the representation of a randomly sampled time series from the dataset, xneg. This type of loss was first introduced by [2]. To evaluate the strength of the learned representations, an SVM classifier is trained using the labels corresponding to these representations. The evaluation is performed on 159 datasets, together constituting themain benchmarking datasets for time series. The first group of datasets, the UCR Archive [3], contains univariate, short (between 15 and 2844 time steps) series; the second group, theUEAdataset [4], containsmultivariate time series of up to 1345 dimensions and varying sample lengths. The last dataset, the Individual Household Electric Power Consumption (IHEPC) dataset from the UCI Machine Learning Repository [5], is a single seven-dimensional time series with more than two million time steps. The authors\u02bc intention with using this variety of datasets is to show the universality of their method, which is one of their main claims. We have re-implemented the authors\u02bc method from scratch as best as we could by using only the paper as instruction. It should be noted that a code repository for the article was made public by the authors, but this was not employed for the purpose of investigating the replicability of the work starting from scratch. This means that we adhere to the Replication track of the Reproducibility challenge for NeurIPS 2019. Our implementation, as well as the authors ,\u0313 was made using the Pytorch [6] library and can be found at https://github.com/lilfelix/reproducibility_NeurIPS19. The main motivation to do a full replication study, avoiding the use of readily provided code, is to better be able to detect if there are parts of the implementation that are crucial for the results, but not presented as such in the original paper. This risk exists generally in machine learning [7], and has in particular been brought to light for deep learning (e.g. [8], [9]), due to the latter s\u0313 typically vast number of hyperparameters to tune.\n1.1 Target questions In this report, we mainly address the following questions.\nCopyright \u00a9 2020 F. Liljefors, M. Sorkhei and S. Broom\u00e9, released under a Creative Commons Attribution 4.0 International license. Correspondence should be addressed to Felix Liljefors (felixlil@kth.se) The authors have declared that no competing interests exist. Code is available at https://github.com/lilfelix/reproducibility_NeurIPS19. \u2013 SWH swh:1:dir:87ee9c9e46eff652b568a19f4d4af1a1663cd55a. Open peer review is available at https://openreview.net/forum?id=HyxQr65z6S.\nReScience C 6.2 (#6) \u2013 Liljefors, Sorkhei and Broom\u00e9 2020 1\n\u2022 Can the classification and regression results be reproduced using only the article s\u0313 description of the method?\n\u2022 Can we reproduce the claimed transferability of the method and its proposed success on sparsely labeled datasets?\n\u2022 Which missing instructions for the experiments might, if relevant, have affected the results?\n\u2022 Which \u02bchidden\u02bc assumptions, that mattered for the results, were made by the authors?\n1.2 Additional contributions We present results for the authors\u02bc method on three additional datasets from the UCR Archive: DodgerLoopDay, DodgerLoopGame and DodgerLoopWekend. These results can be found in the supplementary material. The rest of the article is organized as follows: Section 2 presents the implementation details and the results, in Section 3 we discuss the findings, and Section 4 is dedicated to the conclusions.\n2 Experimental methodology and implementation details\nIn Sections 2.1-2.5, we will group the reproducibility experiments into the three dataset groups (UCR,UEA and IHEPC). First, wewill describe our general experimentalmethodology for this study and some assumptions not mentioned in the original article that were necessary to make for all three groups of datasets. A strength of the original paper is that experimental hyperparameters are listed to a greater detail than is typically done in deep learning papers. Nevertheless, there are a number of design choices of importance for the implementation that were not mentioned in the article. We implemented Algorithm 1 from the paper (described in pseudo-code), to sample three subseries1 xref , xpos and xneg, used to compute the triplet loss. This algorithm is described by the authors as generally applicable to all datasets. However, they also mention a variation of Algorithm 1, which they claim speeds up the training time for datasets with fixed length time series, yet produces no noticeable difference in the computed loss. We did not implement this variation for fixed length time series, but instead used Algorithm 1 for all experiments. In the case of multivariate datasets, we assume that the length of a subseries, e.g. size(xref ), is the same across all dimensions of the time series being sampled from. In other words, when sampling xref from some d-dimensional time series y, size(xiref ) is the same for all i \u2208 [1, d]. Notably, except for IHEPC, the authors state that they have used a batch size of 10 throughout their experiments. Mini-batch training requires sequence padding when there are samples of different lengths. This is the case for these experiments since subseries are explicitly sampled at different lengths from the dataset (see Algorithm 1 in [1]). However, there is no mention of how this padding is carried out in the article. For our experiments, we zero-pad the samples to the maximal sample length for each batch. Another training detail not made explicit in the article is whether the representations used for classification should be obtained from full samples of the datasets (i.e. of length T , if T is the number of time steps for one sample), or rather from randomly sampled subseries of the samples (i.e. of length T \u2032, where T \u2032 \u2208 [1, T ]). For our experiments, we used the full samples (length T ) in order to use the training setmaximally when training the classifier.\n1A subseries here is a contiguous sub-sequence of a time series.\nReScience C 6.2 (#6) \u2013 Liljefors, Sorkhei and Broom\u00e9 2020 2\nRegarding the SVM training, a hyperparameter optimization is performed for theweighting C of the error term using cross-validation across the training set. The authors indicate that they avoided this cross-validation for datasets with a small number of training samples or datasets with few training samples per class. In lack of an exact list of which datasets they refer to, we avoided the hyperparameter search only for the datasets that fell below the size limit set by the cross-validation tool of scikit-learn [10].\nReScience C 6.2 (#6) \u2013 Liljefors, Sorkhei and Broom\u00e9 2020 3\n2.1 The UCR Time Series Classification The UCR Archive contains 128 univariate datasets. Each dataset consists of training and test time series which may have different lengths and missing values. The paper s\u0313 authors decided to not use the three datasets withmissing values (DodgerLoopDay, DodgerLoopGame and DodgerLoopWekend). However, the UCR Archive contains modified versions of these datasets with linearly interpolated values that we used. Our motivation for this is that the UCR Archive briefing document recommends that all available datasets are used, to allow for comprehensive comparisons and avoid cherry-picking results. For the task of time series classification on the UCR Archive datasets, the authors consider other state-of-the art methods, some of which are based on neural networks. More specifically, they compare their method to neural network methods, both unsupervised [11, 12] and supervised [13], and non-neural network methods, both unsupervised [14] and supervised [15, 16, 17, 18]. The use of ResNet as a baseline in the paper is motivated by referencing an extensive review made by [19], who found it to be the best supervised neural network method for time series classification. In Table 1 of the paper, the authors present their accuracy scores on 6 datasets from the UCR Archive, together with baseline2 accuracy scores from non-neural network models (both supervised and unsupervised). In a similar fashion, Table 1 in this report presents our replicated accuracy scores for the same datasets, together with the original results from the paper for comparison. We also introduce a comparison of our replicated scores and the authors\u02bc against neural network models in Table 2. Table 1 and 2 present the highest accuracy for each dataset, across all values of K \u2208 {1, 2, 5, 10}. Results for all values of K and all datasets can be found in the supplementary material. Further, these tables contain accuracy scores for three additional UCR datasets (not presented in the original paper s\u0313 Table 1), for which we also have baseline scores of both RWS and TimeNet (see Table 2). These are presented below the dashed line, and provide a comparison between the non-neural network scores in Table 1 and the neural network scores in Table 2.\ndling of missing values was not described in the paper, we chose to linearly interpolate these values before training the encoder. For the four datasets with the longest samples (EigenWorms, EthanolConcentration, MotorImagery and StandWalkJump), we needed to adjust the batch size to below 10 in order to fit the GPU memory. These adjustments can be seen in Table 3.\nThe authors compare their scores on the UEA datasets to that of DTWD. DTWD, an extension of DTW, is the best baseline known in the multivariate setting, according to the paper. In Table 4, we present our best achieved accuracy (taken among results using different values of K) next to the authors\u02bc scores, along with the scores of DTWD for a random selection of UEA datasets. As can be seen, our best scores closely match the best scores of the authors for these datasets. Our full scores on all theUEAdatasets, except InsectWingBeat, can be found in the supplementarymaterial. InsectWingBeat, with its large number of training samples, was unfeasible for our time horizon to train the SVM with when using scikit-learn, see the discussion in Section 3. For K = 10, the training would have taken more than 12 hours, so we decided to not include this dataset.\n2.5 The IHEPC experiment The Individual Household Electric Power Consumption (IHEPC) is a seven-dimensional time series (not including the temporal dimension) containing over two million measurements. These measurements were split by the authors into train and test sets of 500,000 and \u223c1,500,000 values, respectively. The paper does not mention the multivariate property of the IHEPC dataset and describes that the data was normalized in the same way as the univariate UCR datasets (multivariate normalization is described separately). Further, the plotted result for the IHEPC experiment (Figure 5 in the paper) shows only one feature (active power consumption), leading us to believe that only this feature was used. This belief was also strengthened when we trained the encoder on a single time series of length 500,000measurements, since using all seven features simply requires more GPU memory than the authors specified in the report (16 GB). Lastly, the dataset has missing values which we chose to linearly interpolate, as the paper does not specify how these were handled. The paper mentions that the encoder took no more than a few hours to train on a single Nvidia Tesla P100 GPU. For us, it took 10.5 hours to complete the 400 optimization steps on a Tesla T4 GPU, which has the samememory capacity and, notably, a higher computing performance than the P100. However, it is noteworthy that the triplet loss stopped improving after\u223c50 optimization steps (reaching a loss of 4.5 from an initial loss of 800). After 50 optimization steps, the loss fluctuated up and down, but never went lower than 4.5. The information regarding how the encoder was used for a regression task after being trained on IHEPC was quite sparse in the paper, which led us to spend some time determining the most feasible procedure. What we concluded was the following. The encoder is first trained for 400 optimization steps on a single univariate long time series (500,000measurements), using the featureGlobal active power. Next, the trained encoder is used to generate two tuples of representations which we denote {Rtrainday ,Rtestday } and {Rtrainquarter,Rtestquarter}. To generate Rtrainday , the train set is split into subseries of length 1440, corresponding to the number of measurements in a day, extracted as sliding windows with stride one (a day-longwindow). Each day-longwindow is then passed through the encoder to generate a representation. Next a label for each representation is computed as the difference of the mean measurements in the previous and subsequent day window, producing a set of labels Ltrainday for Rtrainday . Finally, a linear regressor is trained on {Rtrainday ,Ltrainday }, and tested on {Rtestday ,Ltestday }, using minimization of MSE as objective. An analogous experiment can also be performed using subseries of length 12 \u00b7 7 \u00b7 1440 corresponding to a quarter-long window. This experiment thus involves training and testing another regressor on {Rtrainquarter,Ltrainquarter}, {Rtestquarter,Ltestquarter}. The only description in the paper concerning the linear regressors come from the following quote:\nWe compare linear regressors, trained using gradient descent, to minimize themean squared error between the prediction and the target, applied either on the raw time series or on the previously computed representations.\nTherefore, wemade the following assumptions. We used the same optimizer (including its hyperparameters, such as learning rate) for the regressors as we did for the encoder, i.e. Adam [20]. Further, we trained the regressors for 2000 optimization steps (the default number of optimization steps specified in the paper). Since the regressor is trained on representations of the encoder, we had to encode all windows (either day or quarter) of both the train and test time series (\u223c twomillionmeasurements in total). Tomeasure the efficiency of the representations, the authors compared the regressors trained on representations with regressors trained on the raw values of the IHEPC dataset. It is noteworthy that the train and test datasets consisting of representations are orders of\nReScience C 6.2 (#6) \u2013 Liljefors, Sorkhei and Broom\u00e9 2020 7\nmagnitude smaller in size than the raw-valued datasets. The number of scalars representing an encoded window (either day or quarter) is 80, which is the output dimension of the encoder. In contrast, the length of each window of raw values is 1440 and 7 \u00b7 12 \u00b7 1440 for day and quarter windows, respectively. We replicated the experiment by training and testing regressors on both the representations and the raw values, and the results are summarized in Table 5. We observed similar time efficiency when evaluating the regressors as the authors reported. As explained by the authors in their original report, the large discrepancy in the wall time when using representations versus raw values on the quarter task is due to raw-valued windows having much larger size than their corresponding representations. For the raw regressors, we observed test MSEs similar to those reported by the authors. For the regressors trained on the representations however, we saw the training loss drop significantly after only a few optimization steps. These regressors also produced much lower MSEs compared to the those trained on raw values, which was unexpected. The authors didn\u02bct report such a discrepancy in test MSE between representations and raw values, and we aren\u02bct sure what caused this result in our experiment. In summary, our results are not completely aligned with the numbers reported by the authors, yet we saw that the simple linear regressor was more successful in predicting the labels when trained on windows with compact representations.\n3 Discussion of findings\nThemain advantage of the authors\u02bc method is that it is flexible with respect to sequence lengths and domains. Time series with different lengths and dimensionality can be embedded by the encoder architecture. In the bulk of the many experiments, our results closely match those of the authors, both in the univariate and multivariate datasets. In order to formally measure the difference between the results of our experiments and the authors\u02bc experiments, the difference between our scores and those of the authors was computed for each possible value of K, for each dataset in both the UCR and UEA Archives. Then we computed the average difference by taking the average of the differences for each possible value ofK (including the FordA score) in each dataset. We observe that the average difference is less than 5% in 68%, less than 8% in 80% and less than 10% in 88% of the UCR datasets. The corresponding numbers for UEA are: less than 5% in 69%, less than 8% in 76% and less than 10% in 76% of the UEA datasets. An offset is expected since the experiments were only run once for each dataset, as in the article. There is stochasticity in the sampling of the sub-sequences during the training of the encoder. These differences seem to be within the expected range and indicate that we have achieved quite similar results without using their implementation. It should be noted that our focus is to compare our results with the authors\u02bc results only to ensure reproducibility of the experiments, rather than comparing our scores with the state-of-the-art methods. Since we have achieved very similar results to those of the authors, we believe that the distribution of rankings of different methods also generally holds true in our experiments. Thus, for the first target question we conclude that the classification results can be reproduced. Regarding the main advantages of the authors\u02bc model over other state-of-the-art supervisedmethods, we observed in the FordA experiment that the representations generated by the encoder trained on the FordA dataset can be applied for classification in other datasets and achieve acceptable results. This provides support to the authors\u02bc claim that the representations generated by such an encoder are transferable and can be applied to classification tasks for other datasets. Furthermore, we observed in our experiment with the ResNet model that in the case of datasets with limited labeled samples, the authors\u02bc unsupervised method can perform significantly better than the ResNet model. This provides support for the claim of performing well with sparse labeled data. The two last target questions concernmissing instructions and hidden assumptions and whether these have a crucial influence on the results. We were able to reproduce the experiments conducted with the UCR and UEA datasets using the paper s\u0313 instructions, while the long time series experiment (IHEPC dataset) proved more difficult. The long time series experiment lacked many details in its methodology, which led us to make multiple assumptions in order to produce results. For the UCR and UEA experiments, the authors motivated the use of SVMs in the classification step by claiming it allows for efficient training (a matter of minutes in most cases). This held true for the majority of the datasets in our replication study, but for the datasets with the largest number of samples, the training of SVMs took several hours. In the IHEPC experiment, we observed similar efficiency in terms of the wall times when evaluating the regressors on the test set, and we observed similar test errors when training the regressors on the raw values. For the regressors trained on the representations however, we observed lower test errors compared to the numbers reported by the authors. We also saw the training converging much faster with a significant drop in loss after a few optimization steps. Finally, the fact that we managed to reproduce most results indicates that any hidden assumptions made by the authors were not crucial in the end, unless we accidentally made the exact same set of assumptions, which does not seem likely.\nReScience C 6.2 (#6) \u2013 Liljefors, Sorkhei and Broom\u00e9 2020 9\n4 Conclusions\nIn this work, we have presented a replication study of the work by [1] and found that most of the results are replicable. We have reproduced almost a thousand3 (942) results from the original article and they largely follow the results obtained by the authors. Since our experiments were run with a set of additional assumptions, this speaks in favor of the robustness of the authors\u02bc method."}, {"heading": "Supplementary material for [Re] Unsupervised Representation Learning for Multivariate Timeseries", "text": "In this section, we present the full results of our experiments on the UCR and UEA Archives.\n5 UCR Archive Results\nThe following is our full results on the 30 UEA datasets. Note that we were not able to reproduce the SVM results for the InsectWingbeat dataset since it has a very large number of samples (30,000 training and 20,000 test samples), and we could not manage to train the SVM on it in a timely manner (training the SVM was taking more than a day using the scikit-learn library)."}], "title": "[Re] Unsupervised Scalable Representation Learning for Multivariate Time Series", "year": 2020}