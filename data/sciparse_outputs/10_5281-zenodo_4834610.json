{"abstractText": "The authors have introduced a novel method for unsupervised anomaly detection that utilises a newly introduced \u201cMemoryModule\u201d in their paper [1]. We validate the authors\u02bc claim that this helps improve performance by helping the network learn prototypical patterns, and uses the learnt memory to reduce the representation capacity of Convolutional Neural Networks. Further, we validate the efficacy of two losses introduced by the authors, \u201cSeparateness Loss\u201d and \u201cCompactness Loss\u201d presented to increase the discriminative power of the memory items and the deeply learned features. We test the efficacy with the help of t-SNE plots of the memory items.", "authors": [{"affiliations": [], "name": "Kevin Stephen"}, {"affiliations": [], "name": "Varun Menon"}], "id": "SP:be72106ef00936a0ddc86f65713ede0a04bfee1e", "references": [{"authors": ["H. Park", "J. Noh", "B. Ham"], "title": "Learning Memory-guided Normality for Anomaly Detection", "year": 2020}, {"authors": ["W. Luo", "W. Liu", "S. Gao"], "title": "A Revisit of Sparse Coding Based Anomaly Detection in Stacked RNN Framework.", "venue": "IEEE International Conference on Computer Vision (ICCV)", "year": 2017}, {"authors": ["K.Q. Weinberger", "J. Blitzer", "L.K. Saul"], "title": "Distance metric learning for large margin nearest neighbor classification.", "year": 2006}, {"authors": ["O. Ronneberger", "P. Fischer", "T. Brox"], "title": "U-Net: Convolutional Networks for Biomedical Image Segmentation", "year": 2015}, {"authors": ["D.P. Kingma", "M. Welling"], "title": "Auto-Encoding Variational Bayes", "year": 2014}, {"authors": ["H. Guo", "X. Wu", "N. Li", "R. Fu", "G. Liang", "W. Feng"], "title": "Anomaly detection and localization in crowded scenes using short-term trajectories.", "venue": "IEEE International Conference on Robotics and Biomimetics (ROBIO)", "year": 2013}, {"authors": ["C. Lu", "J. Shi", "J. Jia"], "title": "Abnormal Event Detection at 150 FPS in MATLAB.", "venue": "IEEE International Conference on Computer Vision", "year": 2013}], "sections": [{"text": "R E S C I E N C E C"}, {"heading": "Replication / ML Reproducibility Challenge 2020", "text": "[Re] Learning Memory Guided Normality for Anomaly"}, {"heading": "Detection", "text": "Kevin Stephen1, ID and Varun Menon2, ID 1Pune Institute of Computer Technology, Pune, India \u2013 2Courant Institute of Mathematical Sciences, New York University\nEdited by Koustuv Sinha\nReviewed by Anonymous Reviewers\nReceived 29 January 2021\nPublished 27 May 2021\nDOI 10.5281/zenodo.4834610"}, {"heading": "Reproducibility Summary", "text": ""}, {"heading": "Scope of Reproducibility", "text": "The authors have introduced a novel method for unsupervised anomaly detection that utilises a newly introduced \u201cMemoryModule\u201d in their paper [1]. We validate the authors\u02bc claim that this helps improve performance by helping the network learn prototypical patterns, and uses the learnt memory to reduce the representation capacity of Convolutional Neural Networks. Further, we validate the efficacy of two losses introduced by the authors, \u201cSeparateness Loss\u201d and \u201cCompactness Loss\u201d presented to increase the discriminative power of the memory items and the deeply learned features. We test the efficacy with the help of t-SNE plots of the memory items."}, {"heading": "Methodology", "text": "The authors provide a codebase available at https://github.com/cvlab-yonsei/MNADwith scripts for the Prediction task. We reused the available code to build scripts for the Reconstruction task and variants with and without memory. The completed codebase for all tasks available at https://github.com/alchemi5t/MNADrc."}, {"heading": "Results", "text": "We obtain results that are within a 3% range of the reported results for all datasets other than on the ShanghaiTech dataset[2]. The anomalous run stuck out since the \u201dNonMemory\u201d of the same run could scoremarkedly better than the \u201dMemory\u201d variant. This led us to investigate the behaviour of the memory module and found valuable insights which are presented in Section 5."}, {"heading": "What was easy", "text": "\u2022 The authors provide codes to run the Prediction task with memory.\n\u2022 The ideas presented in the paper were clear and easy to understand.\n\u2022 Thedatasetswere easily available, with the exceptionof the ShanghaiTechdataset[2].\nCopyright \u00a9 2021 K. Stephen and V. Menon, released under a Creative Commons Attribution 4.0 International license. Correspondence should be addressed to Varun Menon (vk2148@nyu.edu) The authors have declared that no competing interests exist. Code is available at https:/github.com/alchemi5t/MNADrc \u2013 DOI 10.5281/zenodo.4681515. \u2013 SWH 1:dir:120b52bc3dfe115b3329698a39ea71c7cd5862d1. Open peer review is available at https://openreview.net/forum?id=vvLWTXkJ2Zv.\nReScience C 7.2 (#11) \u2013 Stephen and Menon 2021 1"}, {"heading": "What was difficult", "text": "\u2022 After training the model on the ShanghaiTech dataset[2], we found that the \u201cMemory\u201d variants of the model had issues as described in Sections 4 and 5. We investigated the behaviour of thememorymodule and introduced additional supervision to solve the problem.\n\u2022 We required hyperparameter optimization to hit the reported scores.\n\u2022 The paper also lacked information as to which parts of the pipeline were actually to be credited for the improvement in performance, as discussed in Section 5.\nCommunication with original authors The codebase has scripts only for the Prediction task \u201dMemory\u201d variant of the benchmarks. We reused portions of the codebase to create the other variants. We then validated our modifications with the authors. We also communicated the discrepancies we saw in parts of the results and the authors asked us to wait until they update their repository with the missing scripts.\n1 Introduction\nThe paper that we have tried to reproduce attempts to detect anomalous events in a video sequence. Anomaly detection based on Convolutional Neural Networks (CNNs) usually leverage proxy tasks like reconstruction of input frames or prediction of future frames, i.e., trained only on the \u201dNormal\u201d scenes thereby having a hard time reconstructing anomalies. Conventionally, Peak Signal-To-Noise ratio (PSNR) is used to gauge if the input batch of images has anomalous frames. The authors have introduced a hybrid metric for Abnormality Score calculation, comprising a weighted sum of the PSNR value and L2 loss between the Queries and the Memory items. We utilise the term \u201dReconstruction\u201d to refer to the model and the term \u201dreconstruction\u201d to refer to the output generated by the model (s). The author s\u0313 primary contribution is the introduction of a \u201dMemory Module\u201d which is claimed tomemorize prototypical patterns of normal data, and thesememory items are used to reduce the representation capacity of the CNN which, in theory, decreases the quality of the output (Reconstruction or Prediction) when the input batch has anomalous frames, therebymaking it easier to classify input batches as anomaly or not. The authors also introduce two losses, namely, Compactness loss and Separateness loss which help increase the diversity and discriminative power of thememory items. Thefinal contribution is the introduction of Test-Time Updation of memory to help tune thememory items according to the normal scenes during inference.\n2 Scope of reproducibility\nWe have attempted to reproduce the reported scores of the \u201dMemory\u201d variant and \u201dNon Memory\u201d variant on all datasets for both the Prediction and Reconstruction tasks, alongside reproducing the scores for the ablation studies as well. The author s\u0313 central claim is that the \u201dmemory module\u201d helps the network learn prototypical patterns of normal scenes, and using the learnt memory items as \u201dnoise\u201d reduces the representation capacity of the CNN and makes it harder for the network to reconstruct/predict anomalous output thereby making it increasingly easier to classify anomalous input batch of images.\nReScience C 7.2 (#11) \u2013 Stephen and Menon 2021 2\nWe ran experiments to compare the performance of the Reconstruction and Prediction models with and without memory modules. While benchmarking these models we observed a few deviations in the reported results and reproduced results which lead us to experiments to get more clarity on the behaviour of the memory module, which we shall discuss in Section 4. We provide the hyperparameters with which we got the best results on the provided datasets. Their secondary claims revolve around the Compactness loss and Separateness loss, which they claim helps the network space out and the memory items and decreases the spread between similar deeply learned features. They also claim that the Test-Time updation of the memory items helps tune the memory items to the normal scenes during inference time, subsequently classifying anomalies more accurately. We attempted to reproduce the ablation studies results which covers both the secondary claim and the Test-Time updation scheme claim. The claims we are testing:\n\u2022 \u201dMemorymodule\u201d helps thenetwork to learnprototypical patterns of normal scenes, and using learnt memory items to lessen the capacity of CNNs.\n\u2022 feature compactness and separateness losses to train the memory, ensuring the diversity and discriminative power of the memory items. improves performance.\n\u2022 update scheme of the memory, when both normal and abnormal samples exist at test time, improves performance.\n3 Methodology\n3.1 Model descriptions The authors present a novel algorithm for unsupervised anomaly detection that makes use of amemorymodule. Themodel architecture comprises of three parts: The encoder, the memorymodule and the decoder. Themodel either reconstructs or predicts a video frame with a combination of features from the encoder and the memory items rather than just the features from the encoder. The memory module consists of 10 items of 512 each. A \u201cread\u201d operation generates a tensor of size 32*32*512 from the memory module which is then concatenated to the tensor obtained from the encoder. This obtained matrix is concatenated with the query features to generate a tensor of depth 1024, then passed to the decoder. The key idea here is that the memory item is used to generate a tensor using a weighted average, thus utilising all the memory items. This allows the model to learn diverse patterns. The network and memory module is trained only on normal frames. As a result, the cosine distance between the query and the relevant memory item is less and they are close to each other. On the contrary, since the network has not seen the anomalous frames during the training phase, thememory itemwill be far from the queries obtained. Since they are far, the Query-Memory pair will be sub-optimal thus resulting in poor reconstruction of the input frames. To read the items, the cosine similarity score is computed between each query qkt and all memory items pm, resulting in a 2-dimensional correlation map of size M \u00d7 K, A softmax function is then applied along a vertical direction, and matching probabilities wk,mt are obtained as shown in Equation 1.\nwk,mt = exp\n( (pm) T qkt ) \u2211M\nm\u2032=1 exp ( (pm\u2032) T qkt ) , (1) For each query qkt , the memory items are read by a weighted average of the items pm with corresponding weights wk,mt and obtain p\u0302kt as shown in Equation 2. The p\u0302kt values\nReScience C 7.2 (#11) \u2013 Stephen and Menon 2021 3\ncomputed from each querymakes up the featuremapwhich is then concatenated to the features obtained from the Encoder depth wise.\np\u0302kt = M\u2211 m\u2032=1 wk,m \u2032 t pm\u2032 , (2)\nThe \u201dupdate\u201d operation updates the memory items as it iterates through the dataset and learns the pattern of normal features. To update the memory, for each item, all queries that are closest to thememory item are chosen using thematching probabilities in Equation 1. The set of indices for the corresponding queries form-th item in thememory are denoted byUmt . The item using the queries indexed byUmt is updated as shown in Equation 3. f(\u00b7) is the L2 norm.\npm \u2190 f pm + \u2211 k\u2208Umt v\u2032 k,m t q k t  , (3) A weighted average of the queries is used. The matching probabilities are computed by applying the softmax function to the correlation map of M \u00d7 K along a horizontal direction as shown in Equation 4.\nvk,mt = exp\n( (pm) T qkt ) \u2211K\nk\u2032=1 exp ( (pm) T qk \u2032 t ) (4) The authors choose to use PSNR as a part of the metric to check the output quality from the decoder. Along with this, the authors utilise an L2 distance loss calculated between the queries and its corresponding nearest memory item. The PSNR and L2 distance is coupled together as shown in Equation 5.\nSt = \u03bb ( 1\u2212 g ( P ( I\u0302t, It ))) + (1\u2212 \u03bb) g (D (qt, p)) , (5)\nThe authors also introduce a new pair of losses, Compactness loss and Separateness loss. The Separateness loss minimizes the distance between each feature and its nearest item, while maximising the discrepancy between the feature and the second nearest one, similar to Triplet loss [3]. The feature Compactness loss maps the features of a normal video frame to the nearest item in the memory and forces them to be close. These losses coupled, spread the individual items in the memory and as a result enhance the discriminative power of the features and the memory items. The architecture used in Reconstruction task \u201dMemory\u201d variant is similar to that of the Prediction task \u201dMemory\u201d variant with the exception that it does not have skip connections. The Prediction \u201dNon Memory\u201d variant resemble a U-Net [4] and the Reconstruction models resemble a simple Autoencoder [5]. The authors haveprovided a codebase available at https://github.com/cvlab-yonsei/MNAD. However, the codebase has only the codes for the \u201dMemory\u201d variant of the Prediction task1. We reused a major portion of the available code to build scripts for the Reconstruction task and variants with and without memory. We validated the new scripts and changes by communicating with the authors over email to ensure there is no gap between their intention and our developed codebase. The completed codebase for all tasks available at https://bit.ly/3s3nTIR. The available code for the \u201cPrediction\u201d task could be run on the Ped2[6] and CUHK[7] datasets without any changes. However, the ShanghaiTech dataset[2] required us to generate the frames from the provided videos. We utilise theVideoCapture class available on theOpenCV library to generate 2,74,515 frames. We make the following changes in the available code to create the scripts for the Reconstruction task:\n1Post Submission for review, the authors updated their GitHub repository to include the scripts for the \u201dMemory\u201d variant of the Reconstruction task. Discrepancies in this code are available on our code repository.\nReScience C 7.2 (#11) \u2013 Stephen and Menon 2021 4\n\u2022 We tweak the code to accept one input and generate one output of the same frame by changing the time step parameter.\n\u2022 We remove the skip connections present in the architecture for the Prediction task.\nWe make the following changes for the \u201dNon Memory\u201d variants:\n\u2022 The decoder needs only the query features and hence we change the first layer of the decoder to accept and create a 512 size vector.\n\u2022 For the \u201dNonMemory\u201d variants of themodels, we cannot calculate the L2 distance and only the PSNR value is to be used for Abnormality Score calculation in the evaluation script.\n3.2 Datasets We test and report results on all datasets that the authors have reported scores on. The three datasets used areUCSDPed2[6], CUHKAvenue[7] and the ShanghaiTech[2] anomaly detection dataset. All three datasets are readily available in the author s\u0313 repository. However, only the UCSD Ped2 dataset[6] and CUHK Avenue dataset [7] are available directly in the format required by the authors\u02bc code. The ShanghaiTech dataset[2] contains only videos which we had to split into frames using OpenCV to generate 274,515 frames. We have provided the script used to save the frames on the GitHub repository.\n3.3 Hyperparameters We experimented heavily with the \u03bb hyperparameter to achieve the reported scores. We could not find a fixed trend in its behaviour across the three datasets. We have provided the best results after tweaking \u03bb and the results obtained as per the author s\u0313 recommendation of hyperparameters as discussed in Section 4.1. As for the other hyperparameters, we found that the recommended values worked well and gave us similar results to the paper with the exception of the ShanghaiTech dataset[2].\n3.4 Experimental setup Weusemultiple hardware set-ups for our experiments, namely, vast.ai instances, Google Colab and our personal systems. We used NVidia Tesla T4 and P100 from Google Colab, along with 1080 Ti, 2070, 2070 SUPER, 2080 Ti rented on vast.ai. To maintain uniformity, we benchmark all our scores on a 2080 Ti. We recorded all our training metrics on \u201cwandb\u201d2.\n3.5 Computational requirements For our experiments and benchmarking, we use a rented GPU from vast.ai. All our scores are reported on a 2080Ti with 8 GB vRAM. The training times are as shown in Table 1. The inference times andmodel parameters for the Prediction task \u201dMemory\u201d variant and Reconstruction task \u201dMemory\u201d variant are as shown in Table 2 and Table 3 respectively.\n2wandb logs are available in the repository. 3Denotes the Model size and Memory tensor size\nReScience C 7.2 (#11) \u2013 Stephen and Menon 2021 5\nParameter Category Parameter Value Storage Saved Weights Size 62.7 MB + 20.8 kB 3\nTime (GPU) Inference 0.012s\n4.1 Benchmarking Results The results obtained on the hyperparameter values provided by the authors are provided in Table 4.\nThe best obtained results are provided along with the hyperparameter values in Table 5. We evaluated the models saved at every 5th epoch and tweaked values of \u03bb and Batch Size.\n4.2 Ablation Results In our ablation study experiments, we validate the following:\n\u2022 We test the efficacy of the Separateness Loss, Compactness Loss and Test-Time updation for both theReconstruction andPrediction tasks on theUCSDPed2dataset[6]. The results are as shown in Table 6 and Table 7.\n\u2022 We compare the distribution of query features, learnt with and without the Separateness Loss on the UCSD Ped2 dataset[6] as shown in Figure 1.\n4value of \u03bb used is 1 due to issues discussed in Section 5.\nReScience C 7.2 (#11) \u2013 Stephen and Menon 2021 7\nSeparateness Loss Compactness Loss Test Time Updation \u03bb AUC N N Y 0.87 84.03% N Y Y 1 88.99% Y N Y 0.8 87.24% Y Y N 0.8 87.72% Y Y Y 0.7 87.34%\non the Reconstruction task.\n\u2022 Memory Distribution plots for better understanding the utilisation of memory: The model architecture offers 10memory items of size 512 each. During our evaluation of the ShanghaiTech dataset[2], we found that the Abnormality Score vector had a large number of NaNs, due to which we could not evaluate the model. We further investigated the Prediction \u201dMemory\u201d variant of the model in order to fix this bottleneck. To validate our doubts and concerns, we plotted the distribution of the memory items vs the deeply learned features and observed that the distribution was lopsided and only some of the items were being utilised. We saw this behaviour repeat across the three datasets however the worst affected was ShanghaiTech[2] where only one memory item was being utilised (See Figure 4a). Ped2[6] and CUHK[7] utilised 6 and 9 memory items respectively but the distribution was still lopsided as it seems that some memory items are utilised more than the others as shown in Figure 2. The abnormality score is a proportional summation of the PSNR and L2 distance between theDeeply learned feature and the closestmemory item. In case of ShanghaiTech[2], onememory itemwas responsible for all of the features anddue tominmax normalisation used in the abnormality score, theminimumandmaximumL2 would be computed to the exact same value which meant that the normalization process would have to divide by 0, which resulted in NaNs. We found that other developers had similar issues as shown in this issue. https://github.com/cvlabyonsei/MNAD/issues/6 This problem resulted in Evaluation failing on the ShanghaiTech dataset[2] and returning NaNs in the final anomaly score. A temporary work around was setting the \u03bb value to 1, as a result weighting only the reconstruction or PSNR scores and not using the L2 loss between the query and memory items for the Abnormality Score calculation. As a solution, we added a new supervision to force a uniform distribution across the memory items. We were able to reproduce the reported results on ShanghaiTech[2] without any issues, which previously failed. The methodology used and results are further discussed ahead.\n\u2022 Proposed memory distribution Supervision: We introduce a new supervision to ensure that all the memory items are utilised uniformly. We create a uniformly distributed correlation map of size 10 \u00d7 1024 which is used to supervise the distribution of Query features across the memory items using Mean Squared Error (MSE) Loss as shown in following the code snippet. loss_mse = torch.nn.MSELoss() softmax_score_query, softmax_score_memory = self.get_score(keys,\nquery) mem_dist = torch.argmax(softmax_score_memory,dim = 1)\nReScience C 7.2 (#11) \u2013 Stephen and Menon 2021 9\nmem_dist = F.one_hot(mem_dist, num_classes = 10) mem_dist = torch.sum(mem_dist, dim = 0) query_reshape = query.contiguous().view(batch_size*h*w, dims) mem_dist_loss = loss_mse(softmax_score_memory, self.ideal.float())\nFigure 3 is only a representation of the ProposedMemory distribution supervision. This gives a more uniform distribution of query features across the memory items as shown in Figure 4b.\nthe scores reported by the authors onboth the Prediction andReconstruction tasks. The obtained results are as shown in Table 8. We observed that our obtained scores on the Reconstruction task was more than the Prediction task. The fact that the Reconstruction model performs better than the Prediction model here is deeply concerning as the Prediction model also has Temporal informationwhichhelps recognise Temporal and Spatio-Temporal anomalies while the Reconstruction model does not. Thus, the Reconstruction model completely misses out on Temporal anomalies but is still able to outperform the Prediction model. A possible explanation for the same could be the fact that the ShanghaiTech dataset[2] is too complex for the memory mechanism to be utilised as intended. However, we can say with surety that the \u201dMemory\u201d module does not seem to be helping the performance of the model on the ShanghaiTech dataset[2]. While forcing the model to use each memory item equally is not an ideal solution, it helps the model work without errors and shows the need for a better and dynamic learning scheme for the memory items.\nReScience C 7.2 (#11) \u2013 Stephen and Menon 2021 11\n\u2022 Results with and without Test-Time updation: The authors claim that Test-Time updation of the memory helps improve the performance of the model. However, we found contrary results for the Reconstruction with Memory task on ShanghaiTech[2] and Ped2[6]. For the Prediction task, we empirically found that the 1st skip connection is heavily weighted and is highly responsible for the reconstruction quality. As a result, the intermediate and memory items are not involved in the Prediction task. We passed zeros, ones and random tensors for skip 2,3 and the deeply learnt features from the encoder and the output was optically unchanged for this experiment. As a result, the memory does not seem to affect the output quality much for complex datasets like ShanghaiTech[2]. For Reconstruction task, we can clearly see the drop in performance \u201cwith Test-Time-Updation\u201d in ShanghaiTech[2] as there are no skip connections to overcompensate. The results on ShanghaiTech[2] are as shown in Table 9.\nModel AUC Reconstruction w/ Memory w/ Proposed Supervision w/o Test-Time Updation 71.07% Reconstruction w/ Memory w/o Proposed Supervision w/ Test-Time Updation 64.14%\nPrediction w/ Memory w/ Proposed Supervision w/o Test-Time Updation 70.40% Prediction w/ Memory w/ Proposed Supervision w/ Test-Time Updation 70.35%\n6 Discussion\nOur results as shown in Section 4.1, show that the memory models do perform better than the ones without for Ped2[6] and CUHK[7] datasets. However, the \u201dMemory\u201d variants achieve lower AUC scores than the \u201dNon-Memory\u201d variants on the ShanghaiTech dataset[2] (Table 4). Our Ablation study experiments on Separateness Loss, Compactness Loss and Test-Time updation do not follow the same trend as the authors, shown in Table 6 and Table 7. However, our generated t-SNE plots (Figure 5) show that the Separateness Loss and Compactness Loss help increase the discriminative power of the Memory items by spacing the Query Features out. Our results in Table 9 show that TestTime updation fails on complex datasets like ShanghaiTech[2]. As shown in Section 5, the memory distribution is lopsided. Utilising only one memory item results in similar reconstruction for both anomalous and normal frames. We provide a temporary solution to ensure that the model works smoothly. Our newly introduced Supervision is described in Section 5 and is backed with results (Table 8) and histograms to show the uniform distribution of features among the memory items as shown in Figure 4b . While most of our results corroborate with the claims made by the authors, we believe there are a number of changes that can be brought about to improve performance. As shown in Figure 4a, in Section 5, for a complex dataset like ShanghaiTech[2], only one memory item is being utilised. This is against the core idea of the paper, that is to reduce the representative power of CNNs. The utilisation of just one memory item means that irrespective of the features being close or far, the same tensor obtained after reading the memory is concatenated to the features obtained from the encoder. As a result, there is no difference in reconstruction quality for anomalous features as compared to the normal features. In Figure 10, we\nReScience C 7.2 (#11) \u2013 Stephen and Menon 2021 13\nillustrate how a number of query features correspond to a particular Memory item and how it is concatenated depending on the cosine distance from the query features.\nWhile our proposed supervision is not ideal, we were able to get this script running and were able to achieve the reported scores by the author as shown in Table 8 without any issues described in Section 5. Further, for ShanghaiTech[2], we found that the Reconstructionmodelswith our new Supervision seem to work better than the Prediction with Memory models. Given the fact that the Prediction task has Temporal information as well, it should be outperforming the Reconstruction task due to the correct classification of Temporal anomalies. Thus, the Reconstruction task outperforming the Prediction task shows that the model does not work as intended.\nReScience C 7.2 (#11) \u2013 Stephen and Menon 2021 14\nFinally, we show the difference between the reconstruction of Spatial, Temporal and Spatio-Temporal anomalies from both the Prediction and Reconstruction models as depicted using the Synthetic Dataset discussed in Section 5 and as shown in Figures 6, 7, 8 and 9.\n6.1 What was easy \u2022 Training and evaluation of the Prediction task \u201dMemory\u201d variant is straightforward with the codebase that the authors provide.\n\u2022 The codebase was clear enough for us to easily reuse it for implementing the missing variants discussed in the paper, namely the Prediction \u201dNon Memory\u201d variant and both the \u201dMemory\u201d and \u201dNon Memory\u201d variants of the Reconstruction task.\n\u2022 The ideas presented in the paper were clear and easy to understand.\n\u2022 All benchmarking datasets were readily available in the format required by the codebase, with the exception of the ShanghaiTech dataset[2].\n6.2 What was difficult \u2022 Training on the ShanghaiTech dataset[2] took 25 hrs, and once trained, we observed the \u201dMemory\u201d variants had clear issues (discussed in Section 5) whichmade it difficult to hit the reported scores. We further investigated the behaviour of the memorymodule andhad to introduce additional supervision to helpwith the problems (Section 5).\n\u2022 We found that the hyperparameters specified by the author were not suitable for a number of tasks and thus needed hyperparameter optimization to hit the reported scores.\n\u2022 Therewas also awant of information onwhichparts of the pipelinewere to be credited for the results. The heavy emphasis on benchmarking on the Reconstruction vs Prediction tasks misdirects the reader from why certain inclusions and modifications in the network architecture is the reason for reported state of the art performance (discussed in Section 5).\n6.3 Communication with original authors The codebase has scripts only for the Prediction task \u201dMemory\u201d variant of the benchmarks. We reused the majority of the codebase to create the other variants so as to have minimum variation from what the authors intended. We then validated our modifications with the authors through e-mail communication and will have all correspondence available in the code repository. We also communicated the discrepancies we saw in parts of the results and the authors told us they would update their repository with the missing scripts soon to help match the reported scores.\n7 Reproducibility Recommendations\nWe have the following recommendations:\n\u2022 We believe that the ideal value of the hyperparameter \u03bb is 0.7-0.8 for most datasets as this seems to yield the best results for the \u201dMemory\u201d variant of the Prediction task as compared to 0.6 suggested by the authors.\nReScience C 7.2 (#11) \u2013 Stephen and Menon 2021 15\n\u2022 There also appears to be a problemwith PyTorch versioning where the best results are obtained with PyTorch 1.1.0. This is an issue that other developers noticed as well (https://github.com/cvlab-yonsei/MNAD/issues/1). The reason for this behaviour is unclear."}], "title": "[Re] Learning Memory Guided Normality for Anomaly Detection", "year": 2021}