{"abstractText": "Remya Sankar1,2,3, ID , Nicolas Thou1, ID , Nicolas P. Rougier1,2,3, ID Arthur Leblois3, ID 1INRIA Bordeaux Sud-Ouest, Bordeaux, France \u2013 2LaBRI, Universit\u00e9 de Bordeaux, Institut Polytechnique de Bordeaux, Centre National de la Recherche Scientifique, UMR 5800, Talence, France \u2013 3Institut des Maladies Neurod\u00e9g\u00e9n\u00e9ratives, Universit\u00e9 de Bordeaux, Centre National de la Recherche Scientifique, UMR 5293, Bordeaux, France", "authors": [{"affiliations": [], "name": "Remya Sankar"}, {"affiliations": [], "name": "Nicolas Thou"}, {"affiliations": [], "name": "Nicolas P. Rougier"}, {"affiliations": [], "name": "Arthur Leblois"}, {"affiliations": [], "name": "Beno\u00eet Girard"}, {"affiliations": [], "name": "Daniel Schmid"}, {"affiliations": [], "name": "Robin Gutzen"}], "id": "SP:5a95dd1d0ea596efb9027cc360c5e1d9c833a98f", "references": [{"authors": ["R. Pyle", "R. Rosenbaum"], "title": "A reservoir computing model of reward-modulated motor learning and automaticity.", "venue": "Neural computation", "year": 2019}, {"authors": ["D. Sussillo", "L.F. Abbott"], "title": "Generating coherent patterns of activity from chaotic neural networks.", "venue": "Neuron", "year": 2009}, {"authors": ["G.M. Hoerzer", "R. Legenstein", "W. Maass"], "title": "Emergence of complex computational structures from chaotic neural networks through reward-modulated Hebbian learning.", "year": 2014}, {"authors": ["M.S. Brainard", "A.J. Doupe"], "title": "What songbirds teach us about learning.", "venue": "Nature", "year": 2002}, {"authors": ["B.P. \u00d6lveczky", "T.M. Otchy", "J.H. Goldberg", "D. Aronov", "M.S. Fee"], "title": "Changes in the neural control of a complex motor sequence during learning.", "venue": "Journal of neurophysiology", "year": 2011}, {"authors": ["M.Matsumoto andT.Nishimura"], "title": "Mersenne twister: a 623-dimensionally equidistributed uniformpseudo-random number generator.", "venue": "ACM transactions on modeling and computer simulation (TOMACS)", "year": 1998}], "sections": [{"text": "R E S C I E N C E C"}, {"heading": "Replication / Computational Neuroscience", "text": "[Re] A Reservoir Computing Model of Reward-Modulated"}, {"heading": "Motor Learning and Automaticity", "text": "Remya Sankar1,2,3, ID , Nicolas Thou1, ID , Nicolas P. Rougier1,2,3, ID Arthur Leblois3, ID 1INRIA Bordeaux Sud-Ouest, Bordeaux, France \u2013 2LaBRI, Universit\u00e9 de Bordeaux, Institut Polytechnique de Bordeaux, Centre National de la Recherche Scientifique, UMR 5800, Talence, France \u2013 3Institut des Maladies Neurod\u00e9g\u00e9n\u00e9ratives, Universit\u00e9 de Bordeaux, Centre National de la Recherche Scientifique, UMR 5293, Bordeaux, France\nEdited by Beno\u00eet Girard ID\nReviewed by Daniel Schmid ID Robin Gutzen ID\nReceived 12 March 2020\nPublished 22 November 2021\nDOI 10.5281/zenodo.5718075\nA replication of pyle2019.\n1 Introduction\nPyle and Rosenbaum [1] introduce a novel learning algorithm to the reservoir computing framework, which harnesses the dynamics of a recurrently connected network to generate time series. Most existing algorithms are built on fully supervised learning rules (e.g. FORCE [2]), which limits their potential applications, or themore biologicallyrealistic reinforcement learning techniques (e.g. RMHL [3]) which unfortunately fail to converge on complex spatio-temporal signal generation tasks. Pyle and Rosenbaum [1] use the advantages of these two learning rules, while averting their individual shortcomings, by combining the two algorithms to form the SUPERTREXmodel. The workings of this model are aligned to the theory of motor learning involving the basal ganglia, from rodent and songbird literature [4]. This hypothesises that a cortical pathway works in tandem with the basal ganglia for motor skill acquisition, wherein the basal ganglia pathway functions as a tutor, providing guiding signals that would ultimately be consolidated in the primary cortical pathway in charge of production of the motor commands [5]. Here, the basal ganglia pathway, which uses reward-modulated exploration based learning, akin to the RMHL algorithm, works in parallel with the cortical pathway, modeled using the fully supervised FORCE algorithm. The SUPERTREX model uses both these pathways in parallel, with the RMHL-based pathway providing the supervisory signal that the FORCE-based pathway requires. In this article, we provide amodular and user-friendly Python re-implementation of the model presented by Pyle and Rosenbaum [1]. We were able to successfully reproduce themodel performance in Python, for two tasks out of the three presented in the original article. For the third task, wewere able to do sowith limited robustness. We address this by introducing somemodifications, and discuss how their inclusion vastly improves the robustness as well as scalability of the model."}, {"heading": "Terminology \u2014", "text": "\u2022 Original scripts: the MATLAB scripts used by the authors to produce the results presented in [1].\n\u2022 Python adaptation: our Python adaptation of the original MATLAB scripts.\n\u2022 Python re-implementation: an improved version of our Python adaptation.\nCopyright \u00a9 2021 R. Sankar et al., released under a Creative Commons Attribution 4.0 International license. Correspondence should be addressed to Remya Sankar (Remya.Sankar@inria.fr) The authors have declared that no competing interests exists. Code is available at https://github.com/rsankar9/Reimplementation-SUPERTREX/releases/tag/v3.0 \u2013 DOI https://doi.org/10.5281/zenodo.4596425. Open peer review is available at https://github.com/ReScience/submissions/issues/50.\nReScience C 7.1 \u2013 Sankar et al. 2021 1\n1.1 Framework Pyle and Rosenbaum [1] proposes a model for sensorimotor learning using the framework of reservoir computing. The model is based on two existing reservoir computing techniques: FORCE and RMHL.\n\u03c4 dx dt = \u2212x+ Jr+Qz (1)\nr = tanh(x) + \u03f5 (2)\nFORCE (or first-order reduced and controlled error) is a fully supervised learning rule, which is widely used within the reservoir computing framework [2]. A recurrently connected reservoir, composed of rate-coded neurons is trained to produce a target time series by modifying the readout weights between the reservoir and the output layer (Eq 3, 4). The output, in turn, interacts with the reservoir by providing feedback (Eq 1, 2). FORCE can accurately generate complex dynamical target time-series. However, the model must have explicit knowledge of the target function, as FORCE requires a fully supervisory signal of the correct output in order to compute the error during training.\nz1 = W1r (3)\n\u03c4w1 dW1 dt = \u2212erTP (4)\nRMHL (or Reward-Modulated Hebbian Learning) is built on the concept of reinforcement learning, and uses only a scalar error signal indicating reward, allowing it to be applicable in a wider range of scenarios than FORCE [3]. RMHL introduces perturbations in the performance of themodel, and uses the information gained from this exploration to find the target (Eq 5, 6). This is akin to dopamine-dependent Hebbian learning in the basal ganglia. However, RMHL fails to converge to an accurate solution on several complex tasks. Moreover, it has been observed in songbirds that while the basal ganglia provides a tutor signal in the early stages, learning is eventually consolidated in a parallel cortical pathway, which is primarily responsible for motor activity [5]. RMHL cannot account for such empirical observations.\nz2 = W2r+\u03a8(e)\u03b7 (5)\n\u03c4w2 dW2 dt = \u03a6(e\u0302)z\u0302rT (6)\nSUPERTREX (Supervised Learning Trained by Reward Exploration), themodel proposed by the authors, tries to merge the advantages of both of these algorithms by combining both models. The more wide-ranged applicability of RMHL, owing to its usage of a one dimensional error signal, is used to train the model, while the superior maintenance ability of FORCE is recruited to consolidate the tutoring of the RMHL pathway. This could also potentially support the empirical evidence showing the basal ganglia and cortical pathways working in tandem for motor skill acquisition, discussed above. Thus, the SUPERTREXmodel consists of two parallel pathways, one based on RMHL (exploratory) and one based on FORCE (mastery), each consisting of its own set of weights. The mastery pathway uses the output of the exploratory pathway as its supervisory signal (Eq 7, 8, 9).\nz = z1 + z2 (7)\nReScience C 7.1 \u2013 Sankar et al. 2021 2\n\u03c4w1 dW1 dt = (z\u2212 z1) rTP (8)\n\u03c4w2 dW2 dt = \u03a6(e\u0302)z\u0302rT (9)\nwhere x denotes the reservoir dynamics, J the recurrent connectivitymatrix,Q the feedback weights and r the reservoir activity. The output z of the SUPERTREX model is the combination of the outputs z1 and z2 of the FORCE and RMHL pathways, respectively. W1 andW2 denote the readout weights of the two pathways, respectively, e denotes the squared distance between the output and the target trajectory, \u03c4 is the corresponding timescales for learning, \u03b7 is the exploratory noise, \u03f5 is a small noise term and P is a running estimate of the inverse of the correlation matrix of rates. \u03a8 and \u03a6 are two sublinear functions that serve to damp runaway oscillations during learning and control weight update, respectively. x\u0302 is a high-pass filtered version of x, which represents the recent changes in x.\n1.2 Task The authors test the SUPERTREXmodel on three motor tasks, with increasing difficulty, and compare its performance to those of FORCE and RMHL. The target of each task is to learn to produce a given spatio-temporal signal, under different constraints. Task 1 tests the performance of the model when the target output is known. This task requires the spatio-temporal signal to be produced directly by the model. Thus, the error signal is a direct indicator of the change required in the output of the model, i.e. fully supervisory. Task 2 and Task 3 use the paradigm of exploration by a multi-segmented arm, pivoted at a point. Task 2 tests the performance of the model when the target output is unknown, and only an indirect error signal is provided. The task requires the angles between the arm segments to be generated by the model, which would in turn produce the trajectory of the target spatio-temporal signal. In this case, the error signal is not a direct indicator of the change required in the output of the model. A non-linear inverse transformation of the trajectory would be required to compute the desired angles between the arm segments. It is, thus, not a fully supervisory signal, but simply a reinforcement signal. In Task 3, themovement of the arm segments are penalised variably. This creates the need to choose one frommultiple candidate solutions by optimising the cost of changing the angles between the arm segments. The simulation for each task includes a training phase and a testing phase. In the training phase, for ten periods, the time-series is generated by the model while the weights are being updated according to the current error feedback. After this, in the testing phase (lasting five periods), the readout weights are frozen and the time-series is generated using these frozen weights, without any further feedback-based update. In the SUPERTREX model, the exploratory pathway is also deactivated. It is also worth noting that the authors use teacher-forcing in the testing phase, which considerably improves the model performance by limiting the dependence on the stability of the learned solution (refer to Section \u201cState information provides stability of learned output\u201d in [1]).\nDisclaimer \u2014 Pyle and Rosenbaum [1] proceed to test the model under variations of the above tasks, including disrupted learning and with additional state information. However, these variations have not been replicated by us. We only test the performance of the three learning rules on the three tasks, specified above.\nReScience C 7.1 \u2013 Sankar et al. 2021 3\n2 Comparison with Python Adaptation\nIn this section, we compare the results presented in the paper [1] with the MATLAB implementation by the authors and our Python adaptation. The original scripts, although not available online, are readily available on request. We present our adaptation of this model in the open source framework Python, which has been built based on the paper and the MATLAB scripts provided by the authors. In contrast to the original scripts, it is modular and is easily modifiable with external json descriptor files. We compare the results presented in the paper, with simulations of the MATLAB scripts, provided by the authors, and also with our adaptation in Python 1.\nTo validate our Python adaptation, we test the three algorithms on three tasks by simulating them using both the original scripts and our Python adaptation. For each algorithmtask combination, we produce ten simulations with arbitrary seeds initialising the random generator and one additional simulation using the default seed of MATLAB (equivalent to seed 5489 of the numpy random generator). Except for the default seed, the ten arbitrary seeds are different for the Python and MATLAB simulations, and for each algorithm-task combination. BothMATLABandnumpyuse theMersenneTwister pseudorandom number generator [6]. To evaluate the performance of the algorithm, the authors plot the \u201cdistance from target\u201d, i.e. the square root of the low pass filtered version of the mean squared error, over the progression of the simulation. In order to categorise the model performance as satisfactory or unsatisfactory, we further compute a deviation metric by calculating the mean \u201cdistance from target\u201d over the testing phase. If this deviationmetric is below the threshold of 0.5 (set by visual inspection), themodel is said to have satisfactorily learnt and produced the target output.\n2.1 Task 1 Here, we compare the simulations of the original scripts and our adaptation for Task 1, using FORCE, RMHL and SUPERTREX, with the results presented in the article. Task 1 is designed to test the performance of these three algorithms when generating a known target output. The objective of this task is to produce a time-series of 2-D coordinates required to traverse a target trajectory, in this case, the parameterized curve of a butterfly. The model is trained to generate an output which closely matches the target function.\nThe article claims that:\n\u2022 under the FORCE framework, the target time-series is learned accurately and is maintained in a stable manner during the testing phase (Figure 1a).\n\u2022 under the RMHL framework, the target time-series is generated accurately during the training phase, however is not maintained perfectly during the testing phase (Figure 1b).\n\u2022 under the SUPERTREX framework, the target time-series is learned accurately and is also maintained in a stable manner during testing phase, albeit not as well as FORCE (Figure 1c).\nWe validate these observations with theMATLAB scripts provided by the authors as well as with our Python adaptation. To do so, we run the simulations with the default seed and repeat it ten times with different (arbitrarily chosen) seeds initialising the random number generator. We observe that:\n1In Figures 1- 4, the results presented in the paper have been reused in the column titled \u201doriginal\u201d.\nReScience C 7.1 \u2013 Sankar et al. 2021 4\n\u2022 under the FORCE framework, the target time-series is learned accurately and is maintained in a stable manner during the testing phase, as claimed. The mean deviation over eleven simulations, for both the original scripts (0.003\u00b10.002; n=11) and the Python adaptation (0.004\u00b1 0.003; n=11) is much lower than the threshold of 0.5 (Figure 1a, 2a).\n\u2022 under the RMHL framework, the target time-series is generated accurately during the training phase, however is not maintained perfectly during the testing phase, as claimed. The mean deviation, for both the original scripts (0.168\u00b1 0.038; n=11) and the Python adaptation (0.182 \u00b1 0.046; n=11), is higher than that with FORCE (Figure 1b, 2b).\n\u2022 under the SUPERTREX framework, the target time-series is learned accurately and is also maintained in a stable manner during testing phase, albeit not as well as FORCE, as claimed. Themeandeviation, for both the original scripts (0.006\u00b10.003; n=11) and the Python adaptation (0.006\u00b10.003; n=11), is much better than that for RMHL, but slightly worse than with FORCE (Figure 1c, 2c).\nBoth the original scripts and the Python adaptation are able to successfully closely reproduce the results presented in the paper for Task 1 (Figure 1,2; Table 1, 2).\n2.2 Task 2 Here, we compare the simulations of the original scripts and our Python adaptation for Task 2, using FORCE, RMHL and SUPERTREX, with the results presented in the article. Task 2 is designed to test the performance of these three algorithms when generating an unknown target from an indirect error signal. Using the paradigm of a pivoted multisegmented arm, the objective of this task is to produce a time-series by generating the angles between the arm segments. Motor output does not control the position of the end-effector of the arm, but instead controls the angles of the arm joints, which are nonlinearly related to end-effector position.\nThe article claims that:\n\u2022 the FORCE framework cannot be applied to this task, as FORCE requires the exact target to be provided as a supervisory error, which in this case would be the unknown target angles. Since, we do not have this information beforehand, and require the model to derive it, the FORCE framework is inapplicable to this task.\n\u2022 under the RMHL framework, the target time-series is imitated well by the model during the training phase, however the weights do not converge, and hence, it is unable to maintain the time-series in a stable manner during the testing phase (Figure 3a).\n\u2022 under the SUPERTREX framework, the target time-series is learned accurately and is also generated in a stable manner, with minor divergences, during testing phase, owing to the contribution of the pathway based on the FORCE algorithm (Figure 3b).\nWe verify these observations with the MATLAB scripts provided by the authors as well as with our Python adaptation. To do so, we run the simulations with the default seed of MATLAB and re-simulate it with ten arbitrary seeds initialising the random number generator. We do not modify any task conditions or model hyper-parameters. We observe that:\n\u2022 indeed, the FORCE framework is inapplicable to this task.\nReScience C 7.1 \u2013 Sankar et al. 2021 5"}, {"heading": "MATLAB Original Python", "text": "FO RC E x( t)\ny( t)\n(a) Results for Task 1 with the FORCE algorithm. The target time-series is learned accurately during the training phase and is maintained in a stable manner during the testing phase, in both implementations, as presented in [1].\nRM H L\nx( t)\ny( t)\n(b)Results for Task 1with theRMHLalgorithm. The target time-series is learned accurately during the training phase, though not maintained perfectly during the testing phase, in both implementations, as presented in [1].\nSU PE\nRT RE X x( t)\ny( t)\n1s (c) Results for Task 1 with the SUPERTREX algorithm. The target time-series is learned accurately during the training phase, and is also maintained in a stable manner during testing phase, albeit not as well as FORCE, in both implementations, as presented in [1]."}, {"heading": "MATLAB Original Python", "text": "FO RC E M SE\n10 -4\n10 -2\n10 0\n10\u22124\n10\u22122\n100\n||W ||\n0\n0.2\n0.0\n0.2\n(a) Results for Task 1 with the FORCE algorithm. The target time-series is learned accurately during the training phase and is maintained in a stable manner during the testing phase, as presented in [1].\nRM H L\nM SE\n10 -4\n10 -2\n10 0\n10\u22124\n10\u22122\n100\n||W ||\n0\n0.2\n0.0\n0.2\n(b)Results for Task 1with theRMHLalgorithm. The target time-series is learned accurately during the training phase, though not maintained perfectly during the testing phase, as presented in [1].\nSU PE\nRT RE X M SE\n10 -4\n10 -2\n10 0\n10\u22124\n10\u22122\n100\n||W ||\n0\n0.2\n0.0\n0.2\n1s\n1s (c) Results for Task 1 with the SUPERTREX algorithm. The target time-series is learned accurately during the training phase, and is also maintained in a stable manner during testing phase, albeit not as well as FORCE, as presented in [1].\nFigure 2. Comparison of the performances of the MATLAB scripts (left column) and the Python adaptation (right column)with the results presented in the original article (center column), for the three learning algorithms on Task 1 [1]. All simulations shownhere use theMATLABdefault (5489) as the seed for the random number generator. In each subfigure, the top row shows the target trajectory (red) with the trajectory generated by the model (blue) throughout the test phase. The second row shows the error metric (blue) over the simulation (x and y coordinates, in this case), using the log scale for the y axis. The bottom row shows the progression of the corresponding weight matrices (SUPERTREX: W1 in purple; W2, in green). The horizontal grey line, in the test phase, indicates the deviation metric. ReScience C 7.1 \u2013 Sankar et al. 2021 7\n\u2022 under the RMHL framework, the target time-series is imitated well by the model during the training phase, however the weights do not converge, and hence, it is unable to maintain the time-series in a stable manner during the testing phase. The mean deviation over eleven simulations, for both the original scripts (0.759\u00b1 0.284; n=11) and the Python adaptation (0.814 \u00b1 0.288; n=11) is higher than the threshold of 0.5 (Figure 3a).\n\u2022 under the SUPERTREX framework, the target time-series is learned accurately and is also generated in a stablemanner, withminor divergences, during testing phase, owing to the contribution of the pathway based on the FORCE algorithm. The mean deviation over eleven simulations, for both the original scripts (0.011\u00b10.003; n=11) and the Python adaptation (0.012\u00b10.005; n=11) is below the threshold of 0.5 and much lower than that with RMHL (Figure 3b).\nThe MATLAB scripts provided by the authors and the Python adaptation are able to successfully closely reproduce the results presented for Task 2 in the paper, with the default seed as well as with the 10 arbitrary seeds (Figure 3; Table 1, 2).\n2.3 Task 3 Here, we compare the performance of the MATLAB scripts and our Python adaptation on Task 3, for the three algorithms, with the results presented in the article. Task 3 is an extension of Task 2, designed to test the constraint optimisation ability of these three algorithms when generating an unknown target from an indirect error signal. Using the paradigm of a pivoted multi-segmented arm, the objective of this task is to produce a time-series by generating the angles between the arm segments, while also optimising the movement cost of each arm segment. Hence, the arm is required to traverse the butterfly, while carefully choosing the segment to rotate, in order to minimise the movement cost of its segments. Post the training phase, the readout weights are frozen and in the SUPERTREX model, the exploratory pathway is deactivated.\nThe article claims that:\n\u2022 FORCE can\u02bct be applied to this task, as explained for Task 2.\n\u2022 under the RMHL framework, the target time-series is imitated well by the model during the training phase, however the weights do not converge, and hence, it poorly maintains the time-series during the testing phase (Figure 4a).\n\u2022 under the SUPERTREX framework, the performance is much better than RMHL. The target time-series is learned accurately and is also generated with minor divergences, during testing phase (Figure 4b).\nWe verify these observations with the MATLAB scripts provided by the authors as well as with our Python adaptation. To do so, we run the simulations with the default seed of MATLAB and re-simulate it with ten arbitrary seeds initialising the random number generator. We observe that:\n\u2022 indeed, the FORCE framework is inapplicable to this task, as claimed.\n\u2022 under the RMHL framework, the target time-series is imitated well by the model during the training phase, however the weights do not converge, and hence, it poorly maintains the time-series during the testing phase, as claimed. The mean deviation over eleven simulations, for both the original scripts (0.850\u00b10.313; n=11) and the Python adaptation (0.658\u00b10.216; n=11) is higher than the threshold of 0.5. All eleven simulations with different seeds did not generate the target output in a satisfactory manner (i.e. deviation > 0.5 for 11/11 seeds) (Figure 4a).\nReScience C 7.1 \u2013 Sankar et al. 2021 8"}, {"heading": "MATLAB Original Python", "text": "RM H L\n\u03b8 1 \u03b8 2\nM SE\n10 -3\n10 -1\n10 1\n10\u22123\n10\u22121\n101\n(a) Results for Task 2 with the RMHL algorithm. The target time-series is imitated well by the model during the training phase (not shown), however, it is unable to maintain the time-series in a stable manner during the testing phase, in both implementations, as presented in [1].\nSU PE\nRT RE X \u03b8 1\n\u03b8 2 M SE\n10 -3\n10 -1\n10 1\n10\u22123\n10\u22121\n101\n1s (b) Results for Task 2 with the SUPERTREX algorithm. The target time-series is learned accurately during the training phase, and is also maintained in a stable manner, during the testing phase, in both implementations, as presented in [1].\n\u2022 under the SUPERTREX framework, the performance is notmuchbetter thanRMHL, contrary to the article s\u0313 claim. The target time-series is not generated in a satisfactory manner, during the testing phase, for more than 50% of the tested simulations (Original scripts: 6/11 and Python adaptation: 7/11). The mean deviation over eleven simulations, for both the original scripts (0.881\u00b1 0.224; n=11) and the Python adaptation (0.837 \u00b1 0.241; n=11) is above the threshold of 0.5 and comparable with that of RMHL (Figure 4b).\nThe original scripts and the Python adaptation are able to successfully reproduce the results presented in the paper with the default seed as well as with the 10 arbitrary seeds for the RMHL algorithm, but not for the SUPERTREX algorithm (Figure 3; Table 1, 2).\n3 Modification\nThe Python adaptation is a close adaptation of the original MATLAB scripts provided by the authors. However, on simulating their performance on the three tasks, we observed that while, for the first two tasks, the models performed as described in Pyle and Rosenbaum [1], the performance of the SUPERTREX algorithm on Task 3 was not consistent, and was dependent on the seed used for the random number generator. On inspecting\nReScience C 7.1 \u2013 Sankar et al. 2021 10"}, {"heading": "MATLAB Original Python", "text": "RM H L\n(a)Results for Task 3with theRMHLalgorithm, using the default seed (5489) for the randomnumber generator. The target trajectory is imitated well by the model during the training phase (not shown), however, it poorly maintains the time-series during the testing phase, in both implementations, as presented in [1].\n1s (b) Results for Task 3 with the SUPERTREX algorithm using the default seed (5489) for the random number generator. The target time-series is learned accurately during the training phase, but is notmaintained during the testing phase, in both implementations, in contrast to the results presented in [1]. (c) Results for Task 3 with the SUPERTREX algorithm using different implementations (MATLAB, left and Python adaptation, right) and different seeds (295728336, left and 5624282, right) for the random number generator. The target trajectory is learned accurately during the training phase, and is also maintained in a stable manner, with slight divergences (Deviation: 295728336: 0.215 \u00b1 0.073; 5624282: 0.190 \u00b1 0.054) , during the testing phase, in both implementations, similar to the results presented in [1].\nfurther, we notice that this was, in some cases, due to the uncontrolled exponential increase in the readout weights.\nTo look into the robustness of the implementations further, we test the performance of the RMHL and SUPERTREX algorithms on Task 2 with certain modifications to the task parameters, specifically, the number of arm segments and the length of the arm segments. It would be expected for the behaviour to be comparable with the performance on the original task performance, or undergo a gradual decline. We test Task 2 on the arm parameters, which were used in Task 3, i.e. by increasing the number of arm segments from two to three and changing the length of each arm segment. We observe that RMHL performance is comparable to the original Task 2, wherein the time series is generated during the training phase, but is not maintained beyond (Original scripts: 0.846\u00b1 0.299, Python adaptation: 0.738\u00b1 0.256; n=11). On the other hand, simulations of the SUPERTREX model, with 2 out of 11 seeds, were able to produce the target output satisfactorily (Original scripts: 0.016\u00b1 0.007, Python adaptation: 0.009\u00b1 0.003; n=2) (Figure 5). However, in simulations with 9 out of 11 seeds, the weights increase exponentially, rendering the simulation unable to progress in a meaningful manner (Table 1, 2).\nIn order to improve the model performance, make the model more scalable in terms of task parameters, and also more robust (as seen in Task 3, with respect to reproducibility with different random seeds), we introduce two minor alterations.\n1. We introduce a compensation factor to the update of the readout weights in the exploratory pathway, inversely proportional to the number of segments. Specifically, when the number of segments is greater than two, we multiply the weight update by 0.1/n_segs for Task 2 and by 0.5/n_segs for Task 3.\n2. The SUPERTREX model transfers the information from the exploratory pathway to the mastery pathway, only if the error is consistently below a certain threshold. In the original scripts, this threshold is set at 1.5e-3 for Task 1 and Task 2, while at 1.5e-2 for Task 3. We change the transfer threshold for Task 2 from 1.5e-3 to 1.5e-2.\nThese slight modifications address the shortcomings we encountered earlier with the performance of SUPERTREX in Task 2 and 3. Alteration #1, by including a compensation factor for the change in number of arm segments, prevents the weights from increasing exponentially, and lets the simulation proceed in a meaningful manner. Alteration #2, by increasing the error threshold governing the transfer of information to the mastery pathway, makes the model more tolerant of fluctuations, while continuing to explore and learn a good solution. Although this does not lead to a critical change for Task 1 (Original scripts: 0.006 \u00b1 0.003, n=11; Modified Python re-implementation: 0.004 \u00b1 0.003, n=11) and Task 2 (Original scripts: 0.011 \u00b1 0.003, n=11; Modified Python re-implementation: 0.010\u00b10.004, n=11), this alteration improves the performance of SUPERTREX on Task 3 (Original scripts: 0.881\u00b10.224, n=11; Modified Python re-implementation: 0.140\u00b1 0.071, n=11). Simulations with 10 out of 11 seeds had satisfactory performance (Deviation < 0.5), compared to 6 out of 11 simulations for the original scripts. Further, it unlocks the potential for the model to be more scalable. We find that with these alterations, on merely increasing the number of time-steps per training cycle and with no further fine tuning of hyper-parameters, the model is able to proceed without an exponential increase in weights over a wider range of task parameters. For instance, on adding surplus segments with length 0.1 each, the model is able to perform in a satisfactory manner, for most cases, with up to 50 arm segments (Table 3, Figure 6). Better accuracy can be achieved by further fine tuning of the hyper-parameters.\nReScience C 7.1 \u2013 Sankar et al. 2021 12\nMATLAB Python with modification\n1s\nReScience C 7.1 \u2013 Sankar et al. 2021 13\nReScience C 7.1 \u2013 Sankar et al. 2021 14\nM SE\ny( t)\nx( t)\n20 segments 30 segments\n10\u22123\n10\u22121\n101\n10\u22123\n10\u22121\n101\nM SE\ny( t)\nx( t)\n40 segments 50 segments\n10\u22123\n10\u22121\n101\n10\u22123\n10\u22121\n101\n1s 1s Figure 6. (Continued from previous page.) Scalability of the performance of the modified Python re-implementation using the SUPERTREX algorithm on Task 2. The lengths of the arm segments are 1.8, 1.2 and 0.6 for the first three segments (akin to Task 3) and 0.1 for each additional segment. Here, the simulations for Task 2with 5 to 50 segments are shown, all using the default seed 5489 for the random number generator. In each subfigure, the top panel shows the produced trajectory, the middle panels show the evolution of the x and y coordinates of the end-effector of the arm (blue) throughout the training and test phase, along with the target coordinates (red). The grey vertical line marks the separation of the training and testing phase. The bottom panel shows the progression of the distance from target metric (blue) over the simulation, using the log scale for the y axis. The horizontal grey line, in the test phase, indicates the deviation metric.\nReScience C 7.1 \u2013 Sankar et al. 2021 15"}, {"heading": "3 10000 0.057 0.032 0.069", "text": ""}, {"heading": "4 10000 0.242 0.221 0.136", "text": ""}, {"heading": "5 10000 0.141 0.080 0.147", "text": ""}, {"heading": "6 10000 0.181 0.160 0.133", "text": ""}, {"heading": "7 10000 0.173 0.129 0.130", "text": ""}, {"heading": "8 10000 0.253 0.230 0.137", "text": ""}, {"heading": "9 10000 0.331 0.297 0.168", "text": ""}, {"heading": "10 10000 0.417 0.409 0.151", "text": ""}, {"heading": "15 10000 0.538 0.512 0.179", "text": ""}, {"heading": "20 15000 0.366 0.324 0.188", "text": ""}, {"heading": "30 20000 0.549 0.489 0.236", "text": ""}, {"heading": "40 20000 0.372 0.313 0.228", "text": ""}, {"heading": "50 30000 0.375 0.283 0.281", "text": "4 Discussion\nIn this article, we discussed the SUPERTREX model presented by Pyle and Rosenbaum [1]. We compared the results presented in the paper, both with the results obtained using the original scripts, and with our modular and user-friendly Python adaptation. Furthermore, we were able to improve the robustness and scalability of the model with two minor alterations. The Python adaptation strives to be a close adaptation of the original scripts in MATLAB and differs mainly in the method of initialisation of the reservoir connectivity matrix. This is due to the usage of the function sprandn in the original scripts, whose internal implementation is not freely available. Figure 7 shows that, on importing initialisation matrix fromMATLAB, the exact same results can be obtained in the Python adaptation, as well. Most of the details for the implementation of the models are also described in the paper. Only two necessary details were missing, both concerning the update of the readout weights of the exploratory pathway in the RMHL and SUPERTREXmodels: One, the inclusion of a crucial learning rate of 0.0005, for Tasks 1-3, and two, an additional compensatory factor of 0.5, for Task 3. There is another discrepancy in the function psi(x) for Task 3. The scripts provided by the authors use a factor of 0.005, whereas the article mentions this factor to be 0.025. The three algorithms (FORCE, RMHL and SUPERTREX) have been tested on three tasks, presented in Pyle and Rosenbaum [1]. For Task 1 and 2, we verify that the three algorithms function as presented in the paper, and validate that our Python re-implementation produces comparable results. For Task 3, the SUPERTREX model s\u0313 behaviour is also reproducible, although the performance is dependent on the seed used for the random number generator. Furthermore, we observed that this implementation is quite sensitive to changes in task parameters, such as the number of arms. This was due to the uninhibited increase in the readout weights. We propose the inclusion of a compensation factor for the number of arm segments, which inhibits the growth of the readout weights, and allows the simulation to proceed in a meaningful manner. This considerably improves the robustness and the scalability of the original model.\nReScience C 7.1 \u2013 Sankar et al. 2021 16\nWe conclude that the results presented in the paper are reproducible for two tasks, using the original MATLAB scripts provided by the authors, and also, replicable in Python for all tasks with comparable performance.\n1s\nReScience C 7.1 \u2013 Sankar et al. 2021 17"}], "title": "[Re] A Reservoir Computing Model of Reward-Modulated Motor Learning and Automaticity", "year": 2021}