{"abstractText": "Variational autoencoders (VAE), first introduced in the works of [1], sparked a trend in designing generative models in order to approximate the intractable posterior distribution. Many recent papers have provided ingenious schemes for improving upon VAE, among some ([2, 3, 4, 5]), by achieving tighter log-likelihood bounds on the marginal likelihood (explained in greater detail below). The original bottom-up and top-down architecture has been experimented with ([4]), as well as employing chains of transformations on an, in VAE, assumed simplistic prior distribution ([3, 5]). The importance weighted variational autoencoder (IWAE; [2]) utilized averaging over multiple samples, as opposed to VAE s\u0313 single-sample objective, to tighten the mentioned bound while being able to model a richer latent space \u2013 in effect, this multi-sample scheme allows for a more complex approximate posterior. In light of IWAE, tensor Monte-Carlo ([6]; TMC) was recently proposed as an attempt to improve upon IWAE by sampling exponentially many importance samples. For each of the n latent variables in the TMC,K samples are drawn yieldingK marginal log-likelihood evaluations. Averaging over this large number of samplesmight appear computationally impossible, but via clever tensor products computed in parallel, the TMC is approximately as fast as the less importance sample exhausting IWAE. In this work, we reproduce what we believe are the most important results presented in the Tensor Monte Carlo paper ([6]), where we also provide our reimplementation code. The original results in the TMC paper were attained via a PyTorch ([7]) implementation.1 In an attempt to ease understanding for those unfamiliar with PyTorch, we contribute with a TensorFlow2 ([8]) implementation. Early on in our work, a connection was established with the author in order to bring our reproducibility work to their attention, as well as ensuring that we progress by clearing potential ambiguities. Due to resource and time constraints, we chose to reproduce those results that, in our meaning, appeared most informative and fundamental in the TMC paper. Additionally, as we found the TMC architecture non-trivial to understand, we aim to ease understanding for future users by complementing the textual description of the model with an algorithmic description in Alg. 1 and a depiction of the model in Fig. 4 (figure in Appendix B). Furthermore, we supplement the original paper by visualizing the TMC s\u0313 reconstruction and clustering capabilities (Appendix C and D, respectively), while contrasting them to the capabilities of the baseline, IWAE.", "authors": [{"affiliations": [], "name": "Oskar Kviman"}, {"affiliations": [], "name": "Linus Nilsson"}, {"affiliations": [], "name": "Martin Larsson"}, {"affiliations": [], "name": "Koustuv Sinha"}], "id": "SP:1f09c2f7a032603453b624676da13e67c73ddc90", "references": [{"authors": ["D.P. Kingma", "M. Welling"], "title": "Auto-Encoding Variational Bayes", "year": 2013}, {"authors": ["Y. Burda", "R. Grosse", "R. Salakhutdinov"], "title": "Importance weighted autoencoders", "year": 2015}, {"authors": ["D.J. Rezende", "S. Mohamed"], "title": "Variational inference with normalizing flows", "year": 2015}, {"authors": ["C.K. S\u00f8nderby", "T. Raiko", "L. Maal\u00f8e", "S.K. S\u00f8nderby", "O. Winther"], "title": "Ladder variational autoencoders.", "venue": "Advances in neural information processing systems", "year": 2016}, {"authors": ["D.P. Kingma", "P. Dhariwal"], "title": "Glow: Generative Flow with Invertible 1x1 Convolutions", "year": 2018}, {"authors": ["L. Aitchison"], "title": "Tensor Monte Carlo: particle methods for the GPU era.", "venue": "Advances in Neural Information Processing Systems", "year": 2019}, {"authors": ["A. Paszke", "S. Gross", "S. Chintala", "G. Chanan", "E. Yang", "Z. DeVito", "Z. Lin", "A. Desmaison", "L. Antiga", "A. Lerer"], "title": "Automatic differentiation in PyTorch.", "venue": "NIPS-W", "year": 2017}, {"authors": ["Mart\u0131\u0301n Abadi"], "title": "TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems. Software available from tensorflow.org", "year": 2015}, {"authors": ["P. D"], "title": "KingmaandM.Welling.An Introduction toVariational Autoencoders", "year": 1906}, {"authors": ["G. Roeder", "Y. Wu", "D.K. Duvenaud"], "title": "Sticking the landing: Simple, lower-variance gradient estimators for variational inference.", "venue": "Advances in Neural Information Processing Systems", "year": 2017}, {"authors": ["G. Tucker", "D. Lawson", "S. Gu", "C. J"], "title": "Maddison.Doubly reparameterized gradient estimators forMonte Carlo objectives. 2018", "year": 2018}, {"authors": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "title": "Gradient-based learning applied to document recognition.", "venue": "Proceedings of the IEEE", "year": 1998}, {"authors": ["D.P. Kingma", "J. Ba"], "title": "Adam: A Method for Stochastic Optimization", "year": 2014}, {"authors": ["T. Salimans", "D.P. Kingma"], "title": "Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks", "year": 2016}, {"authors": ["X. Glorot", "Y. Bengio"], "title": "Understanding the difficulty of training deep feedforward neural networks.", "venue": "Proceedings of the thirteenth international conference on artificial intelligence and statistics", "year": 2010}], "sections": [{"text": "R E S C I E N C E C"}, {"heading": "Replication / NeurIPS 2019 Reproducibility Challenge", "text": "[Re] Tensor Monte Carlo: Particle Methods for the GPU"}, {"heading": "Era", "text": "Oskar Kviman1, ID , Linus Nilsson1, ID , and Martin Larsson1, ID 1KTH Royal Institute of Technology, Stockholm, Sweden\nEdited by Koustuv Sinha ID\nReviewed by Anonymous Reviewers\nReceived 15 February 2020\nPublished 21 May 2020\nDOI 10.5281/zenodo.3818617\n1 Introduction\nVariational autoencoders (VAE), first introduced in the works of [1], sparked a trend in designing generative models in order to approximate the intractable posterior distribution. Many recent papers have provided ingenious schemes for improving upon VAE, among some ([2, 3, 4, 5]), by achieving tighter log-likelihood bounds on the marginal likelihood (explained in greater detail below). The original bottom-up and top-down architecture has been experimented with ([4]), as well as employing chains of transformations on an, in VAE, assumed simplistic prior distribution ([3, 5]). The importance weighted variational autoencoder (IWAE; [2]) utilized averaging over multiple samples, as opposed to VAE s\u0313 single-sample objective, to tighten the mentioned bound while being able to model a richer latent space \u2013 in effect, this multi-sample scheme allows for a more complex approximate posterior. In light of IWAE, tensor Monte-Carlo ([6]; TMC) was recently proposed as an attempt to improve upon IWAE by sampling exponentially many importance samples. For each of the n latent variables in the TMC,K samples are drawn yieldingKn marginal log-likelihood evaluations. Averaging over this large number of samplesmight appear computationally impossible, but via clever tensor products computed in parallel, the TMC is approximately as fast as the less importance sample exhausting IWAE. In this work, we reproduce what we believe are the most important results presented in the Tensor Monte Carlo paper ([6]), where we also provide our reimplementation code. The original results in the TMC paper were attained via a PyTorch ([7]) implementation.1 In an attempt to ease understanding for those unfamiliar with PyTorch, we contribute with a TensorFlow2 ([8]) implementation. Early on in our work, a connection was established with the author in order to bring our reproducibility work to their attention, as well as ensuring that we progress by clearing potential ambiguities. Due to resource and time constraints, we chose to reproduce those results that, in our meaning, appeared most informative and fundamental in the TMC paper. Additionally, as we found the TMC architecture non-trivial to understand, we aim to ease understanding for future users by complementing the textual description of the model with an algorithmic description in Alg. 1 and a depiction of the model in Fig. 4 (figure in Appendix B). Furthermore, we supplement the original paper by visualizing the TMC s\u0313 reconstruction and clustering capabilities (Appendix C and D, respectively), while contrasting them to the capabilities of the baseline, IWAE.\n1Code is available at: https://github.com/anonymous-78913/tmc-anon. 2Code is available at: https://github.com/LinuNils/TMC_reproduced.\nCopyright \u00a9 2020 O. Kviman, L. Nilsson and M. Larsson, released under a Creative Commons Attribution 4.0 International license. Correspondence should be addressed to Oskar Kviman (okviman@kth.se) The authors have declared that no competing interests exist. Code is available at https://github.com/LinuNils/TMC\\_reproduced. \u2013 SWH swh:1:dir:7aa1df905b5c3f0a77a8b0c01123dc0013c79280. Open peer review is available at https://openreview.net/forum?id=BJxUSaczTH.\nReScience C 6.2 (#5) \u2013 Kviman, Nilsson and Larsson 2020 1\n2 Background in Variational Autoencoders\nAs in traditional variational inference (VI), the VAE specifies a proposal distribution q\u03d5(z|x) for approximating an intractable posterior distribution p\u03b8(z|x). The VAE is comprised of two parts, where the first part, q\u03d5(z|x), is often referred to as the representation (recognition or encoder) model ([1, 6, 2]), and it learns a mapping from the input space X to the parameter set \u03d5 = {\u00b5, \u03c32}. As the aim of generative modeling is to learn the joint probability distribution, using the notation aligned with this paper, p\u03b8(x, z) = p\u03b8(z)p\u03b8(x|z) ([1, 5, 9]), the second part, conveniently called the generative model, tries to achieve just this. In VAE, the two parameter sets, \u03d5 and \u03b8, are optimized in order to tighten the lower bound for the log-likelihood\nlogp\u03b8(x) = logEq\u03d5(z|x) [ p\u03b8(x, z)\nq\u03d5(z|x)\n] \u2265 Eq\u03d5(z|x) [ log p\u03b8(x, z)\nq\u03d5(z|x)\n] = LVAE(x), (1)\nwhere the lower bound is achieved via Jensens\u0313 Inequality ([1]). Here, we only consider one latent sample, z, per given data point x, i.e. LVAE is a single-sample objective. Since we sample from the approximate posterior (proposal distribution) q\u03d5, wewant to choose this distribution such that we can operate on in a convenient manner. Therefore, we often resort to a Gaussian distribution ([2, 1, 9]. Since this is a somewhat strong assumption of the true distribution ([5, 3]) modern VAE s\u0313 try tomake themodel more expressive, tightening the bound in Eq. (1). One such modern VAE is IWAE ([2]), which employs a multi-sampling objective\nlog p\u03b8(x) = logEq\u03d5(z1|x)\u2026q\u03d5(zk|x)\n[ 1\nk \u2211 i p\u03b8(x, z i) q\u03d5(zi|x) ] \u2265 Eq\u03d5(z1|x)\u2026q\u03d5(zk|x) [ log 1 k \u2211 i p\u03b8(x, z i) q\u03d5(zi|x) ] = LkIWAE(x),\n(2)\nwhere k is the number of samples drawn per data point. In [2], the authors state two fundamental benefits of this approach:\n\u2022 The authors prove that logp\u03b8(x) \u2265 Lk+1 \u2265 Lk.\n\u2022 When training using gradient descent, how to update the network parameters is based on importance weighting between the k z s\u0313 marginal log-likelihoods. I.e. the learning rule putsmore emphasis on sampleswith largermarginal log-likelihoods.\n3 Analysis of the Tensor Monte-Carlo Algorithm\nIn this section, we aim to provide a sufficient background of the TMC algorithm in order for the reader to be able to follow the reproduced results. Additionally, we outlinewhich aspects of the algorithm and results we intend to reproduce. We do so in order to restrict whatwork in the TMCpaperweneed to explain. For completeness, we next briefly touch upon most of the novelties proposed by Aitchison. In the TMC paper, Aitchison extends IWAE s\u0313 multi-sample objective by considering exponentially many important samples, barely without increasing the computational cost compared to IWAE. Aitchison in essence proposes two different models, one where the proposal distribution is factorised, and the other one with non-factorised proposals. Furthermore, two existing variance reduction techniques, STL ([10]) and DReGS ([11]), were employed, actually worsening performance for the TMC, according to the author.\nReScience C 6.2 (#5) \u2013 Kviman, Nilsson and Larsson 2020 2\n3.1 What we reproduce Although there are many potential results to be reproduced, due to time and resource constraints we limit our work to reproducing the following\n\u2022 For all tests we assume a non-factorized proposal distributions. This approach creates a recognition model in the same manner as the baseline algorithm, IWAE, was originally proposed.\n\u2022 We omit the use of the variance reduction techniques STL and DReGS. The results in the TMC paper were presented with and without these techniques, why wemay still compare our results with those presented in the TMC paper. Given our tradeoff between time and quantitative testing, we argue that this approach is most informative if no prior knowledge of the techniques is assumed. Additionally, the best performingmodels in the TMCpaper did not include these variance reduction techniques.\n\u2022 The baseline, an IWAE model, has been reproduced from scratch in TensorFlow.\n\u2022 An ablation study is provided, where we remove the intermediate layers in the recognition and generative models, effectively attaining one of the originally proposed number of layers for the IWAE ([2]). Since the effects of TMC becomes apparent only when we have intermediate layers ([6]), we expect the IWAE and TMC to produce approximately the same results.\n\u2022 We provide a non-exhaustive hyper-parameter search using grid-search over the number of samples (k; particles) and the learning rate.\nAs stated above, we provide the complete reproducibility code publicly on GitHub.\n3.2 Tensor Monte-Carlo For reproducibility, we begin this section with providing an algorithmic description of the TMC, see Alg. 1, which we next complement with some important details. As seen in Alg. 1, the TMC evaluates exponentially many importance samples as the IWAE ([6, 2]). Doing this, in turns, gives rise to a new objective function. In order to average over all different combinations ofmarginal log-likelihoods, Aitchison ([6]), defines the new marginal likelihood estimator as\nPTMC = 1\u220f\ni=1\nKi \u2211 k1,k2,...,kLq\np\u03b8(x|zk11 ) \u220f j p\u03b8(z kj j |z\nkj+1 j+1 )\u220f\ni\nq\u03b8(z ki i |z ki\u22121 k\u22121 )\n, (3)\nwhich yields the following multi-sample objective\nlog p\u03b8(x) \u2265 Eq\u03b8 [PTMC] = LTMC(\u21b6). (4)\nNote that the estimator in Eq. (3) only applies when assuming a non-factorized proposal. Also note, in contrast to the notations used when describing IWAE, kl here is a set with cardinalityK and not the number of samples per latent z. To compute the tensor inner-product in a numerically stable way, the author provides a method referred to as logmmexp ([6]; see Appendix A).\n4 Reproducibility\nPlease note, that in the above section we provided a list of what we aim to reproduce. Here, we go through our experimental methodology in greater detail.\nReScience C 6.2 (#5) \u2013 Kviman, Nilsson and Larsson 2020 3\nAlgorithm 1: Non-factorised Tensor Monte-Carlo (TMC) algorithm. d here is the dimension of a data point, b is the batch size, and the number of layers in the recognition model is Lq. Initialize \u03b8 while Not converged do\nx\u2190\u2212 Randomly draw a data minibatch, z0, of size b for l \u2208 {1, ..., Lq} do\nh kl l \u2190\u2212 MLP(z kl\u22121 l\u22121 ) \u00b5 kl l \u2190\u2212 Linear(h kl l ) \u03c1 kl l \u2190\u2212 SoftPlus(Linear(hl)) \u03f5 kl l \u2190\u2212 N (0, 1) // Reparameterization trick, b\u00d7K \u00d7 d-tensor of\nunivariate gaussians z kl l \u2190\u2212 \u00b5l + \u03c1l\u03f5 kl l for i \u2208 {1, ...,K} do for j \u2208 {1, ...,K} do\nCompute log q\u03b8(zjl |z i l\u22121)\u2190\u2212 log q\u03b8(zjl |\u00b5 i l,\u03c1 i l) ;\nStore the log-likelihoods and the sampled latents\nCompute and store log p\u03b8(z kLq Lq ) for zkll \u2208 {z kLq Lq\n, ..., zk22 } do h\nkl\u22121 l\u22121 \u2190\u2212 MLP(z kl l )\n\u00b5 kl\u22121 l\u22121 \u2190\u2212 Linear(h kl\u22121 l\u22121 ) \u03c1 kl\u22121 l\u22121 \u2190\u2212 SoftPlus(Linear(h kl\u22121 l\u22121 )) for i \u2208 {1, ...,K} do for j \u2208 {1, ...,K} do\ncompute log p\u03b8(zjl+1|z i l )\u2190\u2212 logN (zjl+1|\u00b5 i l+1|\u00b5il+1(\u03c1il+1))\nStore the log-likelihoods\n\u00b5k00 \u2190\u2212 Linear(MLP (z k1 1 )) Compute and store log p\u03b8(x|zk1)\u2190\u2212 logB(x|\u00b5k00 ) update \u03b8 using\u2207\u03b8LTMC(x)\n4.1 Dataset The TMC paper uses the MNIST ([12]) handwritten digit database to evaluate the performance of TMC compared to IWAE. Although the author of the TMC paper also conducts an enlightening toy experiment where the true marginal likelihood is known, we restrain our work to the MNIST dataset as these results are arguably most informative when comparing with existing models in the literature. The dataset was downloaded and pre-processed via Keras ([13]), while we normalized all pixels by division with 255. No explicit scheme of how the training proceeded was presented, but the following was done, justified by discussion with the author: similar to that presented in [2], the model was exposed to all the training data, presented in randomly drawn mini-batches. Next the same was done for all the test data, concluding an epoch."}, {"heading": "Comments on reproducibility \u2014", "text": "\u2022 It seems crucial to know the exact training and test scheme, in order to reproduce the results. Perhaps this is a standardized approach when training VAE s\u0313, but we were unaware of such a standard. We suggest that this should be added to the paper for clarity.\nReScience C 6.2 (#5) \u2013 Kviman, Nilsson and Larsson 2020 4\n4.2 Implementation details In order to make qualitative comparisons between the proposed TMC model and the benchmark model, we implemented the algorithm together with its baselines, in TensorFlow. The instructions for running our code are located in our git repository. Each model is self-contained in a individual file. Considering only the non-factorized proposals evaluated on the MNIST dataset, the author employed two models coined small and large. In common for both models in the TMC paper, is the number of hidden layers and dimensionality of the smallest latent space (furthest from the data), i.e. 4 units. Inspired by the works in [4], there are five stochastic layers in both the representation and generative model. In between each stochastic layer is a two-layered perceptron (deterministic layers) with varying numbers of hidden units. For the small model s\u0313 recognitionmodel, each stochastic layer, starting from the final layer, had 4, 8, 16, 32, and 64 stochastic units. The two layers in the multilayer perceptrons (MLPs) both had twice as many units as their preceding stochastic layer. Here, the same architecture goes for the generative model. Concerning the large recognition model, the two layers in the MLPs both had 8 times as many units as their preceding stochastic layer, i.e. 32, 64, 128, 256 and 512 units respectively. Under the large model, the generative model architecture remained the same as when using the small model. We used leaky-relu non-linearities everywhere except when we calculated the standard deviation, for which we used 0.01 + softplus(x) as proposed by [6]. In our ablation study, we utilized the single-stochastic layer architecture proposed in [2], i.e. the recognition and generative model have two deterministic layers each, with 200 units per layer. This should indeed regress the TMC to the IWAE model (as should be clear from Alg. 1). We kept the dimension of the latent space as 4. For all the above experiments parameter optimization was computed via Adam ([14]) with parameters \u03b21 = 0.9, \u03b22 = 0.999 and \u03f5 = 10\u22124 and a mini-batch size of 128, all as used in the TMC paper. Apart from evaluating K = 20, the only choice of number of samples in the TMC paper for the MNIST dataset evaluation, we perform a hyperparameter search over a small set ofK s\u0313 and learning rates. A deficit in our work is that we did not use weight normalization ([15]), in contrast to the author. In the TMC paper, weight normalization is recommended for numerical stability. We only encountered numerical overflow for some specific choices in our hyper-parameter search (discussed in 5. Results). In the TMCmodel, we used float32 bit precision at all points except for the tensor inner productswhereweused float64 precision, as it caused numerical instability. We used the TensorFlow standardweight initialization, i.e. Glorot-uniform initialization or Xavier-initialization ([16]). From what we gathered this also seems to be the default in PyTorch which is what the TMC paper used."}, {"heading": "Comments on reproducibility \u2014", "text": "\u2022 Despite the TMC paper being very well-written, when implementing the proposed models we experienced difficulties in grasping the flow of latent variables, z, in the network. Especially that, indeed, there is no sampling step subsequent the stochastic layers in the generativemodel. We believe this confusion to have arisen as the notations for the latents do not clearly express them being sampled from the recognition model, i.e. when used for marginal log-likelihood evaluation in the generative model. Perhaps it was our limited prior knowledge about the IWAE algorithm that led up to this ambiguity, and this might not cause reproducibility issues for others. Nevertheless, we contributewith an algorithmic description (Alg. 1) and a transformation of the original textual description into a figure (Fig. 4).\n\u2022 The plots presented in the TMC paper are informative in the context of comparison between the baseline and the presented model. For reproducibility reasons,\nReScience C 6.2 (#5) \u2013 Kviman, Nilsson and Larsson 2020 5\non the other hand, we would reason that a table presenting the final negative loglikelihood scores would ease comparison of results. Even plots with more tics on the objective value-axes might be beneficial. We acknowledge that the results presented in the TMC paper has not been averaged over multiple runs, and thus, due to the slight fluctuations in performance, the final scores might not be representative. To express the usefulness of our suggestion, we provide a table, Tab. 1, presenting scores averaged over the ultimate 50 epochs. Through contact with the author, we attained the presented results, and may thus make more precise comparisons with our results.\n4.3 Reproducability cost The MNIST dataset is a relatively small dataset, andK affects the training time for each epoch depending on if it is able to store all computations in the graphical memory. One epoch with K = 20, using the same number of units in the MLP units and stochastic units mentioned in implementation details, took around 50-60 seconds on the different GPU s\u0313 Nvidia Geforce 1060 (6 GB ram), 1070 (8 GB ram), Tesla P4 (8 GB ram). If K = 50, and we use the same architecture and hardware, the time increased to 70-80 seconds. When using a Nvidia Tesla P100 GPU and K = 20, then one epoch using IWAE took 11 seconds and 14 seconds using TMC.\n5 Results\nAs stated in Sec. 4.2, we ran two different models of IWAE and TMC, which we refer to as large and small models, as well as conducted a hyper-parameter search for different values of K for various learning rates. Additionally we performed a smaller ablation study. To evaluate the performance of TMC for both the small and large models, we trained an IWAE over 1200 epochs. We trained the TMC models in the same manner as the IWAE models. The results of these experiments are presented as averages over three runs in Fig. 1, where the IWAE was evaluated on the IWAE objective function and the TMC on its respective objective function, i.e. LTMC . In both cases, the TMC outperforms the IWAE with the difference being most significant for the smaller model case. Graphically displaying precise comparisons between the results we obtained to those in the TMC paper was at first, as discussed in Sec. 4.2, slightly impracticable as there are no explicit scores presented for the different models. Fortunately, we were kindly supplied with the author s\u0313 results and may thus give comprehensible comparisons, as is done in Fig. 1. Concerning the small models (Fig. 1, left), our results seem to align with those presented in the TMC paper in terms of convergence rates and final scores. The author s\u0313 models outperforming the reproduced models, and our speculative guess is that it might partially be explained by the use of weight normalization. It is important to emphasize that our speculative guess stands unsupported. An ablation study, with and without weight normalization, should be done in order to test the hypothesis. Apart from numerical stability, which might explain the author s\u0313 less fluctuating curve, weight normalization speeds up convergence ([15]). Considering the large models (Fig. 1, right), the author s\u0313 TMC clearly outperform the reproduced TMC. The gap seems too large to be caused by weight normalization. Instead we expect the gap to stem from the author not averaging over multiple runs. As such we cannot expect our results to completely coincide with those from the TMC paper due to the stochasticity when sampling, initializing weights etc. But, since, the purpose of the TMC paper was to display the TMC s\u0313 superiority over the IWAE, and not state-of-the-art results, we did not investigate this discrepancy further. Nonetheless, to get a sense of the final scores in our results, we computed the mean and standard deviation of the last 50 epochs for each of our models, these results are presented in Tab. 1. Clearly the averaged scores are larger than those perceived when inspecting our curves in Fig. 1. Furthermore, we also examined\nReScience C 6.2 (#5) \u2013 Kviman, Nilsson and Larsson 2020 6\nthe TMC paper s\u0313 claim of adding negligible time when training an IWAE for each epoch, and found that our implementation of IWAE (consideringK = 20 importance samples) takes roughly 11 s per epoch while our TMC (considering Kn = 205 = 3, 200, 000 effective importance samples) implementation takes roughly 14 s per epoch, this seem to corroborate the claim by the author of TMC paper of adding negligible time to train. For the small IWAE and TMCmodelsmentioned above, we, in Appendix C, present their conditional reconstructions. I.e., given an MNIST test sample, we show what the reconstruction looks like. Note, these are complementary experiments to the original TMC paper. As displayed in Fig. 5, both algorithms produce almost perfect reconstructions. For future work, it would be interesting to see how well they performed on more complex datasets, such as CIFAR-10 ([17]).\nThe scores presented in Fig. 3 were obtained during our ablation study. We expected the two models to give similar results, as they, indeed, did. Averaged over an increasing number of runs, the results should become indistinguishable as TMC and IWAE are in essence the samealgorithmswhen there is only one stochastic layer, i.e. no intermediate stochastic layers in contrast to the model shown in Fig. 4. This since the parameters of the prior distribution are fixed \u2013 the model assumes a standard normal distribution, therefor there is no new information when evaluating the latent variables under the same distribution parameters. To support this claim, we refer to Alg. 1, specifically where we iterate through the distribution parameters. Mathematically, as is shown in [6], the number of evaluated importance samples grows exponentially with the layers, so if we reduce the number of layers to one, we effectively evaluateK1 = K samples in the TMC, the same as for the IWAE. Regarding the worsening of the objective value after approximately 200 epochs, we are not sure how to explain this. Probably, it is due to the insufficiently small model (100 units per deterministic layer and one stochastic layer with four units). The purpose of our ablation study was simply to show that the TMC\nReScience C 6.2 (#5) \u2013 Kviman, Nilsson and Larsson 2020 7\nand IWAE are the same under the mentioned conditions, and so we do not dig deeper into this phenomena. In Appendix D, we display the TMC s\u0313 clustering abilities, compared to the IWAE, via a two-dimensional latent space. By observing Fig. 6, it is clear that the TMC (right) yields less ambiguous clusters than those produced by the IWAE. Especially, note to which small degree digit nine s\u0313 representation mixes with the others digit representations, as opposed to its cluster for the IWAE (left). During our non-exhaustive hyper-parameter search, we evaluated the TMC model for K \u2208 {1, 5, 20, 50}, and for eachKwe tested different learning rates lr \u2208 {1e\u22125, 1e\u22124, 1e\u2212 3}. The results of this hyper-parameter search are presented in Fig. 2. Each of these different experiments was run over 500 epochs for the small TMCmodel, unsurprisingly K = 50with lr = 1e\u2212 3 has the best performance over these 500 epochs. This as we are effectively looking at more samples for each batch. However, setting K = 50 increases the running time significantly as the number of matrix calculations needed are more than doubled to that of K = 20 and the performance is marginally better compared to that of settingK = 20 and lr = 1e\u22123. The parameter settings proposed in the TMCpaper seems well suited for the previous experiments based on our hyper-parameter search. We also tested with lr = 1e \u2212 2, and this produced NaN results for all different values of K we tested, this might be related to the inherent numerical instability of the TMC method, especially since we have not used weight normalization in our experiments. The deployment of weight normalization might very well alleviate this problem and a larger learning rate might be able to perform better than those we have examined.\nReScience C 6.2 (#5) \u2013 Kviman, Nilsson and Larsson 2020 8\n6 Discussion and Conclusions\nAlthough our reproduced results and the TMC paper results differ, the general observations of the authors remain valid. I.e., the TMC clearly outperforms the IWAEmodel on the given dataset. This is unsurprising as the TMC, for K = 20 and five layers, considers a factor millionmore importance samples ([6]). Despite this, we see that the author s\u0313 claim concerning negligible time difference between the two models is also justifiable, according to our findings. We note in our work, that the intentions of [6] were not to achieve state-of-the-art performances, but to compare the proposed the model to the baseline used in the TMC paper. Perhaps for this reason, the results are presented in a way such that they are impractical to study in detail. Although the author very skillfully and clearly describes the novelties in the TMC paper, understanding themodel architecture was, for us, quite challenging. Furthermore, we argue that the specifics regarding the training and testing scheme could have been outlined in the TMC paper, at least for reproducibility. We believe, the discrepancies in test results stems from two sources, us omitting the use of weight normalization, and the presented one-shot performances, i.e. non-averaged. To conclude, we have successfully reproduced the fundamental results of the NeurIPS 2019 paper \u201dTensor Monte Carlo: Particle Methods for the GPU Era\u201d, in order to support the author s\u0313 claims. Additionally, we have complemented the original paper with an algorithmic description, a figure explaining the architecture and experiments displaying the TMC s\u0313 reconstruction and clustering abilities. We applaud the author for an ingenious take to improve on the existing IWAE. We hope future work will be done on the tensor Monte-Carlo algorithm.\n7 Acknowledgements\nWe would like to thank Laurence Aitchison for supplying us with answers to our questions (patiently), and the original results so that we could integrate them here."}, {"heading": "Appendices", "text": "A Numerically stable tensor inner product (in log-domain)\nThe operation of computing ratios of probabilities is typically unstable, especially if the probabilities have small values. In order to average over a large number of importance samples in log-domain, we need a numerically stable method. If we compute\neZik = \u2211 j eXijeYjk ,\nin order to get Zik = log \u2211 j eXijeYjk ,\nvery large or small values of Xij and (or) Yjk could lead to numerical instability. To circumvent this issue, we subtract the largest values along j before computing the sum. Then, to ensure the correctness of our marginal log-likelihoods, we add these values back as follows\nZik = log \u2211 j eXij\u2212maxj(Xij)eYjk\u2212maxj(Yjk) +maxj(Xij) +maxj(Yjk)\nB TMC Architecture\nHere we present a graphical overview of the model architecture we used for the TMC.\nReScience C 6.2 (#5) \u2013 Kviman, Nilsson and Larsson 2020 10\nC Reconstructions\nBelow we display the reconstructions for the small IWAE (left) and small TMC (right) models, given an observation (sample) from the MNIST test set. In both figures, the far left column shows the input test samples (original), and the remaining columns the reconstructions.\nReScience C 6.2 (#5) \u2013 Kviman, Nilsson and Larsson 2020 11\nD Clustering\nWe compared the clustering abilities for the IWAE versus the TMC algorithms, given a two-dimensional latent space (the latent space farthest from the data). In this experiment, there were two stochastic layers in the recognition model with dimensions 50 and 2 (deterministic layers [100, 100] and [100, 100] for the two MLPs). To obtain these data points, we averaged overK for every latent. It is apparent from Fig. 6 that the TMC offers a richer latent space than the IWAE, for this specific architecture. For instance, note how the digit nine s\u0313 representation (view color bar) is not mixing as much, for the TMC (right), with its neighboring clusters as is the case for the IWAE (left).\nReScience C 6.2 (#5) \u2013 Kviman, Nilsson and Larsson 2020 12"}], "title": "[Re] Tensor Monte Carlo: Particle Methods for the GPU Era", "year": 2020}