{"abstractText": "Meng, Bachmann, and Khan1 gives a mathematically principled approach to solve the discrete optimization problem that occurs in the case of BinaryNeuralNetworks and claims to give a similar performance on various classification benchmarks such as MNIST, CIFAR-10, and CIFAR100 as compared to their full-precision counterparts, as well as other recent algorithms to train BNNs like PMF and Bop. The paper also claims that the BayesBiNN method has an application in the continual learning domain as it helps in overcoming catastrophic forgetting of the past by using the posterior approximation of the previous task as a prior for the upcoming task. We try to reproduce all the results presented in the original paper by making a separate and independent codebase.", "authors": [{"affiliations": [], "name": "Prateek Garg"}, {"affiliations": [], "name": "Lakshya Singhal"}, {"affiliations": [], "name": "Ashish Sardana"}], "id": "SP:ca06b8ec75e8e6228f518b943fafda32e4ff2e6e", "references": [{"authors": ["X. Meng", "R. Bachmann", "M.E. Khan"], "title": "Training Binary Neural Networks using the Bayesian Learning Rule", "year": 2020}, {"authors": ["C.J. Maddison", "A. Mnih", "Y.W. Teh"], "title": "The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables.", "venue": "CoRR", "year": 2016}, {"authors": ["O. Shayer", "D. Levi", "E. Fetaya"], "title": "Learning DiscreteWeights Using the Local Reparameterization Trick.", "venue": "CoRR", "year": 2017}, {"authors": ["O. Ronneberger", "P. Fischer", "T. Brox"], "title": "U-Net: Convolutional Networks for Biomedical Image Segmentation.", "venue": "CoRR", "year": 2015}, {"authors": ["T. Moons"], "title": "Two moons datasets description. 2018. URL: https://scikit- learn.org/stable/modules/generated/ sklearn.datasets.make_moons.html", "year": 2018}, {"authors": ["E. Snelson", "Z. Ghahramani"], "title": "Sparse Gaussian Processes Using Pseudo-Inputs.", "venue": "Proceedings of the 18th International Conference on Neural Information Processing Systems. MIT Press,", "year": 2005}, {"authors": ["I.J. Goodfellow", "M. Mirza", "X. Da", "A.C. Courville", "Y. Bengio"], "title": "An Empirical Investigation of Catastrophic Forgeting in Gradient-Based Neural Networks.", "venue": "International Conference on Learning Representations,", "year": 2014}, {"authors": ["E. Strubell", "A. Ganesh", "A. McCallum"], "title": "Energy and Policy Considerations for Deep Learning in NLP.", "venue": "CoRR", "year": 2019}], "sections": [{"text": "R E S C I E N C E C"}, {"heading": "Replication / ML Reproducibility Challenge 2020", "text": "[Re] Training Binary Neural Networks using the Bayesian"}, {"heading": "Learning Rule", "text": "Prateek Garg1, ID , Lakshya Singhal1, ID , and Ashish Sardana2, ID 1Indian Institute of Technology Delhi, New Delhi, India \u2013 2NVIDIA, Bangalore, KA, India\nEdited by Koustuv Sinha\nReviewed by Anonymous Reviewers\nReceived 29 January 2021\nPublished 27 May 2021\nDOI 10.5281/zenodo.4833681"}, {"heading": "Reproducibility Summary", "text": "Meng, Bachmann, and Khan1 gives a mathematically principled approach to solve the discrete optimization problem that occurs in the case of BinaryNeuralNetworks and claims to give a similar performance on various classification benchmarks such as MNIST, CIFAR-10, and CIFAR100 as compared to their full-precision counterparts, as well as other recent algorithms to train BNNs like PMF and Bop. The paper also claims that the BayesBiNN method has an application in the continual learning domain as it helps in overcoming catastrophic forgetting of the past by using the posterior approximation of the previous task as a prior for the upcoming task. We try to reproduce all the results presented in the original paper by making a separate and independent codebase."}, {"heading": "Scope of Reproducibility", "text": "We try to verify the performance of our re-implementation of the BayesBiNN optimizer on various classification and regression benchmarks. We also implemented the STE optimizer which was the central baseline model used in the paper. Finally, we tried to evaluate the results of BayesBiNN on the continual learning benchmark to get a better insight."}, {"heading": "Methodology", "text": "We developed our separate code-base, consisting of an end-to-end trainer with a Keraslike interface, for the reproduction which includes the implementation of the BayesBiNN and STE optimizer. We did refer to the author s\u0313 code open-sourced on GitHub to get some insights about the hyperparameters and other doubts that emerged during code development."}, {"heading": "Results", "text": "We reproduced the accuracy of the BayesBiNN optimizer within less than 0.5% of the originally reported value, which upholds the conclusion that it performs nearly as well as its full-precision counterpart in classification tasks. When we tried this in a semantic segmentation context, we found that the results were very underwhelming and in\nCopyright \u00a9 2021 P. Garg, L. Singhal and A. Sardana, released under a Creative Commons Attribution 4.0 International license. Correspondence should be addressed to Prateek Garg (prateekgarg.iitd@gmail.com) The authors have declared that no competing interests exist. Code is available at https://github.com/prateekstark/training-binary-neural-network \u2013 DOI 10.5281/zenodo.4716863. \u2013 SWH swh:1:dir:904c01b8e4ac0d4acd1f5fe667511d5a9234d8da. Open peer review is available at https://openreview.net/forum?id=bhiGno-Cxq.\nReScience C 7.2 (#5) \u2013 Garg, Singhal and Sardana 2021 1\ncontrast with the seemingly good results by the STE optimizer even with much hyperparameter tuning. We can conclude that, like other Bayesian methods, it is difficult to train BayesBiNN on more complex tasks."}, {"heading": "What was easy", "text": "After we worked out the mathematics behind the BayesBiNN approach, we developed a pseudo-code for the optimization process which alongwith references from the author s\u0313 code, helped us a lot in our reproduction study."}, {"heading": "What was difficult", "text": "Some of the hyperparameters were not mentioned by the authors in their paper so it was difficult to approximate the values of those parameters. The lack of resources was the next big difficulty that we faced.\nCommunication with original authors We had a very fruitful conversation with the authors, which helped us in better understanding the BayesBiNN approach and its extension to the segmentation domain. The detailed pointers are given at the end of this report.\n1 Introduction\nDeep Learning is moving towards larger and larger parameters day-by-day, which often makes it difficult to run on resource-constraint devices like mobile phones. Binary Neural Networks (BNNs) could act as a savior in such situations, helping in largely saving storage and computational costs. The problem of optimizing this binary set of weights is clearly a discrete optimization problem. Previous approaches like Straight-Through Estimator (STE) and Binary Optimizer (Bop) tend to ignore this and use gradient-based methods, which still worked in practice. The paper presents a mathematically principled approach for training BNNs which also justifies the current approaches.\n2 Scope of reproducibility\nThe paper mentions a bayesian approach to solve the discrete optimization problem in the case of Binary Neural Networks (BNNs). The outcome of this approach was a BayesBiNN optimizer which could be used to train BNNs and achieve similar accuracy as compared to their full-precision counterparts. To verify the claims given in the paper, we target to achieve the following objectives:\n\u2022 Work out and present the mathematics behind BayesBiNN in a simpler way and prepare a pseudo-code to the optimizer.\n\u2022 Implement the BayesBiNN optimizer and STE optimizer to verify the accuracy on tasks of varying domains, as reported in the original paper.\n\u2022 Reproduce the results for other baselines present in the paper such as proximal mean-field (PMF) according to the hyper-parameters given in the paper.\n\u2022 Evaluate the performance of BayesBiNN optimizer in more complex domains like semantic segmentation.\nReScience C 7.2 (#5) \u2013 Garg, Singhal and Sardana 2021 2\n3 Methodology\nWe have re-implemented the algorithm proposed in the paper from scratch using PyTorch and created an end-to-end model trainer with a Keras-like interface. We referred to the code given by the authors for the baselinemodel hyperparameters and the source of synthetic datasets. The algorithmpresented by the original authors in their paper can be represented as follows:\nAlgorithm 1: Bayesian Learning rule for BayesBiNN Input: Input: Initialize \u03bb\nfor number of training epochs do for i = 1,...,number of mini-batch examples do\nSample \u03f5 \u223c U(0, 1) and set \u03b4 = 12 log \u03f5\n1\u2212\u03f5 Initialize wb = tanh((\u03bb+ \u03b4)/\u03c4) Compute following using gumbel-softmax trick\ngi := 1\nM \u2207wb l(yi, fwr (xi))\nsi := N(1\u2212 w2b )\n\u03c4(1\u2212 tanh(\u03bb)2)\nend Update \u00b5 and \u03bb using following equation\n\u00b5\u2190 tanh(\u03bb)\n\u03bb\u2190 (1\u2212 \u03b1)\u03bb\u2212 \u03b1[ M\u2211 i=1 (si \u2299 gi)\u2212 \u03bb0]\nend\nThis would make the paper easier to interpret and this implementation on code. Some of the mathematical expressions mentioned in the original paper were presented from various sources andmissed out several intermediate steps whichwe found to be very important while reproducing the paper from scratch. Here we present a step-wise derivation of some important expressions written in the original paper: Bayesian formulation of the discrete optimization problem, in which loss has to be minimized w.r.t posterior q(w), given prior p(w) can be written as:\nEq(w) [ N\u2211 i=1 l( yi, fw(xi) ) ] +DKL[ q(w)\u2225p(w)]\nTo solve the above optimization problem, Bayesian learning rule given in Khan and Rue2 is applied, assuming solution to be a part of minimal exponential family of distribution, given by:\nq(w) = h(w)exp[\u03bbT\u03d5(w)\u2212A(\u03bb)]\nwhere base measure h(w) is assumed to be 1. Following is the update rule used to learn \u03bb:\n\u03bb\u2190 (1\u2212 \u03c1)\u03bb\u2212 \u03c1[\u2207\u00b5Eq(w)[l(yifw(xi))]\u2212 \u03bb0]\nwhere \u03c1 is the learning rate, \u00b5 = Eq(w)[\u03d5(w)]. Bernoulli distribution being a special case of minimal exponential family distribution, we assume prior p(w) \u223c Bern(p) with\nReScience C 7.2 (#5) \u2013 Garg, Singhal and Sardana 2021 3\np = 0.5, and posterior q(w) to be mean-field Bernoulli distribution:\nq(w) = W\u220f j=1 p 1+wj 2 j (1\u2212 pj) 1\u2212wj 2\nFor weight j,\nq(wj) = exp( 1\n2 (1 + wj) log pj +\n1 2 (1\u2212 wj) log(1\u2212 pj))\n= exp( wj\ufe38\ufe37\ufe37\ufe38 \u03d5(w) 1 2 log p 1\u2212 p )\ufe38 \ufe37\ufe37 \ufe38\n\u03bb\n+ 1\n2 log(p(1\u2212 p))\nComparing above expression with minimal exponential family distribution, we can say:\n\u03bb = 1\n2 log\np\n1\u2212 p and\u03d5(w) = w.\nWe defined \u00b5 = Eq(w)[\u03d5(w)],\n\u00b5 = \u222b wq(w)dw = E[q(w)] = \u2211 wi\u2208{\u22121,1} wiq(wi) = \u2211\nwi\u2208{\u22121,1}\nwip 1+wi 2 (1\u2212 p) 1\u2212wi 2 = \u2212(1\u2212 p) + p\n= 2p\u2212 1\nFrom above derivations we can say that, p = 1/(1 + exp(\u22122\u03bb)) = Sigmoid(2\u03bb) and q(w) \u223c Bern(p). To implement the update rule, we need to compute the gradient with respect to \u00b5. Original paper uses a reparamaterization trick called gumbel-softmax trick Maddison, Mnih, and Teh3, which is used to relax the discrete random variables of a concrete distribution (for eg, bernoulli distribution). Binary concrete relaxation Maddison, Mnih, and Teh3 of binary concrete random variableX \u2208 (0, 1) with distributionX \u223c BinConcrete(\u03b1, \u03bb) with temperature \u03bb and location \u03b1,\nX = 1\n1 + exp(\u2212(log\u03b1+ L)/\u03bb)\nwhere L \u223c Logistic. And its density is given by\np\u03b1,\u03bb(x) = \u03bb\u03b1x\u2212\u03bb\u22121(1\u2212 x)\u2212\u03bb\u22121\n(\u03b1x\u2212\u03bb + (1\u2212 x)\u2212\u03bb)2\nUsing above expressions, for binary weights wj \u2208 {0, 1}, relaxed variable w \u03f5j ,\u03c4 r (pj) \u2208 (0, 1) can be used with temperature \u03c4 and \u03b1 = e2\u03bb given by\nw\u03f5j ,\u03c4r (pj) = 1\n1 + exp(\u2212 2\u03bbj+2\u03b4j\u03c4 ) ,\nwhere \u03b4j \u223c Logistic and its density is given by\np(w\u03f5j ,\u03c4r (pj)) = \u03c4e2\u03bbw \u03f5j ,\u03c4 r (pj) \u2212\u03c4\u22121(1\u2212 w\u03f5j ,\u03c4r (pj))\u2212\u03c4\u22121\n(e2\u03bbw \u03f5j ,\u03c4 r (pj)\u2212\u03c4 + (1\u2212 w \u03f5j ,\u03c4 r (pj))\u2212\u03c4 )2\nReScience C 7.2 (#5) \u2013 Garg, Singhal and Sardana 2021 4\n4 Experimental setup\n4.1 Model descriptions We kept the model architectures the same as mentioned in the original paper to maintain uniformity and implemented them ourselves. For the MNIST classification task we used the BinaryConnect architecture and for the CIFAR classification task we used the VGGBinaryConnect architecture. The authors also compared their BayesBiNN method with the LR-Net method in Shayer, Levi, and Fetaya4. We implemented the same model architecture as in the LR-Net paper. The detailed architectures are mentioned in the supplementary material provided with this report. For the segmentation task, we used the original U-Net architecture detailed in Ronneberger, Fischer, and Brox5 with a minor difference that we introduced a BatchNorm layer after every convolution layer.\n4.2 Datasets The datasets used for image classification tasks are MNIST, CIFAR-10, and CIFAR-100. For generating visualizations for the BayesBiNN and STE methods, we used small toy datasets, the Snelson dataset6 for regression problems, and TwoMoons\u0313 dataset Snelson and Ghahramani7 for classification problems. For the segmentation part, we used the Brain Tissue segmentation dataset from Ronneberger, Fischer, and Brox5, and for the continual learning visualizations we used the permuted MNIST dataset Goodfellow et al.8. The pre-processing of inputs has been kept the same as mentioned in the original paper and has been detailed below. Pre-processing: For the MNIST dataset we simply normalize the images and do not perform data augmentation. We keep our validation split as 0.1 uniformly across all sets of experiments except the comparison with the LR-Net method Shayer, Levi, and Fetaya4. For the CIFAR datasets also, we perform the normalization of images along with dataaugmentation where we generate images by randomly cropping a 32x32 image from a 40x40 padded image. Finally, for our semantic segmentation task, we had a very small dataset of 30 images out of which 24 were chosen for training and 6 for validation. No other pre-processing has been done.\n4.3 Hyperparameters We have used the hyper-parameters given in the original paper. Table Table 1 contains the list of all the parameters we used for our experiments:\nReScience C 7.2 (#5) \u2013 Garg, Singhal and Sardana 2021 5\n4.4 Computational requirements All our final experimental results were performed on a machine having 1 NVIDIA Tesla V100 GPU with 16 GB memory. Training the Binary Network with BayesBiNN optimizer for a single run, takes around 2.5 GPUhours forMNIST, 5.5 GPUhours for CIFAR-10, and around 8.5 GPU hours for the CIFAR-100 dataset, in the current experimental setup.\n5 Results\nIn Table Table 1we report our results for various classificationbenchmarks using our implemented BayesBiNN and STE optimizer. We notice that we get a difference of less than 0.1% as compared to that in the original paper. We generated the results for baseline STE optimizer and full-precision networks by evaluating our implementation of these methods. We also generated the results of PMF, by modifying its original open-sourced code and using the hyperparameters mentioned in the original paper.\n5.1 Comparison with LR-Net Authors compared their BayesBiNN approach to the LR-Netmethod presented in Shayer, Levi, and Fetaya4. We tried to reproduce the result for the same setting. In this comparison, the data pre-processing and augmentationmethods remain the same asmentioned in section 4.2, but we do not split the data in training and validation sets in this case. We\nReScience C 7.2 (#5) \u2013 Garg, Singhal and Sardana 2021 6\ndenote the test accuracies after 190 epochs in the case of MNIST and 290 epochs in the case of CIFAR-10, as done in the original paper to maintain uniformity. Note that, our accuracy ismatchingwith that of the original authors in the case ofMNIST but not in the case of CIFAR-10. We suspect that this is due to some difference in Batch-Norm layers used.\nOptimizer MNIST CIFAR10 BayesBiNN (ours) 99.52% 84.49% BayesBiNN (orig.) 99.50% 93.97%\nLR-net Shayer, Levi, and Fetaya4 99.47% 93.18%\nmodels are intrinsically very difficult to train. For the results shown in Table Table 5 and Figure Figure 5, we have used the hyperparameters denoted in Table Table 1.\n6 Discussion\nWe reproduced almost all the experiments given in the original paper and most of our results match with the original claims. While this BayesBiNN approach is mathematically principled, we tried to take a step forward by using that optimizer on a single segmentation task. However the results were against our expectation and the result of segmentation was a zoomed segmented image of the input with lots of noise. In addition to this, in the case of comparison with the LR-Net method, our accuracy differs from that of the original authors, which we feel might be due to some difference in architecture chosen. Themajor contribution of our work is developing a code base library\nReScience C 7.2 (#5) \u2013 Garg, Singhal and Sardana 2021 8\nbased on PyTorch with a Keras type interface for training BNNs with several different methods in its arsenal. This could reduce the coding efforts required for training BNNs and could help in future research as benchmarking library.\n6.1 What was easy The original paper contained a very good explanation of the mathematics behind the BayesBiNN approach. After we worked that out the pseudo-code as pointed out in Algorithm algorithm 1, the basic implementation of the optimizer became easy and easily verifiable by the author s\u0313 original code. The appendix in the original paper contained a list of various hyper-parameters used for experiments. This helped us a lot while running the experiments and deciding the range of hyper-parameters while doing ablation studies.\n6.2 What was difficult Themost difficult part here was running a large number of experiments in lack of many computational resources. This difficulty was increased since we are taking an average of 5 runs while reporting all our results. Apart from this, we also faced some difficulty in taking care of the hyper-parameters, which were not mentioned in the original paper (like momentum coefficient). To cater to that, we had to guess some possible values of the hyper-parameters and run small random searches to find a good candidate. Finally, we also faced difficultywhile reproducing the results for the baselines PMF andBop, and adapting their experimental settings tomatchwith those used in the original BayesBiNN paper. Since their code was written a long time ago and used older software stack, this task took us a lot of time.\n6.3 Communication with original authors We did not understand the intent of the authors for choosing temperature as 1 in the case of experiments on synthetic datasets. We were also curious about the author s\u0313 view on segmentation tasks using BayesBiNN. Hence, we reached out to the authors via email along with the review of their paper, to ask for some pointers. They gave the following major pointers:\n\u2022 It is reasonable that at high temperatures the learned distribution will have high variance. The mode mentioned in the paper refers to the sign(\u0302(w)), where (\u0302w) denotes the expectation of the learned posterior Bernoulli distribution. It is not appropriate to directly use the continuous (\u0302w) as the mode. Another way is to use\nReScience C 7.2 (#5) \u2013 Garg, Singhal and Sardana 2021 9\nmean, which samples from the learned posterior Bernoulli distribution, and then make predictions using ensemble learning.\n\u2022 STE is more stable and suggested by the authors to act as a baseline, in particular, Adam STE first, to make sure binary networks work. As shown in the paper, there is literally very little difference between STE and BayesBiNN but indeed the latter is difficult to train, as most Bayesian optimizers.\nBroader Impact\nRecent researches Strubell, Ganesh, and McCallum9 mention that training a single big transformer model could emit around 626,155 lbs CO2 which is around 5 times of average carbon emission by a car in its total lifetime. Clearly, Deep Learning takes a huge toll on the environment which is why there has been an increased focus on much more energy-efficient \u201dGreen AI\u201d. BNNs intrinsically have far less computational and space complexity as compared to their full-precision counterparts and as we can see above they can also achieve accuracy close to the full-precision networks, at least in the classification tasks, and also show the potential of expanding well to more complex segmentation tasks. This can help us a lot in moving towards cleaner Deep Learning. This field of research also provides a huge set of opportunities in extending AI to edge devices with much smaller and low-energy systems. We feel that its potential impact on the environment and sustainability is at par with its academic importance."}, {"heading": "Acknowledgement", "text": "We would like to thank NVIDIA and IIT Delhi HPC facility for providing necessary computational resources. The computational requirements were also partly met by Google Colab and Code Ocean. We would also like to thankWeights & Biases for providing free teams to people in academic sphere which proved to be most valuable for our experiments and collaboration."}, {"heading": "Appendix", "text": ""}, {"heading": "MLP Binary Connect Architecture", "text": "Dropout p = 0.2 Fully Connected Layer (units = 2048, bias = False)\nReLU Batch Normalization Layer (gain = 1, bias = 0)\nDropout p = 0.2 Fully Connected Layer (units = 2048, bias = False)\nReLU Batch Normalization Layer (gain = 1, bias = 0)\nDropout p = 0.2 Fully Connected Layer (units = 2048, bias = False)\nReLU Batch Normalization Layer (gain = 1, bias = 0)\nDropout p = 0.2 Fully Connected Layer (units = 2048, bias = False) Batch Normalization Layer (gain = 1, bias = 0)\nSoftmax\nReScience C 7.2 (#5) \u2013 Garg, Singhal and Sardana 2021 11"}, {"heading": "VGG Binary Connect Architecture", "text": "Convolutional Layer (channels = 128, kernel-size = 3\u00d73, bias = False, padding = same) ReLU Batch Normalization Layer (gain = 1, bias = 0) Convolutional Layer (channels = 128, kernel-size = 3\u00d73, bias = False, padding = same)\nReLU Max Pooling Layer (size = 2\u00d72, stride = 2\u00d72)\nBatch Normalization Layer (gain = 1, bias = 0) Convolutional Layer (channels = 256, kernel-size = 3\u00d73, bias = False, padding = same)\nReLU Batch Normalization Layer (gain = 1, bias = 0)\nConvolutional Layer (channels = 256, kernel-size = 3\u00d73, bias = False, padding = same) ReLU\nMax Pooling Layer (size = 2\u00d72, stride = 2\u00d72) Batch Normalization Layer (gain = 1, bias = 0)\nConvolutional Layer (channels = 512, kernel-size = 3\u00d73, bias = False, padding = same) ReLU Batch Normalization Layer (gain = 1, bias = 0) Convolutional Layer (channels = 512, kernel-size = 3\u00d73, bias = False, padding = same)\nReLU Max Pooling Layer (size = 2\u00d72, stride = 2\u00d72)\nBatch Normalization Layer (gain = 1, bias = 0) Fully Connected Layer (units = 1024, bias = False)\nReLU Batch Normalization Layer (gain = 1, bias = 0)\nFully Connected Layer (units = 1024, bias = False) ReLU\nBatch Normalization Layer (gain = 1, bias = 0) Fully Connected Layer (units = 10, bias = False) Batch Normalization Layer (gain = 1, bias = 0)\nSoftmax"}, {"heading": "MLP Binary Connect Architecture for Continual Learning", "text": "Fully Connected Layer (units = 100, bias = False) ReLU\nBatch Normalization Layer (gain = 1, bias = 0) Fully Connected Layer (units = 100, bias = False)\nReLU Batch Normalization Layer (gain = 1, bias = 0)\nFully Connected Layer (units = 100, bias = False) ReLU\nBatch Normalization Layer (gain = 1, bias = 0) Softmax\nReScience C 7.2 (#5) \u2013 Garg, Singhal and Sardana 2021 12"}, {"heading": "LRNet Architecture (MNIST)", "text": "Convolutional Layer (channels = 32, kernel-size = 5\u00d75, bias = False, padding = same) Max Pooling Layer (size = 2\u00d72, stride = 2\u00d72)\nBatch Normalization Layer (gain = 1, bias = 0) ReLU\nConvolutional Layer (channels = 64, kernel-size = 5\u00d75, bias = False, padding = same) Max Pooling Layer (size = 2\u00d72, stride = 2\u00d72)\nBatch Normalization Layer (gain = 1, bias = 0) ReLU\nFully Connected Layer (units = 512, bias = False) Batch Normalization Layer (gain = 1, bias = 0)\nReLU Fully Connected Layer (units = 10, bias = False) Batch Normalization Layer (gain = 1, bias = 0)\nSoftmax"}, {"heading": "LRNet Architecture (CIFAR-10)", "text": "Convolutional Layer (channels = 128, kernel-size = 3\u00d73, bias = False, padding = same) Batch Normalization Layer (gain = 1, bias = 0) ReLU Convolutional Layer (channels = 128, kernel-size = 3\u00d73, bias = False, padding = same)\nBatch Normalization Layer (gain = 1, bias = 0) Max Pooling Layer (size = 2\u00d72, stride = 2\u00d72)\nReLU Convolutional Layer (channels = 256, kernel-size = 3\u00d73, bias = False, padding = same)\nBatch Normalization Layer (gain = 1, bias = 0) ReLU\nConvolutional Layer (channels = 256, kernel-size = 3\u00d73, bias = False, padding = same) Batch Normalization Layer (gain = 1, bias = 0) Max Pooling Layer (size = 2\u00d72, stride = 2\u00d72) ReLU Convolutional Layer (channels = 512, kernel-size = 3\u00d73, bias = False, padding = same)\nBatch Normalization Layer (gain = 1, bias = 0) ReLU\nConvolutional Layer (channels = 512, kernel-size = 3\u00d73, bias = False, padding = same) Batch Normalization Layer (gain = 1, bias = 0) Max Pooling Layer (size = 2\u00d72, stride = 2\u00d72)\nReLU Fully Connected Layer (units = 1024, bias = False) Batch Normalization Layer (gain = 1, bias = 0)\nReLU Fully Connected Layer (units = 10, bias = False) Batch Normalization Layer (gain = 1, bias = 0)\nSoftmax\nSemantic Segmentation using BayesBiNN with augmented dataset We generated 1260 images from 30 original images using the rotation, random horizontal flip, random vertical flip operations. The result for BayesBiNN with this extended\nReScience C 7.2 (#5) \u2013 Garg, Singhal and Sardana 2021 13\ndataset was still very poor and inconsistent with the other methods (STE and Full Precision). The results presented in Section 5.4 were to show the extent of difficulty to train BayesBiNN for segmentation task as even with such a small dataset and large number of epochs, it was still not even able to overfit. Following are some of the images obtained by using BayesBiNN with this bigger dataset:\n(a)Mask example 1 (b)Mask example 2\nReScience C 7.2 (#5) \u2013 Garg, Singhal and Sardana 2021 14"}], "title": "[Re] Training Binary Neural Networks using the Bayesian Learning Rule", "year": 2021}