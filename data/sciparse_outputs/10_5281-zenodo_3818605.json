{"abstractText": "Miscalibration of a model is defined as the mismatch between predicting probability estimates and the true correctness likelihood. In this work, we aim to replicate the results reported by [1] on their analysis of the effect of Mixup [2] on a network\u2019s calibration. Mixup is an effective yet simple approach of data augmentation, which generates a convex combination of a pair of training images and their corresponding labels as the input and target for training a network. We replicate the results reported by the authors for CIFAR-100 [3], Fashion-MNIST [4], STL-10 [5], outof-distribution and random noise data. Our implementation code can be found at https://github.com/MacroMayhem/OnMixup.", "authors": [{"affiliations": [], "name": "Aditya Singh"}, {"affiliations": [], "name": "Alessandro Bay"}, {"affiliations": [], "name": "Koustuv Sinha"}], "id": "SP:2cad1ff58c0ba99acf2d57a4e9b96f74d7f074ee", "references": [{"authors": ["S. Thulasidasan", "G. Chennupati", "J.A. Bilmes", "T. Bhattacharya", "S.E. Michalak"], "title": "OnMixup Training: Improved Calibration and Predictive Uncertainty for Deep Neural Networks.", "year": 2019}, {"authors": ["H. Zhang", "M. Cisse", "Y.N. Dauphin", "D. Lopez-Paz"], "title": "mixup: Beyond Empirical Risk Minimization.", "venue": "In: International Conference on Learning Representations", "year": 2018}, {"authors": ["A. Krizhevsky"], "title": "Learning multiple layers of features from tiny images", "venue": "Tech. rep", "year": 2009}, {"authors": ["H. Xiao", "K. Rasul", "R. Vollgraf"], "title": "Fashion-MNIST: a Novel Image Dataset for Benchmarking", "venue": "Machine Learning Algorithms. Aug", "year": 2017}, {"authors": ["A. Coates", "A. Ng", "H. Lee"], "title": "An Analysis of Single-Layer Networks in Unsupervised Feature Learning.", "venue": "Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics", "year": 2011}, {"authors": ["V. Vapnik"], "title": "Principles of Risk Minimization for Learning Theory.", "venue": "Advances in Neural Information Processing Systems 4", "year": 1992}, {"authors": ["V.N. Vapnik"], "title": "The Nature of Statistical Learning", "venue": "Theory. Berlin, Heidelberg: Springer-Verlag,", "year": 1995}, {"authors": ["O. Chapelle", "J.Weston", "L. Bottou", "V. Vapnik"], "title": "Vicinal RiskMinimization.", "venue": "In:Advances in Neural Information Processing Systems 13", "year": 2001}, {"authors": ["C. Guo", "G. Pleiss", "Y. Sun", "K.Q. Weinberger"], "title": "On Calibration of Modern Neural Networks.", "venue": "Proceedings of the 34th International Conference on Machine Learning - Volume 70. ICML\u201917", "year": 2017}, {"authors": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei-Fei"], "title": "ImageNet: A Large-ScaleHierarchical ImageDatabase.", "year": 2009}], "sections": [{"text": "Edited by Koustuv Sinha ID\nReviewed by Anonymous Reviewers\nReceived 15 February 2020\nPublished 21 May 2020\nDOI 10.5281/zenodo.3818605\n1 Introduction\n1.1 Calibration Modern neural networks are miscalibrated, i.e. their predicted confidence value is not reflective of its confidence in prediction. If a model is overconfident, it becomes prone to making wrong predictions with high confidence, thereby depleting the trust on its predictions. It becomes really important in high risk applications (such as in medical diagnosis, automated navigation, etc.) for the neural network\u2019s prediction to be correct as well as trustworthy. If the network generates reliable predictions, it would enable the use of some form of fall back mechanism, such as a human-in-the-loop for life critical scenarios. Finding a solution to this problem of miscalibration hence becomes important due to the widespread applicability of deep neural networks across multitude of domains. Figure 1 displays this phenomenon where the average accuracy and average confidence is computed per interval bin (see section 2 for details). The top row (a\u2013e) corresponds to a training scenario where no Mixup is used. As the training progresses the network tends to become overconfident in its predictions. On the other hand, by training with Mixup, the network is much better calibrated, as can be seen in the bottom row (f\u2013j). Note that the network used to generate the results in fig 1 is different from the original paper, which uses a VGG-16 architecture. However, the underlying message i.e. over confident predictions are avoided when using Mixup is conveyed nonetheless.\n1.2 Risk Minimisation and Mixup Given an input X and an output Y we are interested in finding f \u2208 F which describes the mapping from input to the target output. The loss function \u2113 penalises the differences\nCopyright \u00a9 2020 A. Singh and A. Bay, released under a Creative Commons Attribution 4.0 International license. Correspondence should be addressed to Aditya Singh (aditya.singh@zebra.com) The authors have declared that no competing interests exist. Code is available at https://github.com/MacroMayhem/OnMixup. \u2013 SWH swh:1:dir:824bff001a04ed1c4bb78dc6fbed52fb4470b7a5. Open peer review is available at https://openreview.net/forum?id=JhZOkalsiI.\nReScience C 6.2 (#10) \u2013 Singh and Bay 2020 1\nbetween f(x) and y for samples (x, y) \u223c P (X ,Y). We aim to minimise the expected risk R: R(\u2113) = \u222b \u2113(f(x), y) dP (x, y). (1)\nIn most scenarios the joint probability distribution P (x, y) is unavailable, and, subsequently, the task is altered to minimise the risk over the training data {(xi, yi)ni=1}. Therefore, P is replaced by the empirical distribution P\u0302 , and this approach is referred to as Empirical Risk Minimisation (ERM) [6]. As a result of this substitution, equation (1) can be modified to find an approximate mapping f\u0302 \u2208 F for f , as\nR\u0302(\u2113) = \u222b \u2113(f\u0302(x), y) dP\u0302 (x, y), (2)\nwhere\nP\u0302 (x, y) = 1\nn n\u2211 i=1 \u03b4(x = xi, y = yi), (3)\nand \u03b4(x = xi, y = yi) is a Dirac delta distribution centered at xi, yi. The vicinal probability distribution Pv is the probability of finding a virtual featuretarget pair (x\u0303, y\u0303) in the vicinity of the original pair (xi, yi). Since the support of P\u0302 (x, y) is a one-point set, it fails to approximate P (x, y) if P is a continuous distribution [7, 8]. Vicinal Risk Minimisation (VRM) [7] assumes that the input distribution is smooth in the vicinity of xi, and replaces P\u0302 by the vicinal probability distribution Pv, where\nPv = 1\nn n\u2211 i=1 v(x\u0303, y\u0303 | xi, yi). (4)\nMixup [2] is based on the principle of VRM, and generates the vicinal input and the corresponding target from a convex combination of a pair of original inputs and targets. Formally, the vicinal input and output can be represented as\nx\u0303i = \u03bbxi + (1\u2212 \u03bb)xj (5) y\u0303i = \u03bbyi + (1\u2212 \u03bb)yj , (6)\nReScience C 6.2 (#10) \u2013 Singh and Bay 2020 2\nwhere xi and xj are randomly selected input samples, yi and yj are their corresponding target values, and \u03bb \u2208 [0, 1] is drawn from a symmetric Beta distribution B(\u03b1, \u03b1). The expected risk R\u0302v for Mixup can thus be defined as\nR\u0302v(\u2113) = 1\nn n\u2211 i=1 \u2113(f\u0302(x\u0303i), y\u0303i). (7)\n1.3 Proposed Method Mixup serves as simple yet effective data-augmentation procedure to enhance the performance of deep neural networks. The working of Mixup has been proved empirically via extensive experiments to suggest that its contribution in boosting the performance is significant. However, moving away from analysing the boost in performance or explaining why Mixup works, the authors explore the impact of Mixup on the calibration of deep neural networks. The paper performs considerable experiments across a number of datasets to measure the degree of miscalibration with Mixup and make a comparison with clearly defined baselines. As part of the replication track of NeurIPS Reproducibility challenge 2019, we aim at reproducing the reported results of their hypothesis, i.e. training with Mixup leads to better calibrated neural networks. For the baseline, though not required by the track, we use training without Mixup. The report is structured as follows: Section 2 contains the description of the metrics used by the authors, in Section 3 we provide a detailed information of the implementations which we gathered from the authors\u2019 submission as well as email correspondences. We also state clearly where we make certain reasonable assumptions in implementation. Section 4 contains the results on various datasets. In the end, we provide our concluding remarks in Section 5.\n2 Calibration Metrics\nTo measure the calibration of a network, we follow the approach as described in [9]. We initially define the number of confidence intervals M each of size 1/M . The confidence interval Im corresponds to the confidence range ((m\u2212 1)/M, m/M ]. Let Bm be the set of samples for which the predicted confidence falls in Im. The accuracy and confidence for Bm are defined, respectively, as\nacc(Bm) = 1 |Bm| \u2211 i\u2208Bm 1(y\u0302i = yi)\nconf(Bm) = 1 |Bm| \u2211 i\u2208Bm p\u0302i,\nwhere y\u0302i, yi, and p\u0302i correspond to predicted label, true label, and the confidence (or winning score), respectively. Unless stated explicitly, confidence and accuracy will refer to a bin\u2019s confidence and accuracy. The Expected Calibration Error (ECE) measures the amount of miscalibration in a network by computing the difference between the confidence and accuracy over the M intervals. Formally, ECE is defined as\nECE = M\u2211 m=1 |Bm| n \u2223\u2223\u2223conf(Bm)\u2212 acc(Bm)\u2223\u2223\u2223. The Overconfidence Error (OE) is a weighted measure, which penalises overconfident predictions, and is defined as\nOE = M\u2211 m=1 |Bm| n [conf(Bm) \u2217max (conf(Bm)\u2212 acc(Bm), 0)] .\nReScience C 6.2 (#10) \u2013 Singh and Bay 2020 3\nWe have followed, wherever applicable, the implementation details of authors submission. Though, some of the fine grain details on training a network and generating the reported results were lacking, for this we have made reasonable assumptions and listed below a full table of implementation details, which we followed. The details are provided in Table 1.\n4 Replication Experiments\nWe conduct 4 trials per experiment, and report the mean and standard deviation over the trials. We report results for more than few values of \u03b1 as opposed to the paper. The authors report the best working value of \u03b1 \u2208 [0.2, 0.4] hence, we use \u03b1 = 0.3 when selecting an individual value for \u03b1. Accuracy, ECE, and OE are computed for \u03b1 \u2208 [0, 1]. Important thing to note is \u03b1 = 0 corresponds to no Mixup, which is one of the author\u2019s baseline, and \u03b1 = 1 is simple averaging of two input images. Including \u00b1 standard deviation also provides additional insights to the range of values for accuracy, ECE and OE which are, instead, missing in the paper. We also show scatter plots for \u03b1 = {0, 0.3, 1.0} to compare average bin accuracy against average bin confidence. To provide additional details of the frequency of samples in the bin, we scale the point size on the plot to correspond to the size of the bin |Bm|.\nReScience C 6.2 (#10) \u2013 Singh and Bay 2020 4\n4.1 CIFAR-100 CIFAR-100 consists of 100 classes with input images of dimensions 32x32x3. We use the standard split of the dataset consisting of 50, 000 and 10, 000 as training and test images, respectively. We train the Resnet-34 model following the details provided in the Section 3. The results can be viewed in Figure 2. Though we were not able to obtain the accuracy value 80%, we were able to replicate the trend that the accuracy is higher when the network is trained with Mixup. For ECE and OE there are significant fluctuations in the values. The results reported by the authors lie within one standard deviation due to large value of standard deviation. However, ECE decreases till \u03b1 = 0.3 and then increases, which is also reported in paper. Figure 2 (d\u2013f) represents the shift from the network being overconfident (\u03b1 = 0) to under confident (\u03b1 = 1). The network is better calibrated, if the points in the scatter plot lie close to the line y = x, and it is observed with \u03b1 = 0.3 in Figure 2 (e).\n4.2 STL-10 The dataset consists of labelled as well as unlabelled examples. For the purpose of this experiment, we utilise the labelled split of train and test. The dataset consists of 10 classes with input images of dimensions 96x96x3. We train the VGG-16 model following the details provided in the Section 3. The accuracy matches with the values reported by authors as observed in Figure 3. The trend of improved accuracy due to Mixup is also observed. For ECE, and OE the values for no Mixup were off, but the expected trend of improved calibration was observed. Again, \u03b1 \u2208 [0.2, 0.4] can be seen as the best value w.r.t ECE. Figure 3 (d\u2013f) represents the shift from the network being overconfident (\u03b1 = 0) to under confident (\u03b1 = 1).\n4.3 Fashion-MNIST The dataset consists of a training set of 60, 000 examples and a test set of 10, 000 examples. Each example is a 28x28x1 gray-scale image, associated with a label from 10 classes. We use the standard splits for training and testing.\nReScience C 6.2 (#10) \u2013 Singh and Bay 2020 5\nThe effect of Mixup on accuracy are not quite evident, since the network learns to classify effectively even without Mixup, Figure 4. ECE value is lower for majority of \u03b1 indicating better calibration. The scatter-plot is not a good indicator to analyse the calibration in this case as majority of the samples are easily classified by the neural network with high accuracy. Only OE behaves a expected suggesting that Mixup helps in calibrating the neural network to avoid over confident predictions.\n4.4 Testing on Out-Of-Distribution dataset & Random Noise In the original work, authors have trained a neural network on STL-10 (in-distribution data) and tested on the samples from ImageNet (out-of-distribution data) [10], corresponding to new classes only. On the other hand, we aim to replicate the effectiveness of\nReScience C 6.2 (#10) \u2013 Singh and Bay 2020 6\nthis experiment by training on Fashion-MNIST (in distribution data) and test on MNIST (out-of-distribution data). In principle, the findings of our experiment should go along with the reported results. MNIST, similar to Fashion-MNIST, consists of 50, 000 training and 10, 000 test images of dimensions 28x28x3. We train the Resnet-18 network on the train split of FashionMNIST and report the results on the test split of MNIST. For random noise images, we use Gaussian noise with mean and standard deviation of the Fashion-MNIST dataset to generate 1024 test images. In Figure 5, the plots closely resemble to the outputs of the paper. For both the scenarios, we can observe that the network trained with Mixup is significantly less confident in predicting the test samples. For random noise, the predictions are somewhat separable as reported by the authors.\n5 Conclusion\nIn this report, we were able to confirm the main results reported in [1] by replicating the majority of the experiments. Though the values were not exactly replicated for ECE and OE, the trend of improved calibration when the network is trained using Mixup was confirmed. We were able to obtain the authors finding regarding the value of \u03b1 \u2208 [0.2, 0.4], providing the best calibration in terms of ECE. Through the out-of-distribution and random noise experiments in Section 4.4, we were able to provide some evidence regarding the generalisability of the author\u2019s corresponding experiment, thereby strengthening their claim of improved calibration on unseen data. The decrease of OE with increase in \u03b1 is also confirmed, which the authors attributed to underfitting of the model. An important finding from the replication of the experiments was the volatility of the error values, hence we urge future researchers to report mean and variance for the errors to allow for better comparisons. We also provide our source code 1 for generating the results and setting up the experiments."}], "title": "[Re] Improved Calibration and Predictive Uncertainty for Deep Neural Networks", "year": 2020}