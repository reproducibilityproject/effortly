{"abstractText": "This report covers our reproduction and extensionof the paper \u2018WhenDoes Self\u2010Supervision Improve Few\u2010shot Learning?\u2019 published in ECCV 2020. The paper investigates the ef\u2010 fectiveness of applying self\u2010supervised learning (SSL) as a regularizer to meta\u2010learning based few\u2010shot learners. The authors of the original paper claim that SSL tasks reduce the relative error of few\u2010shot learners by 4% \u2010 27% on both small\u2010scale and large\u2010scale datasets, and the improvements are greater when the amount of supervision is lesser, or when the data is noisy or of low resolution. Further, they observe that incorporating unlabelled images from other domains for SSL can hurt the performance of FSL, and propose a simple algorithm to select unlabelled images for SSL from other domains to provide improvements.", "authors": [{"affiliations": [], "name": "Arjun Ashok"}, {"affiliations": [], "name": "Haswanth Aekula"}, {"affiliations": [], "name": "Koustuv Sinha"}, {"affiliations": [], "name": "Sharath Chandra Raparthy"}], "id": "SP:fa4831faf60e2199a2af6bc67e64ad6c793c1f39", "references": [{"authors": ["J. Snell", "K. Swersky"], "title": "and R", "venue": "Zemel. \u201cPrototypical Networks for Few-shot Learning.\u201d In: Advances in Neural Information Processing Systems. Ed. by I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett. Vol. 30. Curran Associates, Inc.,", "year": 2017}, {"authors": ["M. Norooz"], "title": "and P", "venue": "Favaro. \u201cUnsupervised learning of visual representations by solving jigsaw puzzles.\u201d In: European conference on computer vision. Springer.", "year": 2016}, {"authors": ["W.-Y. Chen", "Y.-C. Liu"], "title": "Z", "venue": "Kira, Y.-C. F. Wang, and J.-B. Huang. \u201cA Closer Look at Few-shot Classification.\u201d In: International Conference on Learning Representations.", "year": 2019}, {"authors": ["F. Sung", "Y. Yang", "L. Zhang", "T. Xiang", "P.H. Torr"], "title": "and T", "venue": "M. Hospedales. \u201cLearning to compare: Relation network for few-shot learning.\u201d In: Proceedings of the IEEE conference on computer vision and pattern recognition.", "year": 2018}, {"authors": ["C. Finn", "P. Abbeel"], "title": "and S", "venue": "Levine. \u201cModel-agnostic meta-learning for fast adaptation of deep networks.\u201d In: International Conference on Machine Learning. PMLR.", "year": 2017}, {"authors": ["P. Welinder", "S. Branson", "T. Mita", "C. Wah", "F. Schroff", "S. Belongie", "P. Perona"], "title": "Caltech-UCSD Birds 200", "venue": "Tech. rep. CNS-TR-2010-001. California Institute of Technology,", "year": 2010}, {"authors": ["J. Krause", "M. Stark", "J. Deng"], "title": "and L", "venue": "Fei-Fei. \u201c3D Object Representations for Fine-Grained Categorization.\u201d In: 4th International IEEE Workshop on 3D Representation and Recognition (3dRR-13). Sydney, Australia,", "year": 2013}, {"authors": ["S. Maji", "J. Kannala", "E. Rahtu", "M. Blaschko", "A. Vedaldi"], "title": "Fine-Grained Visual Classification of Aircraft", "venue": "Tech. rep.", "year": 2013}, {"authors": ["A. Khosla", "N. Jayadevaprakash", "B. Yao"], "title": "and L", "venue": "Fei-Fei. \u201cNovel Dataset for Fine-Grained Image Categorization.\u201d In: FirstWorkshop on Fine-Grained Visual Categorization, IEEE Conference on Computer Vision and Pattern Recognition. Colorado Springs, CO, June", "year": 2011}, {"authors": ["M.-E. Nilsbac"], "title": "and A", "venue": "Zisserman. \u201cAutomated Flower Classification over a Large Number of Classes.\u201d In: 2008 Sixth Indian Conference on Computer Vision, Graphics Image Processing.", "year": 2008}, {"authors": ["O. Vinyals", "C. Blundell", "T. Lillicrap", "K. Kavukcuoglu"], "title": "and D", "venue": "Wierstra. \u201cMatching Networks for One Shot Learning.\u201d In: Proceedings of the 30th International Conference on Neural Information Processing Systems. NIPS\u201916. Barcelona, Spain: Curran Associates Inc.,", "year": 2016}, {"authors": ["A. Kuznetsova", "H. Rom", "N. Alldrin", "J. Uijlings", "I. Krasin", "J. Pont-Tuset", "S. Kamali", "S. Popov", "M. Malloci", "A. Kolesnikov"], "title": "The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale.", "year": 2018}, {"authors": ["G. Van Horn", "O. Mac Aodha", "Y. Song", "Y. Cui", "C. Sun", "A. Shepard", "H. Adam", "P. Perona"], "title": "and S", "venue": "Belongie. \u201cThe inaturalist species classification and detection dataset.\u201d In: Proceedings of the IEEE conference on computer vision and pattern recognition.", "year": 2018}, {"authors": ["Y. Guo", "N.C. Codella", "L. Karlinsky", "J.V. Codella", "J.R. Smith", "K. Saenko", "T. Rosing"], "title": "and R", "venue": "Feris. \u201cA broader study of cross-domain few-shot learning.\u201d In: European conference on computer vision. ECCV.", "year": 2020}], "sections": [{"text": "R E S C I E N C E C Replication / ML Reproducibility Challenge 2021\n[Re] Does Self-Supervision Always Improve Few-Shot"}, {"heading": "Learning?", "text": "Arjun Ashok1, ID and Haswanth Aekula2, ID 1PSG College of Technology, India \u2013 2DTICI, India\nEdited by Koustuv Sinha,\nSharath Chandra Raparthy\nReviewed by Anonymous Reviewers\nReceived 04 February 2022\nPublished 23 May 2022\nDOI 10.5281/zenodo.6574629\n1 Reproducibility Summary"}, {"heading": "Scope of Reproducibility", "text": "This report covers our reproduction and extensionof the paper \u2018WhenDoes Self\u2010Supervision Improve Few\u2010shot Learning?\u2019 published in ECCV 2020. The paper investigates the ef\u2010 fectiveness of applying self\u2010supervised learning (SSL) as a regularizer to meta\u2010learning based few\u2010shot learners. The authors of the original paper claim that SSL tasks reduce the relative error of few\u2010shot learners by 4% \u2010 27% on both small\u2010scale and large\u2010scale datasets, and the improvements are greater when the amount of supervision is lesser, or when the data is noisy or of low resolution. Further, they observe that incorporating unlabelled images from other domains for SSL can hurt the performance of FSL, and propose a simple algorithm to select unlabelled images for SSL from other domains to provide improvements."}, {"heading": "Methodology", "text": "We conduct our experiments on an extended version of the authors codebase. We im\u2010 plement the domain selection algorithm from scratch. We add datasets and methods to evaluate few\u2010shot learners on a cross\u2010domain inference setup. Finally, we open\u2010source pre\u2010processed versions of 3 few\u2010shot learning datasets, to facilitate their off\u2010the\u2010shelf us\u2010 age. We conduct experiments involving combinations of supervised and self\u2010supervised learning onmultiple datasets, on 2 different architectures and perform extensive hyper\u2010 parameter sweeps to test the claim. We used 4 GTX 1080Ti GPUs throughout, and all our experiments including the sweeps took a total compute time of 980 GPU hours. Our codebase is at https://github.com/ashok\u2010arjun/MLRC\u20102021\u2010Few\u2010Shot\u2010Learning\u2010And\u2010Self\u2010 Supervision."}, {"heading": "Results", "text": "On the ResNet\u201018 architecture and a high input resolution that the paper uses through\u2010 out, our results on 6 datasets overall verify the claim that SSL regularizes few\u2010shot learn\u2010 ers and provides higher gains with difficult tasks. Further, our results also verify that out\u2010of\u2010distribution images for SSL hurt the accuracy, and the domain selection algo\u2010 rithm that we implement from scratch also verifies the paper\u2019s claim that the algorithm\nCopyright \u00a9 2022 A. Ashok and H. Aekula, released under a Creative Commons Attribution 4.0 International license. Correspondence should be addressed to Arjun Ashok (arjun.ashok.psg@gmail.com) The authors have declared that no competing interests exist. Code is available at https://github.com/ashok-arjun/MLRC-2021-Few-Shot-Learning-And-Self-Supervision/ \u2013 DOI 10.5281/zenodo.6508499. \u2013 SWH swh:1:dir:90d1c4d52eec769a1a18df5bd1f8bd0955f0ac24. Open peer review is available at https://openreview.net/forum?id=ScfP3G73CY.\nReScience C 8.2 (#3) \u2013 Ashok and Aekula 2022 1\ncan choose images from a large pool of unlabelled images from other domains, and improve the performance. Going beyond the original paper, we also conduct SSL experiments on 5 datasetswith the Conv\u20104\u201064 architecturewith a lower image resolution. Here, wefind that self\u2010supervision does not help boost the accuracy of few\u2010shot learners in this setup. Further, we also show results on a practical real\u2010world benchmark on cross-domain few-shot learning, and show that using self\u2010supervision when training the base models degrades performance when evaluated on these tasks."}, {"heading": "What was easy", "text": "The paper was well written and easy to follow, and provided clear descriptions of the experiments, including the hyperparameters. The authors\u2019 code implementation in Py\u2010 Torch was relatively easy to understand."}, {"heading": "What was difficult", "text": "Since the codebase was incomplete, it took us a lot of time to solve bugs, and reimple\u2010 ment algorithms not present in the code. Further, the datasets needed a lot of prepro\u2010 cessing to be used. The number of hyperparameters being too many but each proving to be important, and evaluating all the claims of the paper on 5 datasets and 2 architec\u2010 tures was difficult to the number of experiment configurations, resulting in a very high computational cost of 980 GPU hours.\nCommunication with original authors Wemaintained contactwith the authors throughout the challenge to clarify implementa\u2010 tion details and questions regarding the domain selection algorithm. The authors were responsive and replied promptly with detailed explanations.\nReScience C 8.2 (#3) \u2013 Ashok and Aekula 2022 2\n2 Content\n3 Scope of reproducibility\nThe paper claims that\n\u2022 With no additional training data, adding self\u2010supervised tasks such as jigsaw/rota\u2010 tion prediction as an auxiliary task improves the performance of existing few\u2010shot techniques on benchmarks across several different domains\n\u2022 The benefits of self\u2010supervision increase with the difficulty of the task, for exam\u2010 ple when training with a base dataset with less labelled data, or when the dataset contains images of lesser quality/resolution\n\u2022 Using unlabelled data from dissimilar domains for self\u2010supervision negatively im\u2010 pacts the performance of few\u2010shot learners\n\u2022 The proposed domain selection algorithm can alleviate this issue by learning to pick images that are similar to the training domain, from a large and generic pool of images\nWe thoroughly reproduce all the experiments, and investigate whether the claims hold true, with the model and the six benchmark datasets used by the authors. Beyond the paper, we find that the results are biased towards the architecture and resolution used, and demonstrate that the gains do not hold when the input resolution and architecture differ from those reported in the paper. We also report results on the more practical cross\u2010domain few\u2010shot learning setup. Here, we find that self\u2010supervision does not help ImageNet\u2010trained few\u2010shot learners generalize to new domains better. Finally, along with our reproducible codebase, we open\u2010source processed versions of 3 datasets that previously required tedious manual processing, to facilitate their off\u2010the\u2010shelf usage.\n4 Methodology\nThe goal of a few\u2010shot learner is to learn representations of base classes that lead to good generalization on novel classes. To this end, the proposed framework combines meta learning approaches for few\u2010shot learning with self-supervised learning. In general, learning consists of estimating functions f , the feature extractor and g, the classifier that minimize the empirical loss \u2113 over the training data from base class Ds = {(xi, yi)}ni=1 consisting of images xi \u2208 X and labels yi \u2208 Y, along with suitable regularization R. This can be written as:\nLs = \u2211\n(xi,yi)\u2208Ds\n\u2113(g \u25e6 f(xi), yi) +R(f, g)\nIn the original paper, the loss of prototypical networks (ProtoNet)[1] are used as part of the supervised loss. During meta\u2010training, ProtoNet computes the mean of the embed\u2010 dings of all samples in a class. Then, a distance metric such as Euclidean distance or cosine distance is used to classify every query sample into one of the classes, using the distance from the class\u2010prototypes. The loss over the query samples is backpropagated to the network, and this procedure is repeated for multiple episodes with n randomly sampled classes in each episode, with k examples in each class, hence referred to as the n\u2010way k\u2010shot setup. Hence the network meta\u2010learns to provide useful class\u2010prototypes from very few examples. At meta\u2010test time, class prototypes are recomputed from the few examples per each class, and query examples are classified based on the distances to the class prototypes.\nReScience C 8.2 (#3) \u2013 Ashok and Aekula 2022 3\nApart from the supervised losses, the paper uses self\u2010supervised losses \u2113ss that are de\u2010 rived from (x\u0302, y\u0302). Let h denote an additional auxiliary classifier used as part of a self\u2010 supervised loss, and Dss denote the dataset used to construct the self\u2010supervised tasks. Then the self\u2010supervised loss is\nLss = \u2211\n(xi)\u2208Dss\n\u2113(h \u25e6 f(x\u0302i), y\u0302i)\nThe jigsaw task splits an image into 9 regions (3x3) and permutes the regions to obtain the input x\u0302. The target label y\u0302 is the index of the permuatation. The total number of indices are 9! which is reduced to 35 indices [2] by grouping the possible permutations to control the difficulty of the task. The rotation task rotates the image by an angle \u03b8 \u2208 0\u25e6, 90\u25e6, 180\u25e6, 270\u25e6 to obtain x\u0302, with y\u0302 being the index of the angle. The paper uses a weighted combination of the supervised and self\u2010supervised losses L = (1\u2212 \u03b1) \u2217 Ls + (\u03b1) \u2217 Lss. The author also propose an algorithm to select images from a large\u2010dataset for self\u2010 supervision when Ds and Dss are different. Here, a classifier is trained to distinguish the ResNet\u2010101 features of images from Ds and images from Dss, and the top\u2010k images according to the ratio p(x \u2208 Ds)/p(x \u2208 Dp) are selected for self\u2010supervision.\n5 Experimental settings\n5.1 Details regarding the code\nThe authors provide a public implementation of the code1, which is built upon a pop\u2010 ular codebase 2 from Chen et al [3]. We find that there are a lot of errors and bugs in the code, which took a lot of time to debug. This took up a considerable part of our time. Further, the code for the domain selection algorithm was not present, and hence we had to reimplement it from scratch. Our code 3 reuses multiple files from the original codebase, corrects several errors, and provides an implementation of the domain selec\u2010 tion algorithm. We also provide interfaces to train models with a different architecture, and to evaluate models in a cross\u2010domain setup.\n5.2 Model descriptions The authors use awell\u2010knownarchitectureResNet\u201018 for their experiments. TheResNet18 gives a 512\u2010dimensional feature for each input. For the jigsaw task, a single fully\u2010connected (fc) layer with 512\u2010units is added in parallel to the classifier. Nine patches of an image give nine 512\u2010dimensional feature vectors, which are concatenated, and projected to 4096 dimensions using an fc layer, and then to a 35\u2010dimensional output using another fc layer, corresponding to the 35 permutations for the jigsaw task. For rotation prediction task, the 512\u2010dimensional output of ResNet\u201018 is passed through three fc layers consecutively with 128, 128, 4 units. The 4 predictions of the last layer correspond to the four rotation angles. Between each fc layer, a ReLU activation and a dropout layer with a dropout probability of 0.5 are added. Apart from the ResNet\u201018 architecture used in the paper, we use another architecture that is equally adapted in many few\u2010shot learning papers [3] [1] [4] [5], the Conv\u20104\u201064 architecture, which is a simpler architecture with 3x3 kernel size and 64 filters at each layer. Similar extensions aremade for the jigsaw and rotation tasks. Inmultipleworks in the literature, this architecture has beenused to process 84 x 84 images, while theResNet\n1https://github.com/cvl-umass/fsl_ssl 2https://github.com/wyharveychen/CloserLookFewShot 3https://github.com/ashok-arjun/MLRC-2021-Few-Shot-Learning-And-Self-Supervision\nReScience C 8.2 (#3) \u2013 Ashok and Aekula 2022 4\nvariants have been used to process 224 x 224 images. We follow the works and report results with the respective resolutions for each architecture. Both the architectures are represented diagrammatically in tables 15 and 16 respectively in the appendix.\n5.3 Datasets Following the few\u2010shot setup, each dataset is split into three disjoint sets namely the base training set, validation set and the test set. The model is trained on the base set, validated on the validation set, and tested on the test set. Following the paper, we exper\u2010 iment with multiple datasets across diverse domains and denote the number of classes in the base, val, test splits inside brackets: CUB\u2010200\u20102011 [6](64, 12, 20) , Stanford Cars [7] (98, 49, 49), FGVC\u2010Aircraft [8] (50, 25, 25), Stanford Dogs [9] (60, 30, 30), Oxford Flowers [10] (51, 26, 26). These 5 datasets are henceforth referred to as \u201dthe smaller datasets\u201d. Apart from these, we also experiment with a benchmark dataset for few\u2010shot learning, the miniImageNet dataset [11] (64, 16, 20). The original paper also reports results on Tiered\u2010ImageNet, but we restrict to evaluating on miniImageNet due to compute and time constraints. We use the same base\u2010validation\u2010novel class splits for every dataset exactly as in the paper, which they provide in their official repository. Implementation\u2010wise, each class contains 3files, one for each in base, val and novel, and lists the classes to be used, along with all the image paths for each class. Among the small datasets, we found that there were no versions of the flowers and cars dataset that could be used directly. Hence we had to preprocess the two datasets ourselves, before we could use them off\u2010the\u2010shelf. With the miniImageNet dataset, we found that all the directly\u2010downloadable versions [12] [13] contained images resized to 84x84, however we needed a dataset that could be resized to either 84x84 or 224x224 adaptively. Hence, we had to download the ImageNet dataset (155 GB) and process the dataset from scratch, which caused storage issues and also took up a significant part of our time. Along with codebase, we also open\u2010source the processed flowers4 and cars5 datasets, and the miniImageNet dataset with image sizes same as that in ImageNet6. For the domain selection algorithm, the authors use the training sets of two large datasets \u2010 Open Images v5 [14] and iNaturalist [15], which are 500 GB and 200 GB in size respec\u2010 tively. Weuse the validation splits of these datasets as unlabelled images for self\u2010supervision.\n5.4 Hyperparameters Thehyperparameter sweepswere conductedusingWeights andBiases. Each sweepuses random search to search over the following 3 hyperparameters:\n\u2022 Learning Rate: Sampled from a uniform distribution (0.0001, 0.03)\n\u2022 Batch normalization mode:\n1. Use batch normalization, accumulate statistics throughout training, and use the statistics during testing\n2. Use batch normalization, but do not track the running mean and variance during training; estimate them from batches during training and test\n3. No batch normalization\n\u2022 Alpha (\u03b1), the weightage of the SSL term in the loss (only where self\u2010supervision is applied)\n4https://www.kaggle.com/arjun2000ashok/vggflowers/ 5https://www.kaggle.com/hassiahk/stanford-cars-dataset-full 6https://www.kaggle.com/arjunashok33/miniimagenet\nReScience C 8.2 (#3) \u2013 Ashok and Aekula 2022 5\nIn the above grid, the batch\u2010norm modes especially designed to verify the claim of the paper that using batch normmode 2was found to be better for a subset of tasks that did not involve jigsaw, and vice\u2010versa. All models are trained with the Adam optimizer with \u03b21 = 0.9 and \u03b22 = 0.999. We then use the configuration which gives the best validation accuracy. Due to com\u2010 putational constraints, we search hyperparameters for certain datasets, and reuse the hyperparameters found for similar datasets. The selected experiment configurations are given in the appendix and the code\u2010base. Across all of our sweeps, we notice that \u03b1 stays below 0.6, and does not go below 0.3 in our runs. Hence, we infer that an ade\u2010 quate amount of supervision is also needed for good performance, and too much self\u2010 supervision hurts accuracy. For the miniImageNet dataset, we find the values close 0.3 work the best, which the paper reiterates. The paper reports that they use 0.5 for all the SSL experiments on the small datasets, which we confirm as our \u03b1 term converges to values 0.4 and 0.6 for the small datasets. All of our reported results are with the best hyperparameters found.\n5.5 Computational requirements We used 4 Nvidia 1080Ti GPUs for all experiments. The run\u2010times differ for each exper\u2010 iment configuration when incorporating self\u2010supervision. We report the average epoch time for each experimental setup (1 epoch = 100 episodes) in table 6 in the appendix. In general, among experiments involving self\u2010supervised learning, rotation took the maximum amount of time. This is because 4 rotations of the same image are needed at every instance, which is more expensive than loading a single image. The jigsaw task took lesser time than rotation, and the combination of jigsaw and rotation took the high\u2010 est amount of time per epoch. Since the paper reports results on the combination only for the first set of experiments (claim 1), we also do the same. Further, the computa\u2010 tional time restricted us from performing more experiments combining the two. In total, apart from the hyper\u2010parameter sweeps, we perform 250 experiments, across different experimental setups and multiple datasets. All of these experiments took ap\u2010 proximately 700 GPU hours. Along with the hyperparameter sweeps which, the experi\u2010 ments took approximately 980 hours of compute time.\n5.6 Experimental setup and code Following the authors, we train, evaluate and report results on the 5\u2010way 5\u2010shot setting; we also explore 20\u2010way 5\u2010shot setting but we could not continue after a few runs, re\u2010 stricted by the large training and testing time of 20\u2010way 5\u2010shot models. Following the paper, we use 16 query examples to evaluate the models. The batch size cannot be set in episodic few\u2010shot learners, and are by default given by n_way \u2217 (n_support + n_query). We use 16 query images following the paper, and as a result, our batch sizes are 105 in 5\u2010way 5\u2010shot experiments, and 420 in 20\u2010way 5\u2010shot experiments. Following all previous work in few\u2010shot learning, we sample 100 episodes (batches) per epoch, and conduct experiments on about 600 \u2010 800 epochs. Following the paper, we use only 5 query images when trainingmodels for experiments that use lesser labelled data since the {20, 40, 60, 80}% splits of dataset do not contain 16 query images in all classes. In every iteration, an equal number of unlabelled images are sampled at random from the respective dataset(s) for self\u2010supervised learning. Following our paper and the base\u2010 line from previous work [3] in few\u2010shot learning and our original paper, we use the fol\u2010 lowing data augmentation: For label and rotation predictions, images are first resized to 224 pixels for the shorter edge whilemaintaining aspect ratio, fromwhich a central crop of 224 is obtained. For jigsaw puzzles, a random crop of 255 is done from the original image with random scaling between [0.5, 1.0], then split into 3\u00d73 regions, from which a\nReScience C 8.2 (#3) \u2013 Ashok and Aekula 2022 6\nCUB Dogs Cars Aircrafts Flowers 80\n82\n84\n86\n88\n90\n92\n94\nAc cu\nra cy\nResults on ResNet-18\nProtoNet ProtoNet + Rotation ProtoNet + Jigsaw ProtoNet + Jigsaw + Rotation\nFigure 1. Results on applying SSL tasks to Pro\u2010 totypical networks, across 6 datasets\nMethod Accuracy ProtoNet 74.07 \u00b1 0.71 ProtoNet + Jigsaw 77.29 \u00b1 0.73 ProtoNet + Rotation 74.93 \u00b1 0.9\nProtoNet + Jigsaw + Rotation 76.23 \u00b1 0.9\nTable 1. miniImageNet Results with ResNet\u2010 18\nrandom crop of size 64\u00d764 is picked. For evaluation at meta\u2010test time, we use 600 ran\u2010 domly sampled episodes, and report the mean accuracy and 95% confidence intervals. We implement the domain selection algorithm following the paper: For each dataset among the small datasets, we select negative images uniformly at randomwith 10 times the size of the positive images. The loss for the positive class is scaled by the inverse of its frequency to account for the significantly larger number of negative examples. We then train a binary logistic regression classifier using LBFGS for 10000 iterations and use the logits to compute the ratio p(x \u2208 Ds)/p(x \u2208 Dp), whereDp represents the large pool of images, andDs represents the supervised training set. We then choose k as 80%of the total dataset size, and sample k images from the negative class (Dp) to use as unlabelled samples. On verifying that the core claim of the paper (claim 1) for all the 5 small datasets, we choose a diverse set of 2 to 3 representative datasets for claims 2 and 3. For the do\u2010 main selection, we evaluate on all the 5 datasets to verify our implementation of the algorithm.\n6 Results\n6.1 Results reproducing original paper Here, we consider the same architecture that the paper uses \u2010 ResNet\u201018, with an input image size of 224.\nSelf-supervision improves few-shot learning \u2014Here, we successfully verify claim 1 of the pa\u2010 per which states that with no additional unlabelled data, SSL improves few\u2010shot learn\u2010 ing when applied as an auxiliary task. We conduct experiments across all the 5 small datasets as well as the large\u2010scale miniImageNet dataset. We also reproduce results on the baseline from [3], andMAML andMAML+Jigsaw. We present results in figure 1, and tables 1 and 2. All results are on 5\u2010way 5\u2010shot classification.\nThe benefits of self-supervision increase with the difficulty of the task \u2014We successfully verify claim 2 of the authors that the relative gains of using SSL aremore when the difficulty of the task is higher. The authors experimentwith two types of difficult tasks: onewith low\u2010 resolution/greyscale images as input, and another with less labelled data from the base training set. We experiment with 3 selected datasets andwere successful in reproducing the results. We report the results on figures 2 and 4. The exact numbers are given on\nReScience C 8.2 (#3) \u2013 Ashok and Aekula 2022 7\n0 20 40 60 80 68\n70\n72\n74\n76\n78\n80\n82\nAc cu\nra cy\n% of images from other domains used for SSL\nCUB Cars Dogs\nFigure 3. Performance on tasks where a portion of the labelled data is replaced with data from other domains\ntables 12 and 10 respectively in the appendix. We find that the claims of the paper hold true, and that self\u2010supervision has higher gains in harder tasks.\nUnlabelled data for SSL from dissimilar domains negatively impacts the few-shot learner \u2014 Verify\u2010 ing claim 3 of the paper, we replace a portion of the labelled data, starting from 20% of the data to 80% of the data, with data from other domains. Here, we combine the data from all other datasets together, and sample images at random. We present results on 3 chosen datasets, again, to save computation and time for other results. Results are given in figure 3 and table 11 (appendix). The claim that using data from dissimilar domains for self\u2010supervision is detrimental to few\u2010shot classification holds true.\nThe proposed domain selection algorithm can alleviate this issue by learning to pick images from a large and generic pool of images \u2014 To verify claim 4, we implement the domain selection algorithm from scratch, and verify it across all 5 small datasets as given in the paper, to make sure that we have got the implementation right. Results are presented in figure 5 and table 13 in the appendix. Results are shown on using only 20% of the labelled data for learning, only selecting images from other domains at random, and on using the proposed domain selection algorithm. We successfully verify and demonstrate that the algorithm proposed by the authors for selecting images from multiple dissimilar domains.\nReScience C 8.2 (#3) \u2013 Ashok and Aekula 2022 8\n6.2 Results beyond original paper\nResults on a different architecture - Conv4 \u2014Here, we aim to investigatewhether the claims of the paper hold when a small architecture that needs a smaller image size (84x84) is used. In particular, we investigate claim 1 of the paper extensively. Note that the authors do not report results with this architecture. Results are given in figure 6. Exact numbers are given in tables 7 and 9 in the appendix. We find that the results do not hold true when a smaller architecture and image size is used, and that claim depends heavily on the architecture and image size. We present results across all the 5 small datasets for completeness, across both SSL tasks. To confirm our claims, we also rerun results with another seed, but get similar results (Table 8 in appendix). We next study the effect of \u03b1 on the results, with the CUB and cars datasets in tables 3 and 4. Here we find that the value of \u03b1 plays an important role in the performance, and that high values leading to too much self\u2010supervision is detrimental when the model is small. Even across training and testing with multiple \u03b1 values, we find that the self\u2010 supervision provides only a marginal boost in 1 out of 4 cases, invalidating claim 1 of the paper that self\u2010supervision provides a stable boost to few\u2010shot learners.\nResults on cross-domain few-shot learning \u2014 In another effort to extend the paper\u2019s results, we test the results of our trained models on the BSCD\u2010FSL benchmark for cross\u2010domain few\u2010shot learning, introducedby [16]with their code 7. Thebenchmark requires ImageNet\u2010 based trained few\u2010shot models to evaluated on four cross\u2010domain datasets: CropDis\u2010 eases, EuroSAT, ISIC2018, and ChestX datasets, which covers plant disease images, satel\u2010\n7https://github.com/IBM/cdfsl-benchmark\nReScience C 8.2 (#3) \u2013 Ashok and Aekula 2022 9\nCUB Dogs Cars Aircrafts Flowers 60\n65\n70\n75\n80\nAc cu\nra cy\nDomain selection results\nNo SSL SSL with random selection SSL with importance weights\nFigure 5. Results of the domain selection algo\u2010 rithm CUB Dogs Cars Aircrafts Flowers\n40\n50\n60\n70\n80\n90\nAc cu\nra cy\nResults on Conv-4\nProtoNet ProtoNet + Rotation ProtoNet + Jigsaw ProtoNet + Jigsaw + Rotation\nFigure 6. Results of using SSL with the Conv4 architecture\nlite images, dermoscopic images of skin lesions, and X\u2010ray images, respectively. The se\u2010 lected datasets reflect real\u2010world use cases for few\u2010shot learning since collecting enough examples from above domains is often difficult, expensive, or in some cases not possi\u2010 ble. We use this benchmark to find out if models trained with self\u2010supervision provide gains over normal supervised models when tested on real-world datasets. We test our mini\u2010ImageNet trained models on this benchmark, to find out if self\u2010supervision im\u2010 proves results on cross\u2010domain datasets. Results on the ResNet\u201018 models are reported in table 5. Results on the Conv\u20104models are given in appendix table 14. We find that self\u2010 supervision results in learning heavily domain\u2010specific representations, and that the re\u2010 sults of the fully\u2010supervised learner are much better than those with auxiliary tasks as self\u2010supervision.\n7 Discussion\nWe find that the central claims of the author as given in Section 3 hold true, when the same architecture is used. Considering the ResNet\u201018 model used in the paper with an input image size of 224, we find that self\u2010supervision \u2010 in particular the jigsaw task, pro\u2010 vides a boost in the case of small datasets. Experimentally, we verify claim 1 of the paper on all small datasets and miniImageNet. However, going beyond the paper\u2019s architec\u2010\nReScience C 8.2 (#3) \u2013 Ashok and Aekula 2022 10\nture, we find that the results depend heavily on the image size and architecture and do not give the same gains with Conv\u20104\u201064, another architecture common in the few\u2010shot learning literature, with an input image size of 84. Further ablation reveals that the jig\u2010 saw task in particular has a strong influence in this setup, and the rotation task requires tuning the \u03b1 parameter to even reach the accuracy of the fully\u2010supervised model. Fu\u2010 ture work may investigate ways to boost the performance of few\u2010shot classifiers when the input sizes are small, and may also find out better architectures to use when the in\u2010 put size is small. Future work may also experiment with other available architectures, and find out if self\u2010supervision increases performances across all configurations. Regarding claims 2 and 3 such as on harder tasks and scenarios with lesser labelled data in the base dataset, our experiments on selected datasets verify that the claims hold true, with the ResNet\u201018 backbone. Further, we verify claim 4 of the paper by implementing the domain selection algorithm from scratch and our experiments on all the 5 datasets show that relative gains are achieved. Future work may also investigate if the same claims hold true when different architectures were used. Finally, we evaluate the miniImageNet\u2010trained models on a more practical setting of cross\u2010domain few\u2010shot learning and find that SSL during the training time does not help few\u2010shot learners generalize across domains better. Future work may investigate why applying SSL results in domain\u2010specific features, and propose methods to apply SSL in a more domain\u2010agnostic manner. We recommend future works in few\u2010shot learning to train and evaluate in multiple architectures with different resolutions and verify their work more thoroughly.\n7.1 Communication with original authors We maintained communication with the authors throughout our implementation and training phase, spanning two months. We were able to clarify many implementation details in the original codebase, and the authors also re\u2010ran an experiment on their side to test if the numbers match. Further, we received a lot of help regarding implementa\u2010 tion of the domain selection algorithm, and could also confirm the implementationwith them. We acknowledge and thank the authors for their help with the reproducibility of their paper.\n8 Acknowledgements\nThe authors are grateful to Weights & Biases and DAGsHub for reproducibility grants that supported this work.\nReScience C 8.2 (#3) \u2013 Ashok and Aekula 2022 11"}, {"heading": "Results on harder tasks \u2014", "text": "ReScience C 8.2 (#3) \u2013 Ashok and Aekula 2022 13\nReScience C 8.2 (#3) \u2013 Ashok and Aekula 2022 14"}, {"heading": "Results on domain selection \u2014", "text": "Results on cross-domain few-shot learning \u2014\n9.4 Architectures\nReScience C 8.2 (#3) \u2013 Ashok and Aekula 2022 15\nReScience C 8.2 (#3) \u2013 Ashok and Aekula 2022 16"}], "title": "[Re] Does Self-Supervision Always Improve Few-Shot Learning?", "year": 2022}