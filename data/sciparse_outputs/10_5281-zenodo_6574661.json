{"abstractText": "We reproduced the results of the paper \u201dDomain GeneralizationUsing CausalMatching.\u201d The standard supervised learning framework considers that the labels assigned to in\u2010 stances seen in the testing process must have appeared during the training phase. How\u2010 ever, real\u2010world designs may violate these considerations. For instance, in e\u2010commerce, new products are released every day with different labels, and the labels may not be part of the model training. A generalized framework should be capable of detecting un\u2010 seen labels. If a framework fails to detect unseen labels, it may face challenges in open domains and thus may not be generalizable. The objective of domain generalization is to learn representations independent of the domain. Previous works model this objective by learning representations by condition\u2010 ing on the class label. The authors provide counterexamples to show that the objective is not sufficient and propose a new objective to learn representations of inputs across domains such that they have the same representations if derived from the same object.", "authors": [{"affiliations": [], "name": "Richard Jiles"}, {"affiliations": [], "name": "Mohna Chakraborty"}, {"affiliations": [], "name": "Koustuv Sinha"}, {"affiliations": [], "name": "Sharath Chandra Raparthy"}], "id": "SP:5117761528e2a44e69d9ba202a975fa89c974839", "references": [{"authors": ["H. Xiao", "K. Rasul", "R. Vollgraf"], "title": "Fashion-MNIST: a Novel Image Dataset for Benchmarking", "venue": "Machine Learning Algorithms. Aug", "year": 2017}, {"authors": ["D. Li", "Y. Yang", "Y. Song", "T.M. Hospedales"], "title": "Deeper, Broader and Artier Domain Generalization.", "year": 2017}, {"authors": ["J. Xu", "L. Xiao", "A. L\u00f3pez"], "title": "Self-supervised Domain Adaptation for Computer Vision Tasks", "year": 2019}, {"authors": ["D. Mahajan", "S. Tople", "A. Sharma"], "title": "Domain Generalization using Causal Matching.", "venue": "CoRR abs/2006.07500", "year": 2020}, {"authors": ["S. Beery", "G.V. Horn", "P. Perona"], "title": "Recognition in Terra Incognita.", "year": 2018}, {"authors": ["Y. Ganin", "E. Ustinova", "H. Ajakan", "P. Germain", "H. Larochelle", "F. Laviolette", "M. Marchand", "V. Lempitsky"], "title": "Domain-adversarial training of neural networks.\u201d In: The journal of machine learning research", "year": 2016}, {"authors": ["M. Ghifary", "W.B. Kleijn", "M. Zhang", "D. Balduzzi"], "title": "Domain generalization for object recognitionwithmulti-task autoencoders.", "venue": "Proceedings of the IEEE international conference on computer vision", "year": 2015}, {"authors": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "title": "Deep Residual Learning for Image Recognition.", "venue": "ReScience C", "year": 2015}, {"authors": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "title": "ImageNet Classification with Deep Convolutional Neural Networks.", "venue": "Advances in Neural Information Processing Systems", "year": 2012}, {"authors": ["X.Wang", "Y. Peng", "L. Lu", "Z. Lu", "M. Bagheri", "andR.M. Summers"], "title": "Chestx-ray8: Hospital-scale chest x-ray database and benchmarks onweakly-supervised classification and localization of common thorax diseases.", "venue": "In:Proceedings of the IEEE conference on computer vision and pattern recognition", "year": 2017}, {"authors": ["J. Irvin", "P. Rajpurkar", "M. Ko", "Y. Yu", "S. Ciurea-Ilcus", "C. Chute", "H. Marklund", "B. Haghgoo", "R. Ball", "K. Shpanskaya"], "title": "Chexpert: A large chest radiograph dataset with uncertainty labels and expert comparison.", "venue": "Proceedings of the AAAI conference on artificial intelligence", "year": 2019}, {"authors": ["Y. Li", "M. Gong", "X. Tian", "T. Liu", "D. Tao"], "title": "Domain generalization via conditional invariant representations.", "venue": "Proceedings of the AAAI Conference on Artificial Intelligence", "year": 2018}, {"authors": ["B. Sun", "K. Saenko"], "title": "Deep coral: Correlation alignment for deep domain adaptation.", "venue": "European conference on computer vision. Springer", "year": 2016}, {"authors": ["Y. Li", "X. Tian", "M. Gong", "Y. Liu", "T. Liu", "K. Zhang", "D. Tao"], "title": "Deep domain generalization via conditional invariant adversarial networks.", "venue": "Proceedings of the European Conference on Computer Vision (ECCV)", "year": 2018}, {"authors": ["S. Motiian", "M. Piccirilli", "D.A. Adjeroh", "G. Doretto"], "title": "Unified deep supervised domain adaptation and generalization.", "venue": "Proceedings of the IEEE international conference on computer vision", "year": 2017}, {"authors": ["V. Piratla", "P. Netrapalli", "S. Sarawagi"], "title": "Efficient domain generalization via common-specific low-rank decomposition.", "venue": "In: International Conference on Machine Learning. PMLR", "year": 2020}, {"authors": ["Q. Dou", "D. Coelho de Castro", "K. Kamnitsas", "B. Glocker"], "title": "Domain generalization via model-agnostic learning of semantic features.", "venue": "Advances in Neural Information Processing Systems", "year": 2019}, {"authors": ["F.M. Carlucci", "A. D\u2019Innocente", "S. Bucci", "B. Caputo", "T. Tommasi"], "title": "Domain Generalization by Solving Jigsaw Puzzles", "year": 2019}, {"authors": ["K. Zhou", "Y. Yang", "T. Hospedales", "T. Xiang"], "title": "Deep Domain-Adversarial Image Generation for Domain Generalisation.", "venue": "Proceedings of the AAAI Conference on Artificial Intelligence", "year": 2020}, {"authors": ["H. Nam", "H. Lee", "J. Park", "W. Yoon", "D. Yoo"], "title": "Reducing Domain Gap by Reducing Style Bias", "year": 2021}, {"authors": ["I. Albuquerque", "J. Monteiro", "M. Darvishi", "T.H. Falk", "I. Mitliagkas"], "title": "Generalizing to unseen domains via distribution matching", "year": 2021}, {"authors": ["Z. Huang", "H. Wang", "E.P. Xing", "D. Huang"], "title": "Self-Challenging Improves Cross-Domain Generalization", "year": 2020}, {"authors": ["N. Asadi", "A.M. Sarfi", "M. Hosseinzadeh", "Z. Karimpour", "M. Eftekhari"], "title": "Towards Shape Biased Unsupervised Representation Learning for Domain Generalization", "year": 2020}], "sections": [{"text": "R E S C I E N C E C Replication / ML Reproducibility Challenge 2021\n[Re] Domain Generalization using Causal Matching\nRichard Jiles1, ID and Mohna Chakraborty1, ID 1Iowa State University, Ames, Iowa, USA\nEdited by Koustuv Sinha,\nSharath Chandra Raparthy\nReviewed by Anonymous Reviewers\nReceived 04 February 2022\nPublished 23 May 2022\nDOI 10.5281/zenodo.6574661"}, {"heading": "Reproducibility Summary", "text": "We reproduced the results of the paper \u201dDomain GeneralizationUsing CausalMatching.\u201d The standard supervised learning framework considers that the labels assigned to in\u2010 stances seen in the testing process must have appeared during the training phase. How\u2010 ever, real\u2010world designs may violate these considerations. For instance, in e\u2010commerce, new products are released every day with different labels, and the labels may not be part of the model training. A generalized framework should be capable of detecting un\u2010 seen labels. If a framework fails to detect unseen labels, it may face challenges in open domains and thus may not be generalizable. The objective of domain generalization is to learn representations independent of the domain. Previous works model this objective by learning representations by condition\u2010 ing on the class label. The authors provide counterexamples to show that the objective is not sufficient and propose a new objective to learn representations of inputs across domains such that they have the same representations if derived from the same object."}, {"heading": "Methodology", "text": "The open\u2010source code of the paper has been used. The authors provided detailed in\u2010 structions to reproduce the results on their GitHub page. We reproduced almost every table in the main text and a few of them from the appendix. In case of a mismatch of the results, we also investigated the cause and proposed possible explanations for such behavior. For the extensions, we wrote extra functions to check the paper\u2019s claim on other open\u2010source standard datasets. We mainly used the infrastructure offered by the publicly available GPUs offered by Google Colab and GPU\u2010assisted desktop computers to train the models."}, {"heading": "Results", "text": "Most of our results closelymatch the reported results in the original paper for theRotated\u2010 MNIST [1], Fashion\u2010MNIST [2], PACS [3, 4], and Chest\u2010Xray [5] datasets. However, in some cases, as described later, we obtained better results quantitatively than the ones reported in the paper. By investigating the root cause of such mismatches, we provide a possible reason to avoid such a gap. We performed additional experiments by making necessary modifications for the Rotated\u2010MNIST and Rotated Fashion\u2010MNIST dataset. In\nCopyright \u00a9 2022 R. Jiles and M. Chakraborty, released under a Creative Commons Attribution 4.0 International license. Correspondence should be addressed to Richard Jiles (rdjiles@iastate.edu) The authors have declared that no competing interests exist. Code is available at https://github.com/rjiles/causalmatching \u2013 DOI 10.5281/zenodo.6529518. \u2013 SWH swh:1:dir:08875ab42adddf57b8019c82f4e5889d1009743c. Data is available at https://github.com/rjiles/causalmatching \u2013 DOI 10.5281/zenodo.6529518. Open peer review is available at https://openreview.net/forum?id=r43elaGmhCY.\nReScience C 8.2 (#18) \u2013 Jiles and Chakraborty 2022 1\ngeneral, our results still support the main claim of the original paper, even though the results differ for some of the training/testing instances."}, {"heading": "What was easy", "text": "The authorized GitHub page of the paper has the open\u2010source code, which was bene\u2010 ficial as it was well organized into multiple files. Thus, it was easy to follow. The ex\u2010 periments described in the paper were done on widely\u2010used benchmark open\u2010source datasets. Therefore, implementing each experiment was relatively easy to do. Likewise, since most of the parameters were reported in the scripts, we did not needmuch tuning in most experimentations."}, {"heading": "What was difficult", "text": "Though running each experiment is relatively simple, the numerosity of experiments was a demanding task. In particular, each experiment in the actual setting requires training a network for a significant number of iterations. Having restricted access to computational resources and time, we sometimes changed the settings, sacrificing gran\u2010 ularity. Nevertheless, these changes did not impact the interpretability of the final re\u2010 sults.\nCommunication with original authors We emailed the authors and received prompt responses to our questions regarding the provided Jupyter reproduction notebooks. Some tables had multiple runs for the same technique, but it was unclear how to execute the alternative runs.\n1 Introduction\nLearning is a dynamic process in an open environment where some new labels may not belong to any training set; therefore, recognizing these novel labels during classifica\u2010 tion presents a vital problem. The purpose of domain generalization is to learn a single classifier with training data sampled from M domains that generalize well to data from unseen domains. For example, a prototype trained on certain attributes of one region may be deployed to another, or an image classifier may be deployed on slightly rotated images. This proposition assumes that stable (causal) features lead to an optimal classi\u2010 fier uniform to the domains. The paper illustrates that the class\u2010conditional domain invariant objective for represen\u2010 tations is not always sufficient. They provide simple counterexamples to validate the class\u2010conditional domain invariance deficit theoretically and empirically. Differing dis\u2010 tributions of stable causal features within the same class label are commonly observed in real\u2010world datasets, e.g., in digit recognition, the stable feature like shape may differ based on people\u2019s handwriting, or medical images may have variations due to differing body characteristics in the sample. The paper proposes the importance of assuming within\u2010class variation in stable features. This report repeats the original paper\u2019s experiments and compares them with the re\u2010 ported results. Also, we expand the original paper results by investigating the effect of data augmentation on Rotated\u2010MNIST and Rotated Fashion\u2010MNIST datasets under vari\u2010 ous settings. We report and discuss our results in later sections. Domain generalization is a phenomenon that can generalize to unseen data distribu\u2010 tions after training on more than one data distribution. For example, a model trained on one domain may be deployed to another, i.e., domain adaptability, or an image clas\u2010 sifier may be deployed on slightly rotated images. The goal is to \u201dlearn representations independent of the domain after conditioning on the class label\u201d [6].\nReScience C 8.2 (#18) \u2013 Jiles and Chakraborty 2022 2\nThe paper analyzes the observation through a structural causal model (SCM) and dis\u2010 cusses the importance of modeling within\u2010class variations for generalization. The au\u2010 thors [6] propose newmethods RandMatch, MatchDG, andMDGHybrid to increase per\u2010 formance over the previous state\u2010of\u2010the\u2010art methods for various ML problems. In ad\u2010 dition to reproducing the original paper\u2019s results, we propose different state\u2010of\u2010the\u2010art datasets where the analogy can be implemented and evaluate the efficacy of the propo\u2010 sition.\n2 Scope of reproducibility\nThe paper broadly dives into the issue of spurious correlation, where some predictive attributes in the training time might not be predictive at the test time. For example, in Figure 1, we can observe the two different domains in which a cow could appear and/or be trained. If a learning algorithm does not use domain\u2010independent attributes and has most if not all training images of an object in one domain, it may fail when attempting to identify it in other domains.\nHence, there is a need to design ways to prevent machine learning models from retain\u2010 ing these spurious correlations, confining their generalization capability. Since amodel cannot generalize to any arbitrary unseen domains, therefore an assumption has been made by the authors that we have an invariant predictor based on the stable causal fea\u2010 tures across domains. Prior works like [8] propose an additional domain classifier trained from the representa\u2010 tions learned by the feature extractor module. The network is then trained to minimize the label prediction loss and maximize the domain classification loss hence learning domain invariant representations. However, it has been seen that the domain invariant representations fail when the domain and the label are correlated. We investigate the subsequent claims from the original paper:\n\u2022 Claim 1: The paper proposes an object invariant condition to estimate stable fea\u2010 tures to overcome the loopholes of the prior works.\n\u2022 Claim 2: The paper proposes a novel 2\u2010phase iterative algorithm to approximate the object\u2010based matches.\nReScience C 8.2 (#18) \u2013 Jiles and Chakraborty 2022 3\n3 Methodology\nWe utilize the code made available by the original authors for our study. Our major emphasis was to verify that the providedmodels and descriptions stay true to the claims made in the paper. We further retrain their models on the provided dataset of Rotated\u2010 MNIST and Rotated Fashion\u2010MNIST.\n3.1 Method descriptions The problem statement that the paper is trying to solve is domain generalization, where we have access to data from numerous domains and distributions. The objective is to generalize to unseen domains during the testing phase. In order to overcome the flaws of the priorworks, the authors in thepaper further analyzewhether the class conditional domain invariance objective is sufficient or not.\nAn easy counterexample has been shown in Figure 2, which illustrates a binary predic\u2010 tion task with two class labels on the slab dataset [9]. It has two types of features where the first kind of feature, X1, leads to a linear classifier separating the labels from the slab. The second feature, X2, leads to a more complex piecewise linear classifier splitting the labels. The slab feature also has a little noise represented by the low density of the oppo\u2010 site label. Overall, all the odd numbers slab correspond to the red colored points, and all the even numbers slab correspond to the blue colored points. The noise in the slab feature does not alter across domains. On the other hand, the linear feature X1 has very low noise in the source domain, but it is completely noisy in the target domain. Due to the simplicity of the linear feature, a model might still learn the spurious linear feature over the stable slab feature. One of the proposed methods is perfmatch. The method of perfmatch involves mini\u2010 mizing the loss L across m\u2010dimensions of the mapping function h of the learnt repre\u2010 sentation of X (denoted as \u03a6(X)) to the output Y . The function also minimizes the distance between the learnt representations \u03a6() objects of the same class j, k that exist in different domains d, d\u2032 where the learnt matching \u2126() of the same class objects j, k \u2126(j, k) is 1 for the different domains d \u0338= d\u2032.\nfperfmatch = argminh,\u03a6 m\u2211 d=1 Ld(h(\u03a6(X)), Y ) + \u03bb \u00b7 \u2211 \u2126(j,k);d \u0338=d\u2032 dist((\u03a6(x (d) j )), (\u03a6(x (d\u2032) k )))\nThe causal diagram in Figure 3 details the backdoor pass from an object to a domain, with the objects and features separated into two categories, domain\u2010dependent, and domain\u2010independent. From the equation, the objective is to learn the correct Y for a given X, and this is achieved by using the domain\u2010independent featuresXc to generalize across domains.\nReScience C 8.2 (#18) \u2013 Jiles and Chakraborty 2022 4\n3.2 Datasets Thepaper assessed thematching\u2010basedmethods onRotated\u2010MNIST andFashion\u2010MNIST, PACS, and Chest X\u2010ray datasets.\nRotated-MNIST and Fashion-MNIST: It contain rotations of grayscaleMNISThandwritten digits and fashion images from 0\u25e6 to 90\u25e6 with an interval of 15\u25e6 [10]. Here, each angle represents a domain, and the task is to predict the class label. Following CSD, the paper reports the accuracy of 0\u25e6 and 90\u25e6 together as the test domain and the rest as the train domains. PACS dataset: It contains a total of 9991 images from four domains: Photos (P), Art paint\u2010 ing (A), Cartoon (C), and Sketch (S). The task is to classify objects over 7 classes. Inspired by [3, 4], the paper trains 4 models with each domain as the target using Resnet\u201018 [11], Resnet\u201050 [11], and Alexnet [12] network. Chest X-ray: The paper introduces a harder real\u2010world dataset based on Chest X\u2010ray im\u2010 ages from three different sources: NIH [13], ChexPert [14], and RSNA [15]. The objec\u2010 tive is to identify patients with pneumonia. The original authors inserted a spurious correlation in the test domain by vertically translating class 0 in the training domains downwards, withholding the transformation from the test domain.\n3.3 Hyperparameters We used hyperparameters stated in the original paper for most of our experiments. In cases where we deviated from the reported values, mostly due to computational re\u2010\nReScience C 8.2 (#18) \u2013 Jiles and Chakraborty 2022 5\nsources and time limitations, we reported them in the discussion section. If a hyperpa\u2010 rameter is not reported in the original paper, we either communicated with the authors to ask about the hyperparameters or try out different values and report the result for all of them.\n3.4 Experimental setup and code We reran the code of the original authors on both public cloud infrastructures, such as Google Colab, and private GPUs that were available to us. We closely follow the exper\u2010 imental setup in the original paper for our experiments. Our scaling extension can be easily integrated with the source code and optimized similarly. Our implementations for all the experiments in this work are available in the supplementary material and further support the reproducible research.\n3.5 Computational requirements We reran the code of the original authors on both public cloud infrastructures, such as Google Colab, and private GPUs that were available to us. Google Colab provides a single 12GBNVIDIA Tesla K80 GPU that can continuously be used for 12 hours. We also ran the code locally on two different machines. The first machine: The GPU in question is an Nvidia GeForce RTX 3080 10Gb GDDR6X. The CPU in thismachine is an AMDRyzen(TM) 7 5800 (8\u2010Core, 36MB Total Cache, Max Boost Clock of 4.6GHz). The memory used was 32.0 GB DDR4 3466MHz, XMP. The second machine: i9\u20109900k, 1080Ti with 128 Gb DDR4 2666Mhz. We followed the setup in the original paper and implemented the network with the same number of iterations. Evaluating all the results with the saved models takes a good amount of time. It nearly took two days for some of the tables to generate the results. In conclusion, the code is not fast, but it can be run on a local machine. A GPU is heavily recommended because the code is slower without access to GPU.\n4 Results\nTo reproduce the authors\u2019 experiments, we achieve approximately similar results to the original paper. We describe the results in the following sections:\nResult 1 \u2014 Table 1 presents an empirical analysis of various algorithms on the slab dataset to understand which invariance criteria can help to capture the stable (causal) features. The algorithms are evaluated based on the domain invariance and class conditional do\u2010 main invariance criteria and experiment with the perfect match\u2019s new approach, which aims for domain invariance conditioned on the stable features. The results show that the perfect match approach does better than the domain invariance and class condi\u2010 tional domain invariance objective in learning stable features, emphasizing the need to choose the correct invariant criteria. The original authors made the observation that in\u2010 variant representation learning by unconditional (DANN [8], MMD [16], CORAL [17]) and conditional distributionmatching (CDANN [18], C\u2010MMD [16], C\u2010CORAL [17]), andmatch\u2010 ing same\u2010class inputs (Random\u2010Match [19]) have poor performance for the Target. We also observed this from our repeated experiment.\nResult 2 \u2014 Table 2 shows the replicated results for Rotated\u2010MNIST & Rotated Fashion\u2010 MNIST for test domains 0\u25e6 & 90\u25e6. MatchDG outperforms the comparison baselines for most of source distribution(CSD [20], MASF [21], IRM [22]). Source domains having the following angles (30, 45, 60) for Rotated Fashion\u2010MNIST, MatchDG achieves an accuracy of 45.0%, and the next best method, CSD, achieves 38.9%.\nReScience C 8.2 (#18) \u2013 Jiles and Chakraborty 2022 6\nAs of December 2021, the MatchDG algorithm holds the #1 ranking on the PapersWith\u2010 Code website for Rotated Fashion\u2010MNIST, with CSD as #2. The results we got for table 2 confirm that MatchDG performs better than the previous state\u2010of\u2010art technique CSD [20].\nResult 3 \u2014 Table 3 shows the repeated results whereby MatchDG outperforms ERM for overlap %. The table shows the benefit of PerfMatch for all 3 metrics over the default MatchDG variant for all metrics, and each metric aligns with the other metrics for all baselines and models. This aligns with the results from the original authors as well.\nResult 4 \u2014 Table 4 shows that for PACS dataset with ResNet\u201018 architecture, the results are competitive to the authors selected state of the art baselines (JiGen [23], DDAIG [24], SagNet [25], G2DM [26], CSD [20], RSC [27]) averaged over all domains. The MDGHybrid has the 3rd highest average, being beaten by DDEC [28] and RSC [27]. The paper reports\nReScience C 8.2 (#18) \u2013 Jiles and Chakraborty 2022 7\nMatchDG and MDGHybrid using a test domain validation, where MDGHybrid obtains comparable results to the best\u2010performing baseline.\nThe authors original results for MatchDG also claim high rankings for the PACS [3, 4] dataset for both resnet18 and resnet50 on the PapersWithCode website. Our replicated results confirm these claims.\nResult 5 \u2014 Table 5 implementMatchDGonResnet50model usedby theERM inDomainBed. Adding MatchDG loss regularization improves the accuracy of DomainBed, from 84.79 to 87.86 with MDGHybrid. Also, MDGHybrid performs better than the prior approaches using Resnet50 architecture.\nResult 6 \u2014 Table 6 provides results for the Chest X\u2010rays datasets from 3 different sources: RSNA, ChexPert and NIH. MDGHybrid outperforms other baselines for RSNA and Chex\u2010 pert. Nevertheless, NIHMDGHybrid is outperformed by both ERM and CSD. The paper reasons these inconsistent trends due to the intrinsic variability in \u201dsource domains, in\u2010 dicating the challenges of buildingdomain generalizationmethods for real\u2010world datasets\u201d. The replicated results commonly alignwith the original paper, butMDGHybrid exceeded Chexpert for our results. The original paper underperformed in the same manner that our results had an under\u2010performance for NIH even though the original paper MDGHy\u2010 brid attained the best result for NIH. Generally speaking, the results hold.\nAdditional Result 1 \u2014 Table 8 contain the results for Rotated MNIST datasets using the LeNet architecture [16]. In this setup, there are six domains in total (0\u25e6, 15\u25e6, 30\u25e6, 45\u25e6, 60\u25e6, 75\u25e6). The remaining five domains are used as source training domains for each test domain. Matching\u2010based training methods RandMatch andMatchDG outperform prior work on all the domains.\nReScience C 8.2 (#18) \u2013 Jiles and Chakraborty 2022 9\nTables 9, 10, and 11 contain the results for appendix section results of DomainBed, frac\u2010 tion of perfect matches and overlap % when training on all domains.\nAdditional Result 2 \u2014 The chars74k [29] dataset in Figure 5 offers an additional dataset to test the proposed algorithm in the paper. It contains characters A\u2010Z, a\u2010z, 0\u20109 from several domains, more specifically 64 classes (0\u20109, A\u2010Z, a\u2010z), 7705 characters obtained from natu\u2010 ral images, 3410 hand\u2010drawn characters using a tablet PC, 62992 synthesized characters from computer fonts. With the characters gathered from various sources, these sources can be considered in different domains. Thus, the algorithm should extract the causal features and be domain\u2010independent, reflected in the results. Comparison to baselines should show it has an advantage. Unfortunately, time did not allow this testing, but it should be easy to see why this would be a fair comparison for domain generalization.\nReScience C 8.2 (#18) \u2013 Jiles and Chakraborty 2022 10\n5 Discussion\nWe observed several problems in the code; for example, in the dataset generation pro\u2010 cess, the authors randomly flipped the digits of the MNIST dataset during training, i.e., when they rotated a digit by 45\u25e6, it is not consistent with whether it will be clockwise or anticlockwise rotation. The issue was because they were using an inbuilt library of PyTorch, and because of that, when we modified the code to make the rotation con\u2010 sistent, the results improved. Also, for Table 1, during code execution, we observed several errors andmade necessary modifications. For instance, there were errors in the paths in slab_data.py. The same error was rectified by adding the correct path in the file: base_dir= os.getcwd() + \u2019/data/datasets/slab/\u2019. Secondly, during executingdata_gen_syn.py for preparing slab dataset, datasets with spurr_list of 1.0 were not created. Therefore, in the file data_gen_syn.py we appended 1.0 i.e., themodified spur_corr_list is [0.0, 0.10, 0.20, 0.90, 1.0]. On Windows machines, a freeze_support() error was encountered, and thus train.py and test.py needed to have themain()method added (problem is specific to windows only, believed to be an underlying issue with python). Some basic installations were needed for the libraries like torchcsprng and opacus.\nReScience C 8.2 (#18) \u2013 Jiles and Chakraborty 2022 11\n5.1 What was easy The official GitHub page of the paper has the authors\u2019 open source code, which was helpful. The experiments described in the paper were done on widely\u2010used standard datasets. Therefore, implementing each experiment was relatively easy to do. Further\u2010 more, since many of the parameters were reported in the scripts, we did not needmuch tuning in most experiments.\n5.2 What was difficult Though implementing each experiment is relatively simple, the numerosity of experi\u2010 ments proved to be demanding. In particular, each experiment in the original setting requires training a network for many iterations. We sometimes changed the settings in these cases. However, these changes did not affect the interpretability of the final results.\n5.3 Communication with original authors We emailed the authors and received prompt responses to our questions regarding the provided Jupyter reproduction notebooks. Some tables had multiple runs for the same technique, but it was unclear how to execute the alternative runs. For reproducing Ta\u2010 ble 1 in the original paper, it was unclear how we could obtain quantitative values for source 1, source 2, and target. As per the script, it was producing values for source and target. Therefore, we communicated with the authors via email and asked them to explain the condition used in the experiments more clearly. They stated that the num\u2010 bers obtained are evaluated on the target domain/test dataset under different validation strategies. Accordingly, we cannot break them down into source 1 and source 2. Execut\u2010 ing the script with the evaluate flag would evaluate the trained model and provide per domain accuracy (source 1, source 2)."}], "title": "[Re] Domain Generalization using Causal Matching", "year": 2022}