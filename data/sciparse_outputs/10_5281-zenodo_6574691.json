{"abstractText": "In this reproducibility study we focus on the paper \u201dFair Selective Classification via Suf\u2010 ficiency\u201d. Our experiments focus on the following claims: 1. Sufficiency is able to miti\u2010 gate disparities in precision across the entire coverage scale and inmargin distributions, and will not increase these disparities compared to a baseline selective classification model in any case. 2. Using sufficiency may decrease overall accuracy in some cases, but still mitigates the disparity between groupswhen looking at individual classification scores. 3. The sufficiency\u2010regularised classifier exhibits better fairness performance on traditional fairness datasets.", "authors": [{"affiliations": [], "name": "Nils Peters"}, {"affiliations": [], "name": "Joy Crosbie"}, {"affiliations": [], "name": "Rachel vant\u0301 Hull"}, {"affiliations": [], "name": "Marius Strampel"}, {"affiliations": [], "name": "Koustuv Sinha"}, {"affiliations": [], "name": "Sharath Chandra Raparthy"}], "id": "SP:8e628f6d59a575c4d8f23d76eabcbff53f332e7a", "references": [{"authors": ["J.K. Lee", "Y. Bu", "D. Rajan", "P. Sattigeri", "R. Panda", "S. Das", "G.W. Wornell"], "title": "Fair Selective Classification Via Sufficiency.", "venue": "Proceedings of the 38th International Conference on Machine Learning", "year": 2021}, {"authors": ["E. Jones", "S. Sagawa", "P.W. Koh", "A. Kumar", "P. Liang"], "title": "Selective classification can magnify disparities across groups.", "year": 2010}, {"authors": ["R. Kohavi"], "title": "Scaling up the accuracy of naive-bayes classifiers: A decision-tree hybrid.", "venue": "In: Kdd. Vol", "year": 1996}, {"authors": ["Z. Liu", "P. Luo", "X. Wang", "X. Tang"], "title": "Deep Learning Face Attributes in theWild.", "venue": "Proceedings of International Conference on Computer Vision (ICCV)", "year": 2015}, {"authors": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "title": "Deep Residual Learning for Image Recognition.", "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "year": 2016}, {"authors": ["D. Borkan", "L. Dixon", "J. Sorensen", "N. Thain", "L. Vasserman"], "title": "Nuanced metrics for measuring unintended bias with real data for text classification.", "venue": "Companion proceedings of the 2019 world wide web conference", "year": 2019}, {"authors": ["J. Devlin", "M.-W. Chang", "K. Lee", "K. Toutanova"], "title": "Bert: Pre-training of deep bidirectional transformers for language understanding.", "year": 2018}, {"authors": ["I. Turc", "M. Chang", "K. Lee", "K. Toutanova"], "title": "Well-Read Students Learn Better: The Impact of Student Initialization on Knowledge Distillation.", "venue": "CoRR abs/1908.08962", "year": 2019}, {"authors": ["P. Bhargava", "A. Drozd"], "title": "Rogers.Generalization in NLI: Ways (Not) To Go Beyond Simple Heuristics", "year": 2022}], "sections": [{"text": "R E S C I E N C E C"}, {"heading": "Replication / ML Reproducibility Challenge 2021", "text": "[\u00acRe] Reproducing \u2019Fair Selective Classification via Sufficiency\u2019\nNils Peters1,2, ID , Joy Crosbie1,2, ID , Rachel vant\u0301 Hull1,2, ID , and Marius Strampel1,2, ID 1University of Amsterdam, Amsterdam, Netherlands \u2013 2Equal contributions\nEdited by Koustuv Sinha,\nSharath Chandra Raparthy\nReceived 04 February 2022\nPublished 23 May 2022\nDOI 10.5281/zenodo.6574691"}, {"heading": "Reproducibility Summary", "text": ""}, {"heading": "Scope of Reproducibility", "text": "In this reproducibility study we focus on the paper \u201dFair Selective Classification via Suf\u2010 ficiency\u201d. Our experiments focus on the following claims: 1. Sufficiency is able to miti\u2010 gate disparities in precision across the entire coverage scale and inmargin distributions, and will not increase these disparities compared to a baseline selective classification model in any case. 2. Using sufficiency may decrease overall accuracy in some cases, but still mitigates the disparity between groupswhen looking at individual classification scores. 3. The sufficiency\u2010regularised classifier exhibits better fairness performance on traditional fairness datasets."}, {"heading": "Methodology", "text": "As the authors have not made their code publicly available, all code was written from scratch, based on the instructions and pseudocode given in the original paper. Our code reconstruction contains code for training both the sufficiencymodel and a baseline model performing standard selective classification."}, {"heading": "Results", "text": "We were not able to fully reproduce the results of the original paper in this setting. The numbers (accuracies, precisions andmargin distributions) obtained in our experiments differ significantly from those reported in the original paper. Though differences be\u2010 tween the baseline model and the sufficiency model are not as significant as in the original paper, our results do support the main claims about sufficiency being able to increase the worst\u2010group precision and thus causing disparities between groups to de\u2010 crease."}, {"heading": "What was easy", "text": "The authors made the importance of implementing fair selective classification with suf\u2010 ficiency very clear. Moreover, the authors provided an in\u2010depth mathematical back\u2010 ground to sufficiency and selective classification, making their reasoning explicit. Fi\u2010\nCopyright \u00a9 2022 N. Peters et al., released under a Creative Commons Attribution 4.0 International license. Correspondence should be addressed to Nils Peters (nils.peters@kpnmail.nl) The authors have declared that no competing interests exist. Code is available at https://github.com/MLRC2022FSCS/FSCS \u2013 DOI 10.5281/zenodo.6479342. \u2013 SWH swh:1:dir:effcbb5800e91db9053cb59c68bbc097a10da7cf. Open peer review is available at https://openreview.net/forum?id=r9Leh2M7hCt.\nNone 8.2 (#33) \u2013 Peters et al. 2022 1\nnally, the authors presented their results in such a manner that allowed for straightfor\u2010 ward comparison once we had trained the model."}, {"heading": "What was difficult", "text": "Many technical details and model parameters were not specified in the original paper, and as no code was provided by the authors, these initially had to be determined by experimentation. Furthermore, someof the figures in the paper caused confusion about the exact implementation of the model.\nCommunication with original authors As soon as we noticed we needed clarification on the hyperparameters, datasets and models, we contacted the authors via email. Initially we did not receive a reply, and eventually the authors were only able to answer some of our questions on the Tuesday before the deadline. While we re\u2010implemented our model based on the newly supplied information, timewas too short to fix the new issues that became apparent with the new model.\nNone 8.2 (#33) \u2013 Peters et al. 2022 2\n1 Introduction\nFair classification problems emerge when one wishes to ensure that underprivileged groups sharing some sensitive attribute, such as race or gender, are not disadvantaged against any other group with the same sensitive attribute [1]. A variant of the fair clas\u2010 sification problem is selective classification, where a model is allowed to abstain from making a decision. This is usually implemented via confidence thresholding. When the confidence threshold is higher, one should expect to see better performance on the remaining samples, as the system is only making decisions when it is very confident with regards to some confidence measure [2]. However, it has been shown that while decreasing the coverage can increase overall performance, it can additionally magnify disparities between groups [2]. The paper by Lee et al.1 that is central to this reproducibility study proposes a method for enforcing fairness during selective classification, consisting of a sufficiency criterion and a regulariser based on mutual information. The authors claim that the method en\u2010 sures that a classifier is fair, even if it abstains from classifying on a large number of samples. They demonstrate their method on four datasets, each consisting of a differ\u2010 ent type of data.\n2 Scope of reproducibility\nIn this reproducibility study we focus on several claims. The first claim is that suffi\u2010 ciency is able to mitigate disparities in precision across the entire coverage scale and in margin distributions, and will not increase these disparities compared to a base\u2010 line selective classification model in any case. The second claim is that using suffi\u2010 ciency may decrease overall accuracy in some cases, but still mitigates the disparity between groups when looking at individual classification scores. Finally, the authors claim that sufficiency\u2010regularised classifier exhibits better fairness performance on tra\u2010 ditional fairness datasets. Our study consists of two components:\n\u2022 Code reconstruction: Since the author\u2019s code is not publicly available, all code was written from scratch in Python 3, using the instructions and pseudocode as described in the paper. Models, code and datasets are described in Section 3. Our code can be found on GitHub1.\n\u2022 Replication: The main part of our study is focused on reproducing the results in Lee et al.1, and to validate their observations and conclusions. Our replication results are presented in Section 4.\n3 Methodology\nFirst, an overview of the general sufficiency model is given, after which we discuss how the model was adapted to each of the datasets. This is followed by a discussion on how we evaluated our implementation, and finally we discuss the computational require\u2010 ments.\n3.1 Model descriptions As mentioned before, the original paper uses a selective classification model to which the sufficiency criterion has been applied during training. The sufficiency criterion en\u2010 sures that the predictive accuracy is the same for each group at each confidence level,\n1https://github.com/MLRC2022FSCS/FSCS, accessed 04\u201002\u201022\nNone 8.2 (#33) \u2013 Peters et al. 2022 3\nthat precision increases for each group when using selective classification and helps prevent disparities between groups when decreasing coverage. For a binary target Y and sensitive attribute D, the sufficiency criterion imposes a con\u2010 ditional independence between Y and D conditioned on the learned features \u03a6, thus requiring:\nP (Y = 1|\u03a6(x), D = a) = P (Y = 1|\u03a6(x), D = b), \u2200a, b \u2208 D.\nAn overview of the general sufficiency model is given in Figure 1. When training the model, depending on which data set is used, the data is first passed through either or both a pre\u2010trained deep neural network and a two\u2010layer neural network. The first layer serves as a feature extractor and the second one serves as a classifier. From these fea\u2010 tures, in addition to training a joint classifier, a group\u2010specific classifier is trained for each d \u2208 D. For each data point, a group\u2010specific loss and a group\u2010agnostic loss are computed. To obtain the first, the datapoint is assigned to the correct group\u2010specific classifier, that is the one corresponding to the input\u2019s sensitive attribute D = d, while for the second the input is assigned to either of the classifiers based on the marginal distribution PD. A combination of these losses is then used as a sufficiency regulariser:\nLR \u225c 1\nn n\u2211 i=1 ( log q(yi|\u03a6(xi); \u03b8di)\u2212 log q(yi|\u03a6(xi); \u03b8\u223cdi) ) The overall loss function then becomes:\nmin 1\nn n\u2211 i=1 ( L(T (\u03a6(xi)), yi) + \u03bb log q(yi|\u03a6(xi); \u03b8di)\u2212 \u03bb log q(yi|\u03a6(xi); \u03b8\u223cdi) ) and is used to update the feature extractor and joint classifier. By minimising the difference between the group\u2010specific and group\u2010agnostic loss, \u03a6(x) will be such that the group\u2010specific models trained on it will decrease their individual biases and converge towards the samemodel. In binary selective classification, an input\nX is classified as belonging to a certain class when the confidence exceeds some thresh\u2010 old. The softmax response s(x) is monotonically mapped to the confindence score k(x) with the following formula, which maps [0.5, 1] to [0, inf] and provides much higher resolution on the values close to 1 [1]:\n\u03ba(x) = 1\n2 log\n( s(x)\n1\u2212 s(x) ) When y\u0302 = y, the margin M(x) is \u03ba(x) and \u2212\u03ba(x) otherwise. Given a threshold \u03c4 , the classifier makes a correct prediction whenM(x) \u2265 \u03c4 and an incorrect prediction when M(x) \u2264 \u2212\u03c4 .\nNone 8.2 (#33) \u2013 Peters et al. 2022 4\n3.2 Code reconstruction\nFollowing the paper, our code was implemented in PyTorch2. This was achieved by cre\u2010 ating the featuriser for each dataset, a joint classifier and two fully connected layers: one for the privileged and one for the unprivileged protected group. No activation lay\u2010 ers were added to the joint classifier and group\u2010layers, since cross\u2010entropy loss requires logits as input. However, for evaluation of the model a softmax layer was applied to the predictions of the joint classifier as this was required for selective classification. Three separate Adam optimisers were used: one for the featuriser, one for the joint clas\u2010 sifier and one for both layers in the group classifiers. The loss regulariser \u03bb was set to 0.7 for all datasets as was stated in the paper. The learning rate was not provided in the paper, but later clarified by the authors to be 0.001 for each of the three featurisers. Moreover, there was no mention of what range was used for the confidence threshold, which determines the coverage. As such, testing starts with a threshold \u03c4 of 0 (i.e. with a coverage of 100%), and increaseswith some threshold step size until we reach a coverage of under 0.19. The cut\u2010off point for coverage at 0.19was chosen somewhat arbitrarily, as it lies a little past 0.20, which seems to be roughly the point beyondwhich neither the ac\u2010 curacy, nor the precision change much at all. This is in line with both our observations, as well as the data presented by Lee et al.1.\n3.3 Dataset-specific models We ran the experiments on three of the four binary classification datasets used in the paper. For each of the datasets, we used the predetermined train/test splits if available.\nAdult dataset \u2014 The Adult3 dataset [3] consists of 48.842 entries containing tabular cen\u2010 sus data, such as age, sex and education. The first step in preprocessing the dataset was removing all entries with missing values. The data was then split into 29092 training examples and 15060 test examples. Categorical variables within the data were one\u2010hot encoded, and the continuous variables were normalised to be between 0 and 1, the lat\u2010 ter of which was done to remove the outliers that could incorrectly skew the gradient learning of the parameters. Following the original paper, we only kept the first 50 sam\u2010 ples for women with a high income, that is D = 0 and Y = 1, to stimulate disparities between groups. The resulting data, X, was used to predict the target label Y , which in this case is an individual\u2019s income. Classification is binary: an income of over 50k is viewed as high income and assigned label 1, and every other income was assigned label 0. Sex was designated as the sensitive attributeD. For this dataset, we followed the original paper and used a two\u2010layer neural network with a hidden layer consisting of 80 nodes. The first layer is a feature extractor using a Scaled Exponential Linear Unit (SELU) activation function, and the second layer serves as the joint classifier. The network is trained for 20 epochs.\nCelebA dataset \u2014 The CelebA4 dataset [4] consists of 202.599 images of 10.177 different celebrities, alongwith a list of attributes depicted in the images. Itwas not clear from the original paperwhether the aligneddataset or the original onewasused. Moreover, it was only specified that the images were resized to 224x224, but it was not explained how this was done and whether there were any other preprocessing steps, such as normalisation. After communicationwith the authors it became clear that the aligned dataset was used, and that the images were to be normalised with 0.5 mean and 0.5 standard deviation. Because the Pytorch dataloader loaded in 38 extra columns that were unnecessary in our research, wemanually resized the images to 224x224 with a Pytorch transformation.\n2https://pytorch.org/docs/stable/index.html, accessed 04\u201002\u201022 3https://archive.ics.uci.edu/ml/datasets/adult, accessed 04\u201002\u201022 4http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html, accessed 04\u201002\u201022\nNone 8.2 (#33) \u2013 Peters et al. 2022 5\nIn order to be able to use the cross\u2010entropy function in a later stage, all \u20101 values of the binary \u2019blond\u2019 and \u2019male\u2019 variables were mapped to 0. The resulting images were used as data X, the hair colour (blond or not) was used as the target variable Y and the sex variable was used as the sensitive attribute D. To obtain features from the images, we trained a ResNet\u201050 model [5], initialised with the pre\u2010trained ImageNet weights, for 10 epochs. The features were then extracted from the second to last layer. The last layer was removed and replaced with a layer consisting of two output nodes to form the classifier.\nCivil Comments dataset \u2014 TheCivil Comments dataset5 [6] is a text\u2010based dataset consisting of 1.999.514 online comments on various news articles, along with metadata about the commenter and a label indicating whether the comment displays toxicity or not. The Kaggle repository does not provide a test set with labels nor a validation set. This meant that exclusively datapoints from the training set were used in our study. Following Lee et al.1, we let X be the comment text, Y be the binary toxicity label, and D be whether Christianity is mentioned. The dataset does not include mention\u2010of\u2010Christianity values for each data point and therefore all data points without one were dropped. A total of 235.087 comments remained. These were subsequently split into a training, validation and test set using ratios of 0.8, 0.1, 0.1 respectively. Additionally, the targets Y and men\u2010 tions of ChristianityD were converted to binary values, where values above or equal to 0.5 were mapped to 1 and values below 0.5 were mapped to 0. Lastly, the comments X were tokenised using the BERT tokeniser6, with max length set to 512, truncation set to true, and padding set to the max length. To obtain features from the texts, the tokenised data was passed through a BERT model [7] using the pretrained parameters. The exact BERT model was not specified in the original paper. Due to time constraints, the Tiny BERT model from Hugging Face7 [8, 9] was used, which had previously been adapted to Pytorch. Similarly to the Adult dataset, we then applied a two\u2010layer neural network to the BERT output with 80 nodes in the hidden layer. The first layer was treated as a feature extractor and the second layer as the classifier. Following the original paper, we trained the model for 20 epochs.\n3.4 Evaluation Tomake sure our implementation is correct, we also implement a standard classification baseline where we only optimise the cross\u2010entropy loss function. This can be observed in the lower part of Figure 1. Moreover, we plot the margin distributions of our suffi\u2010 ciency implementation and compare them to that of the original paper. To measure the effectiveness of our selective classification implementation, we follow the evaluation method of the authors and plot the accuracy\u2010coverage and precision\u2010 coverage curves, and then compute the area under the curves to summarise the per\u2010 formance across coverage values.\n3.5 Computational requirements The experiments were run using a Nvidia RTX 3090 with 24 GB VRAM at 1785 MHz. The batch sizes were not provided in the original paper, and so they were chosen based on memory constraints. As the Adult dataset consists of relatively little data, the batch size was set to 32 in order to perform enough gradient steps to fit the parameters. This resulted in a total training runtime of about 10 seconds for the baseline and 5 minutes for the sufficiency implementation across all 20 epochs. For the CelebA dataset, the largest batch size that fit in memory was 96, which resulted in a total training runtime\n5https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/data, accessed 04\u201002\u201022 6https://huggingface.co/docs/transformers/model_doc/bert, accessed 04\u201002\u201022 7https://huggingface.co/prajjwal1/bert-tiny, accessed 04\u201002\u201022\nNone 8.2 (#33) \u2013 Peters et al. 2022 6\nof 1 hour and 30 minutes for the baseline and 3 hours and 30 minutes for sufficiency for 10 epochs. For the Civil Comments model, a batch size of 48 was used, resulting in a total of 30 minutes of training time for the baseline model and 1 hour and 38 minutes for the sufficiency implementation when running for 20 epochs.\n4 Results\n4.1 Overall accuracy-coverage graphs Figure 2 displays the overall accuracy plotted against the coverage for different datasets and for both the baseline and the sufficiency\u2010regularisedmodel. From the Adult dataset graph, we can infer that accuracies are the same for both models across all coverages. For the CelebA dataset, the sufficiency model increases the accuracy for most of the coverage scale, only converging with the baseline at a coverage of around 0.25. In the Civil Comments dataset graph, the baseline model outperforms the sufficiency model across the entire coverage scale. In the original paper, the authors claim that sufficiency may decrease accuracy in some cases. Specifically, they show that the baselinemodel outperforms the sufficiencymodel on overall accuracy for the Adult dataset. Our results do support this specific result on the CelebA dataset, though for the Adult dataset the baseline and sufficiency models perform equally. For the Civil dataset, however, it is the case that sufficiency decreases accuracy, which thus confirms the general claim that sufficiency does not necessarily improve accuracy.\n4.2 Group-specific precision-coverage curves Figure 3 shows the group\u2010specific precisions across the entire coverage scale. When comparing the baseline model to the sufficiency model, Figure 3a shows that, from a coverage of below about 0.7, sufficiency leads to a smaller gap between the female and male precisions on the Adult dataset. Theworst\u2010group precision, i.e. themale precision, improves most. For the CelebA dataset in Figure 3b, we observe both groups\u2019 precisions increasingwhen using sufficiency. The precisions increase equally however, causing the gap between the genders to remain the same. As was to be expected from Figure 2c, both precisions decrease when using sufficiency on the Civil Comments dataset. However, the gap between the two groups very slightly decreases for coverages between 0.7 and 1.0. These findings are mostly in line with the findings in the original paper: while for the Adult dataset and the Civil Comments dataset the gaps between the two group do de\u2010 crease when including sufficiency, and while for the CelebA dataset this is not the case, sufficiency does not increase the gap but does significantly improve accuracy. These\nNone 8.2 (#33) \u2013 Peters et al. 2022 7\nresults neither confirm nor deny the authors\u2019 claim that the sufficiency criterion intro\u2010 duces a method for mitigating the disparity in precision, though we do note that the differences in precision in our results are much less significant than those as reported in the original paper.\n4.3 Margin distributions In Figure 4, themargindistributions for both groups are displayed for eachof the datasets. For the Adult dataset, the margins do not appear to be affected much by sufficiency.\nConversely, in the CelebA dataset, both margin distributions become more positively centred, causing the distributions to bemore similar. Especially themale groupmargin shifts more towards the positive side, obtaining a smaller range in the negative region and a wider peak in the positive region. The number of samples in the female group with a negative margin has decreased. We also observe an increase in the number of outliers in the positive region. Finally, the Civil Comments dataset shows the Non\u2010Christian group\u2019s margin becoming more normally centred around a margin of around 1, and also shows the two groups\u2019 distributions becoming more aligned. Our results show sufficiency mitigating and in any case not worsening disparities be\u2010 tween the two groups, with the Adult dataset distributions staying the same and the other two datasets confirming that sufficiency causes a slight reduction of the gap be\u2010 tween the margin distributions of different groups. This thus confirms the claim that sufficiency helps mitigate disparities in margin distributions, however, again, the dif\u2010 ferences between the models\u2019 distributions are not as clear as in the original paper.\n4.4 Numerical evaluations In Table 1, the areas under the accuracy curves and the areas between the precision curves are presented for each of the datasets. For the Adult dataset, the area under the accuracy curve virtually remains the samewhen using sufficiency, in linewith Figure 2a. The area between the precision curves slightly increases. While this seems to contradict\nNone 8.2 (#33) \u2013 Peters et al. 2022 8\nFigure 4a, note that we only observed a decrease in the precision gap for coverages of below 0.7, and the numbers in Table 1 concern the entire coverage scare. For the CelebA dataset, the area under the accuracy curve increases, resulting in an increase in overall accuracy as previously observed in Figure 2b. However, as already suggested by Figure 4b, the area between the precision curves stays virtually the same when using the sufficiency method. Once again confirming the results observed in Figure 3, when using sufficiency, the area under the Civil Comments dataset\u2019s accuracy curve decreases. The area between the precision curves effectively stays the same. As mentioned before, in the original paper sufficiency causes the Adult dataset accu\u2010 racy to diminish, while in our results both models achieve the same performance. In contrast, while for the Civil Comments dataset the original paper\u2019s accuracy increases, our results show a decrease in accuracy. The CelebA results both exhibit an increase in accuracy, though this increase is more prominent in the original paper. The area between the precision curves significantly decreases for theAdult dataset in the original paper, which is not the case for our results. The same holds for the precision curves of the the CelebA and Civil Comments dataset: although less so than for the Adult dataset, the original paper\u2019s result show that disparities are decreased when using sufficiency, while our results do not show any significant change. The Civil Comments results show that accuracy can reduce when using sufficiency, but disparities will not increase. Furthermore, although the claim about disparity in pre\u2010 cision mitigating is not directly confirmed by our results as the areas stay the same, it does show that sufficiency will not (significantly) worsen disparities in any case.\n5 Discussion\nTo summarise, the numbers (accuracies, precisions, margin distributions etc.) obtained in our experiments differ significantly from those reported in the original paper. How\u2010 ever, although differences between the baseline model and the sufficiency model are not as significant as in the original paper, our results do support the main claims about sufficiency being able to increase theworst\u2010group precision and thus causing disparities between groups to decrease. It is worth mentioning that the Figures 4b and 4c show the largest increase inmargin alignment, and these are also the datasets that either improve in overall accuracy, or decrease in disparities between groups. Moreover, the authors claimed that the sufficiency\u2010regularised classifier exhibited better fairness performance on traditional fairness datasets. Though we were not able to reproduce their results in\nNone 8.2 (#33) \u2013 Peters et al. 2022 9\nthis study, we do believe we can validate this claim, as sufficiency is either able to de\u2010 crease the disparities in precision between groups (Figures 3a and 3c), or increase the precision for both groups in an equalmanner aswe traverse the coverage scale,meaning that no group is penalised for the sake of improving the other group\u2019s precision. The fact that we were not able to precisely reproduce the results from the original pa\u2010 per is likely due to the fact that not all technical details required to fully replicate the original paper were provided by the authors in the paper. Specifically the learning rate, selective classification threshold and optimiser algorithms had to be decided upon our\u2010 selves. While a well\u2010informed guess of what parameters to use was made possible due to experimentation, it could well be possible that the authors\u2019 implementation differs on these fronts, and that this caused our results to differ from the ones in the paper. We also did not have time to run all the experiments we would have liked to. For ex\u2010 ample, testing on the CheXpert8 dataset, or experiments beyond replication, such as applying the sufficiency method to a new dataset. This was due to the fact that we spent a large amount of time trying to improve our original results, because we wanted to make sure these were stable before attempting to generalise further.\n5.1 Reproducibility of the paper\nWhat was easy \u2014 The authors provided a strong and logically structured theoretical back\u2010 ground that made the importance of implementing fair selective classification with suf\u2010 ficiency very clear. Moreover, the authors provided an in\u2010depth mathematical back\u2010 ground to sufficiency and selective classification, making their reasoning explicit. Fi\u2010 nally, the authors provided clear explanations of the evaluation method and presented their results in such amanner that allowed for straightforward comparison once we had trained the model.\nWhat was difficult \u2014 As mentioned previously, many crucial technical details (e.g. pre\u2010 trained models and hyperparameters) required to replicate the original paper were not provided by the authors. Furthermore, we found the overview of the model shown in Figure 1 (Figure 2 in the original paper) difficult to interpret. This caused the implemen\u2010 tation of the model to take more time than we had anticipated. The first issue was the use of \u201dex\u201d in the deep network and joint loss depictions, which is generally short for \u201dexcluding\u201d. In section 4.1 of the original paper, it appears that the ResNet\u201050 model is modified in place, leading to the features being extracted and classifiedwithinResNet\u201050 itself. This would indeed indicate \u2019ex\u2019 meaning \u2019excluding\u2019, as there is no separate fea\u2010 turiser in this case. However, this interpretation means that cross\u2010entropy is excluded from the joint loss, though it is explicitly mentioned in section 4.1. This would indicate \u201dex\u201d is short for \u2019exemplum\u2019, which is a contradiction. Moreover, the image does not make immediately clear that the fully connected layers FC0 and FC1 are the same for both the group\u2010specific and the group\u2010agnostic classifier. There was also no mention of the loss functions or activation functions used for the fully connected layers in the group\u2010specific classifiers. Finally, it was not explicitly mentioned whether the featuris\u2010 ers were the same for the Adult and Civil datasets.\n5.2 Communication with original authors As soon as we noticed we were missing crucial information about the hyperparameters and the CelebA dataset and we needed some clarifications on the workings of themodel, we contacted the authors via email. Initially we did not receive a reply, and so we sent a follow\u2010up email. We received an answer from the authors that they needed more time to verify the information we asked for and were currently working towards a deadline\n8The fourth dataset from the original paper: https://stanfordmlgroup.github.io/competitions/chexpert/, accessed 04\u201002\u201022\nNone 8.2 (#33) \u2013 Peters et al. 2022 10\nthemselves andwe eventually received an email on 01\u201002\u20102022. In this email, the authors were only able to answer some of our questions. While we re\u2010implemented our model based on the newly supplied information, time was too short to fix the new issues that became apparent with the new model."}, {"heading": "1. J. K. Lee, Y. Bu, D. Rajan, P. Sattigeri, R. Panda, S. Das, and G. W. Wornell. \u201cFair Selective Classification Via", "text": "Sufficiency.\u201d In: Proceedings of the 38th International Conference on Machine Learning. Ed. by M. Meila and T. Zhang. Vol. 139. Proceedings of Machine Learning Research. PMLR, June 2021, pp. 6076\u20136086. URL: https: //proceedings.mlr.press/v139/lee21b.html. 2. E. Jones, S. Sagawa, P. W. Koh, A. Kumar, and P. Liang. \u201cSelective classification can magnify disparities across groups.\u201d In: arXiv preprint arXiv:2010.14134 (2020). 3. R. Kohavi et al. \u201cScaling up the accuracy of naive-bayes classifiers: A decision-tree hybrid.\u201d In: Kdd. Vol. 96. 1996, pp. 202\u2013207. 4. Z. Liu, P. Luo, X. Wang, and X. Tang. \u201cDeep Learning Face Attributes in theWild.\u201d In: Proceedings of International Conference on Computer Vision (ICCV). Dec. 2015. 5. K. He, X. Zhang, S. Ren, and J. Sun. \u201cDeep Residual Learning for Image Recognition.\u201d In: 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2016, pp. 770\u2013778. DOI: 10.1109/CVPR.2016.90. 6. D. Borkan, L. Dixon, J. Sorensen, N. Thain, and L. Vasserman. \u201cNuanced metrics for measuring unintended bias with real data for text classification.\u201d In: Companion proceedings of the 2019 world wide web conference. 2019, pp. 491\u2013500. 7. J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. \u201cBert: Pre-training of deep bidirectional transformers for language understanding.\u201d In: arXiv preprint arXiv:1810.04805 (2018). 8. I. Turc, M. Chang, K. Lee, and K. Toutanova. \u201cWell-Read Students Learn Better: The Impact of Student Initialization on Knowledge Distillation.\u201d In: CoRR abs/1908.08962 (2019). arXiv:1908.08962. URL: http://arxiv.org/abs/ 1908.08962. 9. P. Bhargava, A. Drozd, and A. Rogers.Generalization in NLI: Ways (Not) To Go Beyond Simple Heuristics. 2021. arXiv:2110.01518 [cs.CL].\nNone 8.2 (#33) \u2013 Peters et al. 2022 11"}], "title": "[\u00acRe] Reproducing \u2019Fair Selective Classification via Sufficiency\u2019", "year": 2022}