{"abstractText": "Recurrent neural networks have the capacity to exhibit complex spatio-temporal patterns that can be used to produce motor patterns. The research field of reservoir computing [2, 4] focuses on forcing recurrent networks to produce a desired read-out output through supervised learning. In the high-gain regime, recurrent networks can even exhibit rich and complex patterns in the absence of stimulation, but they become chaotic, as they can not reproduce twice the same trajectory in the presence of noise [5]. In Laje and Buonomano [3], the authors proposed a method to tame the chaos in such networks, by training the recurrent weights to minimize the error between a desired trajectory spontaneously generated by the network in an initial trial and the current trajectory. By selectively creating stable dynamic attractors, this method allows to benefit at the same time from the rich dynamics of a chaotic network and from the reproducibility of a deterministic one.", "authors": [{"affiliations": [], "name": "Julien Vitay"}, {"affiliations": [], "name": "Nicolas P. Rougier"}], "id": "SP:f31b00b8ed6b987c5cdd52616df4d61b8692534b", "references": [{"authors": ["Simon S. Haykin"], "title": "Adaptive filter theory", "venue": "isbn: 013322760X", "year": 2002}, {"authors": ["Herbert Jaeger"], "title": "The \u201decho state\u201d approach to analysing and training recurrent neural networks", "venue": "Tech. rep. German National Research Center for Information Technology,", "year": 2001}, {"authors": ["Rodrigo Laje", "Dean V Buonomano"], "title": "Robust timing and motor patterns by taming chaos in recurrent neural networks.", "venue": "In: Nat. Neurosci", "year": 2013}, {"authors": ["Wolfgang Maass", "Thomas Natschl\u00e4ger", "Henry Markram"], "title": "Real-time computing without stable states: a new framework for neural computation based on perturbations.", "venue": "Neural Comput", "year": 2002}, {"authors": ["H. Sompolinsky", "A. Crisanti", "H.J. Sommers"], "title": "Chaos in Random Neural Networks", "venue": "In: Phys. Rev. Lett. 61.3", "year": 1988}, {"authors": ["David Sussillo", "L F Abbott"], "title": "Generating coherent patterns of activity from chaotic neural networks.", "venue": "(Aug", "year": 2009}], "sections": [{"text": "[Re] Robust timing and motor patterns by taming chaos in recurrent neural networks Julien Vitay1\n1 Professorship for Artificial Intelligence, Department of Computer Science, Chemnitz University of Technology, D-09107 Chemnitz, Germany\njulien.vitay@informatik.tu-chemnitz.de\nEditor Nicolas P. Rougier\nReviewers Xavier Hinaut Pierre Enel\nReceived Jul, 1, 2016 Accepted Oct, 7, 2016 Published Oct, 7, 2016\nLicence CC-BY\nCompeting Interests: The authors have declared that no competing interests exist.\n Article repository\n Code repository\nA reference implementation of\n\u2192 Laje, R. and Buonomano, D.V. (2013). Robust timing and motor patterns by taming chaos in recurrent neural networks. Nat Neurosci. 16(7) pp 925-33. doi:10.1038/nn.3405."}, {"heading": "Introduction", "text": "Recurrent neural networks have the capacity to exhibit complex spatio-temporal patterns that can be used to produce motor patterns. The research field of reservoir computing [2, 4] focuses on forcing recurrent networks to produce a desired read-out output through supervised learning. In the high-gain regime, recurrent networks can even exhibit rich and complex patterns in the absence of stimulation, but they become chaotic, as they can not reproduce twice the same trajectory in the presence of noise [5]. In Laje and Buonomano [3], the authors proposed a method to tame the chaos in such networks, by training the recurrent weights to minimize the error between a desired trajectory - spontaneously generated by the network in an initial trial - and the current trajectory. By selectively creating stable dynamic attractors, this method allows to benefit at the same time from the rich dynamics of a chaotic network and from the reproducibility of a deterministic one."}, {"heading": "Methods", "text": "The network proposed by Laje and Buonomano [3] is a pool of 800 rate-coded neurons sparsely connected with each other with a fixed probability. They receive random inputs from specific impulse neurons, which are only used to shortly force the network into a deterministic state at the beginning of a trial or to perturb its state. One or more output neurons (called read-out neurons) receive inputs from the recurrent units and learn to produce a specific target activation through supervised learning. 60% of the recurrent weights are plastic and evolve according to the recursive least squares (RLS) algorithm [1], as well as all the read-out weights between the recurrent units and the read-out ones.\nThe training procedure is split into three separate phases: 1) an initial trajectory is acquired by giving an input impulse and recording the activation of all recurrent units for a given duration in the absence of noise, 2) the recurrent weights are trained for 20 or 30 trials to reproduce this initial trajectory using RLS in the presence of noise,\nReScience | rescience.github.io 1 Oct 2016 | Volume 2 | Issue 1\n3) the read-out weights are trained for 10 trials to generate a desired pattern (e.g. a delayed impulse, see the Results section), using the recurrent units as inputs.\nThe Python code for the network simulation and training is provided in the file RecurrentNetwork.py. It is based on both the original article and the Matlab code provided by the authors as supplementary material1. Contrary to the original code where the network is defined as a script (obligatory with Matlab), the network is here implemented as a Python class, allowing to reuse the same code when some parameters change between two simulations (e.g. the number of output neurons). The computations are mostly standard linear algebra operations on vectors and matrices, so the network is simply defined by a set of Numpy arrays stored as attributes. This class is then instantiated by each script reproducing the figures (Fig1.py, Fig2.py, Fig3.py), with different inputs and/or training procedures. The rest of this section describes the network in more details and presents the implementation."}, {"heading": "Structure of the network", "text": ""}, {"heading": "Recurrent network", "text": "The recurrent network is composed of N = 800 neurons, receiving inputs from a variable number of input neurons Ni and sparsely connected with each other. Each neuron\u2019s firing rate ri(t) applies the tanh transfer function on an internal variable xi(t) which follows a first-order linear ordinary differential equation (ODE):\n\u03c4 \u00b7 dxi(t) dt + xi(t) = Ni\u2211 j=1 W inij \u00b7 yj(t) + N\u2211 j=1 W recij \u00b7 rj(t) + Inoisei (t)\nri(t) = tanh(xi(t))\nThe weights of the input matrix W in are taken from the normal distribution, with mean 0 and variance 1, and multiply the rates of the input neurons yj(t). The variance of these weights does not depend on the number of inputs, as only one input neuron is activated at the same time. The recurrent connectivity matrix W rec is sparse with a connection probability pc = 0.1 (i.e. 64000 non-zero elements) and existing weights are taken randomly from a normal distribution with mean 0 and variance g/ \u221a pc \u00b7N , where g is a scaling factor. It is a well known result for sparse recurrent networks that for high values of g, the network dynamics become chaotic. Inoisei (t) is an additive noise, taken randomly at each time step and for each neuron from a normal distribution with mean 0 and variance I0. I0 is chosen very small in the experiments reproduced here (I0 = 0.001) but is enough to highlight the chaotic behavior of the recurrent neurons (non-reproducibility between two trials).\nThe build() method of the Network class initializes all these elements as Numpy arrays and stores them as attributes. For clarity, we present the code here as a regular script:\n# Input I = np.zeros((Ni, 1))\n# Recurrent population x = np.random.uniform(-1.0, 1.0, (N, 1)) r = np.tanh(x)\n1http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3753043\nReScience | rescience.github.io 2 Oct 2016 | Volume 2 | Issue 1\n# Weights between the input and recurrent units W_in = np.random.randn(N, Ni)\n# Weights between the recurrent units W_rec = np.random.randn(N, N) * g/np.sqrt(pc*N)\n# The connection pattern is sparse with p=0.1 connectivity_mask = np.random.binomial(1, pc, (N, N)) connectivity_mask[np.diag_indices(N)] = 0 W_rec *= connectivity_mask\nEverything is straightforward here: population variables are vertical vectors and weights are represented by matrices. They are initialized using Numpy\u2019s random library. In order to ensure the sparseness of the recurrent weights. connectivity_mask is a binary mask of size N \u2217 N with ones where synapses should exist and zeros otherwise, generated using the binomial distribution. The diagonal is removed to avoid self-connections. The weight matrix is a regular Numpy matrix here, but the scipy.sparse library could have been used instead.\nTo perform the simulation, the ODE of the recurrent neurons is discretized using the forward Euler method, with an implicit time step dt of 1 ms:\nfor t in range(duration): x += (np.dot(W_in, I) + np.dot(W_rec, r)\n+ Io * np.random.randn(N, 1) - x)/tau r = np.tanh(x)"}, {"heading": "Read-out neurons", "text": "The read-out neurons simply sum the activity of the recurrent neurons using a matrix W out:\nzi(t) = N\u2211 j=1 W outij \u00b7 rj(t)\nThe read-out matrix is initialized randomly from the normal distribution with mean 0 and variance 1/ \u221a N :\n# Read-out population z = np.zeros((No, 1)) # Output weights W_out = np.random.randn(No, N) / np.sqrt(N)\nThe firing rate of the read-out neurons is simply the product between the weight matrix and the rates of the recurrent neurons at each time step:\nz = np.dot(W_out, r)\nReScience | rescience.github.io 3 Oct 2016 | Volume 2 | Issue 1"}, {"heading": "Learning rule", "text": ""}, {"heading": "Recursive least squares (RLS)", "text": "The particularity of the reservoir network proposed by Laje and Buonomano [3] is that both the recurrent weights W rec and the read-out weights are trained in a supervised manner. More precisely, in this implementation, only 60% of the recurrent neurons have plastic weights, the 40% others keep the same weights throughout the simulation.\nLearning is done using the recursive least squares (RLS) algorithm [1]. It is a supervised error-driven learning rule, i.e. the weight changes depend on the error made by each neuron: the difference between the firing rate of a neuron ri(t) and a desired value Ri(t).\nei(t) = ri(t)\u2212Ri(t)\nFor the recurrent neurons, the desired value is the recorded rate of that neuron during an initial trial (to enforce the reproducibility of the trajectory). For the readout neurons, it is a function which we want the network to reproduce (e.g. handwriting as in Fig. 2).\nContrary to the delta learning rule which modifies weights proportionally to the error and to the direct input to a synapse (\u2206wij = \u2212\u03b7 \u00b7 ei \u00b7 rj), the RLS learning uses a running estimate of the inverse correlation matrix of the inputs to each neuron:\n\u2206wij = \u2212ei \u2211\nk\u2208B(i)\nP ijk \u00b7 rk\nEach neuron i therefore stores a square matrix P i, whose size depends of the number of weights arriving to the neuron. Read-out neurons receive synapses from all N recurrent neurons, so the P matrix is N \u2217 N . Recurrent units have a sparse random connectivity (pc = 0.1), so each recurrent neuron stores only a 80 \u2217 80 matrix on average. In the previous equation, B(i) represents those existing weights.\nThe inverse correlation matrix P is updated at each time step with the following rule:\n\u2206P ijk = \u2212 \u2211 m\u2208B(i) \u2211 n\u2208B(i) P i jm \u00b7 rm \u00b7 rn \u00b7 P ink 1 + \u2211 m\u2208B(i) \u2211 n\u2208B(i) rm \u00b7 P imn \u00b7 rn\nEach matrix P i is initialized to the diagonal matrix and scaled by a factor 1/\u03b4, where \u03b4 is 1 in the current implementation and can be used to modify implicitly the learning rate [6]."}, {"heading": "Implementation", "text": "Linear algebra: For an efficient implementation in Python, the previous weightspecific update rules have to be turned into matrix/vector operations. Unfortunately, for the recurrent units, each matrix P i has a different size (80 \u2217 80 on average), so we will still need to iterate over all post-synaptic neurons. If we note W the vector of weights coming to a neuron (80 on average), r the corresponding vector of firing rates (also 80), e the error of that neuron (a scalar) and P the inverse correlation matrix (80*80), the update rules become:\n\u2206W = \u2212e \u00b7 P \u00b7 r\n\u2206P = \u2212 (P \u00b7 r) \u00b7 (P \u00b7 r) T\n1 + rT \u00b7 P \u00b7 r In Haykin [1], it is shown that P converges towards the inverse correlation matrix\nof the inputs to the neuron, plus a regularization term:\nReScience | rescience.github.io 4 Oct 2016 | Volume 2 | Issue 1\nP = ( \u2211 t r(t) \u00b7 rT (t)\u2212 \u03b4I)\u22121\nHowever, by looking at the original Matlab code, one notices that the weight update \u2206W is also normalized by the denominator of the update rule for P:\n\u2206W = \u2212e \u00b7 P \u00b7 r 1 + rT \u00b7 P \u00b7 r\nRemoving this normalization from the learning rule impairs learning completely, so we kept this variant of the RLS rule in our implementation.\nInitialization: The RLS rule is applied to both recurrent and read-out weights. For the recurrent neurons, we need to first build a list of the actual inputs to each neuron:\nN_plastic = int(0.6*N) W_plastic = [list(np.nonzero(connectivity_mask[i, :])[0])\nfor i in range(N_plastic)]\nHere, only 60% of the recurrent neurons have plastic synapses, so we restrict learning to the first 480 neurons (the exact indexes are irrelevant). We use the non-zero elements of connectivity_mask to find out which synapses actually exist.\nWe can then initialize the P matrices for each recurrent neuron, depending on how many weights they receive:\nP = [np.identity(len(W_plastic[i]))/delta for i in range(N_plastic)]\nInitializing the P matrix for the read-out neurons is easier, as they all receive exactly N weights from the recurrent neurons:\nP_out = [np.identity(N)/delta for i in range(No)]\nTraining: Once the P matrices are initialized to the correct value, implementing the learning rule only consists of getting the algebraic operations right. One particularity of the original Matlab code is that the learning rules are only applied every 2 time steps during the training window instead of every step. As the computations are heavy, we kept this implementation trick which does not change much the results.\nFor the recurrent weights, we need to build a vector r_plastic for each neuron that contains the firing rates of the neurons connected to it. Once we have this vector, we can apply the RLS rule. The error is computed as a vector of shape (800, 1) (one scalar value per recurrent unit), but in practice only used for the first N_plastic units:\n# Compute the error of the recurrent neurons error = r - target\nReScience | rescience.github.io 5 Oct 2016 | Volume 2 | Issue 1\n# Apply the RLS learning rule to the recurrent weights for i in range(N_plastic): # for each plastic post neuron\n# Get the rates from the plastic synapses only r_plastic = r[W_plastic[i]] # Multiply with the inverse correlation matrix P*R PxR = np.dot(P[i], r_plastic) # Normalization term 1 + R'*P*R RxPxR = (1. + np.dot(r_plastic.T, PxR)) # Update the inverse correlation matrix # P <- P - ((P*R)*(P*R)')/(1+R'*P*R) P[i] -= np.dot(PxR, PxR.T)/RxPxR # Learning rule W <- W - e * (P*R)/(1+R'*P*R) W_rec[i, W_plastic[i]] -= error[i, 0] * (PxR/RxPxR)[:, 0]\nThe weight update uses the error of the neuron i (a scalar) and a vector corresponding to (P*R)/(1+R'*P*R). The shape of W_rec[i, W_plastic[i]] is (80,) on average, but PxR/RxPxR is (80, 1), so we need to slice this array to the correct shape using [:, 0]. The implementation for the read-out weights is similar, except we can use directly r instead of r_plastic in the update rules."}, {"heading": "Training procedure", "text": "The training procedure is split into different trials, which differ from one experiment to another (Figs 1, 2 and 3). Each trial begins with a relaxation period of t_offset = 200 ms, followed by a brief input impulse of duration 50 ms and variable amplitude. This impulse has the effect of bringing all recurrent neurons into a deterministic state (due to the tanh transfer function, the rates are saturated at either +1 or -1). This impulse is followed by a training window of variable length (in the seconds range) and finally another relaxation period. In Figs 1 and 2, an additional impulse (duration 10 ms, smaller amplitude) can be given a certain delay after the initial impulse to test the ability of the network to recover its acquired trajectory after learning.\nIn all experiments, the first trial is used to acquire an innate trajectory for the recurrent neurons in the absence of noise (I0 is set to 0). The firing rate of all recurrent neurons over the training window is simply recorded and stored in an array without applying the learning rules. This innate trajectory for each recurrent neuron is used in the following trials as the target for the RLS learning rule, this time in the presence of noise (I0 = 0.001). The RLS learning rule itself is only applied to the recurrent neurons during the training window, not during the impulse or the relaxation periods. Such a learning trial is repeated 20 or 30 times depending on the experiments. Once the recurrent weights have converged and the recurrent neurons are able to reproduce the innate trajectory, the read-out weights are trained using a custom desired function as target (10 trials)."}, {"heading": "Results", "text": "Fig. 1 shows that an initially chaotic network can become deterministic after training. The target function for the single read-out neuron is a smooth impulse peaking 2 second after the initial impulse. Before training, two successive trials do not produce similar dynamics in the recurrent population, so the read-out neuron cannot reproduce the desired function. However, after training, the recurrent dynamics become identical, even in the presence of noise, allowing the read-out neuron to reproduce the target. Moreover, a short perturbation impulse to the network given 500 ms after the initial\nReScience | rescience.github.io 6 Oct 2016 | Volume 2 | Issue 1\nimpulse only transiently perturbs the network\u2019s dynamics, which quickly recover the learned trajectory.\nNote that in the original article, the recurrent weights are trained for 20 trials, while here we use 30 trials. Only a small percentage of initial configurations (i.e. initial value of the connectivity matrix) converge after 20 trials. In our experiments, 30 trials allow for an almost systematic convergence of the weights. As shown in the original article, the longer the network is trained, the more stable the trajectories will be. During learning, the code displays the averaged mean-square error over a learning trial to show how well the weights have converged.\nFig. 2 shows that using two read-out neurons, the network can robustly learn trajectories in the 2D space, such as written words (\u201cchaos\u201d and \u201cneuron\u201d) acquired as sequences of points. The setup was similar as for Fig. 1, although 4 input neurons were used (2 per word: one for the initial impulse, one for the perturbation) and\nReScience | rescience.github.io 7 Oct 2016 | Volume 2 | Issue 1\n2 read-out ones (x and y axes of the word). The training window was different for the two words (1.322 and 1.234 s respectively). As in the original paper, the scaling factor g was chosen smaller than in the previous experiment (1.5 instead of 1.8) to accelerate training: the network is less chaotic and it becomes easier to find values for the recurrent weights that lead to stable trajectories. Using g = 1.8 would require more training trials. Despite the complexity of the target output, the network is able to robustly learn the two trajectories. Perturbations (of small amplitude) during the sequence reproduction only briefly impair the trajectory.\nFig. 3 investigates the timing capacity of the recurrent network, i.e. the maximal duration of the target function that can be successfully learned by the network in a limited number of trials (here 20). For this experiment, the target function is chosen analog to Fig. 1: a flat function (value of 0.2) with a Gaussian peak after a certain duration. The position of the peak within the training window is systematically varied from 250 ms to 8 s, what primarily influences the total duration of a trial. The exact shape of the output function is not very important: only the length of the training window actually matters as we want the recurrent units to exhibit stable trajectories long enough. Other functions with the same total duration would lead to similar results.\nFor each duration, 10 randomly initialized networks were trained on the corresponding target function with the same protocol as in Fig. 1, and the correctness of the read-out activity in a test trial after learning is measured using the Pearson correlation coefficient R2. We observe that durations smaller than 4 seconds are perfectly learned by all networks, while this performance decreases (with a high variance) for longer durations. Longer durations actually necessitate more training trials to allow the convergence of the recurrent weights (the read-out weights are not difficult to train as long as the recurrent neurons are stable).\nNote that the protocol differs slightly from the original one: in Laje and Buonomano [3], the same 10 initial networks were used to learn the different durations, while here 10 new networks are re-created every time. This may explain why in our results\nReScience | rescience.github.io 8 Oct 2016 | Volume 2 | Issue 1\nlearning a duration of 8 seconds leads to a better performance than for 7 seconds (luckier initializations). As running the simulations for this figure already took three days on a standard computer, we decided not to correct the learning protocol and leave the figure as is.\nThe rest of the figures in Laje and Buonomano [3] focuses on a more specific analysis of the computational properties of the model (effect of noise, Lyapunov exponents, distribution of weights\u2026). As the match between the reproduction and the original model is already high on the three first figures, we did not perform those experiments."}, {"heading": "Conclusion", "text": "The reproduction of the model proposed by Laje and Buonomano [3] was successful. Although the original article is already quite detailed and self-explanatory, the fact that the authors released the original Matlab code on Pubmed Central allowed to resolve some small ambiguities and speed up the reproduction process:\n1. The initialization of the inverse correlation matrices P to the identity matrix is not specified in the manuscript, only in the original code.\n2. The RLS learning rule is described in a weight-specific way (\u2206wij), but Numpy (as well as Matlab) is much more efficient when using matrix/vector operations. Having the code already in this format saved a lot of time and mistakes.\n3. In the implementation, the weight change is again normalized by 1 + rT \u00b7 P \u00b7 r, which is not mentioned in the article. As the network does not converge without this normalization, the re-implementation would not have been possible.\n4. The amplitude of the perturbations in Figs. 1 and 2 was not specified in the article nor in the code. The reproduction uses wild guesses for theses values.\nReScience | rescience.github.io 9 Oct 2016 | Volume 2 | Issue 1\nThis highlights the importance of open-sourcing the implementation of computational models for a correct reproducibility. As reproducibility is the most important quality of a computational model, this suggests that all national funding agencies should require their researchers to provide freely their articles (as the NIH does with PMC) together with the corresponding data (whether it is recordings or source code) in order to encourage the development of high-quality science."}], "title": "[Re] Robust timing and motor patterns by taming chaos in recurrent neural networks", "year": 2016}