{"abstractText": "DOI 10.5281/zenodo.3528175 Cross-validation is the gold standard for evaluatingmachine learning algorithms or finetuning their parameters. The results of this technique, however, are not always reproducible andmay depend on the computing platform and the number of parallel threads, especially if the underlying learning algorithm uses a pseudo-random number generator (PRNG). This paper gives a recipe for solving these reproducibility problems and applies it to LIBLINEAR1, a popular software library that implements randomized learning algorithms based on support vectormachines2. The proposed approach solves these problems by using a cross-platform PRNG and bymaking the PRNG state private in each thread. The cross-validation results obtained with the modified version of LIBLINEAR are the same across platforms. Furthermore, the parallelized cross-validation results are no longer affected by random fluctuations arising from the sharing of the PRNG state by parallel threads.", "authors": [{"affiliations": [], "name": "Vladimir Sukhoy"}, {"affiliations": [], "name": "Alexander Stoytchev"}, {"affiliations": [], "name": "Konrad Hinsen"}, {"affiliations": [], "name": "Georgios Detorakis"}], "id": "SP:391b6505df29ceed204d4cefebb2f608fb904a92", "references": [{"authors": ["R.-E. Fan", "K.-W. Chang", "C.-J. Hsieh", "X.-R.Wang", "andC.-J. Lin"], "title": "LIBLINEAR: A library for large linear classification.", "venue": "Journal of Machine Learning Research", "year": 2008}, {"authors": ["V. Vapnik"], "title": "Statistical Learning Theory", "venue": "New York: Wiley,", "year": 1998}, {"authors": ["N. Barnes"], "title": "Publish your computer code: It is good enough.", "venue": "Nature", "year": 2010}, {"authors": ["R. Peng"], "title": "Reproducible research in computational science.", "venue": "Science", "year": 2011}, {"authors": ["D. Ince", "L. Hatton", "J. Graham-Cumming"], "title": "The case for open computer programs.", "venue": "Nature", "year": 2012}, {"authors": ["M. Hutson"], "title": "Artificial Intelligence faces reproducibility crisis.", "venue": "Science", "year": 2018}, {"authors": ["P. Henderson", "R. Islam", "P. Bachman", "J. Pineau", "D. Precup", "D. Meger"], "title": "Deep reinforcement learning that matters.", "venue": "Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence", "year": 2018}, {"authors": ["S.O. Gundersen"], "title": "Kjensmo. \u201cState of the art: Reproducibility in Artificial Intelligence.", "venue": "Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence", "year": 2018}, {"authors": ["D. Sculley", "G. Holt", "D. Golovin", "E. Davydov", "T. Phillips", "D. Ebner", "V. Chaudhary", "M. Young", "J.-F. Crespo", "D. Dennison"], "title": "Hidden technical debt in machine learning systems.", "venue": "Advances in Neural Information Processing Systems", "year": 2015}, {"authors": ["M. Saito", "M. Matsumoto"], "title": "SIMD-oriented Fast Mersenne Twister: A 128-bit pseudorandom number generator.", "venue": "Monte Carlo and Quasi-Monte Carlo Methods 2006. Ed. by A. Keller et al. Errata: http://www.math. sci.hiroshima-u.ac.jp/~m-mat/MT/ARTICLES/errata.pdf. Berlin, Germany: Springer,", "year": 2008}, {"authors": ["M. Matsumoto", "T. Nishimura"], "title": "Dynamic creation of pseudorandom number generators.", "venue": "Monte Carlo and Quasi-Monte Carlo Methods", "year": 1998}, {"authors": ["D. Lewis", "Y. Yang", "T. Rose", "F. Li"], "title": "RCV1: A new benchmark collection for text categorization research.", "venue": "Journal of Machine Learning Research", "year": 2004}], "sections": [{"text": "R E S C I E N C E C Letter / Machine Learning"}, {"heading": "Eliminating the Variability of Cross-Validation Results", "text": "with LIBLINEAR due to Randomization and Parallelization Vladimir Sukhoy1, ID and Alexander Stoytchev1, ID 1Iowa State University, Department of Electrical and Computer Engineering, Ames, IA 50011, USA\nEdited by Konrad Hinsen ID\nReviewed by Georgios Detorakis ID\nXavier Hinaut ID\nReceived 15 July 2019\nPublished 04 November 2019\nDOI 10.5281/zenodo.3528175\nCross-validation is the gold standard for evaluatingmachine learning algorithms or finetuning their parameters. The results of this technique, however, are not always reproducible andmay depend on the computing platform and the number of parallel threads, especially if the underlying learning algorithm uses a pseudo-random number generator (PRNG). This paper gives a recipe for solving these reproducibility problems and applies it to LIBLINEAR1, a popular software library that implements randomized learning algorithms based on support vectormachines2. The proposed approach solves these problems by using a cross-platform PRNG and bymaking the PRNG state private in each thread. The cross-validation results obtained with the modified version of LIBLINEAR are the same across platforms. Furthermore, the parallelized cross-validation results are no longer affected by random fluctuations arising from the sharing of the PRNG state by parallel threads.\n1 Introduction\nThe reproducibility ofmachine learning results has been questioned in the top scientific journals3,4,5,6. Similar issues have been brought up in the Artificial Intelligence7,8 and Machine Learning9 communities. According to Henderson et al.7, reproducibility problems canbe extrinsic, i.e., unrelated to the algorithms or their implementations, or intrinsic, i.e., directly associated with the algorithms, their implementations, or the computing environments in which they run. This paper describes a technique that eliminates two intrinsic problems that may impede the reproducibility of cross-validation results for randomized learning algorithms. The first problem stems from using a platformdependent pseudo-random number generator (PRNG). The second problem is due to sharing of the PRNG state by parallel threads. Cross-validation (CV) is a statistical technique that is often used to evaluate machine learning algorithms or to select their parameters (see Figure 1). It assigns data instances intoN folds and then picksK folds for testing andN\u2212K folds for training. This process is repeated R times and the results are averaged. Typically,K = 1 and R = N . Because a PRNG is often used to assign instances to folds, the resultsmay depend on the platform (i.e., OS, programming language, compiler, runtime libraries, etc.). It is straightforward to parallelize CV by distributing the R repetitions over T threads, but the results can be affected by PRNG state sharing. In particular, this is true for algorithms implemented in C or C++ that use the standard function rand(). Furthermore, rand() may not even be thread-safe (POSIX 7, 2018, p. 1767). The proposed technique solves the first reproducibility problemby replacing a platformdependent PRNG with a cross-platform PRNG. In the experiments, we used the SIMD-\nCopyright \u00a9 2019 V. Sukhoy and A. Stoytchev, released under a Creative Commons Attribution 4.0 International license. Correspondence should be addressed to Vladimir Sukhoy (sukhoy@iastate.edu) The authors have declared that no competing interests exists. Code is available at https://github.com/sukhoy/cvrep. Open peer review is available at https://github.com/ReScience/submissions/issues/6.\nReScience C 5.3 (#1) \u2013 Sukhoy and Stoytchev 2019 1\noriented Fast Mersenne Twister (SFMT) library11, but the approach should work with any cross-platform PRNG. The second reproducibility problemwas solved by re-seeding the PRNG in each thread before each repetition and holding the PRNG state variables in thread-local storage (TLS). These modifications do not require synchronizing the PRNG calls in parallel threads. That is, they don\u02bct reduce the scalability of parallelized CV. This technique was evaluated bymodifying LIBLINEAR1, which is a popular library that implements randomized learning algorithms based on linear support vector machines2. The original library is affected by both reproducibility problems described above. The experiments showed that the results obtained with the modified version of the library remained reproducible across platforms and compilers. Furthermore, the results were not affected by the number of parallel threads used during the CV.\n2 A Recipe for Reproducible Parallelizable Cross-Validation\nThe choice of a PRNG may affect the results of a parallelized CV when it is used with a randomized learning algorithm. More specifically, this choice affects: 1) the generation of CV folds; and 2) the PRNG output used by the learning algorithm in each thread. In both cases, the following recipe makes the results reproducible:\n1. Use reproducible CV folds. This can be achieved by using a predetermined assignment of instances to folds or by using a known random seed for a randomized assignment.\n2. Use thread-local storage (TLS) to hold the PRNG state in each thread without sharing it with any other thread. Alternatively, each thread can allocate the memory for its PRNG state dynamically, so that it is not shared with any other thread.\n3. Re-seed the PRNG in each thread before processing each CV repetition. That is, if a thread runsmore than one CV repetition, then use a known random seed to initialize the threads\u0313 PRNG before processing each repetition. The simplest approach is to use the same random seed in all cases. Another possibility is to determine the random seed from the data, e.g., by deriving the seed from the value of a cryptographic hash function applied to the data in the test fold. Yet another possibility is to use a PRNG that is independent of other PRNGs for each CV repetition12, which prevents exhausting the state space when all PRNGs are structurally the same and only their seeds are different.\nThe next section describes how to apply this recipe to the LIBLINEAR library.\nReScience C 5.3 (#1) \u2013 Sukhoy and Stoytchev 2019 2\n3 Modifications for LIBLINEAR\nThis section describes how to patch LIBLINEAR (version 2.21) to ensure CV reproducibility. This version of the patch assumes that a modern compiler is used (i.e., GCC 4.2 and later or LLVM/Clang 3.9 and later). The patch replaces all calls to rand() with a different PRNG based on the SFMT library11. The first five steps ensure that the crossvalidation results are reproducible across platforms. The last step makes it possible to parallelize the cross-validation using multiple threads while preserving reproducibility.\n1. Create a sub-directory called SFMT in the top-level LIBLINEAR directory and unpack the SFMT library there (we used SFMT v. 1.5.1). Then, compile it as follows: $ cc -c -fPIC -DSFMT_MEXP=19937 SFMT.c\n2. Extend LIBLINEARs\u0313Makefile to link it with SFMTby inserting the following line before the line that starts with all: override LIBS += SFMT/SFMT.o\n3. Redirect all rand() calls to its SFMT-based replacement using the C/C++ preprocessor so that each threaduses a local private PRNGstate by inserting the following snippet after all #include directives at the beginning of linear.cpp: #include \u201dSFMT/SFMT.h\u201d #define rand sfmt_random #define RAND_MAX 0x7fffffff static __thread sfmt_t sfmt = {}; static const int default_sfmt_seed = 1234; static inline int sfmt_random() {\nreturn sfmt_genrand_uint32(&sfmt) % RAND_MAX; } void seed_liblinear_PRNG(int seed) { sfmt_init_gen_rand(&sfmt, seed); } static void __attribute__((constructor)) seed_sfmt_startup() { seed_liblinear_PRNG(default_sfmt_seed); }\n4. Add a function that initializes the random seed to LIBLINEARs\u0313 interface by inserting the following line in linear.h: void seed_liblinear_PRNG(int seed);\nThis change is useful when LIBLINEAR is used as a library by another application.\n5. To ensure that CV results are reproducible evenwith parallel processing, the PRNG should be re-seeded before processing each repetition. To achieve this, modify the last for-loop in the function cross_validation() in linear.cpp as follows:\nfor(i=0;i<nr_fold;i++) {\nseed_liblinear_PRNG(default_sfmt_seed); ...\n}\n6. To enable parallel processing, use OpenMP (OpenMP v.3, 2008) to distribute the fold combinations to the worker threads by adding the following #pragma option before the same for-loop as in the previous step:\n#pragma omp parallel for for(i=0;i<nr_fold;i++)\nTo enable OpenMP it may be necessary to modify the compilation options, e.g., by adding -f openmp to the CFLAGS variable.\nReScience C 5.3 (#1) \u2013 Sukhoy and Stoytchev 2019 3\n4 Results\nTable 1 compares the cross-validated accuracies for the rcv1_train data set14. These results were obtained using LIBLINEAR-2.21 and its patched version described in the previous section (excluding step 6). The results are shown for three platforms: 1) Linux (32 cores, RedHat 4.4.7-18 with GCC 4.4.7), 2) macOS X (4 cores, version 10.13.6 with Xcode 10.1), and 3) Windows (2 cores, version 10 with Visual Studio 2017). The results (shown in bold) imply that the modified version of the library produced the same crossvalidation results on all three platforms. Table 2 shows the accuracy statistics for the parallelized CV. To enable parallel processing, the original version of LIBLINEAR was patched using only step 6 from Section 3; the PRNG remained unchanged. The modified version was patched using all six steps from Section 3 and the results with this version are shown in bold. The table shows that random fluctuations are introduced by parallelization and that the proposed technique eliminates them (i.e., the standard deviation is zero). The results in both tables were obtained using the following command line:\n$ train -c 4 -e 0.1 -v <n_folds> rcv1_train.binary\nwhere <n_folds> specified the number of folds, i.e., 5, 10, or 20. On Windows, the PRNG state in Visual Studio is already thread-local, which prevented fluctuations (see the last row of Table 2). Without re-seeding the PRNGs, however, the results still depend on the number of threads T . To show this, we performed another experiment on Windows that used only step 6 from Section 3 and also varied T from 1 to the number of folds N , where N was set to 5, 10, and 20. The results in terms of average and standard deviation (in %) were as follows: 96.860 (0.0044) for 5-fold CV; 96.760 (0.0068) for 10-fold CV, and 96.800 (0.0043) for 20-fold CV. Table 2 shows that the results with the modified version (i.e., with all six steps) depend on N but not on T .\n5 Conclusion\nThis paper described a technique that solves intrinsic reproducibility problems of randomized learning algorithms that stem from: 1) using a platform-dependent PRNG; and 2) sharing the PRNG state across parallel threads. A recipe for patching parallelized cross-validation was described for LIBLINEAR, which is a popular machine learning library. After applying this patch to LIBLINEAR, the CV results became reproducible on three different platforms, i.e., Linux, macOS, and Windows, because the random fluctuations arising from PRNG state sharing were eliminated.\nReScience C 5.3 (#1) \u2013 Sukhoy and Stoytchev 2019 4"}], "title": "Eliminating the Variability of Cross-Validation Results with LIBLINEAR due to Randomization and Parallelization", "year": 2019}