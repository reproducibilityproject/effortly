{"abstractText": "This study is a reference implementation of Keramati, Dezfouli, and Piray [2] that proposed an arbitration mechanism between a goal-directed strategy and a habitual strategy, used to model the behavior of rats in instrumental conditionning tasks. The habitual strategy is the Kalman Q-Learning from Geist, Pietquin, and Fricout [1]. We replicate the results of the first task, i.e. the devaluation experiment with two states and two actions. The implementation is in python with numpy, scipy and matplotlib library. The authors couldn\u2019t provide the original implementation and we are not aware of other implementations elsewhere.", "authors": [{"affiliations": [], "name": "Guillaume Viejo"}, {"affiliations": [], "name": "Beno\u00eet Girard"}, {"affiliations": [], "name": "Mehdi Khamassi"}, {"affiliations": [], "name": "Julien Vitay"}, {"affiliations": [], "name": "Georgios Detorakis"}], "id": "SP:3e02dfe986fff5acb24c3003bf302defd6107860", "references": [{"authors": ["Matthieu Geist", "Olivier Pietquin", "Gabriel Fricout"], "title": "Kalman Temporal Differences: The deterministic case", "venue": "IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning", "year": 2009}, {"authors": ["Mehdi Keramati", "Amir Dezfouli", "Payam Piray"], "title": "Speed/accuracy trade-off between the habitual and the goal-directed processes", "year": 2011}], "sections": [{"text": "[Re] Speed/accuracy trade-off between the habitual and the goal-directed processes Guillaume Viejo1, Beno\u00eet Girard1, and Mehdi Khamassi1\n1 Sorbonne Universit\u00e9s, UPMC Univ Paris 06, CNRS, Institute of Intelligent Systems and Robotics (ISIR), F-75005 Paris, France\nguillaume.viejo@isir.upmc.fr\nEditor Nicolas P. Rougier\nReviewers Julien Vitay Georgios Detorakis\nReceived Jan, 20, 2016 Accepted Feb, 9, 2016 Published Feb, 10, 2016\nLicence CC-BY\nCompeting Interests: The authors have declared that no competing interests exist.\n Article repository\n Code repository\nA reference implementation of\n\u2192 Speed/accuracy trade-off between the habitual and the goal-directed processes, M. Keramati, A. Dezfouli, P. Piray, PLoS computational biology, 7, 2011"}, {"heading": "Introduction", "text": "This study is a reference implementation of Keramati, Dezfouli, and Piray [2] that proposed an arbitration mechanism between a goal-directed strategy and a habitual strategy, used to model the behavior of rats in instrumental conditionning tasks. The habitual strategy is the Kalman Q-Learning from Geist, Pietquin, and Fricout [1]. We replicate the results of the first task, i.e. the devaluation experiment with two states and two actions. The implementation is in python with numpy, scipy and matplotlib library. The authors couldn\u2019t provide the original implementation and we are not aware of other implementations elsewhere."}, {"heading": "Methods", "text": "We used the description of the model from the original article except for the implementation of the Kalman Q-Learning which we took from Geist, Pietquin, and Fricout [1]. We used the same parameters as the original article except for the update rate of the transition function \u03d5, the initialization of the covariance matrice and an uncentered transform parameter \u03ba that were not mentionned in the original article. The largest uncertainty about the model concerned the devaluation procedure. Besides setting the reward r to null, the authors stated that \u201cFor modeling the devaluation of the outcome in the first two simulations, R(S1, EM) is set to -1.\u201d As this notation (R(S1, EM)) is not defined in the rest of the article, we assumed that it is R\u0302(S1, EM) updated by equation (14) in the original article.\nThe parameters are as follows :\nName Description Value \u03c3 Updating rate of the average reward 0.02 \u03b7 Variance of evolution noise 0.0001 Pn Variance of observation noise 0.05 \u03b2 Rate of exploration 1.0 \u03c1 Update rate of the reward function 0.1 \u03b3 Discount factor 0.95\nReScience | rescience.github.io 1 Feb 2016 | Volume 2 | Issue 1\nName Description Value \u03c4 Time step of graph exploration 0.08 depth Depth of search in graph exploration 3 \u03d5 Update rate of the transition function 0.5 init cov Initialisation of covariance matrice 1.0 \u03ba Unscentered transform parameters 0.1\nWe describe the algorithm of our implementation in details. The process of action selection and reward update are separated for clarity.\nInitialization\nQ(s, a)Goal\u2212Directed = {0, . . .}\nQ(s, a)Habitual = {0, . . .}\n# Covariance matrix\n\u03a3 =  cov \u00d7 \u03b7 0 . . . 0 0 cov \u00d7 \u03b7 . . . ...\n... . . . . . . 0 0 . . . 0 cov \u00d7 \u03b7  R(S1, EM) = 1 # Reward value\nR\u0304 = 0 # Reward rate\nR\u0302(s, a) = {0, . . .} # Reward function\nMain Loop"}, {"heading": "FOR i = 1 : T", "text": "st = S0 # Initial state\nIF i = Tdevaluation # Moderate / Extensive training\nR(S1, EM) = 0\nR\u0302(S1, EM) = \u22121\nWHILE st \u0338= S1 \u2227 at \u0338= EM\nat = Selection(st)\nrt = R(st, at)\nst+1 = transition(st, at)\nUpdate(st, at, st+1, rt)\nSelection\n# Sort the Q-values in descending order\n{a1, . . . , ai, . . .} \u2190 sort(Q(st, ai))\n# VPI : Value of Precise Information\nReScience | rescience.github.io 2 Feb 2016 | Volume 2 | Issue 1\nV PI(st, a1) = (Q(st, a2) H\u2212Q(st, a1)H)P (Q(st, a1)H < Q(st, a2)H)+ \u03c3(st,at)\u221a 2\u03c0 e \u2212 (Q(st,a2) H\u2212Q(st,a1) H )2 2\u03c3(st,at) 2\nV PI(st, ai) = (Q(st, ai) H\u2212Q(st, a1)H)P (Q(st, ai)H > Q(st, a1)H)+ \u03c3(st,at)\u221a 2\u03c0 e \u2212 (Q(st,a1) H\u2212Q(st,ai) H )2 2\u03c3(st,at) 2\nFOR i \u2208 {a1, a2, . . . , ai, . . .}\nIF V PI(st, ai) \u2265 \u03c4R\u0304\n# Q-Value from Goal-directed system is evaluated\nQ(st, ai) = R\u0302(st, ai)+\u03b3 \u2211 s\u2032 pT ({s, a} \u2192 s\u2032)max b\u2208A Q(s\u2032, b)Goal\u2212directed\nELSE\n# Q-Value from Habitual system is retrieved\nQ(st, ai) = Q(st, ai) Habitual\nat \u2190 SoftMax(Q(st, a), \u03b2)\nUpdate"}, {"heading": "R\u0304 = (1\u2212 \u03c3)R\u0304+ \u03c3rt # Reward Rate", "text": "R\u0302(st, at) = (1\u2212 \u03c1)R\u0302+ \u03c1rt # Reward function\npT (st, at, st+1) = (1 \u2212 \u03d5)pT (st, at, st+1) + \u03d5 # Probability of transition\nSpecific to Kalman Q-Learning\n# Sigma-points sampling\n\u0398 = {\u03b8j , 0 \u2265 j \u2265 2|S.A|}\nW\u030c = {wj , 0 \u2265 j \u2265 2|S.A|}\nR\u030c = {r\u030cj = \u03b8j(st, at)\u2212 \u03b3 max b\u2208A \u03b8j(st+1, b), 0 \u2265 j \u2265 2|S.A|} rpredicted = 2|S.A|\u2211 j=0 wj r\u030cj\n# Covariance computation P\u03b8j r\u030cj = 2|S.A|\u2211 j=0 wj(\u03b8j \u2212QHabitualt )(r\u030cj \u2212 rpredicted) Pr\u030cj = 2|S.A|\u2211 j=0 wj(r\u030cj \u2212 rpredicted)2 + Pn\nKt = P\u03b8j r\u030cjP \u22121 r\u030cj # Kalman gain \u03b4t = rt \u2212 rpredicted # Reward-prediction error\nQHabitualt+1 = Q H t +Kt\u03b4t\nPHt+1 = P H t \u2212KtP\u03a3tKTt\nReScience | rescience.github.io 3 Feb 2016 | Volume 2 | Issue 1"}, {"heading": "Results", "text": "We only reproduced the results of Figure 3 A, B, G, H in a qualitative manner. Results are presented in Figure 1. We can observe the strategy shift (from goal-directed to habitual) after extensive training around 50 time steps. In the original article, the strategy shift occurs after 100 time steps.\nHowever we can observe a difference between the probabilities of action for the goal-directed model. In our implementation,\np(s0, pl) \u2243 0.7\nand p(s0, em) \u2243 0.3 before devaluation. In the original article,\np(s0, pl) \u2243 0.6\nand p(s0, em) \u2243 0.4 Nevertheless, the probabilities of action from the Kalman Q-Learning after strategy shifting are equivalent."}, {"heading": "Conclusion", "text": "We were able to qualitatively reproduce the first simulations of the article. Despite the small differences in the exact timing of the strategy shifting and in the probabilities of action, the behavior of our implementation is similar to the original article. Thus, we confirm the correctness of the model presented in the original article.\nReScience | rescience.github.io 4 Feb 2016 | Volume 2 | Issue 1"}], "year": 2016}