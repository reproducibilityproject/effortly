{"abstractText": "We reproduce the work in Zero-shot Knowledge Transfer via Adversarial Belief Matching, which describes a novel approach for knowledge transfer. A teacher network trained on real samples distills knowledge to a student network that is trained solely on pseudo data extracted from a generator network, with the student trying to mimic the teacher\u2019s outputs on these datapoints. To this end, we additionally re-implement Wide Residual Networks which are used as the main framework for both teacher and student networks and train them from scratch on CIFAR10 and SVHN. We compare the results of the proposed method with a few-shot knowledge distillation attention transfer setting implemented and trained from scratch. We suggest an approach for further exploitation of the learnt mechanics of the generator network in the zero-shot setting, which operates on top of the main method, and briefly discuss the benefits and drawbacks of this approach. Our code can be found publicly available in https://github.com/AlexandrosFerles/NIPS_2019_Reproducibilty_ Challenge_Zero-shot_Knowledge_Transfer_via_Adversarial_Belief_Matching.", "authors": [{"affiliations": [], "name": "Alexandros Ferles"}, {"affiliations": [], "name": "Alexander N\u00f6u"}, {"affiliations": [], "name": "Leonidas Valavanis"}, {"affiliations": [], "name": "Koustuv Sinha"}], "id": "SP:bd5f0d86d1b384d083592c7005c32ea5e4dedf12", "references": [{"authors": ["P. Micaelli", "A.J. Storkey"], "title": "Zero-shot Knowledge Transfer via Adversarial Belief Matching.", "venue": "Advances in Neural Information Processing Systems 32", "year": 2019}, {"authors": ["S. Zagoruyko", "N. Komodakis"], "title": "Wide Residual Networks.", "venue": "Proceedings of the British Machine Vision Conference (BMVC)", "year": 2016}, {"authors": ["S. Zagoruyko", "N. Komodakis"], "title": "Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer.", "venue": "In: 5th International Conference on Learning Representations,", "year": 2017}, {"authors": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "title": "Deep Residual Learning for Image Recognition.", "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "year": 2016}, {"authors": ["G. Hinton", "O. Vinyals", "J. Dean"], "title": "Distilling the knowledge in a neural network.", "year": 2015}, {"authors": ["D. Kingma", "J. Ba"], "title": "Adam: A Method for Stochastic Optimization.", "venue": "In: International Conference on Learning Representations (Dec", "year": 2014}, {"authors": ["I. Loshchilov", "F. Hutter"], "title": "SGDR: Stochastic Gradient Descent with Warm Restarts.", "venue": "ICLR", "year": 2017}, {"authors": ["S. Ahn", "S.X. Hu", "A.C. Damianou", "N.D. Lawrence", "Z. Dai"], "title": "Variational Information Distillation for Knowledge Transfer.", "year": 2019}, {"authors": ["M. Tan", "Q.V. Le"], "title": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks.", "venue": "CoRR abs/1905.11946", "year": 2019}, {"authors": ["S. Jetley", "N.A. Lord", "N. Lee", "P.H.S. Torr"], "title": "Learn to Pay Attention.", "year": 2018}], "sections": [{"text": "Edited by Koustuv Sinha ID\nReviewed by Anonymous Reviewers\nReceived 15 February 2020\nPublished 21 May 2020\nDOI 10.5281/zenodo.3818623"}, {"heading": "1 Introduction", "text": "Knowledge distillation is a model compression technique which attempts to transfer the knowledge of large cumbersome models to smaller models. Many recent successful deep networks are extremely large and contain millions of parameters, which limits their usage to machines with more powerful hardware. For such networks to be available to a wider range of devices, model compression techniques are vital. In many cases, data availability concerning a specific task is limited, due to a variety of reasons ranging from corporateowned datasets to the preservation of privacy of the individuals that participated in the creation of a dataset. This has in fact motivated the emerge of few/zero shot distillation approaches, where a pre-trained model can be used for distillation with little or no access to the data it was trained on.\nIn this work, we reproduce the paper Zero-shot Knowledge Transfer via Adversarial Belief Matching [1], where the authors present a method for distilling the knowledge of a larger pre-trained network to a smaller one, without the use of real data from the side of the student network. Our work comprises of a full re-implementation and reproduction of this method and any other methods and experiments described in this paper, including re-training the Wide Residual Network[2] teacher networks from scratch on CIFAR10 and SVHN and reproducing the few-shot knowledge distillation via attention transfer of [3]. Additionally, we propose a modification of the main method in an attempt to yield better zero-shot knowledge transfer results. We present our results, analyze our\nCopyright \u00a9 2020 A. Ferles, A. N\u00f6u and L. Valavanis, released under a Creative Commons Attribution 4.0 International license. Correspondence should be addressed to Alexandros Ferles (ferles@kth.se) The authors have declared that no competing interests exist. Code is available at https://github.com/AlexandrosFerles/NIPS_2019_Reproducibilty_Challenge_Zeroshot_Knowledge_Transfer_via_Adversarial_Belief_Matching. \u2013 SWH swh:1:dir:2c366708175b2ed7c83ce6b33a80dd43c8aad915.\nReScience C 6.2 (#2) \u2013 Ferles, N\u00f6u and Valavanis 2020 1\nfindings, and discuss the reproducibility process of the paper with comments concerning discrepancies compared to the source code."}, {"heading": "2 Methods", "text": ""}, {"heading": "2.1 Wide Residual Networks", "text": "Wide Residual Networks (WRNs) were originally proposed in [2] and are used as the main framework for both the teacher and student network in the few-shot knowledge distillation setting of [3] and zero-shot knowledge transfer setting of [1]. The main motivation of WRNs is to provide a network with similar performance to much deeper neural networks by taking advantage of less yet wider residual layers. WRNs are uniquely defined by two hyperparameters: the depth n of the network and the width factor w of each layer.\nWRNs comprise of a single convolutional layer, followed by 3 blocks of convolutional layers that extract features which are subsampled by a global average pooling layer before being fed to a linear layer to generate class predictions. The number of convolutional layers at each block is the same, and is defined by the factor n of the network. Additionally, each convolutional layer which lies inside the blocks of WRNs, learns a residual function[4] on its input. The initial convolutional layer on all WRN versions is the same and performs a convolution operation that outputs 16 feature maps. In their simplest form (n = 16, k = 1) WRNs use 16, 32 and 64 output feature maps at each block in respect. Wider version multiply each of these values with k to define the amount of feature maps that will be used at each block.\nAt each individual block, the first convolution operation is responsible for the subsampling of its input and the increase of the number of feature maps, when necessary. Finally, the operations of batch normalisation and ReLU activation are applied in a reverse order compared to most deep convolutional networks, as in WRNs each batch normalisation layer precedes the non-linearity activation function."}, {"heading": "2.2 Knowledge Distillation and Attention Transfer", "text": "The zero-shot method is evaluated through comparison with a few-shot knowledge distillation method proposed in [5]. A student network matches the outputs of a pre-trained teacher network by feeding real data to the teacher and using the output probabilities as targets for training the student. The original paper uses cross entropy loss to train the student with softened teacher probability outputs\nqi = exp(zi/T )\u2211 j exp(zj/T ) , (1)\nwhere temperature T yields a softer probability distribution of classes and (1) corresponds to standard softmax activation of the teacher outputs zi when T = 1. To make use of the true labels of the data, a weighted combination of cross entropy losses with labels and teacher outputs as targets serve as objective for training the student. In the experiments of [1], the Kullback-Leibler divergence is used. Moreover, the baseline model is augmented by adding an attention transfer loss [3]:\n\u03b2 NL\u2211 l \u2223\u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223\u2223 f(A (t) l )\u2223\u2223\u2223\u2223\u2223\u2223f(A(t)l )\u2223\u2223\u2223\u2223\u2223\u2223\n2\n\u2212 f(A (s) l )\u2223\u2223\u2223\u2223\u2223\u2223f(A(s)l )\u2223\u2223\u2223\u2223\u2223\u2223\n2 \u2223\u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223\u2223 2\n(2)\nThe additional loss takes a subset of activation blocks Al and computes the squared mean over channels in order to get spatial attention maps f(Al). By adding (2) to the objective, the student is encouraged to match the spatial attention maps of the teacher.\nReScience C 6.2 (#2) \u2013 Ferles, N\u00f6u and Valavanis 2020 2\nAs in the zero-shot setting that follows, WRNs will be used for both the teacher and student network. They can easily be integrated in attention-based knowledge transfer methods since we can make use of the output feature maps of each block as a point of comparison, while the spatial resolution of the output of each block is the same regardless of the WRN version. Hence there is no need for interpolation. Additionally, since we aggregate over the filter dimension in order to create the spatial attention maps of each output, we can effectively compare teacher and student WRNs of different widths without extra operations on this dimension, such as a linear mapping to the same filter dimension.\nTo follow the notation of [3] and [1] for the rest of this paper we refer to this method as KD-AT."}, {"heading": "2.3 Zero-Shot Knowledge Transfer", "text": "The proposed idea of zero-shot knowledge transfer from teacher to student network is to introduce a generator network, and train the student and the generator adversarially, by using the representation learned from the teacher network. Following the notations of [1], we let T (x), S(x; \u03b8) and G(z;\u03d5) be pretrained teacher network, student network and generator, where the weights \u03b8 and \u03d5 parameterize their respective networks that are to be trained. In order to train the student to match the teacher without real data, we sample noise z \u223c N (0, I) and let G(z) generate fake samples xp. Gradient updates are alternated between student and generator to optimize the Kullback-Leibler divergence:\nDKL(T (xp) || S(xp)) = \u2211 i t(i)p log\n( t (i) p\ns (i) p\n) , (3)\nwhere tp, sp are the teacher and student output probabilities given pseudo-data, and i denotes the class. The student minimizes (3) to force it to match the output probabilities of the teacher, which the authors call \"belief matching\". For the generator, the objective is instead maximized so that it learns to produce samples where the student and teacher disagree the most. The adversarial belief matching is balanced through an appropriate choice in numbers of gradient steps nG, nS when alternating the training. In addition, the authors experiment with extra loss functions. The attention transfer term (2) used for the baseline is also applied to the zero-shot model for the main experiments of the paper.\nThus, zero-shot knowledge transfer acts based on the samples drawn from the generator network, and distills knowledge to a student network by matching the outputs of the teacher much like the knowledge distillation approach in [5]. The advantage, however, becomes clear compared to the case when only a few training samples are available: Instead of learning to match the teacher on the same limited number of samples at each iteration, the generator is trained alongside the student and will continue to generate challenging pseudo-data to further close the gap between student and teacher."}, {"heading": "2.4 Modification of the Zero-Shot Method", "text": "In the training setting of the main method, for each iteration the generator creates a single batch of samples (when nG) on which we then train the student network ns times to match the teacher\u2019s feature maps and outputs on this sole batch. The motivation for using only one batch is clear and justified, since we first force the samples in a position of the sample space which makes it difficult for the student to learn, which requires multiple student updates to balance the training. However, we propose a slight modification on the zero-shot training setting. Instead of using the same sample that the generator was updated on, we use the updated generator to provide us with ns different batches (keeping nG the same as before), each of them used only once to update the student based on the teacher\u2019s outputs on them. This way, we intend to create a more diverse pseudo-training dataset which could provide an improved training setting for the student network.\nReScience C 6.2 (#2) \u2013 Ferles, N\u00f6u and Valavanis 2020 3"}, {"heading": "2.5 Measuring Belief Matching", "text": "In order to measure the student network\u2019s degree of belief matching (network\u2019s ability to match output probabilities of teacher) with its teacher, the following procedure is executed: Test samples are progressively changed in the direction of the student\u2019s decision boundary until they resemble input data of another class. As the student\u2019s confidence in a sample belonging to the other class increases, we monitor the predictions of teacher as well. Ideally, we would expect the teacher to follow closely the behaviour of the student. We thus iterate over a number of test samples whose predicted labels are the same for student and teacher. Then, iterating over all possible classes that are not the predicted one, we take K steps of gradient updates on the sample to alter it towards the \"fake\" class j with learning rate \u03be. We get the gradient by feeding the sample to the student and computing the cross entropy with class j as target. In each step we let both student and teacher predict the progressively altered sample, and store their respective probability pj of the sample belonging to class j. Finally, the mean over fake classes and test data size results in a transition curve of pj over K steps. Mean Transition Error is introduced to quantify this matching capability, and computed through the absolute difference |psj\u2212ptj | between networks and taking the mean over K steps, fake classes and sample size."}, {"heading": "3 Implementation", "text": ""}, {"heading": "3.1 Discussion on Reproducibility Issues", "text": "For all of the methods used to derive the results of this paper, we used the PyTorch framework to train our deep networks along with external components such as Adversarial Belief Matching. We first designed each method on our own, and then consulted the official codes of the zero-shot and few-shot knowledge transfer to find hyperparameter values and fine-tune our networks. In detail, we had to integrate the following settings in our work, which were not mentioned in the paper[1] but implemented in the official repository of the authors:\n\u2022 To our knowledge, there is no mention about weight initialization in [2] or [3] from the authors of Wide ResNets. We thus used the weight initialization presented on GitHub1.\n\u2022 We initially treated the hyperparameters of Temperature T and \u03b1 value on knowledge distillation between the teacher and student network as presented in [5], and then changed it to the values used by the authors on [3]. In particular, T is equal to 4, while \u03b1 is equal to 0.9.\n\u2022 In attention transfer, the authors in [3] suggest that the best way to extract the spatial attention map would be to use the sum of the square of each individual pixel per channel, but the authors of [1] use the squared mean instead. Furthermore, the distance between student and teacher maps is quantified by taking the squared mean over batch and spatial size, as opposed to Euclidean distance which they state in their paper.\n\u2022 In [3] and [5], cross entropy is used for the student\u2019s loss term with teacher outputs as targets. However, in both the few-shot KD and zero-shot settings of [1] teacher and student are compared with the use of KL divergence between the softmax activations of the former and the log-softmax of the latter (KL for the zero-shot model is stated in the paper).\n\u2022 There is no description of the Generator network in [1] apart from \"We use a generic generator with only three convolutional layers, and our input noise z has 100 dimensions\". Thus, we consulted the official code for more details in order to\n1https://github.com/szagoruyko/wide-residual-networks\nReScience C 6.2 (#2) \u2013 Ferles, N\u00f6u and Valavanis 2020 4\ndesign this network. The structure of the generator can de found in the Appendix Section B.\n\u2022 In the zero-shot method of [1] the paper does not mention that weight clipping is performed on both the student and generator networks. We proceeded with integrating weight clipping to our training too."}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Data and Preprocessing", "text": "The network is evaluated on two commonly used data sets, CIFAR-10 and SVHN, that include 60000 and approximately 100000 32x32 images respectively. On CIFAR-10. 50000 images are allocated to its training set, while the remaining 10000 images comprise its test set and are used for evaluation purposes. On SVHN, 73257 and 26032 are allocated to its training and test set respectively. The only pre-processing method applied on SVHN is mean/std normalization. On the other hand, we perform a few methods of data augmentation on CIFAR-10 in addition to normalization, namely reflect mode image padding, random cropping and random horizontal flipping."}, {"heading": "4.2 Training WRN Scratches", "text": "The batch size on both datasets is equal to 128, and in order to match the update steps claimed on [1], we trained CIFAR10 for 200 epochs and SVHN for 100 epochs respectively. For both datasets, we apply a Stochastic Gradient Descent (SGD) optimizer with Nesterov momentum (equal to 0.9) and a weight decay of 5 \u2217 10\u22124. The initial learning rate is equal to 0.1 and divided by 5 when 30%, 60% and 80% of the update steps have been completed. Most of the steps were directly motivated from [1], while we also consulted [3] and [2] when some settings were not clear to us. We apply three seeds on each training, namely 0, 1 and 2, and apart from our own method described in 2.4 we use the same 3 seeds for the rest of this work. As in [1], we train 4 variants of WRN, namely WRN-16-1, WRN-16-2, WRN-40-1 and WRN-40-2.\nWe also trained few-shot scratches of WRN-16-1 on M samples per class (M drawn from {10, 25, 50 , 75, 100}) on each dataset under the same configurations, to generate the \u2019No Teacher\u2019 models mentioned in [1]. In order to train for the same number of update steps, we scale the number of original epochs based on each training dataset size and the value of M, by the following formula:\nepochs\u2032 = Dataset_Size\n10 \u2217M \u2217 epochs (4)\nLastly, we evaluate WRN-16-1 directly on each test set to mimic the \u2019No Teacher\u2019 model with M=0. Since this is exact setting of KD-AT with M=0, we only train this setting once per seed for both cases."}, {"heading": "4.3 Few-Shot Knowledge Attention Transfer", "text": "For few-shot knowledge distillation with attention transfer, we train WRN-16-1 under the same hyperparameter settings for each dataset and values of M drawn from {10, 25, 50 , 75, 100} for WRN-16-1 for both CIFAR and SVHN, and M=5000 for knowledge distillation when trained on full data. Additionally, we combine all the possible teacherstudent pairs of the 4 variants of WRN (listed in table 1) to train the KD-AT setting for M=200 on CIFAR10. Formula (4) is once again used to define the number of training epochs for each dataset and value of M.\nReScience C 6.2 (#2) \u2013 Ferles, N\u00f6u and Valavanis 2020 5"}, {"heading": "4.4 Zero-Shot and Modified Zero-Shot Training", "text": "The zero-Shot training setting relies on training with fake samples, so we do not need to scale the number of epochs. Instead, for both CIFAR10 and SVHN we train for 80000 iterations, sample a pseudo-batch and update the generator once per iteration (ng = 1) and then update the student ns = 10 times per iteration. For the modified zero-shot model, the generator produces a new batch for each student update. As in [1] we use Adam optimizer[6] with cosine annealing[7] in these settings, with an initial learning rate of 2 \u2217 10\u22123 for the student network and 1 \u2217 10\u22123 for the generator. Noisy inputs are sampled from a standard normal distribution with 100 dimensions, and fed to the generator which extracts pseudo batches of size 128*3*32*32, like the input batches of WRNs when trained on real data. In case we wish to use extra M samples for the zero-shot methods, the models are in addition fine-tuned few-shot by using the KD-AT procedure for a further 200 epochs. While the value of M is not clearly stated on [1] for the SVHN data, we perform a KD-AT training with M=200 to match the case with the CIFAR data."}, {"heading": "4.5 Measuring Belief Matching", "text": "For the belief matching experiment, we make use of a WRN-40-2 teacher and three WRN16-1 students, one trained from the KD-AT setting one from the zero-shot setting and one from the modified zero-shot setting. The paper does not state which M is used for KD-AT. Hence, we choose M = 200 for fair comparison as it has similar test accuracy to the zero-shot model. In order to compute the probability transition curves described in section 2.5, the process is guided from a learning rate \u03be equal to 1, and 100 update steps are performed per sample and fake label. For each of CIFAR10 and SVHN, we use 1000 test set samples, and average over the extracted probability transition curves to display our results. We also compute the Mean Transition Error (MTE) between the teacher and each of the students on each dataset as in [3] via the formula:\n1\nNsamples Nsamples\u2211 n=1 1 C \u2212 1 C\u22121\u2211 n=1 1 K |pstudent \u2212 pteacher| (5)\nwhere Nsamples represents the 1000 samples from each dataset, C is the number of different classes (equal to 10 for both CIFAR10 and SVHN), K represents the 100 updates steps, pstudent and pteacher are the probability estimations of the student and teacher for each of the K update steps on C-1 fake labels and Nsamples samples."}, {"heading": "5 Results", "text": ""}, {"heading": "5.1 Evaluation of the Knowledge Transfer Methods", "text": "We first reproduce the zero-shot and few-shot experiments for the teacher and student architectures WRN-40-2 and WRN-16-1 on SVHN and CIFAR-10. The results are presented in Figure 1, which shows test accuracy of the baseline model KD-AT trained with M samples or all data, as well as the zero-shot model and a student trained from scratch with M samples. The test accuracy are the means over three trials. In addition, the results include the performance of Variational Information Distillation (VID) of [8]. We can see in Figure 1 that the results of the paper are reproduced, where the zero-shot model outperforms KD-AT and VID trained with M = 100 samples, and almost reaches the accuracy of KD-AT with M = 5000 on SVHN.\nReScience C 6.2 (#2) \u2013 Ferles, N\u00f6u and Valavanis 2020 6\nTable 1 shows reproduced results of the experiment investigating architecture dependence on CIFAR-10. The mean test accuracy over three trials is close to the results of the paper, with small discrepancies. Similar to the official results, we also notice that the zero-shot distilling setting from WRN-40-2 to WRN-16-2 performs better than distilling from the same teacher to WRN-40-1, suggesting that deeper student networks with similar number of parameters not necessarily perform better. The opposite can be seen for KD-AT, with the deeper student network performing best (but with larger standard deviations than the paper). Moreover, we include results of our modified zero-shot algorithm, which show improved performance for all network architectures. Training our modified algorithm requires multiple generated batches per iteration, and results in higher complexity in terms of speed. However, it converges to a similar or higher accuracy in fewer iterations of the training process, making it run in a similar time or sometimes faster than the original zero-shot algorithm. Due to the complexity of the task, we did not have enough resources to further evaluate the performance of the algorithm.\nOverall, we observe that even in the reproducibility part we get slightly better results on the same settings as [1]. We tried to stay as close as possible to the methods that were reported, and mostly attribute the small improvements to the data augmentation that we applied on CIFAR when both optimizing the scratch teacher networks and training the student networks in the few-shot, zero-shot and modified zero-shot settings. Additionally, we observed that the modified zero-shot setting brings improvements even close to 3 percentage points for some cases. Our intuition is that this can be attributed to the greater diversity of samples drawn from the generator, which was our main motivation for introducing this method. The accuracy of both zero-shot settings can slightly increase\nReScience C 6.2 (#2) \u2013 Ferles, N\u00f6u and Valavanis 2020 7\nif we switch to few-shot training by taking extra update steps on the student on a few real samples, however this increase stays limited (at a few cases there was no improvement at all) which hints us that the majority of the necessary features have already been learned by the student when trained on the zero-shot settings."}, {"heading": "5.2 Visual Inspection of Learned Patterns from the Generator", "text": "Samples drawn from different generator networks at different stages of their training can be seen in the following figure. Through visual inspection, we observe that starting from random noise (as expected), features start to grow dependencies and form patterns that are useful for network training and can serve as a substitute of real data, when the latter are not available."}, {"heading": "5.3 Adversarial Belief Matching", "text": "We finally measure the belief matching between teacher and student in both the zeroshot and few-shot settings for both datasets. Figure 3 depicts the reproduced transition curves for all four cases, and Table 2 shows MTE (equation 5). The performance of the zero-shot model is very similar to the paper, but the transition error of KD-AT is higher. We observe the same pattern as the authors, that similarity in predictions between student and teacher as samples are altered is much worse for KD-AT, despite having comparable test accuracy to the zero-shot model. This is surprising since the procedure is using real data, which KD-AT uses for distillation. We provide a possible explanation for this: The process of manipulating samples towards the student decision boundary might result in images outside the space of real data. Examples of images after K update steps towards the student decision boundary of other classes can be found in Figure 5 of the appendix. The images look like noisy versions of the original class, but are now predicted as another class with almost full certainty by the student. For KD-AT, the student matches the teacher and true labels solely on real samples, whereas the zeroshot student is trained on pseudo-data which is not limited to this space, as is shown in Figure 2. A toy experiment is also conducted in [1], demonstrating how the generator produces samples that follow the decision boundary of the teacher in order to make it more difficult for the student, which could explain the high degree of belief matching in our experiments.\nThe transition curve plots concerning zero-shot and modified zero-shot on CIFAR show that the deviation between the teacher and student predictions is higher in our method. This is also confirmed by the Mean Transition Error values in Table 2, and is expected since in our setting more images are used to train the student, and at each batch iteration only one update is performed per image. On the other hand, the original zero-shot method focuses on a single image per batch iteration, where the student in updated ns = 10 times on this single image to match the teachers predictions.\nWe finally perform an extra ablation study on CIFAR for both the original zero-shot and KD-AT methods, where we replicate the setting of measuring belief matching, with the core difference that samples are updated based on the gradients of the teacher network.\nReScience C 6.2 (#2) \u2013 Ferles, N\u00f6u and Valavanis 2020 8\nThis way, the manipulated samples will be the same for both methods. We observe that the performance under this setting is different. The teacher network grows full confidence in the \u2019new\u2019 class after a few updates, while the student network reaches up to a low threshold in its confidence for that class. Thus, mean transition errors are kept high for both cases, with the zero-shot method resulting in a lower error value (0.64) compared to KD-AT (0.78). Plots for the transition curves can be found on section D of the Appendix."}, {"heading": "6 Conclusions and Future Work", "text": "In this work, we reproduced the zero-shot knowledge transfer proposed in [1]. Training a generator to produce images on which a student fails to match its teacher and then training the same student to mimic the decisions of its teacher on these pseudo data, ends up with similar or better performance in datasets such as SVHN and CIFAR-10. Moreover, we modified the training setting and sampled new images from the generator at each student gradient update instead of once in the beginning of the iteration. Consequently, the dataset is more diverse for the student to learn and the algorithm converges, resulting in better performance than the original method.\nThe initial work along with the proposed modifications leave room for further exploration and analysis. For example, the generator of a shallow network but with a more thorough designed generator, better quality adversarial features can be constructed. In-depth analysis of generated pseudo data and their diversity could also be performed, so that the resulting modified zero-shot model can provide additional insight to what effect sampling multiple batches has on the student network.\nFuture work can also focus on the selection of the teacher and student network architecture. Frameworks with higher representation learning capabilities compared to WRNs have emerged (a recent example would be EfficientNets[9]) which can be alternatively used to build a better teacher network. In this direction different frameworks can be\nReScience C 6.2 (#2) \u2013 Ferles, N\u00f6u and Valavanis 2020 9\ncombined to match intermediate layer representation with access to the same receptive field of the original image, along with matching the distribution of the output class predictions. Another research direction, would be to further explore the usability of the fact that intermediate feature maps are also optimized through the attention transfer loss. In [10], visual attention is applied to the VGG network[11] by scaling middle and coarse layer feature maps in combination with the output feature maps to improve its performance compared to its baseline version."}, {"heading": "A Wide Res Net architecture", "text": "The architecture of Wide ResNet is summed on the following figure:"}, {"heading": "B Generator Network", "text": "The following table shows the layer structure of the generator network:\nReScience C 6.2 (#2) \u2013 Ferles, N\u00f6u and Valavanis 2020 10\nFor all the convolutional layers, the kernel size k is equal to 3,while the stride s and padding p are equal to 1.\nReScience C 6.2 (#2) \u2013 Ferles, N\u00f6u and Valavanis 2020 11"}, {"heading": "C Full Experiments", "text": "C.1 Training scratches of Wide ResNets In order to use Wide ResNets of different depth and width as teacher networks for the Few-Shot Attention Knowledge Distillation and the Zero-Shot Knowledge Transfer, we trained 4 variants of Wide ResNet from scratch. The results on CIFAR10 are shown in Table 4 below.\nThe results of training teachers on SVHN are shown in Table 5 below.\nC.2 Training Wide ResNet 16-1 with no Teacher We also trained WRN-16-1 from scratch on small subsets of M images per class on CIFAR10 and SVHN and without the use of a teacher network to assist in the learning process. We firstly show the results on CIFAR10 in Table 6 below.\nReScience C 6.2 (#2) \u2013 Ferles, N\u00f6u and Valavanis 2020 12\nThe results on SVHN are the following in Table 7:\nC.3 Few-Shot Knowledge Distillation with Attention Transfer (KD-AT) Few-Shot Knowledge Distillation with Attention Transfer is trained using different pairs of Teacher-Student and different values of M for CIFAR-10 and SVHN datasets. We firstly show the results on CIFAR10 using WRN-40-2 for the Teacher and WRN-16-1 for the Teacher, for different values of M in Table 8."}, {"heading": "10 39.08 35.33 36.49", "text": ""}, {"heading": "25 60.05 58.94 63.05", "text": ""}, {"heading": "50 70.9 65.83 68.68", "text": "The results on SVHN using WRN-40-2 for the Teacher and WRN-16-1 for the Student, for different values of M are shown in Table 9."}, {"heading": "10 37.35 31.32 33.88", "text": ""}, {"heading": "25 48.71 48.89 47.44", "text": ""}, {"heading": "50 68.84 65.33 66.48", "text": ""}, {"heading": "75 78.51 78.4 79.28", "text": "ReScience C 6.2 (#2) \u2013 Ferles, N\u00f6u and Valavanis 2020 13\nThe results for different pairs of Teacher-Student for M=200 is shown in Table 10.\nC.4 Zero-Shot Knowledge Transfer We trained the zero-show Knowledge transfer algorithm for various pairs of Teacher Student for CIFAR-10 and SVHN. In Table 11 the results for CIFAR-10 is shown for various seeds and Teacher Student pairs and in Table 12 the experiment for SVHN is shown.\nC.5 Zero-Shot Knowledge Transfer with modified generator Table 13 shows the results for the modified zero-shot we tried.\nReScience C 6.2 (#2) \u2013 Ferles, N\u00f6u and Valavanis 2020 14\nC.6 Zero-Shot Knowledge Transfer with extra M real samples Our results in CIFAR10 when extra samples are drawn from the generator, are presented in Table 14.\nThe same setting is repeated on the SVHN dataset in Table 15 with the following results:\nC.7 Modified Zero-Shot Knowledge Transfer with extra M real samples Our results in CIFAR10 when extra samples are drawn from the generator, are presented in Table 16.\nReScience C 6.2 (#2) \u2013 Ferles, N\u00f6u and Valavanis 2020 15"}, {"heading": "D Samples for Measuring Belief Matching", "text": "The following figure shows the evolution of images when manipulated at different steps regarding the belief matching experiment conducted in section 4.5:"}, {"heading": "E Transition Curves of Teacher Updates", "text": "ReScience C 6.2 (#2) \u2013 Ferles, N\u00f6u and Valavanis 2020 16"}], "title": "[Re] Zero-Shot Knowledge Transfer via Adversarial Belief Matching", "year": 2020}