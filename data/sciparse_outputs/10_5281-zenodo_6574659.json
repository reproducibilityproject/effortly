{"abstractText": "Weshow that the theoretical assumption regarding eigenspace alignment and symmetry hold also for a different dataset other than the oneused in the original paper. In addition, we reproduce ablations regarding learning rate, weight decay and Exponential Moving Average. Since we used CIFAR\u201010 in all experiments we can not directly compare accuracies. However, we show the same relative behaviour of different networks given hyperparam\u2010 eter changes. We can directly compare performance for one of the experiments (Table 8. in [1] bottom left part). Our models, namely SGD Baseline, DirectPred (with and without frequency=5), achieve comparable accuracy which differ by at most 1%. We also con\u2010 firm the claim thatDirectPred outperforms its one\u2010layer SGDalternative. Our code canbe accessedunder the following link: https://anonymous.4open.science/r/SelfSupervisedLearning-FD0F.", "authors": [{"affiliations": [], "name": "Tobias H\u00f6ppe"}, {"affiliations": [], "name": "Agnieszka Miszkurka"}, {"affiliations": [], "name": "Dennis Bogatov Wilkman"}, {"affiliations": [], "name": "Koustuv Sinha"}, {"affiliations": [], "name": "Sharath Chandra Raparthy"}], "id": "SP:7bb4504ce78ca5a7eabc9ddcfe00e25db2dcf4ca", "references": [{"authors": ["Y. Tian", "X. Chen", "S. Ganguli"], "title": "Understanding self-supervised Learning Dynamics without Contrastive Pairs. 2021", "year": 2021}, {"authors": ["J.-B. Grill"], "title": "Bootstrap your own latent: A new approach to self-supervised Learning", "year": 2020}, {"authors": ["X. Chen", "K. He"], "title": "Exploring Simple Siamese Representation Learning", "year": 2020}, {"authors": ["J. Bromley", "J. Bentz", "L. Bottou", "I. Guyon", "Y. Lecun", "C. Moore", "E. Sackinger", "R. Shah"], "title": "Signature Verification using a \u201dSiamese", "venue": "Time Delay Neural Network.\u201d In: International Journal of Pattern Recognition and Artificial Intelligence", "year": 1993}, {"authors": ["T. Chen", "S. Kornblith", "M. Norouzi", "G. Hinton"], "title": "A Simple Framework for Contrastive Learning of Visual Representations", "year": 2020}, {"authors": ["T. Chen", "S. Kornblith", "K. Swersky", "M. Norouzi", "G. Hinton"], "title": "Big Self-Supervised Models are Strong SemiSupervised Learners", "year": 2020}, {"authors": ["K. He", "H. Fan", "Y.Wu", "S. Xie", "andR"], "title": "Girshick.MomentumContrast for UnsupervisedVisual Representation Learning", "year": 2020}, {"authors": ["A. van den Oord", "Y. Li", "O. Vinyals"], "title": "Representation Learning with Contrastive Predictive Coding. 2019", "year": 2019}, {"authors": ["D.P. Kingma", "M. Welling"], "title": "Auto-Encoding Variational Bayes", "year": 2014}, {"authors": ["P. Vincent", "H. Larochelle", "Y. Bengio", "P.-A. Manzagol"], "title": "Extracting and composing robust features with denoising autoencoders.", "year": 2008}, {"authors": ["J. Donahue", "P. Kr\u00e4henb\u00fchl", "T. Darrell"], "title": "Adversarial Feature Learning. 2017", "year": 2017}, {"authors": ["J. Donahue", "K. Simonyan"], "title": "Large Scale Adversarial Representation Learning. 2019", "year": 1907}, {"authors": ["X. Wang", "X. Chen", "S.S. Du", "Y. Tian"], "title": "Towards Demystifying Representation Learning with Non-contrastive Self-supervision", "year": 2021}, {"authors": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "title": "Deep Residual Learning for Image Recognition", "year": 2015}, {"authors": ["D.P. Kingma", "J. Ba"], "title": "Adam: Amethod for stochastic optimization.", "year": 2014}, {"authors": ["I. Loshchilov", "F. Hutter"], "title": "Decoupled weight decay regularization.", "venue": "ReScience C", "year": 2017}], "sections": [{"text": "R E S C I E N C E C Replication / ML Reproducibility Challenge 2021\n[Re] Understanding Self-Supervised Learning Dynamics without Contrastive Pairs Tobias H\u00f6ppe1, ID , Agnieszka Miszkurka1, ID , and Dennis Bogatov Wilkman1, ID 1KTH Royal Institute of Technology, Stockholm, SWE\nEdited by Koustuv Sinha,\nSharath Chandra Raparthy\nReviewed by Anonymous Reviewers\nReceived 04 February 2022\nPublished 23 May 2022\nDOI 10.5281/zenodo.6574659"}, {"heading": "Reproducibility Summary", "text": ""}, {"heading": "Scope of Reproducibility", "text": "The authors in [1] claim that with the underlying learning dynamics of BYOL [2] and SimSiam [3], a new method DirectPred can be derived. We investigate the assumptions made for this derivation and also compare the quality of the produced encoder repre\u2010 sentations through linear probing of these networks."}, {"heading": "Methodology", "text": "We reimplemented BYOL, SimSiam and DirectPred from scratch as well as their ablations in TensorFlow. We checked original repository in written PyTorch for some implemen\u2010 tation details. In all experiments we used the CIFAR\u201010 train set for training and the test set for evaluation. We were running our experiments for more than 100 hours on GCP\u2019s V100 GPU."}, {"heading": "Results", "text": "Weshow that the theoretical assumption regarding eigenspace alignment and symmetry hold also for a different dataset other than the oneused in the original paper. In addition, we reproduce ablations regarding learning rate, weight decay and Exponential Moving Average. Since we used CIFAR\u201010 in all experiments we can not directly compare accuracies. However, we show the same relative behaviour of different networks given hyperparam\u2010 eter changes. We can directly compare performance for one of the experiments (Table 8. in [1] bottom left part). Our models, namely SGD Baseline, DirectPred (with and without frequency=5), achieve comparable accuracy which differ by at most 1%. We also con\u2010 firm the claim thatDirectPred outperforms its one\u2010layer SGDalternative. Our code canbe accessedunder the following link: https://anonymous.4open.science/r/SelfSupervisedLearning-FD0F."}, {"heading": "What was easy", "text": "The architecture of the Siamese network and training schemes were both straightfor\u2010 ward to implement and easy to understand.\nCopyright \u00a9 2022 T. H\u00f6ppe, A. Miszkurka and D.B. Wilkman, released under a Creative Commons Attribution 4.0 International license. Correspondence should be addressed to Dennis Bogatov Wilkman (dwilkman@kth.se) The authors have declared that no competing interests exist. Code is available at https://github.com/miszkur/SelfSupervisedLearning \u2013 DOI 10.5281/zenodo.6508184. \u2013 SWH swh:1:dir:ec5169f4713c6c67088d980c76f1c25bc1c399bc. Data is available at https://www.cs.toronto.edu/~kriz/cifar.html. Open peer review is available at https://openreview.net/forum?id=r4xe3nMQ3AY.\nReScience C 8.2 (#17) \u2013 H\u00f6ppe, Miszkurka and Wilkman 2022 1"}, {"heading": "What was difficult", "text": "We could not run our code on STL\u201010 dataset due to time and resource constraints. Due to differences between PyTorch and TensorFlow libraries, we had to implement some parts by hand to keep our code as close to the original work as possible. Also, original repository is not easy to read and does not cover all the experiments (e.g. eigenspace alignment experiment). Correctly applying data\u2010augmentation was also a hard task due to assumptions of how the individual data augmentations functions actually work.\nCommunication with original authors We did not contact authors of the paper since we did not encounter any major issues during the reproducibility study.\nReScience C 8.2 (#17) \u2013 H\u00f6ppe, Miszkurka and Wilkman 2022 2\n1 Introduction\nSelf\u2010Supervised learning has become an important task in many domains, since labeled data is often rare and expensive to get. Many modern methods of Self\u2010Supervised learn\u2010 ing are based on Siamese\u2010networks [4] which areweight sharingNeural networks for two or more inputs which representations then will be compared in latent space. The rep\u2010 resentation created by this approach can then be used for classification by fine\u2010tuning on fewer labelled data\u2010points. Traditionally, during pre\u2010training positive pairs (same image, or two images from the same class) and negative pairs (different images or two images from a different class) are used. The distance of the representation of positive pairs is minimized while the distance of the representation of negative pairs is maxi\u2010 mized, which prevents the networks from collapse (i.e mapping all inputs to the same representation). These methods have shown quite some success in the past [5], [6], [7], [8]. However, these methods rely on negative pairs, and large batch sizes which makes the training less feasible. Recently, new methods have been proposed which rely only on positive pairs and yet don\u2019t collapse [2], [3]. In the paper \u201dUnderstanding Self\u2010Supervised Learning Dynamics without Contrastive Pairs\u201d by Tian et.al. [1] the underlying dynamics are explored and based on the theoretical results, a new method, DirectPred, was proposed which does not need an update of the predictor via gradient descent but instead is set directly each iteration. The focus of this work is to test several assumptionsmade in [1] for the theoretical analy\u2010 sis and see if they hold. For this, wewill concentrate especially on the eigenvalues of the predictor network and the eigenspace alignment with its input. Also, we will reproduce the results from [1], [2] and [3] on CIFAR\u201010 to compare their learned representation using linear probing.\n2 Related work\nA common approach to representation learning without Siamese networks is genera\u2010 tive modelling. Typically these methods model a distribution over the data and a la\u2010 tent space, fromwhich then embeddings can be drawn as data representations. Usually these approaches rely on Auto\u2010encoding [9, 10] or Adversarial networks [11, 12]. How\u2010 ever, generative models are often computationaly heavy and hard to train. Discriminativemethods using Siamese networks like SimCLR [5, 6] andMoco [7] outper\u2010 form generative models and have lower computational cost. However, these methods rely on very large batch sizes since they use contrastive pairs. Most recent methods, replicated in this work, like BYOL [2] and SimSiam [3], only rely on positive pairs and therefore can make use of smaller batch sizes. To understand why these methods do not collapse, the dynamics of these networks are analysed with linear models in [1, 13]. From this analysis, the authors could derive ablations of BYOLwhere part of the network is directly set to its optimal solution instead of being trained by gradient descent.\n3 Method\nIn this section we will describe the methods of BYOL and SimSiam as well as their suc\u2010 cessor DirectPred.\n3.1 BYOL & SimSiam The network architecture of the models is shown in Figure 1. First, two augmented viewsX \u20321 andX \u20322 of an imageX are created and fed into the online networkW and target\nReScience C 8.2 (#17) \u2013 H\u00f6ppe, Miszkurka and Wilkman 2022 3\nnetwork Wa respectively. Both of these networks have the same architecture, a ResNet\u2010 18 (W xenc) as encoder [14], which is supposed to create hidden features and a projector headW xpro, which is a two layerMLP, with purpose tomap the feature space into a lower dimensional hidden space. The online network also has an additional predictor head, again consisting of a two layer MLP. The target network has a StopGrad function instead of a predictor head. Therefore during back propagation, only the weights of the online network are updated via gradient decent. The loss between the output of the online and target network is equal to the cosine\u2010similarity loss function.\nL(Z\u0302(O)1 , Z\u0302 (T ) 2 ) = \u2212 \u27e8Z \u20321, Z \u20322\u27e9 ||Z \u20321||2||Z \u20322||2\n(1)\nNote, that the final loss of one image is the symmetric lossL(Z\u0302(O)1 , Z\u0302 (T ) 2 )+L(Z\u0302 (O) 2 , Z\u0302 (T ) 1 ), since each augmentation is given to both networks. As mentioned, the target network is not updated with gradient descent, but with an exponential moving average (EMA). After each batch the target network will be set to Wa = Wa + (1 \u2212 \u03c4)(W \u2212 Wa). In SimSiam the target network is set directly to the online network after each update, i.e \u03c4 = 0.\n3.2 DirectPred [1] derives a one layer predictor head analytically with the analysis of the underlying learning dynamics of the models presented in Section 3.1 with an approximation of the actual network as a purely linear model. The learning dynamics of the networks are\nW\u0307p = \u03b1p(\u2212WpW (X +X \u2032) +WaX)W\u22a4 \u2212 \u03b7Wp (2) W\u0307 = WTp (\u2212WpW (X +X \u2032) +WaX)\u2212 \u03b7W (3) W\u0307a = \u03b2(\u2212Wa +W ) (4)\nWith X = E[x\u0302x\u0302\u22a4], where x\u0302 is the average augmented view of a datapoint and X \u2032 is the covariance matrix of the augmented views. \u03b1p and \u03b2 are multiplicative learning rate ratios, i.e \u03b1p = \u03b1pred \u03b1 and \u03b2 = 1\u2212\u03c4 \u03b1 (here \u03b1 and \u03b1pred are the learning rates forW andWp respetively). In addition to the linearity of the network, three simplifying assumptions where made:\n\u2022 The target network is always in a linear relationship with the online network (e.g. Wa(t) = \u03c4(t)W (t)\nReScience C 8.2 (#17) \u2013 H\u00f6ppe, Miszkurka and Wilkman 2022 4\n\u2022 The original data distribution p(X) is Isotropic and its augmentation p\u0302(X \u2032|X) has meanX and covariance \u03c3I\n\u2022 The predictorWp is symmetric\nBased on these assumptions, one can show, that the eigenspaces of the output of the online network and the predictor Wp align. Let F = WXW\u22a4 (i.e. the output of the on\u2010 line network when it is approximated as a linear model), then it follows with the three assumptions, that the eigenspaces of these two matrices align over time (e.g. for all non\u2010zero eigenvalues \u03bbWp , \u03bbF ofWp and F , the corresponding normalized eigenvectors vWp , vF are parallel, v\u22a4WpvF = 1). With this alignment one can derive decoupled dynam\u2010 ics for the eigenvalues of W and Wp. By analysing this system, it can be shown that it has, depending on the weight decay parameter, several fixpoints, from which some are stable and some not. The trivial solution (the collapse) is one of them and the basin of attraction of these fixpoints varies with the relative learning rate of the predictor \u03b1pred\u03b1 . With this analysis, [1] derives conditions under which the trivial fixpoint can be avoided. For a thorough mathematical analysis, we refer to [1]. In Section 5.1 we will present em\u2010 pirical evidence, that the symmetry assumption holds, and that the eignenspaces align. Furthermore, in Section 5.3 we will investigate the role of weight decay and the learning rate. From the decoupled dynamics of the eigenvalues, we can also derive an analytical ex\u2010 pression for the predictor Wp. Let F = U\u2126U\u22a4 be the eigen\u2010decomposition of F with \u2126 = diag(\u03bb(1)F , ..., \u03bb (d) F ) the diagonal matrix with the eigenvalues of F , then we can ap\u2010 proximate the eigenvalues ofWp with\n\u03bb (j) Wp = \u221a \u03bb (j) F + \u03f5maxj \u03bb (j) F (5)\nand therefore setWp to Wp = Udiag(\u03bb (1) Wp , ..., \u03bb (d) Wp )U\u22a4 (6)\nNote, that we cannot compute F directly, which is why we use a running average F\u0302 as approximation in practice\nF\u0302 = \u03c1F\u0302 + (1\u2212 \u03c1)Z\u0302 (7)\nwhere Z\u0302 = Z\u0302(O)1 Z\u0302 (O)\u22a4 2 . Wedenote thismethodDirectPred and in Section 5.2we show, thatDirectPred canperform similar to BYOL and SimSiam\n4 Data & Configurations\nWe ran our experiments on Google Cloud Platform using Virtual Machine with a V100 GPU. All experiments are conducted on CIFAR\u201010 [15], which contains 60 000 RGB images uni\u2010 formly distributed over 10 classes. The pre\u2010training and the linear evaluation are done on the entire training set, which consists of 50 000 images. For the linear evaluation, only a linear layer is used on top of the encoder, where the weights of the encoder are frozen (i.e. we test how linearly separable the encoders output is). The reported accu\u2010 racy results are produced from a test set containing 10 000 images. Also, to account for the small dimension of the CIFAR\u201010 images (32\u00d7 32\u00d7 3) we use 3\u00d7 3 convolutions and stride 1 without maximum pooling in the first block of the encoder. To augment each image, we first do a random flip, take a random crop (up to 8% of the original size) of the image. Then we randomly adjust brightness, saturation, contrast\nReScience C 8.2 (#17) \u2013 H\u00f6ppe, Miszkurka and Wilkman 2022 5\nand hue of the RGB image by a random factor 1. Finally with a 20% chance we convert the image to grey scale.\nSelf\u2010supervised pretraining In the basic setting, the online network use ResNet\u201018 as encoder, two layer projector MLP, two layer predictor MLP, where the first layer consists of 512 nodes, followed by BatchNorm and ReLU, and then a linear output layer with 128 nodes. For BYOL we use EMA to update target network and for SimSiam we directly set encoder and projector of target network to the weights of the online one (\u03c4 = 0). We use SGD optimizer with learning rate 0.03, momentum 0.9 and weight decay (L2 penalty) of 0.0004. The predictor of DirectPred is set directly and are not trained with gradient descent and consist of one linear layer with 128 nodes. By SGD baseline for those methods we mean a network pre\u2010trained with a one linear layer predictor with or without EMA. In all experiments, we use batch size of 128. For updating the target network we used the EMA parameter \u03c4 = 0.996. For DirectPred we use \u03f5 = 0.1 and \u03c1 = 0.3.\nLinear evaluation In order to test the performance of the different models, we use lin\u2010 ear evaluation, i.e. we train a linear layer on top of the ResNet\u201018 encoder with frozen weights for 100 epochs. This measures how linearly separable the learned representa\u2010 tions of the encoder are. We use Adam optimizer [16] with polynomial decay of learning rate from 5e\u20102 to 5e\u20104. Images are normalized but we do not use augmentation for this part of training just as in the original repository for DirectPred.\n5 Experiments and findings\nIn this section, we will first show that the assumptions and theoretical findings from Section 3.2 hold in practice. Finally, we will pre\u2010train and use linear evaluation on the different models presented in Section 3 in order to test their performances.\n5.1 Eigenspace alignment First, we pre\u2010train BYOL and SimSiam keep track of the predictor heads symmetry and eigenspace alignment. In Figure 2 we can see, that the assumption of an symmetric predictor Wp holds. Even without symmetry regularisation, Wp approaches symmetry during training. Also, we can see that for all non\u2010zero eigenvalues ofWp the eigenspaces between F andWp align as the training progresses. We ran the same Experiment for SimSiam, and can also see the same effect on the pre\u2010 dictor and the alignment (Figure 3). If we don\u2019t use a symmetric predictor, we also see that the eigenspaces for the non\u2010zero eigenvalues align. However, once we use symme\u2010 try regularisation on Wp, all eigenvalues become zero, which shows that the network collapses. We will see later in Section 5.3 that we can prevent this collapse by using different learning rates \u03b1, \u03b1pred and weight decay \u03b7, \u03b7pred forW andWp respectively.\n5.2 Performance Byol & SimSiam In table 1 we can see that the performance of BYOL increases slightly when using symmetry regularisation on the predictor. However, as already seen in Fig\u2010 ure 3, whenusing noEMA,we observe that the network collapses. We observe in general better performance formodels trainedwith EMA, given the same hyperarameters. How\u2010 ever, we did not use extensive hyperparameter tuning, as performance is not the focus of our work.\n1for brightness, saturation and contrast we chose a value uniformly at random between 0.6 and 1.4. For adjusting the hue, we set the maximal value to 0.1\nReScience C 8.2 (#17) \u2013 H\u00f6ppe, Miszkurka and Wilkman 2022 6\nDirectPred As we can see in Figure 2 & 3, the eigenspaces for both models align and therefore the theoretical assumptions of [1] hold. As we can see in Table 2, all mod\u2010 els perform reasonably well, and can achieve almost the same performance as BYOL or SimSiam. However, as already mentioned earlier, we can see that models with EMA out\u2010 perform models without EMA. I addition, we run an experiments where the predictor is only updated every 5th step according to Equation 6 and otherwise is updated with gradient decent, we call this method DirectPred5. We can see that the hybrid method DirectPred5 does not increase performance, however, according to [1] when training for 500 epochs,DirectPred5 can outperformDirectPred. Due to computational constraints we cannot reproduce this experiment.\n5.3 Influence of weight decay and learning rate As we can see in Figure 3, SimSiam with symmetric predictor does collapse. However, we can prevent this by adjusting the weight decay and learning rate. To make sure the network does converge to a stable non\u2010collapsing fix\u2010point, the weight decay of the pre\u2010\nReScience C 8.2 (#17) \u2013 H\u00f6ppe, Miszkurka and Wilkman 2022 7\ndictor should be set higher than the rest of the network (\u03b7pred > \u03b7, for mathematical analysis see [1]). By omitting weight decay, we are not able to stabilize the training of SimSiam with symmetric predictor and we can also see, that methods without weight decay performworse, than with weight decay (Table 3). Also, to decrease the basin of at\u2010 traction, of the trivial fixpoints, the learning rate of the predictor should be rather large compared to the learning rate of the rest of the network, i.e \u03b1pred\u03b1 >> 1 (see Section 3.2 in [1]).\n6 Challenges\nThe original paper describes the methods and mathematical derivations well. Authors also share which hyperparameters they used in most of the experiments. Since the au\u2010 thors provided the open\u2010source repository for the paper, we could check some of the details of the experiments there. However, as the code is not well\u2010structured it was at times challenging to analyse. Furthermore, not all of the experiments are shared in the repository, for example there is no codewhich produces eigenspace experiments results or config for weight decay experiment. The reproducedpaper didnot outlined self\u2010containeddescriptionon themethods it used as it built upon previous works. Thanks to the detailed description of BYOL by Grill et. al. [2] we were able to reproduce the paper achieving similar results as the authors. Due to time constraints we decided to use CIFAR\u201010 instead of STL\u201010 which was used in most of the experiments in the reproduced paper. However, claims tested by us in this work are not restricted to one dataset and we shown that they indeed hold in a different setting. One of the main challenges was the large amount of computations required for all the experiments, it took around 4 hours and 30 minutes to pre\u2010train and fine tune a single model, and in total we trained for around 100+ hours.\nReScience C 8.2 (#17) \u2013 H\u00f6ppe, Miszkurka and Wilkman 2022 8\nOur work is implemented in TensorFlow and one of the challenges was differences be\u2010 tween TensorFlow and PyTorch libraries. For instance, in PyTorch one of the parame\u2010 ters of the SGD optimizer is weight decay (L2 penalty), in TensorFlow we had to imple\u2010 ment it by hand as TensorTlow\u2019s SGDW implements only Decoupled Weight Decay Reg\u2010 ularization [17]. Furthermore, image augmentation methods such as ColorJitter from PyTorch do not have exact corresponding methods in Tensroflow. We used a custom way to do it so that augmentations are as close as possible to the original version.\n7 Conclusion\nIn this work we study and reimplement three architectures used to give insight into self\u2010 supervised representation learning without contrastive pairs namely BYOL, SimSiam, DirectPred and their ablations. Our experimental results aligned well with both the theo\u2010 retical analysis about the eigenspaces and the symmetric assumptions made in [1] and translate to other dataset than used in the paper. Lastly, we confirmed that SimSiam can be prevented from collapsing with the use of weight decay and adjusting a learning rate of predictor. Furthermore, we confirm the claim that DirectPred outperforms its one\u2010layer SGD alter\u2010 native. However, we cannot report that DirectPred could outperform Byol. This may be due to the fact that we used CIFAR\u201010 as opposed to STL\u201010 in the original paper. This leaves us with the conclusion, that DirectPred gives valuable insights into the dynamics of unsupervised representation learning without contrastive pairs, but do not necessar\u2010 ily build new state of the art models themselves.\n8 Ethical considerations\nSelf\u2010supervised learning circumvents label scarcity which is one of the most common problems when applyingML to new scenarios. This can have both positive and negative consequences. On one hand, it can accelerate important developments for example in medical diagnosis. However, it can also be used in unethical ways such as in surveil\u2010 lance or military equipment. Furthermore, there will be less need for people labelling datasets which will result in reduction of job positions in this area."}], "title": "[Re] Understanding Self-Supervised Learning Dynamics without Contrastive Pairs", "year": 2022}