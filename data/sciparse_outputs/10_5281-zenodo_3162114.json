{"abstractText": "Recurrent neural networks1 are widely used for processing sequences of information. Recurrent neural networks are very hard to optimize due to the exploding and vanishing gradient problem [2]. This problem becomes more evident in tasks where training data has dependencies that exist over long time scales. A number of architectures have been proposed to mitigate this problem. These architectures include Long Short Term Memory3, Gated Recurrent Unit4. These networks introduce a linear temporal path that allow gradients to flow freely across time-steps. The LSTM is the most widely used architecture. It has been shown that LSTMs have greater representational power as compared to the other architectures5. It is still hard to optimize LSTMs to learn long term dependencies. The authors say that when the weights of the LSTM are large, the gradients through the linear temporal path get suppressed. This path is important as it was introduced to carry information about long termdependencies. The authors prove this empirically. To prevent this, the authors provide a simple stochastic algorithm (h-detach). The authors experiment with this algorithm on tasks that require capturing long-term dependencies. Examples of such tasks include the copying task2,6 and the sequential-mnist task7. The authors claim that their approach results in faster convergence and stable training. In this work, we attempt to reproduce the results for the copying task and the sequential mnist task (permuted and non-permuted). We also conduct experiments to prove their claims mentioned in the ablation study.", "authors": [{"affiliations": [], "name": "Aniket Didolkar"}, {"affiliations": [], "name": "Koustuv Sinha"}], "id": "SP:9c3996953c4fc387e932e442633014de68597db8", "references": [{"authors": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "title": "Neurocomputing: Foundations of Research.", "venue": "Chap. Learning Representations by Backpropagating Errors,", "year": 1988}, {"authors": ["Y. Bengio", "P. Simard", "P. Frasconi"], "title": "Learning long-term dependencies with gradient descent is difficult.", "venue": "IEEE transactions on neural networks / a publication of the IEEENeural Networks", "year": 1994}, {"authors": ["S. Hochreiter", "J. Schmidhuber"], "title": "Long Short-Term Memory.", "venue": "Neural Comput", "year": 1997}, {"authors": ["J. Chung", "\u00c7. G\u00fcl\u00e7ehre", "K. Cho", "Y. Bengio"], "title": "Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling.", "year": 2014}, {"authors": ["G. Weiss", "Y. Goldberg", "E. Yahav"], "title": "On the Practical Computational Power of Finite Precision RNNs for Language Recognition.", "year": 2018}, {"authors": ["R. Pascanu", "T. Mikolov", "Y. Bengio"], "title": "Understanding the exploding gradient problem.", "year": 2012}, {"authors": ["Q.V. Le", "N. Jaitly", "G.E. Hinton"], "title": "A Simple Way to Initialize Recurrent Networks of Rectified Linear Units.", "year": 2015}, {"authors": ["B. Kanuparthi", "D. Arpit", "G. Kerg", "N.R. Ke", "I. Mitliagkas", "Y. Bengio"], "title": "h-detach: Modifying the LSTM Gradient Towards Better Optimization.", "venue": "In: International Conference on Learning Representations", "year": 2019}, {"authors": ["M. Arjovsky", "A. Shah", "Y. Bengio"], "title": "Unitary Evolution Recurrent Neural Networks.", "venue": "CoRR abs/1511.06464", "year": 2015}], "sections": [{"text": "R E S C I E N C E C Replication / Machine Learning\n[Re] h-detach: Modifying the LSTM gradient towards better optimization\nAniket Didolkar1, ID 1Manipal Academy of Higher Education, Manipal, India\nEdited by Koustuv Sinha ID\nReviewed by Anonymous reviewers\nReceived 04 May 2019\nPublished 22 May 2019\nDOI 10.5281/zenodo.3162114\n1 Introduction\nRecurrent neural networks1 are widely used for processing sequences of information. Recurrent neural networks are very hard to optimize due to the exploding and vanishing gradient problem [2]. This problem becomes more evident in tasks where training data has dependencies that exist over long time scales. A number of architectures have been proposed to mitigate this problem. These architectures include Long Short Term Memory3, Gated Recurrent Unit4. These networks introduce a linear temporal path that allow gradients to flow freely across time-steps. The LSTM is the most widely used architecture. It has been shown that LSTMs have greater representational power as compared to the other architectures5. It is still hard to optimize LSTMs to learn long term dependencies. The authors say that when the weights of the LSTM are large, the gradients through the linear temporal path get suppressed. This path is important as it was introduced to carry information about long termdependencies. The authors prove this empirically. To prevent this, the authors provide a simple stochastic algorithm (h-detach). The authors experiment with this algorithm on tasks that require capturing long-term dependencies. Examples of such tasks include the copying task2,6 and the sequential-mnist task7. The authors claim that their approach results in faster convergence and stable training. In this work, we attempt to reproduce the results for the copying task and the sequential mnist task (permuted and non-permuted). We also conduct experiments to prove their claims mentioned in the ablation study.\n2 Proposed Method:H-detach\nWe will first introduce the LSTM equations and then analyze the problems with backpropagation according to the authors.\n2.1 Long Short Term Memory Networks LSTMwas introduced to provide free-flowing paths for gradients to propagate. The equations of the LSTM can be defined as follows-\nct = ft \u2299 ct\u22121 + it \u2299 gt (1)\nCopyright \u00a9 2019 A. Didolkar, released under a Creative Commons Attribution 4.0 International license. Correspondence should be addressed to Aniket Didolkar () The authors have declared that no competing interests exists. Code is available at https://github.com/dido1998/h-detach/tree/v1.0 \u2013 DOI 10.5281/zenodo.2657361. Open peer review is available at https://github.com/reproducibility-challenge/iclr_2019/pull/148.\nReScience C 5.2 (#4) \u2013 Didolkar 2019 1\nht = ot \u2299 tanh(ct) (2)\nHere, \u2299 denotes the pointwise product and the gates ft, it, gt and ot are defined as\nft = \u03c3(Wfhht\u22121 +Wfxxt + bf ) (3)\nit = \u03c3(Wihht\u22121 +Wixxt + bi) (4)\ngt = tanh(Wghht\u22121 +Wgxxt + bg) (5)\not = \u03c3(Wohht\u22121 +Woxxt + bo) (6)\nHere, ct and ht are the cell state and hidden state respectively. The output of the LSTM at each time step is \u03d5(ht). The loss is calculated as lt = l(\u03d5(ht)). In an auto-regressive LSTM networks,the output of one time step is fed as input to the next time step. There is a linear recursive relation between the cell states ct. This path is the linear temporal path which allows the free flowing of gradients backwards. More information about LSTMs can be obtained from colahs\u0313 blog.\n2.2 Backpropagation Here we will present the authors analysis of the backpropagation equations in brief. Let w be a weight in one of the weight matrices (Wfh,Wfx,Wih,Wix,Wgh,Wgx,Woh or Wox). Let,\nzt = \u2223\u2223\u2223\u2223\u2223\u2223 dht/dw dct/dw\n1n \u2223\u2223\u2223\u2223\u2223\u2223 (7) Then, zt = (At + Bt)zt\u22121. The exact definitions of At and Bt can be obtained from [8]. The equation for zt can be expanded as\nzt = (At +Bt)(At\u22121 +Bt\u22121)...(A2 +B2)z1 (8)\nHere At carries the gradients that arise due to the linear temporal path. The matrix At is bounded given that the number of time steps is finite and the initialization of c0 is finite. On the other hand, the matrix Bt is obtained through the repeated multiplication of weights. If the weights are large, the matrix Bt can also become very large in magnitude. In this case, the magnitude of At will be very small as compared to Bt, hence the gradients through the linear temporal path (which carry information of long term dependency) will have no effect on the weights.\n2.3 Algorithm : h-detach The above problem can be mitigated by multiplying Bt by a positive number between 0 and 1. The authors provide a simple stochastic algorithm to achieve this which is presented as Algorithm 1.\n3 Implementation Details\nThe authors have made their code public1. The codebase used for the reproducibility experiments is also publicly available2.The implementation is in Pytorch3. The author\n1https://github.com/bhargav104/h-detach 2https://github.com/dido1998/h-detach 3https://pytorch.org/\nReScience C 5.2 (#4) \u2013 Didolkar 2019 2\nAlgorithm 1 h-detach algorithm 1: Input : p, h0, c0, [x0,x1,.....,xT ] 2: l\u2190 0 3: for 1 \u2264 t \u2264 T do 4: if bernoulli(p) == 1 then 5: h\u0303t\u22121 \u2190 stop_gradient(ht\u22121) 6: else 7: h\u0303t\u22121 \u2190 ht\u22121 8: ht, ct \u2190 LSTM(xt, h\u0303t\u22121, ct\u22121) 9: lt \u2190 L(\u03d5(ht)) 10: l\u2190 l + lt 11: return l\nprovides instructions to run the code to perform the experiments. We could get the code running immediately without any changes. The stop-gradient in algorithm 1 can be implemented using the detach()4 method in pytorch. The authors claim that LSTMs trained using h-detach converge in lesser number of epochs as compared to normal LSTMs. We have been able to confirm this claim in our experiments. Even though the convergence is faster in terms of the number of epochs, the implementation (provided by the authors) is slower as compared to the vanilla LSTM implementation provided by pytorch 5. This is because, in the implementation, each input is fed sequentiallywhichmakes it slower compared to feeding inputs at all time steps at once (as is usually done for efficiency). On communication with the authors, it was confirmed that this was done to ensure the correctness of the h-detach algorithm. The authors have themselves implemented the LSTM cell in pytorch. In our experiments, we observed that implementing the LSTM cell in CUDA code. The code can be found in the reproducibility repo6. Implementing the LSTM cell in CUDA code can result in upto 2x speed-up. This CUDA code can be added as an extension in pytorch and used.\n4 Experiments\n4.1 Copying Task This task requires the recurrent network to memorize the network inputs provided at the first few time steps and output them in the same order after a large time delay. Thus the only way to solve this task is for the network to capture the long term dependency between inputs and targets which requires gradient components carrying this information to flow through many time steps. The authors define 10 token {ai}i=9i=0. The input to the LSTM is a sequence of length T + 20 formed using one of the ten tokens at each time step. Input for the first 10 time steps are sampled i.i.d. (uniformly) from {ai}i=7i=0. The next T\u22121 entries are set to a8, which constitutes a delay. The next single entry is a9, which represents a delimiter, which should indicate to the algorithm that it is now required to reproduce the initial 10 input tokens as output. The remaining 10 input entries are set to a8. The target sequence consists of T +10 repeated entries of a8, followed by the first 10 entries of the input sequence in exactly the same order. This copying task is similar to the one described in [9]. We run each experiment for 200-300 epochs. Themain point the authors wanted to prove is that using h-detach results in more stable training and faster convergence. We observe that these properties can be observed in the first 200- 300 epochs. The authors have used the probability values of 0.25 and 0.50 for h-detach.\n4https://pytorch.org/docs/stable/autograd.html#torch.Tensor.detach 5https://pytorch.org/docs/stable/nn.html#torch.nn.LSTM 6https://github.com/dido1998/h-detach/tree/master/h-detach-cuda-extension\nReScience C 5.2 (#4) \u2013 Didolkar 2019 3\nThe original paper does not mention how they arrived at these values. However, on the openreview site7, the authors mention that they tried the probability values 0.1, 0.25, 0.4, 0.5, 0.75 and 0.9. They found that the values between 0.25 and 0.50 work the best.\nWe present our experiments in Figure 1. The top row presents results for the copying task with T=100 and with the probabilities for h-detach set as 0, 0.25 and 0.5. The bottom row presents results for the copying task with T=300 and the same probabilities. As the authors mention, we see the main advantage of using h-detach in the task with T=300. We see that, the models trained with h-detach result in higher accuracies and faster convergence. Here, we also see that p=0.50 works better than p=0.25. For the task with T=100, we observe that both methods achieve high accuracy of close to 100% but h-detach results in faster convergence. The hyperparameters are exactly the same as provided by the authors in the source code.\n4.2 Sequential MNIST This is the sequential version of the MNIST classification task. Each pixel is fed one at a time. The authors consider two versions of this task: one is which the pixels are fed in order (left to right and top to bottom) and the other is in which pixels are fed in random but fixed order. The second one is called permuted mnist. We tested with two different learning rates (0.0005 and 0.001). The results can be seen in Figure 2 The optimizer used was ADAM. The batch-size was set to 100. The number of examples in the training set was set to 50000, 10000 for validation and 10000 for testing. The number of epochs was set to 200. On the sequential MNIST task, vanilla LSTM and training with h-detach give an accuracy of 98.60%(authors report 98.60%) and 98.62%(authors report 98.80%) respectively. The numbers are not very different but we observed that training with h-detach gives faster convergence. For the permuted mnist (pmnist) task we trained with h-detach and obtained the maximum accuracy of 91.96%(authors report 92.92%). The validation accuracy curve for the permuted mnist task can be found in Figure 3\n7https://openreview.net/forum?id=ryf7ioRqFX\nReScience C 5.2 (#4) \u2013 Didolkar 2019 4\n4.3 Ablation Studies The authors first study the effect of removing gradient clipping from the sequential mnist task. This would be insightful as it would confirm that stochastically blocking gradients through the hidden state ht prevents increase in gradient magnitude. As the authors say, we observe that removing gradient clipping makes the training extremely unstable in the case of a vanilla LSTMas compared to LSTMwithh-detach.This is demonstrated in Figure 4. The authors also conduct experiments to prove that the cell state carries information about long termdependencies. This is doneby stochastically blocking gradients through the cell state instead of the hidden state. This new algorithm is tested on the copying task with T=100 and the sequential mnist task. The results are demonstrated in Figure 5. For the copying task, the accuracy does not cross 40% even after 200 epochs. For the sequential mnist task, the accuracy crosses 80% but the training is extremely unstable and the h-detach version completely out performs the c-detach version.\n5 Environment details\nThe experiments were conducted on a NVIDIA GTX 1060 and on google colab8 (tesla K80).\n8https://colab.research.google.com/\nReScience C 5.2 (#4) \u2013 Didolkar 2019 5\n6 Future work\nThough the authors have thoroughly explored the advantages of the h-detach algorithm through their experiments, they have not evaluated their proposed algorithmon stacked LSTM networks and bidirectional RNNs with LSTM cells. It would be interesting to see if we get similar performance improvements on these architectures as in the single layer uni-directional case.\n7 Conclusion\nIn this paper, we reproduce selected experiments of the original paper, [8]. Through communication with the authors, we understood that the main aim of this work was to prove that h-detach achieves better training stability, convergence speed, and it is robust to different learning rates and seeds. Though some of the numbers reported do not match exactly with the original paper, we have successfully proved that h-detach results in more stable and faster convergence."}], "title": "[Re] h-detach: Modifying the LSTM gradient towards better optimization", "year": 2019}