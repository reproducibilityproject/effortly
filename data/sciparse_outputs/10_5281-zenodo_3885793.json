{"abstractText": "Motivation \u2014 Thisworkwas published in 1996, butwith the advances in computing and information theory, and increased emphasis on the importance of context2, new studies3,4 have started building on its foundations. Furthermore, there aremanyways to write the activation functions and only a few simple formulations were analyzed. Therefore, the significance of this work is twofold: 1. To replicate the study in a modern programming language and make it publicly accessible. 2. Enable new activation functions to be readily tested.", "authors": [{"affiliations": [], "name": "Sepehr Mahmoudian"}, {"affiliations": [], "name": "Daniel Schmid"}, {"affiliations": [], "name": "Abdullah Makkeh"}], "id": "SP:64f80a75f33ea04f89bd7db29c4e8d00966efd08", "references": [{"authors": ["D. Smyth", "W.A. Phillips", "J. Kay"], "title": "Measures for investigating the contextual modulation of information transmission.", "year": 1996}, {"authors": ["M. Larkum"], "title": "A cellular mechanism for cortical associations: An organizing principle for the cerebral cortex.", "venue": "Trends in Neurosciences", "year": 2013}, {"authors": ["J. Kay", "W.A. Phillips"], "title": "Coherent Infomax as a Computational Goal for Neural Systems.", "venue": "Bulletin of Mathematical Biology", "year": 2011}, {"authors": ["M. Wibral", "V. Priesemann", "J. Kay", "J. Lizier", "W.A. Phillips"], "title": "Partial information decomposition as a unified approach to the specification of neural goal functions.", "venue": "Brain and Cognition", "year": 2017}], "sections": [{"text": "R E S C I E N C E C Replication / Computational Neuroscience\n[Re] Measures for investigating the contextual modulation of information transmission Sepehr Mahmoudian1,2, ID 1Department of Data-driven Analysis of Biological Networks, Campus Institute for Dynamics of Biological Networks, Georg August University G\u00f6ttingen, G\u00f6ttingen, Germany \u2013 2MEG Unit, Brain Imaging Center, Goethe University Frankfurt, Frankfurt, Germany\nEdited by Olivia Guest ID\nReviewed by Reubs J Walsh ID Daniel Schmid ID\nAbdullah Makkeh ID\nReceived 18 April 2020\nPublished 09 June 2020\nDOI 10.5281/zenodo.3885793\n1 Introduction\nBackground \u2014 Systems such as neurons can have different types of input. The authors1 propose dividing the input to a neuron into receptive (or driving) and contextual. They further present formulations of howcontextual connections canmodulate driving input. Finally, they use information theory to compare and contrast contextual modulation in these different formulations.\nMotivation \u2014 Thisworkwas published in 1996, butwith the advances in computing and information theory, and increased emphasis on the importance of context2, new studies3,4 have started building on its foundations. Furthermore, there aremanyways to write the activation functions and only a few simple formulations were analyzed. Therefore, the significance of this work is twofold: 1. To replicate the study in a modern programming language and make it publicly accessible. 2. Enable new activation functions to be readily tested.\nReplication \u2014 A successful attempt is made to qualitatively and quantitatively replicate the results of the original paper by replicating the three figures that comprise the results. While in all cases the results are qualitatively identical, there are minor quantitative differences in some figures which could in some cases be attributed to the use of a different seed value or the differences between how small or large values are handled in this python implementation and the original authors\u02bc implementation in an old programming environment about three decades ago.\n2 Methods\nInput \u2014 The study is limited to information transmission from a neuron which has only one receptive (R) or contextual (C) connection as input and produces one output (X). There is input at every time step, +1 denotes firing and -1 being silent, which is then multiplied by the weight of their connection. Probability distributions were created in the same way as original authors as described below, where for all figures P(C = 1 | R = 1) was set to 0.889972. This is so that the three components had all the same maximum possible value (0.5 bits). This makes it easier to compare the absolute values.\nCopyright \u00a9 2020 S. Mahmoudian, released under a Creative Commons Attribution 4.0 International license. Correspondence should be addressed to Sepehr Mahmoudian (sepehr.mahmoudian@gmail.com) The authors have declared that no competing interests exists. Code is available at https://github.com/sepehrmn/mahmoudian-2020-rescience. \u2013 SWH swh:1:dir:c707f7af830f2cf32cf42c9515cc95ffd5fc5e2c. Open peer review is available at https://github.com/ReScience/submissions/issues/29.\nReScience C 6.3 (#2) \u2013 Mahmoudian 2020 1\nP (R = +1, C = +1) = 0.5P (C = 1|R = 1) (1)\nP (R = +1, C = \u22121) = 0.5[1\u2212 P (C = 1|R = 1)] (2)\nP (R = \u22121, C = +1) = 0.5[1\u2212 P (C = 1|R = 1)] (3)\nP (R = \u22121, C = \u22121) = 0.5P (C = 1|R = 1). (4)\nActivation Functions \u2014 Three activation functions are studied by the authors. Based on how receptive and contextual information are formulated to interact, they are called Aa (additive), Am (modulatory), and Ab (both additive and modulatory).\nAa(r, c) = r + c (5)\nAm(r, c) = 0.5r[1 + e rc] (6)\nAb(r, c) = 0.5r[1 + e rc] + c (7)\nWhere r is an instance of R, and c an instance of C, multiplied by the weight of the connection. The output of each activation function is passed into the sigmoidal activation function to generate a probability of firing:\np(X = 1|r, c) = 1 1 + e\u2212A(r,c)\n(8)\nInformation Theory \u2014 They use the three-way mutual information as a measure of the information shared between the output and the inputs which is defined as follows:\nI(X;R;C) = \u2211 x,r,c p(x, r, c)log P (x|r)P (x|c) P (x)P (x|r, c)\n(9)\nI(X;R|C) is used by the authors as a measure of information that is shared between the R input and the X output but not the C input and is defined as follows:\nI(X;R|C) = \u2211 x,r,c P (x, r, c)log P (x|r, c) P (x|c)\n(10)\nI(X;C|R) is used by the authors as a measure of information that is shared between the C input and the X output but not the R input and is defined as follows:\nI(X;C|R) = \u2211 x,r,c P (x, r, c)log P (x|r, c) P (x|r)\n(11)\nDependencies \u2014 The replication was done using Ubuntu 19.10 with Intel\u00ae Core\u2122 i7-7500U CPUwithAnacondaPython-3.8.1, numpy-1.18.1, scipy-1.4.1, andmatplotlib-3.1.3. There are no external libraries required for calculating the information theoretic terms as everything is provided in the accompanying code.\nReScience C 6.3 (#2) \u2013 Mahmoudian 2020 2\n3 Results\nActivation functions and transmitted information \u2014 Figure 1 shows the three-way mutual information and conditional mutual information terms for each of the activation functions and across a range of R input weights while C remains constant at 1. These results are produced in a similar way to the paper by using the exact probability distributions. They are qualitatively identical to the original paper. There are minor quantitative differences. For example, in (c) the maximum value of the Ab function is higher in the original paper. In order to rule out the possibility of missing the highermaximum value as a result of not having that particular data point, a small step size of 0.1 was used for each data point which leads to a higher number of data points on this figure than the original publications\u0313.\nInformation surface plots for different input weights \u2014 Figure 2 shows the I(X;C|R) for each of the activation functions and across a range of r and c input weights. These results are produced in a similar way to the paper by using the exact probability distributions. They are qualitatively identical to the original paper. There are no notable quantitative differences.\nReScience C 6.3 (#2) \u2013 Mahmoudian 2020 3\nSampling biases and variances \u2014 Figure 3 compares the mean and standard deviation of three-way measures sampled against the analytical measures for the modulatory activation function. The mean and deviations were computed from 100 trials per sample size and compared with the analytical measure across a range of sample sizes. As the authors claim, sample size of 200 could be sufficient as the improvements beyond that are not much. The error for small sample sizes is higher in the original paper, this can be attributed to use of a different seed value and their programming environment.\nReScience C 6.3 (#2) \u2013 Mahmoudian 2020 4\n4 Discussion\nThe goal of the authors was to study contextual modulation in single binary neurons using information theory. To that end they present figures 1-2 as a way to compare and contrast functions with or without forms of contextual modulation. They use figure 3 to suggest how much data is required to get the required accuracy for the analyses presented in figures 1-2.\nOne example of how the analyses here can be used to study contextual modulation is included as follows: For the modulatory activation function, it is intended that the role of R is to drive the neuron to fire and C is to amplify the firing. Figure 2 (b) shows that increasing C alone does not increase I(X;C|R), but increasing R and C together does and a higher C with a constant R amplifies it. At low R values, C has a higher role in the output so I(X;C|R) begins to diminish at higher R values. This shows that the modulatory function is behaving as intended and expected.\nIn conclusion, this manuscript and the accompanying code present a successful replication of the authors\u02bc work in a modern programming language. Similarly, the code and information-theoretic analyses here could be used to study other activation functions and contextual modulation.\nAcknowledgments \u2014 I would like to thank Bill Phillips and Jim Kay for clarifying my questions about the input. This researchwas supported by the funding initiative Nieders\u00e4chsisches Vorab of the Volkswagen Foundation and the Ministry of Science and Culture of Lower Saxony."}], "title": "[Re] Measures for investigating the contextual modulation of information transmission", "year": 2020}