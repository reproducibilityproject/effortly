{"abstractText": "This work aims to reproduce Lang et al.\u2019s StylEx [1] which proposes a novel approach to explain how a classifier makes its decision. They claim that StylEx creates a post\u2010hoc counterfactual explanation whose principal attributes correspond to properties that are intuitive to humans. The paper boasts a large range of real\u2010world practicality. However, StylEx proves difficult to reproduce due to its time complexity and holes in the informa\u2010 tion provided. This paper tries to fill in these holes by: i) re\u2010implementation of StylEx in a different framework, ii) creating a low resource training benchmark.", "authors": [{"affiliations": [], "name": "Chase van de Geijn"}, {"affiliations": [], "name": "Victor Kyriacou"}, {"affiliations": [], "name": "Irene Papadopoulou"}, {"affiliations": [], "name": "Vasiliki Vasileiou"}, {"affiliations": [], "name": "Koustuv Sinha"}, {"affiliations": [], "name": "Sharath Chandra Raparthy"}], "id": "SP:72aa218729152b0f5ca997e3609eedc9b19fdbdb", "references": [{"authors": ["O. Lang"], "title": "Explaining in Style: Training aGAN to explain a classifier in StyleSpace.", "venue": "In:ArXiv", "year": 2021}, {"authors": ["T. Miller"], "title": "Explanation in artificial intelligence: Insights from the social sciences.", "venue": "Artificial Intelligence", "year": 2019}, {"authors": ["B. Kim", "M. Wattenberg", "J. Gilmer", "C. Cai", "J. Wexler", "F. Viegas"], "title": "Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav).", "venue": "In: International conference on machine learning. PMLR", "year": 2018}, {"authors": ["R.K. Mothilal", "A. Sharma", "C. Tan"], "title": "Explaining machine learning classifiers through diverse counterfactual explanations.", "venue": "Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency", "year": 2020}, {"authors": ["Y. Goyal", "Z. Wu", "J. Ernst", "D. Batra", "D. Parikh", "S. Lee"], "title": "Counterfactual visual explanations.", "venue": "In: International Conference on Machine Learning. PMLR", "year": 2019}, {"authors": ["Y. Goyal", "Z. Wu", "J. Ernst", "D. Batra", "D. Parikh", "S. Lee"], "title": "Counterfactual visual explanations.", "venue": "In: International Conference on Machine Learning. PMLR", "year": 2019}, {"authors": ["Z. Wu", "D. Lischinski", "E. Shechtman"], "title": "Stylespace analysis: Disentangled controls for stylegan image generation.", "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition", "year": 2021}, {"authors": ["T. Karras", "S. Laine", "M. Aittala", "J. Hellsten", "J. Lehtinen", "T. Aila"], "title": "Analyzing and Improving the Image Quality of StyleGAN.", "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition", "year": 2020}, {"authors": ["K. Schutte", "O. Moindrot", "P. H\u00e9rent", "J.-B. Schiratti", "S. J\u00e9gou"], "title": "Using StyleGAN for Visual Interpretability of Deep Learning Models on Medical Images.", "year": 2021}, {"authors": ["T. Karras", "S. Laine", "T. Aila"], "title": "A Style-Based Generator Architecture for Generative Adversarial Networks. 2019", "year": 2019}, {"authors": ["A.G. Howard", "M. Zhu", "B. Chen", "D. Kalenichenko", "W. Wang", "T. Weyand", "M. Andreetto", "H. Adam"], "title": "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications. 2017", "venue": "Geijn et al", "year": 2022}, {"authors": ["I. Gulrajani", "F. Ahmed", "M. Arjovsky", "V. Dumoulin", "A.C. Courville"], "title": "Improved Training of Wasserstein GANs.", "venue": "Advances in Neural Information Processing Systems", "year": 2017}], "sections": [{"text": "R E S C I E N C E C"}, {"heading": "Replication / ML Reproducibility Challenge 2021", "text": "[Re] Explaining in Style: Training a GAN to explain a classifier in StyleSpace\nChase van de Geijn1,2, ID , Victor Kyriacou1,2, ID , Irene Papadopoulou1,2, ID , and Vasiliki Vasileiou1,2, ID 1Universiteit van Amsterdam, Amsterdam, The Netherlands \u2013 2Equal contributions\nEdited by Koustuv Sinha,\nSharath Chandra Raparthy\nReceived 04 February 2022\nPublished 23 May 2022\nDOI 10.5281/zenodo.6574655"}, {"heading": "Reproducibility Summary", "text": ""}, {"heading": "Scope of Reproducibility", "text": "This work aims to reproduce Lang et al.\u2019s StylEx [1] which proposes a novel approach to explain how a classifier makes its decision. They claim that StylEx creates a post\u2010hoc counterfactual explanation whose principal attributes correspond to properties that are intuitive to humans. The paper boasts a large range of real\u2010world practicality. However, StylEx proves difficult to reproduce due to its time complexity and holes in the informa\u2010 tion provided. This paper tries to fill in these holes by: i) re\u2010implementation of StylEx in a different framework, ii) creating a low resource training benchmark."}, {"heading": "Methodology", "text": "We use their provided python notebook to confirm their AttFind algorithm. However, to test the authors\u2019 claims, we reverse engineer their architecture and completely re\u2010 implement their train algorithm. Due to the computational cost of training, we use their pre\u2010trainedweights to test our reconstruction. To expedite training, a smaller resolution dataset is used. The training took 9 hours for 50,000 iterations on a Google Colab Nvidia K80 GPU. The hyperparameters are listed in the proceedings."}, {"heading": "Results", "text": "We reproduce the StylEx model in a different framework and test the AttFind algorithm, verifying the original paper\u2019s results for the perceived age classifier. However, we could not reproduce the results for the other classifiers used, due to time limitations in training and the absence of their pre\u2010trained models. In addition, we verify the paper\u2019s claim of providing human\u2010interpretable explanations, by reproducing the two user studies outlined in the original paper."}, {"heading": "What was easy", "text": "The notebook supplied by the authors loads their pre\u2010trained models and reproduces part of the results in the paper. Furthermore, their algorithm for discovering classifier\u2010\nCopyright \u00a9 2022 C.V.D. Geijn et al., released under a Creative Commons Attribution 4.0 International license. Correspondence should be addressed to Chase van de Geijn (chase.vandegeijn@student.auc.nl) The authors have declared that no competing interests exist. Code is available at https://github.com/irenepap2/Re_StylEx.git \u2013 DOI 10.5281/zenodo.6508290. \u2013 SWH swh:1:dir:f0871f3a14e536717d3225180942c4a385ce39e3. Open peer review is available at https://openreview.net/forum?id=SK8gAhfX2AK.\nNone 8.2 (#15) \u2013 Geijn et al. 2022 1\nrelated attributes, AttFind, is well outlined in their paper making the notebook easy to follow. Lastly, the authors were responsive to our inquiries."}, {"heading": "What was difficult", "text": "A major difficulty was that the authors provide only a single pre\u2010trained model, which makes most of the main claims require training code to verify. Moreover, the paper leaves out information about their design choices and experimental setup. In addition, the authors do not provide an implementation of the models\u2019 architecture or training. Finally, the practical audience is limited by the resource requirements.\nCommunication with original authors We had modest communication with the original author, Oran Lang. Our discussion was limited to inquiries about design choices not mentioned in the paper. They were able to clarify the encoder architecture and some of their experimental setup. However, their training code could not be made available due to internal dependencies.\nNone 8.2 (#15) \u2013 Geijn et al. 2022 2\n1 Introduction\nAs the field of machine learning (ML) develops and its algorithms become more preva\u2010 lent in society, concerns on the explainability of black\u2010box models become pivotal. For problems that have a high societal impact, there is understandable apprehension to\u2010 wards trustingmodels that do not provide justification. For applications such asmedical imaging and autonomous driving, there is a need for some level of human supervision. Even if a model has high performance, such as neural networks, without the ability for human interpretation, its use will be limited. In order to gain trust in systems powered by ML models, the models need to be inter\u2010 pretable and explainable. The two concepts are regularly used interchangeably, yet have subtle differences. Interpretability is the degree to which humans can understand the cause of a decision [2]. Deep neural networks, such as classifiers are often perceived as \u201cblack boxes\u201d whose decisions are opaque and hard for humans to understand. Ex\u2010 plaining the decision of classifiers can reveal model biases[3] and also provide support to downstream human decision\u2010makers. On the other hand, explainability is linked to the internal logic of a model. It focuses on explaining the data representation within that network. Explainability implies interpretability, however, the implication is not bidirectional. In recent years, there has been increasing attention to the field of explainability of deep network classifiers. Among the various ways of explanations, counterfactual explana\u2010 tions are gaining increasing attention [4, 5, 6]. To discover and visualize, the attributes used to generate counterfactual explanations, a natural candidate is generative models. In [7] they observed that StyleGAN2 [8], tends to contain a disentangled latent space (i.e., the \u201cStyleSpace\u201d) which can be used to extract individual attributes. The authors based their proposed methodology [1] on this observation. Though [9] propose a similar archi\u2010 tecture, Lang et al. assert that by integrating the classifier into the training of StylEx they can obtain principal attributes that are specific for the classification task. Additionally, they suggest that StylEx can be applied to a large variety of complex, real\u2010world tasks, which makes its replicability especially intriguing. Our work aims to reproduce the claims made by Lang et al. and confirm their results. Their paper reports in detail many experiments to justify their claims, but does not dive into their experimental setups for architecture and training. Since not all the informa\u2010 tion needed is available without contacting the authors, we argue that this paper cannot be considered fully reproducible. To remedy the holes in reproducibility and aid future work that builds on or applies StylEx, we build their proposed architecture and training algorithm, after correspon\u2010 dence with the authors.\n2 Scope of reproducibility\nTo determine the scope of reproduction, we quote Lang et al.\u2019s main claims:\nClaim 1 [They] propose the StylEx model for classifier\u2010based training of a StyleGAN2, thus driving its StyleSpace to capture classifier\u2010specific attributes\nClaim 2 A method to discover classifier\u2010related attributes in StyleSpace coordinates, and use these for counterfactual explanations.\nClaim 3 StylEx is applicable for explaining a large variety of classifiers and real\u2010world com\u2010 plex domains. [They] show it provides explanations understood by human users.\nTo reproduce Claim 2, a trained model and the AttFind algorithm are sufficient; both of which are contained in the authors\u2019 notebook. Claim 1 requires a network trained con\u2010 ditioned on a classifier and a network trained without, while Claim 3 requires multiple\nNone 8.2 (#15) \u2013 Geijn et al. 2022 3\nnetworks trained on multiple domains. However, to train these models, the architec\u2010 ture and training code is necessary; which, as stated previously, are not open source or thoroughly documented. In addition, the computational cost to train the models is expensive. Thus, to verify these claims our goals will be to:\n\u2022 Reconstruct their architecture and port the pre\u2010trained weights in PyTorch\n\u2022 Evaluate whether the principal attributes we obtain correspond to the same fea\u2010 tures using their pre\u2010trained weights\n\u2022 Retrain on datasets of smaller images and analyze the scalability of their method using fewer training steps and smaller architecture\n\u2022 Conduct two user studies on visual coherence and distinctness to prove that at\u2010 tributes extracted are interpretable by humans\nTo ease reproduction for future work, we built the StylEx architecture into a different framework, to get a deeper understanding of the model, and become more equipped to tackle training. As an addition, this contribution allows StylEx to bemore accessible for classifiers trained in PyTorch.\n3 Background\nThere have been many attempts to extract explanations from classifiers most of which utilize heatmaps of important features. However, heatmaps struggle to visualize fea\u2010 tures that are not spatially localized such as color or shape. Rather than identifying ar\u2010 eas of interest, one can provide an explanation through a \u201dwhat\u2010if\u201d example where the features are slightly altered. These forms of justification have been found to be more in\u2010 terpretable for non\u2010localized features, and are known as counterfactual examples. How\u2010 ever, it often requires domain knowledge and handcrafting examples to be appropriate. Lang et al. automate this and utilize machine learning to generate realistic counterfac\u2010 tual examples. This section will outline how they claim to achieve this with their two major contributions, StylEx and AttFind.\n3.1 StylEx ThewayLang et al. generate examples is through aneural generativemodel they dubbed StylEx. StylEx expands on thepopular generative adversarial network StyleGANv2, which generates realistic images by creating competition between two networks. One of these two networks, referred to as the Generator,G, attempts to generate a realis\u2010 tic image. To this end, the generator samples from a latent space, z \u2208 Rn, with a simple probability distribution such as zi \u223c N (0, 1). The sampled vector is pushed through a series of linear layers called mapping network to create a new latent vector, w, with a more complex probability distribution. This vector is used as input to a number of StyleBlocks based on the logarithmic resolution of the image. StyleBlocks consist of an affine transform and an upsampling layer. The affine transform, Ar, maps w to yet another vector sr, where r denotes the block number or resolution of the block. This concate\u2010 nation of all sr is known as the style, or attribute, vector, and the space that it spans is known as the StyleSpace. The attribute space is emphasized due to recent observations that it is less entangled than the latent space. The second network is the discriminator, D. This network is trained to differentiate between fake and real images. This forces the generator to slowly improve its creation of fake images. In this way, the discriminator can be seen as an adaptive loss function. The flawwith the direct application of StyleGAN is that it generates froma random latent space. To explain a classification, we would like to condition it on a particular image of\nNone 8.2 (#15) \u2013 Geijn et al. 2022 4\ninterest, but StyleGAN has no mechanism for extracting the attributes of an image. To fix this, Lang et al. added a third, encoding network to StylEx, E. Rather than using a randomly sampled z and themapping network to obtainw, StylEx uses the output of the encoder, z = E(x), where x is an input image. StylEx adds an extra loss condition that the reconstructed image, x\u2032 = G(E(x)), should be approximately x. Thus, the encoder combined with the affine transformations allows us to extract the attributes of an input image. StylEx is not unique in adding an encoder to the StyleGAN to explain a classifier. How\u2010 ever, other methods do not include the classifier in the training of the network. Style\u2010 GAN incorporates the classifier into training by appending its output to the encoded z vector. This results in another loss condition C(x) \u2248 C(x\u2032).\n3.2 AttFind Once the attributes of an image have been extracted, a counterfactual explanation can be achieved from the attributes with the most affect on a classifier\u2019s decision. Lang et al. propose attribute find (AttFind) to discover the most influential attributes. The al\u2010 gorithm adjusts all the attributes one at a time by a fixed amount d and observes their effect on the classification\u2206cs. The k attributes with the highest\u2206c create a local expla\u2010 nation for an image\u2019s classification. To approximate a global explanation, the principal attributes are determined by the mean\u2206c across images in a set.\n4 Reproduction approach\nReimplementing StylEx has been split into twomain tasks to ease resource requirements. The first task consists of rebuilding StylEx in a different framework; the second is train\u2010 ing the model from scratch. In this section, we discuss how we rebuilt the model archi\u2010 tecture and training process. Additionally, we include details obtained through corre\u2010 spondence missing from the original paper.\n4.1 Model descriptions To test Claim 1 and Claim 3, at least two models are necessary. Because only one pre\u2010 trained model is available, a new model needs to be trained. However, this is compu\u2010 tationally expensive as it builds on StyleGAN 1. This led us to evaluate reproducibility in two ways. Firstly, we recreate their architecture in PyTorch, using their pre\u2010trained weights to bypass the training limitation. Secondly, we attempt to train a model from scratch using less complex datasets with smaller resolutions to verify claims requiring multiple models. In the following sections, we explain how we reconstruct the StylEx architecture and training process.\nRebuilding StylEx \u2014 The author\u2019s notebook includes a TensorFlow StylEx pre\u2010trained on the FFHQ[10] dataset to find the attributes most influential in age classification. Taking advantage of the pre\u2010trained model\u2019s raw parameters, we reverse engineer the architecture of each component of StylEx and implement it in PyTorch. Subsequently, the pre\u2010trained weights are transferred into the reconstructed StylEx to confirm the cor\u2010 rect implementation of the structure. Transferring the pre\u2010trained parameters from a TensorFlow model to a PyTorch model turned out to be challenging and non\u2010trivial. We start by building the architecture of the MobileNetV1 [11] classifier, as described in the summary of their model, in both TensorFlow and PyTorch. We follow this ap\u2010 proach so that we can compare how the results of each layer differ, depending on the framework. We notice that for the 2D convolutional layers PyTorch and TensorFlow pad\n1StyleGAN can take on the order of 40 days on one GPU for high resolutions [10]\nNone 8.2 (#15) \u2013 Geijn et al. 2022 5\nthe images differently, leading to different results. To address this, we add a Constant\u2010 Pad2D layer in our PyTorch architecture before each convolution with a stride of 2. In addition, we change the default hyperparameters of PyTorch\u2019s BatchNorm2D to match the corresponding TensorFlow defaults. The next step is to follow the same procedure for the encoder and the StyleGAN com\u2010 ponents. We use the official StyleGAN2 implementation in PyTorch by NVlabs[8] and modify the initial architecture to align with the StylEx model. In particular, instead of only using the encoding of an image X as input to the generator, we also concatenate the classifier\u2019s output logits. Additionally, their generator returns the StyleSpace which contains classifier\u2010specific attributes. For the encoder, we use the same architecture as StyleGAN2\u2019s discriminator. Finally, we transfer the pre\u2010trained weights, to our compo\u2010 nents. The last step is to load the rebuilt StylExmodel in the provided notebook to confirm that the conversion of the models is successful and reproduce the results provided in the notebook.\nTraining the model \u2014 Lang et al. asserted that StylEx works for a wide range of classifiers and datasets. The results they show in their paper are all with high\u2010resolution images. The high resolution comes with a high computational cost as StylEx is built on top of a StyleGAN. High\u2010resolution StyleGANs can take over amonth to train on a single GPU sys\u2010 tem. To tackle this, we train our model on a low\u2010resolution MNIST dataset. In this way, we investigate whether their model works well on low\u2010resolution datasets and relieve computational requirements. The training is as outlined in their paper. The loss function for the StylEx model is bro\u2010 ken into seven parts: Lx, Lw, LLPIPS, Ladv, LPLR, LKL, and the LGP . Lx is the L1 loss between the real image, x, and the reconstruction of that image, G(E(x)). LLPIPS is the Learned Perceptual Image Patch Similarity (LPIPS) of the two images. This loss is a metric other than raw pixel value error for the similarity between two images. Lw is the L1 loss between the encoding of the original image, w = E(x), and the encoding of the reconstructed image w\u2032 = E(G(E(x))). Collectively, these three losses make up the reconstruction loss, Lrec, ie,\nLrec = Lw + Lx + LLPIPS .\nIn the implementation, each loss term in Lrec had a weighting coefficient to even out the magnitude of their contributions. The weights are detailed further in Section 5.2. LKL is the KL divergence loss between the classification probabilities of the original image and its reconstructed classification probabilities. LGP and LPLR are the gradient penalty and path length regularization losses described in the WGAN\u2010GP[12] and Style\u2010 GAN2 paper[8] respectively. Ladv is the Wasserstein adversarial generator loss of x\u2032. Fi\u2010 nally, the discriminator\u2019s loss is the Wasserstein adversarial discriminator loss.\n5 Experimental setup\n5.1 Datasets The pre\u2010trainedmodels the authors offer are trained on the Flickr\u2010Faces\u2010HQDataset [10] 2. The dataset contains 70,000 high\u2010quality PNG images at 1024\u00d71024 resolution with large variations in terms of age, ethnicity, and image background. They use it to find the top attributes which contribute to perceiving a person\u2019s age (young or old) or gender (male or female). They also preprocess the images by lowering the resolution to 256x256. The official dataset is unlabeled. It is not clearwhether the authors\u2019 dataset is an internal, labeled Google version or an unofficially labeled dataset.\n2https://github.com/NVlabs/ffhq\u2010dataset\nNone 8.2 (#15) \u2013 Geijn et al. 2022 6\nFor training, the MNIST [13] dataset is used due to its simplicity. Only the examples with labels 8 or 9 are kept and the resolution is increased to 32x32. MNIST was chosen because images compressed to 16x16 or even 8x8 tend to be recognizable for humans. Unfortunately, LPIPS relies on neural networks that have a fixed number of pooling lay\u2010 ers. Without editing reimplementation of LPIPS, the lowest resolution possible is 32.\n5.2 Hyperparameters A complete list of hyperparameters can be found in Table 2 (see Appendix 10). A hyper\u2010 parameter search was not performed for two reasons. First, the training time is long \u2013 even for very low resolutions, this is constraining. Second, the criteria for evaluating success is based on a human user, making automated hyperparameter tuning unintu\u2010 itive.\n5.3 Computational requirements Most of our experiments were conducted on Google Colab along with our systems. For training our models we use Colab\u2019s NVIDIA Tesla K80 GPU. Our code is provided in the following GitHub repository: MLRC_2021_FALL\u2010E358. Thebasic architecture of the StyleGAN2was adapted fromNVlabs\u2019 GitHub repository. As previouslymentioned, wemodify the basic architecture, to align with StylEx\u2019s generator and load Lang et al.\u2019s pre\u2010trained weights. The training code was adapted from labml.ai Annotated Paper Implementations\u2019 StyleGAN implementation. Training the model on MNIST for 50,000 iterations takes on the order of nine hours to train on Colab. The time required for AttFind is dependent on the resolution, latent dimension, and the number of images in the dataset. Finding the attribute of a single image took approximately oneminute for an imagewith resolution 32 and a latent space of 514.\n6 Results\n6.1 Rebuilding StylEx results To support Claim 1, we recreate their pre\u2010trained models to PyTorch and test if our re\u2010 sults agree. In Figure 3 (seeAppendix 8), we compare the results fromour PyTorch StylEx to their TensorFlow implementation. There are minor differences in the probabilities from the PyTorch classifier which are likely caused by differences in default values or module implementations in the two frameworks.\n6.2 AttFind results We are now equipped to test our PyTorchmodels on the AttFindmethod and inspect the principal attributes of the age classifier; meaning the attributes with the highest contri\u2010 bution to young or old classification. To this end, we compute the AttFind algorithm \u2013 with our classifier and generator as inputs \u2013 using the 250 latent variables of the FFHQ dataset. As can be seen in Figures 1 and 5 (see Appendix 9), our model obtains the same attributes as in the original paper. In addition, we implement the Independent selection strategy, to generate image\u2010specific explanations as described in the original paper. This method is a local explanation that returns the top\u2010k attributes affecting a classifier\u2019s decision for a single image rather than the entire dataset. The results are shown in Figure 2. These results support the author\u2019s Claim 2, that AttFind discovers significant attributes for a classifier\u2019s decision. Notably, in 1c the reported probability of the top left image is\nNone 8.2 (#15) \u2013 Geijn et al. 2022 7\n17% in the paper, while the probability we find with our and their notebook classifier is 39%.\nTheirs Ours\nPerceived Gender 0.96(\u00b10.047) 0.94(\u00b10.031) Perceived Age 0.983(\u00b10.037) 0.978(\u00b10.025)\nTable 1 shows that the results we obtain are within a standard deviation of their results; verifying their contribution that StylEx provides attributes that are easily distinguish\u2010 able by humans. Table 3 depicts the three most common words used, to describe the most prominent attribute that changes in the images (see Appendix 12). By inspecting the results, we draw two main conclusions. First, for all coordinates except skin color (i.e. 5th row in Face(age/gender) classifiers), the majority of the users use the same word in their descriptions. Second, the most common word used is different per attribute, proving that each attribute is unique. Our results agree with the results provided in the original paper.\n6.4 Reconstruction Generalization To further investigate the proposed model, we create new latent variables using images from theFFHQdataset on our architectureswith their pre\u2010trainedweights. Then, weuse the obtained latent variables to reconstruct the images using our pre\u2010trained generator. Finally, we follow the same process using their architecture and compare the resulting images. Our StylEx reconstructs a clearer image, compared to theirmodelwhich ismore blurred. This may occur because of some differences in the formatting between the frameworks.\n6.5 Training The training proved quite volatile. The Lrec would get stuck in local minima during training. Examples of the images reconstructed by the fully trainedmodel (seeAppendix 11). Lang et al. experimented with two training regimens. The first regimen was trained us\u2010 ing onlyE(x) as w, the inputs to the generator, and the above loss. The second regimen alternated between usingE(x) and a randomly generated encoding, w\u0304. This w\u0304 is created by applying a mapping network to z, where z \u223c N (0n, 1n) and n is the dimensionality of w. For this randomly generated x\u0304\u2032 = G(w\u0304), only the adversarial loss is calculated. Training using w\u0304 can be viewed as the same as training a vanilla StyleGAN. Because we are unsure which method was used for the results in their paper and notebook, we experimented with both. However, the first regimen was the only one that converged. Though we were able to train a model, due to time constraints, we were unable to fully investigate Claim 1. Again due to time constraints, we were unable to run AttFind on the trained model to fully test Claim 3.\n7 Discussion\nUsing the definition of reproducibility3 by the U.S. National Science Foundation (NSF) subcommittee on replicability in science, it is difficult to determine Lang et al.\u2019s repro\u2010 ducibility. All details regarding the experimental setup, such as the hyperparameters, the hours of training, the number of steps, the labels of the datasets, etc. are omitted, thus recreating the exactmaterials of the original investigators is difficult. Since our def\u2010 inition is an implication and we cannot satisfy the first condition, we cannot determine the reproducibility. Instead, wewill use a looser definition of reproducibility. Wewill refer to reproducibility as the ability for another researcher to test their claims. We found that, given enough time, the StylEx is seemingly reproducible. However, given a limited time budget such\n3\u201creproducibility refers to the ability of a researcher to duplicate the results of a prior study using the same materials as were used by the original investigator\u201d\nNone 8.2 (#15) \u2013 Geijn et al. 2022 9\nas our own, the paper is not fully reproducible. We, therefore, can only provide unit tests of their claims. The following sections will discuss information from the results section 6 and to what degree they confirm reproducibility claim by claim.\n7.1 Claim 1 The most difficult claim to investigate, given a limited time budget, is the effect of classifier\u2010based training on the StyleSpace. The original paper trains three models, the StylEx with and without integration of the classifier in training and the StyleGAN v2. We found, once the training algorithm is implemented correctly, just training all threemod\u2010 els will take at least 24 hours for 50,000 epochs on one GPU even for the simple MNIST dataset. The authors stated that it took approximately a week to train StylEx with 8GPUs. Over two weeks of training time is beyond our time constraints. In addition, we observed that training is volatile.4 The reconstruction error stagnates in a local minimum before suddenly dipping. However, the model was not always able to escape the local minima within 50,000 iterations. This suggests that, though their results are likely replicable, their replicability may be stochastic. This again hinders reproducibility when time is limited.\n7.2 Claim 2 The claim that the authors document the most was Claim 2, their AttFind method. Be\u2010 cause the method was implemented in the notebook provided, testing reproducibility was easy. We were able to verify that for the perceived age classifier, our model obtains the same top attributes. Weconclude that theirmethod candiscover themost influential classifier\u2010 related attributes. In addition to their notebook, we modified the AttFind method to find the principal attributes of a single image as shown in Figure 2. This validated the sub\u2010claim of AttFind that StylEx can provide image\u2010specific explanations. Rather than finding the globally important attributes, the model can find the locally important attributes for a particular image.\n7.3 Claim 3 The authors claim that StylEx is applicable to a variety of real\u2010world problems. Applica\u2010 bility can be interpreted in two different ways. One can interpret it as being possible to apply StylEx to a variety of domains, or as practical to apply StylEx to a variety of domains. From what we have seen in Figures 1, 2, it is possible to use StylEx for explaining an age classifier, thus it can explain a real\u2010world problem. From Figure 6 (see Appendix 11), we found that the StylEx can be trained to, at minimum, reconstruct MNIST data, thus multiple domains. Though we have found that it is possible, we have also found that it is seemingly imprac\u2010 tical. Every domain requires the model to be retrained, meaning every domain requires days or weeks of training.\n7.4 What was easy The open\u2010source notebook is very well structured, which combined with the pseudo\u2010 code outlined in Algorithm 1 of their paper, made the AttFind method easy to replicate. In addition, the provided pre\u2010trained models helped to derive some of the vague com\u2010 ponents of StylEx model.\n4An example of successful training can be found here and one where the model failed to converge here\nNone 8.2 (#15) \u2013 Geijn et al. 2022 10\n7.5 What was difficult As we already emphasized, there are many difficulties in reproducing this paper. StylEx is built on top of several previous papers making the knowledge needed for implemen\u2010 tation substantial. Lang et al. proposed a model without providing code, that is compu\u2010 tationally expensive, and with volatile training behavior. In addition, that is sensitive to hyperparameters, which in our case were unknown. Even when scaling down the com\u2010 plexity of the model using smaller resolutions, the time cost of training exceeds what was feasible with our time constraints. Taking shortcuts to subvert these difficulties had a multitude of challenges. We found loading weights from TensorFlow to PyTorch deceptively complex and far from trivial due to differences between the frameworks. Even evaluating their notebook came with difficulties as the dataset they trained on FFHQ does not officially have labels, so the details of their dataset were unknown.\n7.6 Future Work Theprimary goal of this paperwas to reproduce thework of Lang et al., however, through reimplementing their code, we found two open avenues for future research. Firstly, the paper focused on general image explanations but did not show examples of misclassi\u2010 fied data. It would be interesting to see what insights can be obtained through StylEx. Secondly, the paper compared StylEx only with StyleGAN v2 models. AttFind seems applicable to general autoencoders, and not specific to GANs. Viewing StylEx as an au\u2010 toencoder, rather than a GAN seems like a promising angle for scalability to a similar counterfactual generator."}, {"heading": "12. I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. C. Courville. \u201cImproved Training of Wasserstein GANs.\u201d", "text": "In: Advances in Neural Information Processing Systems. Ed. by I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett. Vol. 30. Curran Associates, Inc., 2017. URL: https://proceedings. neurips.cc/paper/2017/file/892c3b1c6dccd52936e27cbd0ff683d6-Paper.pdf. 13. L. Deng. \u201cThe mnist database of handwritten digit images for machine learning research.\u201d In: IEEE Signal Processing Magazine 29.6 (2012), pp. 141\u2013142.\nNone 8.2 (#15) \u2013 Geijn et al. 2022 12\n8 Our StylEx vs Lang et al.\u2019s\n9 AttFind Lang et al.\u2019s top attributes\nNone 8.2 (#15) \u2013 Geijn et al. 2022 13\n10 Hyperparameters\n11 MNIST Reconstruction\nNone 8.2 (#15) \u2013 Geijn et al. 2022 14\n12 Verbal Description Study\nNone 8.2 (#15) \u2013 Geijn et al. 2022 15"}], "title": "[Re] Explaining in Style: Training a GAN to explain a classifier in StyleSpace", "year": 2022}