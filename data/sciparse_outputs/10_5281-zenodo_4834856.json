{"abstractText": "We reproduce the results of the paper \u201dOn Warm-Starting Neural Network Training.\u201d In many real-world applications, the training data is not readily available and is accumulated over time. As training models from scratch is a time-consuming task, it is preferred to use warm-starting, i.e., using the already existing models as the starting point to obtain faster convergence. This paper investigates the effect of warm-starting on the final model s\u0313 performance. It identifies a noticeable gap between warm-started and randomly-initialized models, hereafter referenced as the warm-starting gap. Furthermore, they provide a solution to mitigate this side-effect. In addition to reproducing the original paper s\u0313 results, we propose an alternative solution and assess its effectiveness.", "authors": [{"affiliations": [], "name": "Klim Kireev"}, {"affiliations": [], "name": "Amirkeivan Mohtashami"}, {"affiliations": [], "name": "Ehsan Pajouheshgar"}, {"affiliations": [], "name": "Koustuv Sinha"}, {"affiliations": [], "name": "Jesse Dodge"}], "id": "SP:46f7ed0977ea5becbc43c02baf35d685bc46d881", "references": [{"authors": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "title": "Deep Residual Learning for Image Recognition.", "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Las Vegas, NV, USA: IEEE,", "year": 2016}, {"authors": ["G.E. Hinton"], "title": "Learning multiple layers of representation.", "venue": "en. In: Trends in Cognitive Sciences", "year": 2007}, {"authors": ["Y. Netzer", "T. Wang", "A. Coates", "A. Bissacco", "B. Wu", "A.Y. Ng"], "title": "Reading Digits in Natural Images with Unsupervised Feature Learning.", "venue": "NIPS Workshop on Deep Learning and Unsupervised Feature Learning", "year": 2011}, {"authors": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "title": "Learning representations by back-propagating errors.", "venue": "en. In: Nature 323.6088 (Oct", "year": 1986}, {"authors": ["D.P. Kingma", "J. Ba"], "title": "Adam: A Method for Stochastic Optimization.", "venue": "International Conference on Learning Representations,", "year": 2015}, {"authors": ["C. Shorten", "T.M. Khoshgoftaar"], "title": "A survey on image data augmentation for deep learning.", "venue": "Journal of Big Data", "year": 2019}, {"authors": ["M. Oquab", "L. Bottou", "I. Laptev", "J. Sivic"], "title": "Learning and Transferring Mid-level Image Representations Using Convolutional Neural Networks.", "venue": "IEEE Conference on Computer Vision and Pattern Recognition", "year": 2014}, {"authors": ["J. Huang", "J. Li", "D. Yu", "L. Deng", "Y. Gong"], "title": "Cross-language knowledge transfer using multilingual deep neural network with shared hidden layers.", "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing. ISSN: 2379-190X", "year": 2013}, {"authors": ["M. Long", "H. Zhu", "J. Wang", "M.I. Jordan"], "title": "Unsupervised domain adaptation with residual transfer networks.", "venue": "Proceedings of the 30th International Conference on Neural Information Processing Systems. NIPS\u201916", "year": 2016}, {"authors": ["S. Ioffe", "C. Szegedy"], "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift.", "venue": "en. In: International Conference on Machine Learning. PMLR,", "year": 2015}, {"authors": ["J. Ash", "R.P. Adams"], "title": "On Warm-Starting Neural Network Training.", "venue": "Advances in Neural Information Processing Systems", "year": 2020}], "sections": [{"text": "R E S C I E N C E C Replication / ML Reproducibility Challenge 2020\n[Re] Warm-Starting Neural Network Training\nKlim Kireev1, ID , Amirkeivan Mohtashami1, ID , and Ehsan Pajouheshgar1, ID 1Ecole Polytechnique F\u00e9d\u00e9rale de Lausanne, Lausanne, Switzerland\nEdited by Koustuv Sinha, Jesse Dodge\nReviewed by Anonymous Reviewers\nReceived 29 January 2021\nPublished 27 May 2021\nDOI 10.5281/zenodo.4834856"}, {"heading": "Reproducibility Summary", "text": ""}, {"heading": "Scope of Reproducibility", "text": "We reproduce the results of the paper \u201dOn Warm-Starting Neural Network Training.\u201d In many real-world applications, the training data is not readily available and is accumulated over time. As training models from scratch is a time-consuming task, it is preferred to use warm-starting, i.e., using the already existing models as the starting point to obtain faster convergence. This paper investigates the effect of warm-starting on the final model s\u0313 performance. It identifies a noticeable gap between warm-started and randomly-initialized models, hereafter referenced as the warm-starting gap. Furthermore, they provide a solution to mitigate this side-effect. In addition to reproducing the original paper s\u0313 results, we propose an alternative solution and assess its effectiveness."}, {"heading": "Methodology", "text": "We reproduced almost every figure and table in the main text and some of those in the appendix. We used our implementation to produce these results. In case of a mismatch of the results, we also investigated the cause and proposed possible explanations. We mainly used GPUs to train our models using infrastructure offered by public clouds and those that were available to us privately."}, {"heading": "Results", "text": "Most of our results closely match the reported results in the original paper. Therefore, we confirm that the warm-starting gap exists in certain settings and that the ShrinkPerturb method successfully reduces or eliminates this gap. However, in some cases, we were not able to completely reproduce their results. By investigating the root of such mismatches, we provide another solution to avoid this gap. In particular, we show that data augmentation also helps to reduce the warm-starting gap."}, {"heading": "What was easy", "text": "The experiments described in the paper were based on regular training of neural networks on a portion of widely-used datasets, possibly from a pre-trained model. Therefore implementing each experiment was relatively easy to do. Furthermore, since many of the parameters were reported in the original paper, we did not need much tuning\nCopyright \u00a9 2021 K. Kireev, A. Mohtashami and E. Pajouheshgar, released under a Creative Commons Attribution 4.0 International license. Correspondence should be addressed to Amirkeivan Mohtashami (amirkeivan.mohtashami@epfl.ch) The authors have declared that no competing interests exist. Code is available at https://github.com/CS-433/cs-433-project-2-fesenjoon. \u2013 SWH swh:1:dir:a7205bcd48196c1391d8c56414a1e20c39b52aa7. Open peer review is available at https://openreview.net/forum?id=N43DVxrjCw.\nReScience C 7.2 (#14) \u2013 Kireev, Mohtashami and Pajouheshgar 2021 1\nin most experiments. Finally, it is straightforward to implement and use the proposed solution."}, {"heading": "What was difficult", "text": "Though implementing each experiment is relatively simple, the numerosity of experiments proved to be slightly challenging. In particular, each of the online experiments in the original setting requires training a deep network to convergence more than 30 times. In these cases, we sometimes changed the settings, sacrificing granularity to reduce computation time. However, these changes did not affect the interpretability of the final results.\nCommunication with original authors We briefly communicated with the authors to clarify the experiments\u02bc details, such as the convergence conditions.\n1 Introduction\nTraining large models from scratch is usually time and energy-consuming, so it is desired to have a method to accelerate retraining neural networks with new data added to the training set. The well-known solution to this problem is warm-starting. WarmStarting is the process of using the weights of a model, pre-trained on a subset of the data, as the starting point of training with the complete data. The paper investigates the effect of warm-starting on the final model s\u0313 accuracy and identifies a generalization gaponwarm-startedmodels. Thepaper also provides amethod to mitigate this gap by shrinking the pre-trained weights and adding a random perturbation. In this report, we repeat the original paper s\u0313 experiments and compare them with the reported results. Also, we extend the original paper results by investigating the effect of data augmentation on this phenomenon. In particular, we establish that using data augmentation might be a second solution to mitigating the generalization gap. We report and discuss our results in Section 2. In section 3, we detail our experimental settings and hyperparameters.\n2 Results & Discussion\n2.1 Warm-Starting Generalization Gap Similar to the paper, we start by demonstrating the existence of a generalization gap when using warm-starting before training. We use the same set of experiments used by the authors. Unless otherwise stated, we follow the settings described in the paper for our experiments. In particular, in the offline setting, we first train our model on half of the training data and then further train the pre-trained model on the whole dataset. We compare the resulting model with a model trained from randomly initialized weights. Figure 1 depicts the test accuracy of ResNet-18 [1] during training in this setting andmatches Figure 1 of the paper. We repeat this experiment with different datasets, models, and optimizers. In particular we perform experiments on CIFAR-10 [2], CIFAR-100 [2], and SVHN [3]. As our model, we experiment with ResNet-18, a three-layer perceptron, and logistic regression. The same models and datasets were used in the original paper. For the optimizers, we compare SGD [4] and Adam [5]. In this particular experiment, we compare SGD with and\nReScience C 7.2 (#14) \u2013 Kireev, Mohtashami and Pajouheshgar 2021 2\nwithout momentum. In the rest of this work, unless explicitly stated, SGD is used without momentum. The final accuracies are reported in Table 1 similar to Table 1 of the original paper. Our results are similar to the paper s\u0313 results for CIFAR-10 and CIFAR100 datasets. In particular, we observe a generalization gap when using a warm-started model instead of training from scratch. However, we did not observe the same gap on the SVHN dataset. Furthermore, we were unable to obtain a reasonable accuracy with the MLP model using SGD without momentum with the reported setting on SVHN. We instead report the result of using 0.005 learning rate. We did not perform hyperparameter tuning for the other experiments.\nIn the online setting, we follow the original paper and train our model in several steps, increasing the amount of data available at each step. This setting is a more accurate simulation of the real-world problems where the training data grows over time. We split the training data into batches of 1000 samples and start adding them, one by one, to the pool of available data. We follow two different scenarios. In one scenario, we reinitialize ourmodel randomly after each batch is added and train it from scratch. In the other scenario, we continue training themodel with the parameters learned in the previous step. After each batch is added, we continue training our model until convergence before adding the next batch. We assume convergence when the model reaches 99% training accuracy. By communicating with the origi-\nnal paper s\u0313 authors, we confirmed that this is the same condition used in the original paper. As in the paper, we optimize the model using Adam optimizer with a learning rate of 0.001 on CIFAR-10. Given the discrepancy of our results on the SVHN datasets in the offline settings, we additionally perform the same experiment on this dataset. For the CIFAR-10 dataset, the generalization gap between random-initialization training and warm-start training is clearly observed (Figure 2a). However, like the offline experiment, we did not observe the gap for the SVHN dataset (Figure 2b). Still, we were able to reproduce the gap by increasing the convergence accuracy threshold to 99.9% (Figure 2c). Note that 99% train accuracy is more challenging to achieve on the CIFAR-10 dataset than on SVHN and therefore requires more training, possibly leading to more over-fitting. Increasing the convergence threshold compensates for this difference. This result and the fact that the authors show that the proposed Shrink-Perturb method is similar to an aggressive regularization brings up the question of whether this gap might be a side-effect of over-fitting when training on partial data. There are various known techniques to prevent overfitting. The original paper investigates the effect of some of these techniques, namely regularization, and early-stopping. We reproduced these experiments and explained the results below. Early-Stopping: To investigate the effect of early-stopping, following the original paper, we trained a ResNet-18 model on half of the CIFAR-10 data and checkpointed its parameters every 20 epochs. The result is plotted in Figure 3, which matches Figure 4 of the original paper and shows the warm-starting gap can be observed even after 20 epochs of training. To decrease computational costs, we used lower granularity than the original paper to perform this experiment, saving parameters every 20 epochs rather than 5 epochs. Also, we only perform each experiment once.\nReScience C 7.2 (#14) \u2013 Kireev, Mohtashami and Pajouheshgar 2021 3\nRegularization: Regularization is commonly used to improve generalization. The original paper explores the effect of various types of regularization. Due to time and resource limitations, we only look into weight decay, which is widely used and is a de-facto standard for training the state-of-the-art models. We repeat the offline setting experiment on CIFAR-10 with a weight decay of 0.1 on both the pre-training and main training. However, contrary to the original paper s\u0313 results, we observe that the warm-starting gap decreases when applyingweight decay. We also test with weight decay values of 0.01 and 0.001. We find out that higher values of weight decay result in lower warm-starting gap. The results are reported in Table 2, which corresponds to Appendix Table 13 of the original paper. Data Augmentation: Data augmentation is widely used to obtain state-of-the-art performance and is known to help generalization [6], but it is not used in the other experiments of this paper. It is specifically important to check the effect of data augmentation since it is widely used in practice. Therefore we extend the original paper s\u0313 experiments by investigating the impact of data augmentation. We report our results in Section 2.4.\n2.2 Effect of Hyperparametes Our results show that in some cases, a generalization gap exists when pre-training our model on a portion of the final dataset. However, when training in the online setting on SVHN, we could only observe this gap with a high enough convergence threshold. The paper investigates the effect of other training hy-\nperparameters, namely learning rate and batch size. To investigate the effect of learning rate and batch size, we train a ResNet-18 with different values for thesehyperparameters. Wechoose the learning rate from {0.1, 0.01, 0.001} and the batch size from {128, 64, 32, 16}. We iterate over all pairs for these values. For each pair, we train over the full CIFAR-10. We also train a different model over 50% of the CIFAR-10 dataset and use it to warm-start a model and train it on the whole dataset. We use a different learning rate and batch size, randomly chosen from the sets of values, in the second part of the training, i.e., for training the warm-started model. We repeat each experiment 9 times. Each model is trained to 99% training accuracy. The test accuracy is plotted against training time in Figure 4, which corresponds with Figure 3 of the paper. Note that the training time for the warm-started model corresponds to the time of the second part of the training. In other words, the time of training on half of the dataset is not included. This is justified because the goal is to assess if warmstarting leads to comparable accuracy while saving training time when a new batch of data arrives. In our results, choosing the right hyperparameters can lead to achieving comparable or even better accuracy, when using warm-starting, faster than training a randomly initialized model. This does not match the results of the paper, where the\nReScience C 7.2 (#14) \u2013 Kireev, Mohtashami and Pajouheshgar 2021 4\nwarm-started models with comparable accuracy take the same amount of training time as the randomly initialized models. While we perform less experiments in the warmstarted setting, we perform the same number of experiments with random initialization as described in the original paper s\u0313 text. However, the number of points in Figure 3 of the original paper corresponding to randomly initializedmodels, is more than what has been described in the text. We note that performing more randomly initialized experiments might be the reason for the mismatch in our results with the original paper.\n2.3 Shrink & Perturb Solution In addition to establishing the warm-starting gaps\u0313 existence and investigating its roots, the paper also provides a method to mitigate this issue. In this method, the training starts from a shrunk and perturbed version of the pre-trained weights, so we reference it as the Shrink-Perturb method. More specifically, for a given \u03bb and \u03c3, the new weight is computed as\nwnew = \u03bbwpretrained + \u03c3wrandom (1)\nwhere wold is the pre-trained weight and wrandom is the corresponding weight from a randomly initializedmodel. Whenever we apply the Shrink-Perturb transformation, we create a new randomly initialized model and use its weights as wrandom. We tested the effectiveness of this method in both offline and online settings. In the offline setting, we applied the Shrink-Perturb transform after pre-training on 50% of CIFAR-10. We used \u03c3 = 10\u22124 and repeated this experiment with different values of \u03bb. We plotted the test accuracy during training on all of the data in Figure 5. It can be seen that the method is effective and leads to even better performance than the randomly initialized model. In the online setting, we applied the Shrink-Perturb transform every time a new batch\nReScience C 7.2 (#14) \u2013 Kireev, Mohtashami and Pajouheshgar 2021 5\nof data is added. To reduce computation cost, we add data in batches of 2500 samples. The result for \u03c3 = 10\u22124 and different values of \u03bb is plotted in Figure 6. Figure 6 matches Figure 7 of the original paper. The result of applying the Shrink-Perturb method in the offline setting is not reported in the original paper.\nTo assess the impact of shrinking weights on the model s\u0313 performance, we fit different models to CIFAR-10. Then, we shrink the weight with different values of \u03bb and evaluate the accuracy. Similar to the paper, we train ResNet18 and an MLP with ReLU activation with and without bias. In addition, we also train an MLP with Tanh activation with and without bias. The result is shown in Figure 7, which corresponds with Figure 6 of the paper. The only difference in our findings with the original paper s\u0313 is that we observe classifier performance damage for MLP with ReLU for \u03bb > 0.6. Though for \u03bb > 0.8 the damage is negligible. Also, note that shrinking the weights of anMLP without bias and ReLU activation only scales the final output, which does not affect the output labels. Therefore its immunity to shrinkage is expected. The\nmore interesting result is that even for ResNet-18 or MLP with Tanh activation, the test accuracy is not significantly damaged for \u03bb values greater than 0.2. In order to explain why the Shrink-Perturb method is effective, the original paper compares the average gradients over the first and second half of the dataset during the train-\nReScience C 7.2 (#14) \u2013 Kireev, Mohtashami and Pajouheshgar 2021 6\ning of the warm-started model in the offline setting. In particular, a ResNet-18 is first trained on half of CIFAR-10. Warm-starting from the pre-trained model, the model is trained on the full dataset while measuring the average gradient over the first and second half of the dataset simultaneously. It is observed that the gradient for the first half, which the model was pre-trained on, is substantially lower than for the second part. However, applying the Shrink-Perturb transformation eliminates this difference. We reproduced this experiment with some slight modifications. In particular, instead of averaging the gradient over part of the dataset after each batch, we did it at the beginning of each epoch. We plotted these values in Figure 8. Our result matches Figure 5 of the original paper. In particular, we confirm that the Shrink-Perturb method successfully eliminates the gap between the gradients.\n2.4 Effect of Data Augmentation Data augmentation is widely used in practice. However, it is not used for the experiments in the original paper. Therefore, we decided to assess its impact on the generalization gap for warm starting training. We perform our experiments on ResNet-18 and CIFAR-10. To augment the data, we first pad the image with 4 pixels on each side and then randomly crop it back to 32x32. We then perform a random horizontal flip with probability 0.5. We also apply color jitter with brightness, contrast, and saturation factor equal to 0.25. Finally, we also apply small random rotations. All experiments were done with SGD and a learning rate equal to 0.001 in order to make the setup consistent with previous warm start experiments. The results are reported on Figure 9 . It can be seen that applying the augmentationmitigates thewarm-starting gap. We allow themodels to train for 350 epochs. However, because the learning rate is low, themodels are not fully converged even after 350 epochs. We did not continue the training because of resource limitations. However, it is visible thatwarm-startingwith data augmentation can achieve good performance faster than training from scratch. To explain the effectiveness of the Shrink-Perturb solution, the original paper s\u0313 authors looked at the differences of the gradient norm for the first and the second part of the dataset, which is heavily reduced after applying Shrink-Perturb (as shown in Figure 8). Following the same direction, we checked if applying data augmentation can affect the difference as well. It can be seen in Figure 10 that, similar to the shrink perturbmethod, the gradient norm difference is also mitigated when using data augmentation. It is clear that applying data augmentation prevented overfitting. The original warm start setup has a large divergence between train and test accuracy from the beginning of the training. On the contrary, the model trained with data augmentation has close performance on train and test datasets. We leave the investigation of other overfitting prevention techniques\u02bc effects as future work. Additionally, we note that data augmentation usually slows down the convergence and it cannot be applied to every task since for some types of data transform set cannot be\nReScience C 7.2 (#14) \u2013 Kireev, Mohtashami and Pajouheshgar 2021 7\ndefined. Due to the limits of this report, we also leave the careful comparison between data augmentations and Shrink & Perturb as future research in this area.\n2.5 Warm-Starting Gap in Transfer Learning Deep learning models require large training sets to perform well. This presents a problem in many practical cases where only limited data is available, and acquiring additional data is expensive. This has encouraged the use of transfer learning [7, 8, 9]; the practice of warm-starting from a model trained on a different dataset. To investigatewhether a similar gap is observed in transfer learning, we trained aResNet18 model on one dataset and used the pre-trained weights to warm-start training on a different dataset. We performed this experiment for all pairs of CIFAR-100, CIFAR-10, and SVHN datasets. To also investigate the effect of the amount of the data available, we also considered subsets of these datasets where only a fraction p of data is available. More accurately, for every two datasets and p \u2208 {0.1, 0.3, 0.6, 1.0}, we chose a random subset from each dataset containing p \u00d7 n data points, where n is the total number of data points in that dataset. We performed the described transfer learning experiment for these subsets and recorded the final test accuracy. To assess the effect of warmstarting and the Shrink-Perturb method, we plotted the final test accuracy of each of the described three settings (random initialization, warm starting, and warm-starting with Shrink-Perturb) with respect to p in Figure 11. In this experiment, we used Adam as our optimizer. This figure corresponds with Figure 9 of the original paper. At each part of the training, we train our models for 200 epochs. When training CIFAR100 starting from weights of a model trained on SVHN or CIFAR-10, the last layer is initialized randomly because of the mismatch in the number of classes. It can be seen that, as mentioned in the paper, the warm-starting gap exists in the transfer learning settings as well, and that it is worsened when the amount of data available is increased. Furthermore, the Shrink-Perturb method proves useful in this setting, as well.\n3 Methodology\nIn this section, we define the setting we used for our experiments. There was no available code for the original paper, and we implemented everything from scratch. We use the PyTorch framework for the implementations.\nReScience C 7.2 (#14) \u2013 Kireev, Mohtashami and Pajouheshgar 2021 8\n3.1 Model descriptions Most of the experiments are performed using ResNet-18 [1]. Some experiments are also performed on a Multi-Layered Perceptron (MLP) and Logistic Regression. We detailed the structure of each of these models below.\n\u2022 ResNet-18: We used an implementation of ResNet-18 tuned for CIFAR-10 dataset. We used the code from https://github.com/huyvnphan/PyTorch_CIFAR-10. In all experiments, batch normalization [10] was enabled.\n\u2022 MLP: The MLP has three hidden layers, each of which has 100 neurons. Either ReLU or Tanh was used as the activation function. Unless explicitly stated, the bias term is added.\n\u2022 Logistic Regression: We implement Logistic Regression as a Multi-Layered Perceptron with no hidden layers.\nWe used either Adam or SGD optimizers for training the models. More accurately, we use Adam in Table 1, Figure 2, Figure 6, and Figure 11. In all other experiments, we use SGD. We use 0.001 learning rate and batches of size 128. Unless otherwise stated, we used SGD without momentum and without weight decay. In cases where momentum wasused (such as in Table 1), the value ofmomentumwas set to 0.9. TheAdamoptimizer was usedwith default parameters fromPyTorchs\u0313 implementation, namely \u03b21 = 0.9, and \u03b22 = 0.999.\n3.2 Datasets Same as the original paper, we perform experiments on CIFAR-10, CIFAR-100, and SVHN datasets. We normalize each of the RGB channels by the mean and standard deviation of that channel in the CIFAR-10 dataset. Except for the data augmentation experiments, we do not apply any data augmentation.\n3.3 Hyperparameters We used hyperparameters stated in the original paper in most of our experiments. In cases where we deviated from the reported values, mostly due to computational resource and time limitation, we have reported them in the text where we described the experiment. In case a hyperparameter is not reported in the original paper, we either communicated with the authors to ask the hyperparameters, pick a value making reasonable assumptions, or try out different values and report the result for all of them. In all these cases, we clarified the parameter we used in the text.\nReScience C 7.2 (#14) \u2013 Kireev, Mohtashami and Pajouheshgar 2021 9\n3.4 Experimental setup We ran our experiments on both public cloud infrastructure, such as Google Colab and private GPUs that were available to us. Therefore the infrastructure varies between different experiments. Our implementations for all the experiments in this work is available in the SupplementaryMaterial and also in https://github.com/CS-433/cs-433-project-2-fesenjoon.\n4 Communication with Authors\nIn the original paper [11], it was not clear what convergence condition was used to stop the training. Therefore, We communicated with the authors via email and asked them to explain the convergence condition used in the experiments more clearly. They stated that convergence happens when the training accuracy reaches 99%. However, for reproducing the Table 1 which also uses simpler models like Logistic Regression and MLP, it is not possible to reach 99% accuracy. They clarified that in this scenario, the convergence condition is met when the training accuracy stops improving.\n5 Conclusion\nWe have verified the existence of the generalization gap in certain training settings. Additionally, we have confirmed that the introduced Shrink-Perturb method can be effective in removing this gap. We did this by repeating experiments of the original paper and performing some experiments of our own. However, we also encountered cases where wewere not able to reproduce the warm-starting gap or where the Shrink-Perturb method was not very successful. In addition, we reproduced several experiments to investigate the effect of hyper-parameters, such as learning rate, on this phenomenon. Finally, we have shown that applying data augmentation can also help to remove this gap. To allow others to reproduce our results, we have detailed our experiments and have released our code."}], "title": "[Re] Warm-Starting Neural Network Training", "year": 2021}