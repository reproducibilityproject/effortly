{"abstractText": "In this work, we experimented with Layer\u2010wise Relevance Propagation and combined it with back\u2010propagation to perform classification and semantic segmentation, follow\u2010 ing the approach proposed by Chefer H. et al., in [1] for computer vision. Moreover, we incorporated the concept of pixel affinities, by using ViT\u2010based explainability as vi\u2010 sual seeds to drive the generation of pseudo segmentation masks by computing pixel affinities, following the approach described by Ahn J. et al. in [2].", "authors": [{"affiliations": [], "name": "Ioannis Athanasiadis"}, {"affiliations": [], "name": "Georgios Moschovis"}, {"affiliations": [], "name": "Alexander Tuoma"}, {"affiliations": [], "name": "Koustuv Sinha"}, {"affiliations": [], "name": "Sharath Chandra Raparthy"}], "id": "SP:558be094ac1d051cc74b147a0665e8ed179cae3d", "references": [{"authors": ["H. Chefer", "S. Gur", "L. Wolf"], "title": "Transformer Interpretability Beyond Attention Visualization.", "venue": "CoRR abs/2012.09838", "year": 2020}, {"authors": ["J. Ahn", "S. Kwak"], "title": "Learning pixel-level semantic affinity with image-level supervision for weakly supervised semantic segmentation.", "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "year": 2018}, {"authors": ["O. Russakovsky"], "title": "ImageNet Large Scale Visual Recognition Challenge.", "venue": "In: International Journal of Computer Vision 115.3 (Dec", "year": 2015}, {"authors": ["M. Guillaumin", "D. Kuttel", "V. Ferrari"], "title": "ImageNet Auto-Annotation with Segmentation Propagation.", "venue": "English. In: International Journal of Computer Vision", "year": 2014}, {"authors": ["L.H. Gilpin", "D. Bau", "B.Z. Yuan", "A. Bajwa", "M.A. Specter", "L. Kagal"], "title": "Explaining Explanations: An Approach to Evaluating Interpretability of Machine Learning.", "year": 2018}, {"authors": ["A. Binder", "G. Montavon", "S. Bach", "K.-R. Myller", "W. Samek"], "title": "Layer-wise Relevance Propagation for Neural Networks with Local Renormalization Layers.", "year": 2016}, {"authors": ["A. Dosovitskiy"], "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.", "venue": "CoRR abs/2010.11929", "year": 2020}, {"authors": ["B. Cheng", "M.D. Collins", "Y. Zhu", "T. Liu", "T.S. Huang", "H. Adam", "L.-C. Chen"], "title": "Panoptic-DeepLab: A Simple, Strong, and Fast Baseline for Bottom-Up Panoptic Segmentation.", "year": 2019}, {"authors": ["K. He", "G. Gkioxari", "P. Dollar", "R. Girshick"], "title": "Mask r-cnn.", "venue": "Proceedings of the IEEE international conference on computer vision", "year": 2017}, {"authors": ["B. Zhou", "A. Khosla", "A. Lapedriza", "A. Oliva", "A. Torralba"], "title": "Learning deep features for discriminative localization.", "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition", "year": 2016}, {"authors": ["Y.-T. Chang", "Q. Wang", "W.-C. Hung", "R. Piramuthu", "Y.-H. Tsai", "M.-H. Yang"], "title": "Weakly-supervised semantic segmentation via sub-category exploration.", "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition", "year": 2020}, {"authors": ["R.R. Selvaraju", "A. Das", "R. Vedantam", "M. Cogswell", "D. Parikh", "D. Batra"], "title": "Grad-CAM: Why did you say that? Visual Explanations from Deep Networks via Gradient-based Localization.", "year": 2016}, {"authors": ["V. Petsiuk", "A. Das", "K. Saenko"], "title": "RISE: Randomized Input Sampling for Explanation of Black-box Models.", "year": 2018}, {"authors": ["A. Vaswani", "N. Shazeer", "N. Parmar", "J. Uszkoreit", "L. Jones", "A.N. Gomez", "L. Kaiser", "I. Polosukhin"], "title": "Attention Is All You Need", "year": 2017}, {"authors": ["H. Hashemi", "H. Zamani", "W.B. Croft"], "title": "Guided Transformer: Leveraging Multiple External Sources for Representation Learning in Conversational Search.", "venue": "CoRR abs/2006.07548", "year": 2020}, {"authors": ["P. Krahenbuhl", "V. Koltun"], "title": "Efficient inference in fully connected crfs with gaussian edge potentials.", "venue": "Advances in neural information processing systems", "year": 2011}, {"authors": ["M. Everingham", "L.V. Gool", "C.K.I. Williams", "J. Winn", "A. Zisserman"], "title": "The PASCAL Visual Object Classes (VOC) challenge", "venue": "ReScience C", "year": 2010}, {"authors": ["S. Abnar", "W.H. Zuidema"], "title": "Quantifying Attention Flow in Transformers.", "venue": "CoRR abs/2005.00928", "year": 2020}, {"authors": ["A. Binder", "G. Montavon", "S. Lapuschkin", "K.-R. Muller", "W. Samek"], "title": "Layer-Wise Relevance Propagation for Neural Networks with Local Renormalization Layers.", "venue": "Artificial Neural Networks and Machine Learning \u2013 ICANN 2016", "year": 2016}, {"authors": ["E. Voita", "D. Talbot", "F. Moiseev", "R. Sennrich", "I. Titov"], "title": "AnalyzingMulti-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned.", "venue": "CoRR abs/1905.09418", "year": 2019}], "sections": [{"text": "R E S C I E N C E C Replication / ML Reproducibility Challenge 2021\n[Re] Weakly-Supervised Semantic Segmentation via"}, {"heading": "Transformer Explainability", "text": "Ioannis Athanasiadis1,2, ID , Georgios Moschovis1,2, ID , and Alexander Tuoma2, ID 1Equal contribution \u2013 2KTH Royal Institute of Technology, Stockholm, SE\nEdited by Koustuv Sinha,\nSharath Chandra Raparthy\nReviewed by Anonymous Reviewers\nReceived 04 February 2022\nPublished 23 May 2022\nDOI 10.5281/zenodo.6574631\n1 Reproducibility Summary\n1.1 Scope of Reproducibility In this work, we experimented with Layer\u2010wise Relevance Propagation and combined it with back\u2010propagation to perform classification and semantic segmentation, follow\u2010 ing the approach proposed by Chefer H. et al., in [1] for computer vision. Moreover, we incorporated the concept of pixel affinities, by using ViT\u2010based explainability as vi\u2010 sual seeds to drive the generation of pseudo segmentation masks by computing pixel affinities, following the approach described by Ahn J. et al. in [2].\n1.2 Methodology In order to reproduce the experiments presented in [1] and [2], we initially examined the authors\u2019 code thoroughly and based on our understanding, we tried to replicate most parts of the pipeline apart from evaluation metrics for positive and negative per\u2010 turbation area\u2010under\u2010curve (AUC) results for the predicted and target classes on the ImageNet [3] validation set, as well as Segmentation performance on the ImageNet\u2010 segmentation [4] dataset, which we borrowed from the authors\u2019 repository for the work of Chefer H. et al., in [1]. Regarding hardware, we used private resources to train our ViT\u2010hybrid architecture and Affinity network, as well as perform inference for all our models; Finally, it took roughly 15 GPU hours to reproduce the vision\u2010related results of [1] whereas it took about 40 GPU hours to train and evaluate the AffinityNet on the Hybrid\u2010ViT architecture.\n1.3 Results Overall, we reproduced the experiments related to the vision task as conducted at [1]. Our results are up to first decimal place identical to those reported in [1] thus support\u2010 ing the authors\u2019 claim of having implemented a relatively sufficient ViT interpretabil\u2010 ity method. When it comes to the AffinityNet [2], the method has been adapted in the context of Hybrid\u2010ViT architectures with our experiments indicating that the weakly\u2010 supervised semantic segmentation performance ofHybrid\u2010ViT architectures are inferior to the CNN\u2010based ones.\nCopyright \u00a9 2022 I. Athanasiadis, G. Moschovis and A. Tuoma, released under a Creative Commons Attribution 4.0 International license. Correspondence should be addressed to Ioannis Athanasiadis (iath@kth.se) The authors have declared that no competing interests exist. Code is available at https://github.com/athaioan/ViT_Affinity_Reproducibility_Challenge. \u2013 SWH swh:1:dir:95342519c89e6957f4f90ee7e51d8724a48d9a56. Open peer review is available at https://openreview.net/forum?id=rcEDhGX3AY.\nReScience C 8.2 (#4) \u2013 Athanasiadis, Moschovis and Tuoma 2022 1\n1.4 What was easy We found particularly easy to run and understand the code provided by the original authors of both [1] and [2] papers. When it comes to replicating [1], the authors provided most of the information required to reproduce the vision\u2010related experiments with the code compensating for what was missing.\n1.5 What was difficult The main difficulty of replicating the study presented in [1] was that details on how to compute the AUC metric were not provided in the paper report.\nReScience C 8.2 (#4) \u2013 Athanasiadis, Moschovis and Tuoma 2022 2\n2 Introduction\nOne of the most exciting technological aspects nowadays is Machine Learning\u2019s promis\u2010 ing capabilities in transforming the world we live in, mainly due to its exciting resur\u2010 gence through Deep Learning. However, as machine learning models become more complex, there is a noticeable trade\u2010offbetweenaccuracy and simplicity or interpretabil\u2010 ity [5] and plenty of cutting\u2010edge research papers have been published in top\u2010tier con\u2010 ferences related to this tension. In this project, we primarily experimented with Layer\u2010 wise Relevance Propagation (LRP), a mechanism of explaining what pixels are relevant within a 2\u2010dimensional image for reaching a classification decision [6] and applied it to a Vision Transformer [ViT] [7], combined with gradient back\u2010propagation to perform classification but also semantic segmentation on the respective data in ImageNet [3, 4], by reproducing the work of Chefer H. et al, in [1]. Furthermore, the task of semantic segmentation refers to clustering the pixels of an in\u2010 put image that correspond to the same semantic category. There are various approaches dedicated to this task with the one proposed in [8] being the current state\u2010of\u2010the\u2010art. However, they all rely on training given ground truth segmentation masks. Consider\u2010 ing that annotating images in the form of segmentationmasks is a rather expensive and tedious process, capitalizing on weak forms of segmentation would be highly benefi\u2010 cial. In order to address these issues, in this project, we investigated using ViT\u2010based explainability as visual seeds to drive the generation of pseudo segmentation masks by computing pixel affinities, following the approach described in [2]. In particular, we trained a Hybrid ViT\u2010base, where the patches are extracted from a CNN feature map, through relevance propagation and used those as seeds to a network computing pixel affinities, in order to improve quality of the generated segmentation masks.\n3 Related Work\nSemantic segmentation has numerous applications, such as self\u2010driving cars ormedical image analysis. Additionally, the evident importance in providing the machines with the ability to perceive the world along with its challenging nature has attractedmany re\u2010 searchers to this domain. Many algorithms have been proposed for this task with Mask R\u2010CNN [9] being among the most frequently employed ones. Although such approaches can be trained to extract semanticwith high precision, they require an extensive amount of semantically annotated training samples. In their work [2], the authors capitalize on image\u2010level supervision to construct competent pseudo\u2010segmentation masks that can be further utilized to train the segmentation approaches requiring ground truth labels. More specifically, they use class activation mapping (CAM) [10] seeds to model the rela\u2010 tion between neighboring pixels, which enables the refinement of the initial CAM cues into segmentation masks of higher quality. Although the previous approach results in relatively accurate segmentation masks, the initial CAMs seeds tend to highlight only the most descriptive part of an instance, which negatively affects the quality of the gen\u2010 erated segmentation masks. With the purpose of mitigating this issue, the essayist of [11] employs a sub\u2010category exploration approach. RegardingDeepNeural Networks (DNNs) interpretability, various approaches have been proposed in the literature. GradCAM [12] is a popular interpretability method applied to various CNN architectures that weighs feature activations in different pixel regions within an image with the average gradient of the class scores. After these gradients are computed through global average pooling, they are passed to a ReLU1 activation function that intensifies pixels contributing towards increasing the target class activa\u2010 tion scores. However GradCAM is restricted to CNN architectures. One more general\n1Rectified Linear Units activation function is: ReLU(x) = max{x, 0}.\nReScience C 8.2 (#4) \u2013 Athanasiadis, Moschovis and Tuoma 2022 3\napproach is RISE [13] that measures pixels\u2019 importance by applying element\u2010wise mul\u2010 tiplications of the original input with a sampled random binary mask to reduce their intensities to zero and only preserve the most important among them. AlthoughCNN\u2010based architectures havedemonstrated competent performance in anum\u2010 ber of vision\u2010related tasks, they come with an increased inductive bias due to the 2D neighboring structure of the images. On the other hand, transformer\u2010based architec\u2010 tures are able to learn spatial relationships detached from the explicit 2D nature of the images. Transformer architecture, since it was proposed in 2017 by Waswani A. et al., [14] has become very popular in various deep learning domains, and it is based solely on attention mechanisms, dispensing recurrence and convolutions entirely and weigh\u2010 ing the influence of different parts of the input data. Following its recent success in NLP, it was recently adopted in computer vision tasks, and in this work, we focus on particularly re\u2010implementing a Vision Transformer [ViT] [7] from scratch. Additionally, we employ the explainability cues derived from a image classification ViT to drive the construction of segmentation masks given solely image\u2010level annotation as we explain hereunder.\n4 Methods\nIn this section, we describe the methods utilized in our work. Precisely, in subsection 4.1, we provide details about Vision Transformer architecture. Subsection 4.2 explains how we perform relevance propagation in our model implementations. Finally, in sub\u2010 section 4.3, we present the AffinityNet framework modeling the affinity of neighboring pixels.\n4.1 ViT Classification Asmentioned earlier, a VisionTransformer [ViT] [7] is an implementation of transformer networks for computer vision tasks. The transformer encoders in ViT are similar to the original transformer architecture introduced in [14] with slight modifications in the order of operations. Similarly to how a sentence is split into tokens, in ViT we split an image into patches and provide the linearization of the patches representations as input to stacked transformer encoders after adding positional embeddings. Positional embeddings are learned during training; while processing the input patches in given order x0, x1, x2, ... we learn the respective positional embeddings x\u03020, x\u03021, x\u03022, ... for the patches and compute the loss in a backward fashion. The input is then propagated to the attention heads, where multi\u2010head attention is calculated as the concatenation of self\u2010 attention scores computed in each head individually as stated in the formulas below:\nAttention(Q,K, V ) = softmax ( QKT\u221a\ndk\n) V\nMultihead(Q,K, V ) = Concat(head1, ...headh)\u0398o\nwhere headi = Attention(Q\u0398Qi ,K\u0398 K i , V\u0398 V i )\nAttention is a mechanism for weighting representations learned in a neural network. It is proportional to the respective weights of the network and really flourished within a variety of NLP tasks, where self\u2010attention and multi\u2010head attention became one of the major breakthroughs in sequencemodeling tasks precisely [15]. In our implementation, we use ViT\u2010Base, the smallest ViT model variant, which consists of 12 stacked encoder layers, as well as 12 attention heads in every layer, as it is illustrated in table 1. We use a [CLS] learnable embedding z00 = xclass to the sequence of embedded patches, whose state at the output of the Transformer encoder zL0 , to which a classification head is at\u2010 tached to represent an image y = LayerNorm(zL0 ). We also employ a hybrid architecture,\nReScience C 8.2 (#4) \u2013 Athanasiadis, Moschovis and Tuoma 2022 4\nwhich again consists of a ViT\u2010Base but the patches are extracted fromaCNN featuremap, while layer normalization is applied before every block and residual connections after every block in our implementation as it is described in [7].\n4.2 ViT Explainability As we explained in section 2, one of our main goals in this project was to apply LRP [6] to a ViT\u2010Base model [7], combined with classic gradient back\u2010propagation regime to perform classification but also semantic segmentation on the respective data in Ima\u2010 geNet [3, 4], by reproducing the work of Chefer H. et al, in [1]. Considering the input feature map and weights of layer n in form of tensors, X,\u03a8we compute the Deep Taylor Decomposition R(n)j for relevance propagation as formulated below. This expression satisfies the conservation rule that broadly suggests that relevance will be maintained in consecutive layers.\nR (n) j = G\n( X,\u03a8, R(n\u22121) ) = \u2211 i Xj \u2202L (n) i (X,\u03a8) \u2202Xj\nMoreover, in cases we have two operators (e.g. skip connections and matrix multiplica\u2010 tion) the above expression is used for both the input pairs (u, v) and (v, u) to compute Ru (n)\nj andRv (n)\nj . Given two such tensors u and v, if we add them in layer n the conserva\u2010 tion rule ismaintained but not in other cases of operations such asmatrixmultiplication. To address this lack of conservationwe normalize the relevances and get R\u0304u (n)\nj and R\u0304v (n) j\nrespectively. In addition, there is a special case related to the matrix multiplication op\u2010 eration, where we get two attribution maps for each of the matrices we multiply, and the sum of the relevances of each matrix equals R. Furthermore, to actually normalize the CAMs, all we need to do is divide each of them by 2, which is what the normalization below would do since Ru (n)\nj and Rv (n) j have identical sums.\nRu (n) j = G(u, v,R(n\u22121))\nRv (n) j = G(v, u,R(n\u22121))\nR\u0304u (n) j = R u(n) j\n| \u2211\nj R u(n) j | | \u2211\nj R u(n) j |+ | \u2211 k R v(n) k |\n\u00b7 \u2211 i R (n\u22121) i\u2211\nj R u(n) j\nR\u0304v (n) k = R v(n) k\n| \u2211\nk R v(n) k | | \u2211\nj R u(n) j |+ | \u2211 k R v(n) k |\n\u00b7 \u2211 i R (n\u22121) i\u2211\nk R v(n) k\nFollowing the above formulas, we have computed relevances for all layers of our ViT\u2010 Base and have implemented relevance propagation, in order to perform semantic seg\u2010 mentation on the ImageNet\u2010segmentation [4] dataset following the experiments described in [1]. An example of a CAM generated by our Hybrid ViT\u2010base, where the patches are extracted from a CNN feature map, through relevance propagation is illustrated in Fig. 1(b).\nReScience C 8.2 (#4) \u2013 Athanasiadis, Moschovis and Tuoma 2022 5\n4.3 AffinityNet At this stage, we employed the AffinityNet proposed in [2] with the purpose of refining the initially incomplete explainability cues, derived from the Hybrid\u2010ViT network, into segmentation masks of higher quality. In more detail the AffinityNet aims at modelling the relation between adjacent pixels through leveraging the images\u2019 feature representa\u2010 tion f aff and computing the similarity of ith and jth pixels as:\nWi,j = exp(\u2212||f affi \u2212 f affj ||)\nConceptually, the AffinityNet is trained to predict the inter\u2010pixel semantic affinities, in a class\u2010agnostic manner, by learning to extract meaningful representations for each pixel. Evidently, target labels are required in order to drive the AffinityNet\u2019s weights towards accurately predicting the affinities.\nSemantic Affinity Targets \u2014 Training the AffinityNet to model the inter\u2010pixel relationships, requires supervision in the form of segmentation masks. In our scenario, ground truth segmentations labels were not provided and thus the generated ViT explainability seeds are utilized as our best available source of supervision. Admittedly, the generated ex\u2010 plainability cues can be quite incomplete and by no means precisely capture the whole instances, however, we can use the most confident pairs in terms of belonging to the same instance. Assuming C classes withMc corresponding to the explainability cue of class c, we construct the background activation mapMbg as:\nMbg(x, y) = [1\u2212max c\u2208C Mc(x, y)] \u03b1\nThe parameter\u03b1 controls how confident the generated background cues are. Intuitively, when the \u03b1 parameter is relatively high, a pixel of high activation in the Mbg would be a strong indication of the pixel belonging to the background category. On the con\u2010 trary, when the \u03b1 parameter is relatively low, a high background activation suggests that background is the dominant semantic of that pixel but notwith asmuch confidence. Next, wemake use of the common practice of applying dense conditional random fields (dCRF) [16] to refine the activation responses for all C + 1 classes. Applying the dCRF on these classes\u2019 activations with theMbg having been derived from a low \u03b1, favors clas\u2010 sifying the pixels as background. On the other hand, when a high \u03b1 is used, the dCRF is more prone to classifying a pixel as its most activated class. Having said that, apply\u2010 ing dCRF on low \u03b1 gives rise to the confident pixel of foreground instance while on the other, a high \u03b1 allows for identifying confident background pixels. In our experiments, we set \u03b1low = 4 and \u03b1high = 32 respectively. Below we provide an indicative illustration of confident background and foreground pixels.\nNext, we extract pairs of pixels belonging to the same category with high confidence. Additionally, we also consider as neutral, those pixels that were classified by the dCRF\nReScience C 8.2 (#4) \u2013 Athanasiadis, Moschovis and Tuoma 2022 6\nas background in the presence of low \u03b1 and as foreground in the opposite case. Finally, the construction of confident common\u2010instance pairs is now feasible. We consider pairs of positive and negative affinity, in a class\u2010agnostic manner, while we ignore any pair containing neutral pixels. It is worth highlighting that only neighboring pairs are ex\u2010 tracted with a radius of 5 pixels. An intuitive figure, showcasing the possible affinities is displayed below.\nTraining AffinityNet \u2014 After having generated the explainability\u2010based affinity targets, we can now train a neural network to generate insignificant W values to those pixels that are semantically unrelated. More specifically, we utilized the CNN\u2010backbone as trained in the Hybrid\u2010ViT image classification task for feature representation f aff purposes. In order to adapt to affinity\u2010assignment task, we employed two 1 \u00d7 1 convolutions on top of the feature map extracted from the Hybrid backbone. The loss used for training the network incorporates three different types of affinities, namely the negative, the fore\u2010 ground positive and background positive affinities. Additionally, we weighted the loss contributions of these three types based on the amount of negative, foreground, and background affinity labels on each training batch. The intuition behind this approach was to avoid only accounting for themost frequent case of background positive relation\u2010 ships due to images containing mostly background content. Based on these the overall loss was computed as :\nL+fg = \u2212 1\nN+fg \u2211 i,j log(Wi,j)I(i,j\u2208T + fg )\nL+bg = \u2212 1\nN+bg \u2211 i,j log(Wi,j)I(i,j\u2208T + bg )\nL\u2212 = \u2212 1 N\u2212 \u2211 i,j log(Wi,j)I(i,j\u2208T \u2212)\nL\u2212 = L+fg + L + bg + 2L \u2212\nwith I being the indicator of ith and jth pixel sharing the target relationship T . Note that the L\u2212 contributes twice in order avoid unbalance between positive and negative relationships.\nRefining the Explainability seeds \u2014 At this stage, we utilized the predicted pixel\u2010wise affini\u2010 ties to propagate high explainability activations towards the pixels of identical semantic affinity. In more detail, we regarded the predicted affinities as transition probabilities in a random\u2010walk process. By employing this approach, we were able to propagate the highly activated regions based on the semantic relationships predicted fromAffinityNet. The transition matrix derives from the predicted affinities as:\nTrw = D \u22121 w W o\u03b2\nwith Dw being a diagonal array applying row\u2010wise normalization to W . Additionally, the o\u03b2 operator is applied so that low transitional probabilities are ignored. Naturally,\nReScience C 8.2 (#4) \u2013 Athanasiadis, Moschovis and Tuoma 2022 7\nthe \u03b2 hyperparameter has to be an integer value larger than one. Next, we compute the expected transitional probabilities of t+ 1 iterations of the random walk process as:\nTrw = T t rw\nFinally, we extract the semantic segmentationmasks through refining the explainability seedsMc for each c class as:\nvec(Mnewc ) = Trwvec(Mc)\nwith vec(.) being the array flatten operator. In our experiments, we used values of 16 and 8 for the hyperparameters \u03b2 and t respectively.\nAdditionally, in table 3 we evaluate the segmentation quality of the extracted cues by comparing them with the provided ground truth segmentation masks. In Appendix 6.1 we provide qualitative results corresponding to explainability cues in ImageNet; gener\u2010 ated using our ViT\u2010Base implementation.\n2ImageNet segmentation dataset was obtained from calvin\u2010vision.net.\nReScience C 8.2 (#4) \u2013 Athanasiadis, Moschovis and Tuoma 2022 8\nrollout raw GradCAM LRP partial LRP Target paper Ours [18] attention [12] [19] [20] [1]\npixel accuracy 73.54 67.84 64.44 51.09 76.31 79.70 79.73 mAP 84.76 80.24 71.60 55.68 84.67 86.03 86.03 mIoU 55.42 46.37 40.82 32.89 57.94 61.95 62.01\nIn the context of this study, we replicated the ViT explainability approach proposed in [1]. Additionally, we capitalized on the explainability seeds derived from a Hybrid\u2010ViT architecture to generate competent semantic segmentation labels for weak\u2010supervision. More specifically, the AffinityNet [2] was employed with the purpose of refining the ini\u2010 tially incomplete explainability cues into segmentation masks of higher quality. The quantitative results provided in tables 2 and table 3 indicate that we have successfully implemented the explainability method described in [1] since our results are identical to those originally reported in the latter for all the considered metrics. Regarding the AffinityNet, we evaluated the class\u2010wisemIoU performance that we have achieved based on the explainability seeds as generated by the Hybrid\u2010ViT architecture. Furthermore, according to table 4, we observe that the performance we achieved is lower compared to the one reported in [2], however segmentation masks of improved\nReScience C 8.2 (#4) \u2013 Athanasiadis, Moschovis and Tuoma 2022 9\nquality were generated. One reason for that could be the lower quality of ViT explain\u2010 ability seeds compared to the CNN\u2010based ones. Another potential reason for the lacking performance of the AffinityNet, when given explainability cues from ViT architecture, could be that the feature map f aff in our case, derives from low\u2010level image representa\u2010 tion where as in the original paper [2] feature representation from multiple levels were aggregated. Such multi\u2010level aggregation was not feasible in our scenario due to the nature of the transformer architecture. Concluding, in this work we have demonstrated the feasibility of using ViT\u2010derived ex\u2010 plainability cues with the purpose of training the AffinityNet. Although, wewere able to increase the quality of the ViT explainability cues by refining them with the AffinityNet, the CNN\u2010based architectures perform better while using lighter models."}, {"heading": "Appendix", "text": "The code of our reproducibility attempt can be found at https://github.com/athaioan/ViT_ Affinity_Reproducibility_Challenge\n6.1 Qualitative Results on ImageNet - ViT Explainability [1] In here, we provide qualitative results of the reproduced ViT explainability approach as proposed in [1]\nFigure 3. Image of a bug from ImageNet segmentation dataset [4].\nFigure 4. Segmentationmap generated by ourViT\u2010 base for the bug image.\nFigure 5. Image of a cow from ImageNet segmentation dataset [4].\nFigure 6. Segmentationmap generated by ourViT\u2010 base for the cow image.\nReScience C 8.2 (#4) \u2013 Athanasiadis, Moschovis and Tuoma 2022 12\nFigure 7. Image of a reindeer from ImageNet segmentation dataset [4].\nFigure 8. Segmentation map generated by our ViT\u2010base for the reindeer image.\nFigure 9. Image of a sheep from ImageNet segmentation dataset [4].\nFigure 10. Segmentation map generated by our ViT\u2010base for the sheep image.\nFigure 11. Image of a squirrel from ImageNet segmentation dataset [4].\nFigure 12. Segmentation map generated by our ViT\u2010base for the squirrel image.\n6.2 Qualitative Results on Pascal VOC - AffinityNet on Hybrid ViT In here, we provide qualitative results of the reproduced ViT explainability approach as proposed in [1]\nReScience C 8.2 (#4) \u2013 Athanasiadis, Moschovis and Tuoma 2022 13\nReScience C 8.2 (#4) \u2013 Athanasiadis, Moschovis and Tuoma 2022 14"}], "title": "[Re] Weakly-Supervised Semantic Segmentation via Transformer Explainability", "year": 2022}