{"abstractText": "We fine\u2010tuned a transformer model, pre\u2010trained on Reddit data2, using the ParlAI API3 with counterfactual data augmentation, positively biaseddata collection, bias controlled training, and all three biasmitigation techniques combined, as discussed in the original paper1. We implemented counterfactual data augmentation and bias controlled train\u2010 ing ourselves. All models were trained and evaluated using a single NVIDIA Tesla P100 PCIe GPU, which took between 1.3 and 4.6 GPU hours approximately.", "authors": [{"affiliations": [], "name": "Erica Eaton"}, {"affiliations": [], "name": "Pirouz Naghavi"}, {"affiliations": [], "name": "Koustuv Sinha"}, {"affiliations": [], "name": "Sharath Chandra Raparthy"}], "id": "SP:3b823fea084519cd8a29a45703fe6874a38c5711", "references": [{"authors": ["E. Dinan", "A. Fan", "A. Williams", "J. Urbanek", "D. Kiela"], "title": "and J", "venue": "Weston. \u201cQueens are Powerful too: Mitigating Gender Bias in Dialogue Generation.\u201d In: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). Online: Association for Computational Linguistics, Nov.", "year": 2020}, {"authors": ["A.H. Miller", "W. Feng", "A. Fisch", "J. Lu", "D. Batra", "A. Bordes", "D. Parikh"], "title": "and J", "venue": "Weston. ParlAI: A Dialog Research Software Platform.", "year": 2018}, {"authors": ["S. Roller"], "title": "ParlAI Tutorial", "venue": "https://colab.research.google.com/drive/1bRMvN0lGXaTF5fuTidgvlAl-Lb41F7AD.", "year": 2020}, {"authors": ["J. Devlin", "M.-W. Chang", "K. Lee"], "title": "and K", "venue": "Toutanova. \u201cBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.\u201d In: Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). Minneapolis, Minnesota: Association for Computational Linguistics, June", "year": 2019}, {"authors": ["A. Radford", "J. Wu", "R. Child", "D. Luan", "D. Amodei", "I. Sutskever"], "title": "Language Models are Unsupervised Multitask Learners.", "year": 2019}, {"authors": ["J. Urbanek", "A. Fan", "S. Karamcheti", "S. Jain", "S. Humeau", "E. Dinan", "T. Rockt\u00e4schel", "D. Kiela"], "title": "A", "venue": "Szlam, and J.Weston. \u201cLearning to Speak and Act in a Fantasy Text Adventure Game.\u201d In:", "year": 2019}, {"authors": ["J. Zhao", "Y. Zhou"], "title": "Z", "venue": "Li, W.Wang, and K.-W. Chang. \u201cLearning Gender-Neutral Word Embeddings.\u201d In: Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Brussels, Belgium: Association for Computational Linguistics, Oct.", "year": 2018}, {"authors": ["S. Humeau"], "title": "K", "venue": "Shuster, M.-A. Lachaux, and J.Weston. Poly-encoders: Transformer Architectures and Pre-training Strategies for Fast and Accurate Multi-sentence Scoring.", "year": 2020}, {"authors": ["E. Dina"], "title": "et al", "venue": "The Second Conversational Intelligence Challenge (ConvAI2).", "year": 2019}, {"authors": ["A. Celikyilmaz", "E. Clark"], "title": "and J", "venue": "Gao. Evaluation of Text Generation: A Survey.", "year": 2020}], "sections": [{"text": "R E S C I E N C E C"}, {"heading": "Replication / ML Reproducibility Challenge 2021", "text": "[Re] Reproduction and Extension of \u201dQueens are Powerful too: Mitigating Gender Bias in Dialogue Generation\u201d\nErica Eaton1, ID and Pirouz Naghavi1, ID 1University of Washington, 185 E Stevens Way NE, Seattle, WA 98195\nEdited by Koustuv Sinha,\nSharath Chandra Raparthy\nReviewed by Anonymous Reviewers\nReceived 04 February 2022\nPublished 23 May 2022\nDOI 10.5281/zenodo.6574651"}, {"heading": "Reproducibility Summary", "text": ""}, {"heading": "Scope of Reproducibility", "text": "The main claims we are trying to reproduce are that bias controlled training or com\u2010 bining counterfactual data augmentation, the positively biased data collected by Dinan et al.1, and bias controlled training for the LIGHT dataset yields generated dialogue in which the percent of gendered words and male bias closely match the ground truth."}, {"heading": "Methodology", "text": "We fine\u2010tuned a transformer model, pre\u2010trained on Reddit data2, using the ParlAI API3 with counterfactual data augmentation, positively biaseddata collection, bias controlled training, and all three biasmitigation techniques combined, as discussed in the original paper1. We implemented counterfactual data augmentation and bias controlled train\u2010 ing ourselves. All models were trained and evaluated using a single NVIDIA Tesla P100 PCIe GPU, which took between 1.3 and 4.6 GPU hours approximately."}, {"heading": "Results", "text": "Overall, our results support themain claims of the original paper1. Although the percent gendered words and male bias in our results are not exactly the same as those in the original paper1, the main trends are the same. The main difference is lower male bias for the baseline model in our results. However, our findings and the trend similarities between our results and those obtained byDinan et al.1 demonstrate that bias controlled training or combining all three bias mitigation techniques can effectively control the amount of gender bias present in the model generated responses, supporting Dinan et al.\u2019s claims1."}, {"heading": "What was easy", "text": "When reproducing the original paper1, implementing counterfactual data augmenta\u2010 tion and bias controlled training was easy since these techniques were well\u2010described\nCopyright \u00a9 2022 E. Eaton and P. Naghavi, released under a Creative Commons Attribution 4.0 International license. Correspondence should be addressed to Erica Eaton (eatone3@uw.edu) The authors have declared that no competing interests exist. Code is available at https://github.com/Pnaghavi/Mitigating-Gender-Bias-in-Generated-Text. \u2013 SWH swh:1:dir:320f7080ccd0edd611da07e9dbd9dbe4bbd18758. Open peer review is available at https://openreview.net/forum?id=StblE2MQ3AY.\nReScience C 8.2 (#13) \u2013 Eaton and Naghavi 2022 1\nin the original paper1. Also, combining all three bias mitigation techniques was sim\u2010 ple, as we applied the same techniques used to implement each bias mitigation method individually."}, {"heading": "What was difficult", "text": "The only difficulty we encountered, albeit minor, was learning how to use ParlAI, which was necessary to use the same model as in the original paper1. However, after reading through the ParlAI documentation and experimenting with the ParlAI Google Colabora\u2010 tory tutorial4, we understood how to use ParlAI to fine\u2010tune the model, pre\u2010trained on Reddit conversations2, for the datasets we create.\nCommunication with original authors\nWe communicated with Emily Dinan, an author of the original paper1, who clarified whatmodel was used in the original paper1 and provided us with the command to down\u2010 load the model as well as the hyperparameter settings used when fine\u2010tuning.\n1 Introduction\nAd\u2010hocmethods formitigating social bias in natural language data remain an active area of modern research. As transfer learning with pre\u2010trained models such as BERT5 and GPT\u201026 continue to be pervasive, the inherent issues in their training data have come to light. Large corpora of unstructured text from the Internet reflect the biases and inequal\u2010 ities of society, and are consequently learned by these models and their fine\u2010tuned vari\u2010 ants. To this end, Dinan et al.1 proposed three techniques to specificallymitigate gender bias in fine\u2010tuned languagemodels, using the LIGHTdataset7 as an example. The LIGHT dataset is a crowdsourced collection of dialogues spoken between \u201dpersonas,\u201d characters played by either humans or models, in a fantasy adventure game, LIGHT7. Dinan et al. applied the following techniques to this dataset: 1) counterfactual data augmentation, in which genderedwords are replacedwith their opposite, i.e., replacing \u201dhe\u201d with \u201dshe\u201d; 2) positively biased data collection, in which new, less biased female character personas and dialogues are created via crowd\u2010sourcing; and 3) bias controlled training, in which the dialogue is placed in groups based on the number of gendered words it contains and this group number is included with the dialogue as a special token when training the model1. The model itself is a transformer pre\u2010trained on a dataset of Reddit con\u2010 versations2 and then fine\u2010tuned on LIGHT using the three techniques described above, individually, as well as one combining all three techniques.\n2 Scope of reproducibility\nThe aim of this paper is to evaluate the following hypotheses made by Dinan et al.1 by reproducing their experiments.\n\u2022 Combining counterfactual data augmentation, the positively biased data collected by Dinan et al.1, and bias controlled training for the LIGHT dataset yields gener\u2010 ated dialogue in which the percent of genderedwords andmale bias closelymatch the ground truth.\n\u2022 Bias controlled training for the LIGHT dataset yields generated dialogue in which the percent of gendered words and male bias closely match the ground truth.\nReScience C 8.2 (#13) \u2013 Eaton and Naghavi 2022 2\n3 Methodology\nWe fine\u2010tuned the transformer model, pre\u2010trained on Reddit data2, using the ParlAI API3 with counterfactual data augmentation, positively biased data collection, bias con\u2010 trolled training, and all three bias mitigation techniques combined, as discussed in the original paper1. We generated training, test, and validation datasets for counterfac\u2010 tual data augmentation and bias controlled training from the original LIGHT dialogue dataset. We also formatted the dataset used for each bias mitigation technique, extract\u2010 ing the dialogue from each dataset and placing it in the proper format, such that every\u2010 thing said in the dialogue so far is used to predict the next response in the dialogue, which is the label. All models were trained and evaluated using a single NVIDIA Tesla P100 PCIe GPU.\n3.1 Model descriptions\nDinan et al.1 used a transformer with 8 encoder layers, 8 decoder layers, embedding dimension of 512, and 16 attention heads. This model was pre\u2010trained on Reddit con\u2010 versations from the pushshift.io Reddit dataset, which contains 2.2 billion samples for training after removing comments that contain URLs or that are less than 5 characters long1. Specifically, the model was trained on all comments in each thread and learned to predict the next comment in the thread1. Thus, this pre\u2010training makes the model well\u2010suited for the dialogue generation task2. The model contains 87, 508, 992 trainable parameters and the training objective is to minimize the cross entropy loss on the origi\u2010 nal and augmented LIGHT dialogues.\n3.2 Datasets\nWe used the ParlAI API command from the paper\u2019s ParlAI project page8 to obtain the fol\u2010 lowing data: the LIGHT dataset7, a list of counterfactuals, a list of gendered words9, and the positively biased data collected by Dinan et al.1. The LIGHT dataset and positively biased data collected by Dinan et al. contain information about interactions between characters in the game, LIGHT, such as the character names and personas, dialogue, and environment where the interaction took place, to name a few. The LIGHT dataset contains approximately 11, 000 interactions and 111, 000 utterances7. An utterance is a single occurrence of a character talking during a dialogue. The LIGHT dataset is used to fine\u2010tune the baseline model. Each biasmitigationmethod employed by Dinan et al.1 also requires fine\u2010tuning the pre\u2010 trainedmodel on a new dataset. For counterfactual data augmentation, we used the list of counterfactuals to replace every gendered word, according to the list of gendered words from Zhao et al.9, in the LIGHT dialogue dataset with its counterfactual. The list of genderedwords9 has 1, 049words. The list of counterfactuals contains each gendered word and its opposite gendered counterpart. For example, the counterfactual for \u201dhe\u201d is \u201dshe\u201d. In addition, the list of counterfactuals, containing 421 words, was constructed by Dinan et al.1 using the list of gendered words from Zhao et al.9. For positively biased data collection, Dinan et al. crowdsource new dialogue data, ask\u2010 ing workers to create dialogue assuming gender equality1. This dataset contains 507 interactions and 6, 658 utterances. Given the time and resource constraints, we used Dinan et al.\u2019s positively biased data1 rather than crowdsourcing the data ourselves. For bias controlled training, we appended \u201dfx my\u201d after the last utterance in an episode, which is a portion of a dialogue between two characters, based on the label, which is the next utterance in the dialogue. In \u201dfx my,\u201d x is 1 if there is at least one female gen\u2010 dered word in the label and 0 otherwise, and y is 1 if there is at least one male gendered word in the label and 0 otherwise. Thus, each label falls into one of four bins: \u201df0 m0\u201d which has no gendered words; \u201df0 m1\u201d which has no female gendered words but at least\nReScience C 8.2 (#13) \u2013 Eaton and Naghavi 2022 3\none male gendered word; \u201df1 m0\u201d which has at least one female gendered word but no male gendered words; and \u201df1m1\u201d which has at least one female and onemale gendered word. Placing the dialogue labels in these bins causes themodel to learn the gender bias present in an utterance, allowing us to specify the desired gender bias in the model\u2019s generated dialogue using one of the four bins. We used the list of gendered words from Zhao et al.9 to determine the number of gendered words and proper bin for each label and model generated utterance. We split the datasets used for fine\u2010tuning each model into approximately 90% for train\u2010 ing and 10%for anunseen test set. The training setwas further split into 80%for training and 20% for validation.\n3.3 Hyperparameters As previouslymentioned, themodel, pre\u2010trained onReddit conversations, has 8 encoder layers, 8 decoder layers, 16 attention heads, and an embedding dimension of 5122. In addition, this model has 2, 048 nodes in the hidden layer, uses GeLU activation function, and truncates each dialogue to at most 512 characters and each label to at most 128 char\u2010 acters. Other hyperparameters for each model are an initial learning rate of 3.1e \u2212 7, memory\u2010efficient Adam optimizer, gradient clipping of 0.1, inverse square root learn\u2010 ing rate scheduler with a decay factor of 0.5 and patience of 3, no activation or attention dropout, batch size of 20, and dropout of 0.1 or 0.15 depending on hyperparameter tun\u2010 ing results. Emily Dinan, one of the authors of the original paper1, provided some of the hyperparameter values, but we reduced the batch size due to memory constraints with Google Colaboratory resources. Since most hyperparameters were provided by Emily Dinan and the learning rate is adjusted by the inverse square root learning rate sched\u2010 uler and batch size could not be increased due to GPU limitations, the only remaining hyperparameter that we could effectively tune to improve perplexity, based on our expe\u2010 riencewith deepNLPmodels, particularly pre\u2010trained transformers, was dropout. Thus, we tuned dropout, applied to the embeddings and before layer normalization, for the model combining all three bias mitigation techniques, since this model provided the best results according to the original paper1, to obtain lower perplexity on the valida\u2010 tion set. In order to tune dropout, we increased dropout in increments of 0.025, starting from a value of 0.1, which was given by Emily Dinan, up to 0.2. After training a number of models with different dropouts, we found that 0.15 dropout resulted in the lowest perplexity. In addition, for the extension with neutral, generated data, we again tuned dropout, and found 0.15 to be the optimal value.\n3.4 Experimental setup and code Similar to the Reddit dataset used for pre\u2010training themodel as well as the training done by Dinan et al.1, we generated the datasets based on the entire history of conversations so far, predicting the next utterance in each conversation. For each biasmitigation tech\u2010 nique and combining all three techniques, we generated the datasets from the original conversations in the LIGHT dataset7 for training, evaluation, and response generation. Using ParlAI\u2019s API, we fine\u2010tuned 5 versions of the model, pre\u2010trained on Reddit conver\u2010 sations2: baseline, counterfactual data augmentation, positively biased data collection, bias controlled training, and all three bias mitigation techniques combined. When fine\u2010 tuning eachmodel, the best model is saved according to the perplexity on the validation set. As long as the perplexity on the validation set continues to improve, the model con\u2010 tinues training and at every quarter epoch, the version of themodel achieving the lowest perplexity on the validation set is saved. If the model does not improve after 10 quarter epochs, training will be automatically stopped to avoid overfitting or unnecessary train\u2010 ing. After training is complete, we run further evaluation to obtain F1 scores on the validation and test datasets as well as F1 scores pertaining to the labels for each bin for these two datasets. Finally, we pass every dialogue episode in the test set through the\nReScience C 8.2 (#13) \u2013 Eaton and Naghavi 2022 4\nmodel to generate responses. These generated responses are used to compute statistics defined by Dinan et al.1 to evaluate gender bias in generated responses from themodel.1 All experiments were run on Google Colaboratory using a single NVIDIA Tesla P100 PCIe GPU. After fine\u2010tuning each model, the labels in the test set are split into the bias con\u2010 trolled training bins and within these bins, each model\u2019s generated utterances are also grouped into the same bins. This allowed us to compute the percent gendered words and male bias for the generated utterances within each bin of labels for the test set. In addition, we computed the F1 score for predicted tokens in generated responses sepa\u2010 rately for each bin of test labels.\n3.5 Computational requirements\nThe model used by Dinan et al. in the original paper1 was pre\u2010trained on Reddit con\u2010 versations in the same manner as the polyencoder transformer model from Humeau et al.10, and contains the same number of encoder layers, decoder layers, attention heads, and embedding dimension size. Training the polyencoder transformer on the ConvAI2 dataset, which has about 131, 000 elements11, took 2.7 hours using 8 NVIDIA Volta 100 GPUs10. Since the polyencoder transformer has about 20% more parameters than the model used by Dinan et al. and the LIGHT dataset is about 15% smaller than the Con\u2010 vAI2 dataset, we estimated it tookDinan et al. about 2.3 hours or less, which is 85%of 2.7 hours, using 8 GPUs to fine\u2010tune each model or about 11.5 hours total for all 5models.\nWe initially estimated we could also fine\u2010tune all 5 models in approximately 11.5 hours using Google Cloud Platform. Instead, we used a single NVIDIA Tesla P100 PCIe GPU on Google Colaboratory. During training, each model required about 16 GB of GPU mem\u2010 ory, maximizing the GPU memory available with the aforementioned batch size of 20. Table 1 lists runtime information for fine\u2010tuning each model, where the model combin\u2010 ing all three bias mitigation techniques uses dropout of 0.15 for the embeddings and before layer normalization, as previously mentioned. The runtime for this model with other values for dropout was approximately the same. The actual training time for our models was substantially lower than our estimate, likely due, at least in part, to the un\u2010 predictability of Google Colaboratory providing the full computational GPU resources assigned to a particular session.\n1The GitHub repository for our project is located at https://github.com/Pnaghavi/Mitigating\u2010Gender\u2010Bias\u2010 in\u2010Generated\u2010Text\nReScience C 8.2 (#13) \u2013 Eaton and Naghavi 2022 5\n4 Results\nBelow are the results from reproducing and extending the experiments in the original paper1. Overall, our results support the hypotheses previously identified. Further dis\u2010 cussion of the results in relation to the hypotheses is provided below. We also imple\u2010 ment 3 extensions to the original paper1, two of which are aimed at addressing the high time andmonetary cost of positively biased data collection, which requires crowdsourc\u2010 ing data. Figure 1 shows the percent gendered words, percent male bias, and F1 score of each model\u2019s generated utterances for conversations in the test set, separated according to the test label bins, where \u201dBaseline\u201d is themodel trained only on the LIGHT dataset, \u201dCDA\u201d is counterfactual data augmentation, \u201dPos Data\u201d is positively biased data collection, \u201dBias\u201d is bias controlled training, and \u201dAll\u201d combines all three bias mitigation techniques. In Figure 1, each set of three graphs corresponds to one of the four bias controlled training bins for test labels. The results shown in Figure 1 are quite similar to those in Figure 1 of the original paper1 in terms of how the percent gendered words, percent male bias, and F1 score for each model in each bin compare. Although our results are not exactly the same as those in the original paper1 in terms of values, the main trends in our results\nReScience C 8.2 (#13) \u2013 Eaton and Naghavi 2022 6\nare the same as those in the original paper1. The main differences between our results and those in the original paper1 are lower male bias in each bin for the baseline and a percent gendered words for \u201dCDA\u201d that is closer in value to the baseline in our results.\n4.1 Results for First Hypothesis According to the first hypothesis, the number of gendered words in the generated ut\u2010 terances for the \u201dAll\u201d model for each bin should be similar to the number of gendered words in the labels of the test set. This is observed in all four bins in Figure 1. Specif\u2010 ically, for the F0M0 bin, the test labels have no gendered words, which means the gen\u2010 erated utterances for both models should have a very low number of gendered words and approximately 50% male bias. The \u201dAll\u201d model satisfies these two requirements, as depicted in the first set of charts in Figure 1, because the generated utterances from this model are less than 1%gendered words and the percentmale bias is approximately 44%. For the F+M0 bin, the test labels have at least one female gendered word and no male gendered words, which means the generated utterances should have a higher number of gendered words and a smaller percentage of male bias. This is observed for the \u201dAll\u201d model in the second set of charts in Figure 1, since the percent gendered words for the \u201dAll\u201d model is higher than the baseline and the percentmale bias is under 5%, compared to about 42%male bias for the baseline. Similarly, in the F0M+ bin, the test labels have at least one male gendered word and no female gendered words. Thus, the generated utterances for the \u201dAll\u201d model should have a higher number of gendered words and a larger percentage of male bias, which is depicted in the third set of charts in Figure 1. In the F0M+ bin, the percent of gendered words for the \u201dAll\u201d model is about 1% higher than the baseline and themale bias is approximately 97%, compared to only 52% for the baseline. For the last bin, F+M+, the test labels have at least one male and one female gendered word. As a result, the generated utterances for the \u201dAll\u201d model should have a higher percentage of gendered words and closer to 50% male bias. As shown in the last set of charts in Figure 1, the \u201dAll\u201d model does have a higher percentage of gendered words than the baseline, specifically 13%, compared to 8% for the baseline. However, the male bias is about 43% for the \u201dAll\u201d model, which is not as close to an even gender bias split, 50% male and 50% female, as the baseline, which has about 46% male bias. In the discussion section, we give a possible cause for this discrepancy in our results.\n4.2 Results for Second Hypothesis Based on the second hypothesis, the number of gendered words in each utterance gen\u2010 erated by the \u201dBias\u201d model should be similar to that of the labels in the test set for each dialogue. This can be clearly seen for all four bins in Figure 1. In the F0M0 bin, the test labels have no gendered words. If the model has learned from bias controlled train\u2010 ing, producing properly gender biased text according to the bin appended to the end of the dialogue, then the generated text for the \u201dBias\u201d model in the F0M0 bin should have very few gendered words and about 50%male bias. As depicted in the first set of charts in Figure 1, for the F0M0 bin, the \u201dBias\u201d model has less than 1% gendered words and approximately 57% male bias, as desired. For the F+M0 bin, the generated text should have more female gendered words and few to no male gendered words, matching the gender bias in the test set label. This is observed in the second set of charts in Figure 1, since the \u201dBias\u201d model yields a higher percent of gendered words than the baseline and less than 5% male bias, compared to 42% male bias for the baseline. Generated text in the F0M+ test label bin should have more male gendered words and few to no female gendered words, which is depicted in the third set of charts in Figure 1. Specif\u2010 ically, the percent gendered words for the \u201dBias\u201d model is 1% higher than the baseline and male bias is approximately 94%, compared to only 52% for the baseline. In the last bin, F+M+, the generated text should ideally have an even distribution of male and fe\u2010 male gendered words and a higher percentage of gendered words overall. This is shown\nReScience C 8.2 (#13) \u2013 Eaton and Naghavi 2022 7\nin the last set of charts in Figure 1, since the \u201dBias\u201d model has a higher percentage of gendered words than the baseline, specifically 11% for the \u201dBias\u201d model and 8% for the baseline, although male bias is 36% for the \u201dBias\u201d model compared to 46% for the base\u2010 line, which is not an even distribution. A possible cause for this discrepancy in our results is described in the discussion section.\n4.3 Effect of Removing Positively Biased Data Collection Given the time and monetary cost involved in crowdsourcing data, specifically the pos\u2010 itively biased data Dinan et al. collected1, a natural question is whether adding this positively biased data to counterfactual data augmentation and bias controlled training is worth the cost. In other words, what is the performance loss if positively biased data collection is excluded from the model, instead relying only on counterfactual data aug\u2010 mentation and bias controlled training.\nImplementation and Experimental Setup \u2014We fine\u2010tuned the model, pre\u2010trained on Reddit conversations2, on the data generated fromcounterfactual data augmentation andusing bias controlled training. The implementation and experimental setup is the same as that for themodel that combines all three biasmitigation techniques, exceptwe excluded the positively biased data collected by Dinan et al.1.\nResults and Discussion \u2014 Figure 2 depicts, for each bin, the percent gendered words and percent male bias in the generated utterances as well as the F1 score for the \u201dAll\u201d model, which combines all three bias mitigation techniques, the \u201dCDA + Bias\u201d model, which uses counterfactual data augmentation and bias controlled training, and the baseline. As expected, for all four bins, the percent gendered words, percent male bias, and F1 score for \u201dAll\u201d achieves better results than \u201dCDA + Bias,\u201d in terms of higher F1 scores and the percent gendered words and male bias being closer to ground truth, except \u201dCDA + Bias\u201d achieves a slightly higher F1 score for the F0M0 bin. However, results for \u201dCDA + Bias\u201d are always within about 2% of the results for \u201dAll\u201d and the overall F1 score for \u201dCDA + Bias\u201d is within 0.25% of the overall F1 score for \u201dAll,\u201d specifically an F1 score of 15.31 for \u201dCDA + Bias\u201d and 15.56 for \u201dAll.\u201d Although incorporating positively biased data collection does yield better results, given how small the difference is between including vs. excluding this technique, it may not be worth the necessary time or money. Instead, one could simply use counterfactual data augmentation and bias controlled training or find a less costly way to collect positively biased data, which is the focus of the next extension.\n4.4 Generating Gender Neutral Data In the previous section, we created a model incorporating counterfactual data augmen\u2010 tation and bias controlled training, removing positively biased data collection. Instead of completely removing this additional, positively biased data, an alternative, which\nReScience C 8.2 (#13) \u2013 Eaton and Naghavi 2022 8\nstill avoids the cost of crowdsourcing data, is to generate new, gender neutral data us\u2010 ing code. Incorporating gender neutral data can help shift the gender bias of the data, whether male or female, closer to 50%.\nImplementation and Experimental Setup \u2014We fine\u2010tuned the model, pre\u2010trained on Reddit conversations2, using counterfactual data augmentation and bias controlled training, then generated responses from this model for all dialogue episodes in the training data. For each generated response, we set the response to be either the model\u2019s generated response or the actual label. If the generated response is neutral, meaning it contains approximately the same number of male and female gendered words or no gendered words, we use the generated response 90% of the time, selecting the actual label in all other cases. These neutral generated responses were used to reconstruct the conversa\u2010 tions. We then created new training and validation datasets from these conversations that partially included neutral model generated utterances. Finally, a new model was fine\u2010tuned on these datasets. The experimental setup is the same as that for the model that combines all three bias mitigation techniques, except we excluded the positively biased data collected by Dinan et al.1 and used the gender neutral data we generated instead. An important point to note is that the test dataset for this new model is the original test dataset. Thus, the F1 scores obtained for each bin and the overall F1 score are from the original test dataset, containing 100% natural conversations.\nResults and Discussion \u2014 Figure 3 shows, for each bin, the percent gendered words and percent male bias in the generated utterances as well as the F1 score for the \u201dAll\u201d model, which combines all three bias mitigation techniques, the baseline, and the \u201dCDA + Bias + Our Gen Data\u201d and \u201dCDA + Bias\u201d models, which use counterfactual data augmentation and bias controlled training with and without our neutral, generated data, respectively. Results for our new model, \u201dCDA + Bias + Our Gen Data,\u201d are within 2% of the results for \u201dAll\u201d in all cases except male bias for F0M0, F+M0, and F0M+. For F0M0, our model yields male bias closer to 50% than \u201dAll\u201d by 6%, specifically male bias of about 43% for \u201dAll\u201d and 49% for our model. Also, our model results in about 4% higher male bias than \u201dAll\u201d for the F+M0 bin and about 4% lower male bias for the F0M+ bin. However, these are actually the desired results because for each bin, the male bias for our model is closer to 50%, at least slightly, than \u201dAll.\u201d Thus, our model results in more gender neu\u2010 tral responses overall, which was the goal of this method. In addition, all results for our new model are still relatively close to the results of \u201dAll,\u201d demonstrating the effective\u2010 ness of our new method, as it did not require any crowdsourced data, only additional training. One concern with using model generated responses is that they may not be as coherent as natural dialogue, but the F1 scores for our new model are comparable to those for the \u201dAll\u201d model. For future work, if we repeatedly use the dialogues with our neutral, generated responses to create new generated responses, coherency will be\u2010 come a greater concern and necessitate the use of a coherency assessment model, such as some of the machine\u2010learned evaluation metrics highlighted by Celikyilmaz et al.12. Given that adding our neutral, generated data to counterfactual data augmentation and bias controlled training yields approximately the same or slightly higher F1 scores than the \u201dAll\u201d model, using only neutral, generated responses with high coherency, accord\u2010 ing to themetrics introduced by Celikyilmaz et al.12, in the reconstructed conversations, we can continue to shift the model towards gender neutrality, while maintaining high F1 scores.\n4.5 Percent Generated Responses with Respect to Bins To better evaluate the degree towhich our extensions generate gender neutral responses in comparison to the \u201dAll\u201d model, we placed the generated responses from these three models into one of the bias controlled training bins based on the presence of gendered\nReScience C 8.2 (#13) \u2013 Eaton and Naghavi 2022 9\nwords in the generated response, and computed the percent of generated utterances in each bin for each of the three models.\nResults and Discussion \u2014 Figure 4 depicts the percent of generated responses in each bin for the baseline, when combining all bias mitigation techniques, denoted \u201dAll,\u201d and us\u2010 ing counterfactual data augmentation and bias controlled trainingwith andwithout our neutral, generated data, denoted \u201dCDA + Bias + Our Gen Data\u201d and \u201dCDA + Bias,\u201d respec\u2010 tively. These results demonstrate that the \u201dCDA + Bias + Our Gen Data\u201d model generates more gender neutral responses overall, compared to \u201dAll\u201d and \u201dCDA + Bias.\u201d Specifically, for the F0M0 and F+M+ bins, which are themore gender neutral bins, \u201dCDA + Bias + Our Gen Data\u201d has the highest, or near highest, percentage of generated responses. For the F+M0 and F0M+ bins, which are not gender neutral, \u201dCDA + Bias + Our Gen Data\u201d has the lowest percent of generated responses. In addition to generating more neutral re\u2010 sponses, \u201dCDA + Bias + Our GenData\u201d achieves approximately the same F1 score for each bin as \u201dAll,\u201d as depicted in Figure 3, demonstrating that the control over gender bias pro\u2010 vided by bias controlled training is still present despite the responses beingmore gender neutral overall. This indicates an opportunity for future work to shift the overall bias of the model\u2019s generated responses to any direction, male biased, female biased, or neu\u2010 tral, by selectingmodel generated responses that belong to the bin with the desired bias to infuse the original dialogues with this bias and train a model to generate more re\u2010 sponses with the desired bias. By repeating this process, we can reinforce the model to generate more responses biased in the desired direction, as long as we can still achieve a high F1 score and maintain coherency, which can be checked by machine\u2010learned co\u2010 herency metrics12 as a form of second or outsider opinion on the generated responses during the infusion process.\n5 Discussion\nGiven how closely our experimental results for bias controlled training and combining all three original bias mitigation methods matched the ground truth, these two tech\u2010 niques can be used to control the gender bias of these models\u2019 generated text. Thus, gender neutral dialogue could be created by constructing ground truth data with either no gendered words or 50% male bias and 50% female bias within the gendered words. Given that we reproduced the results from the original paper1 for bias controlled train\u2010 ing and combining all three bias mitigation techniques, we feel that overall our results support the claims in the original paper1, despite the differences in value between our results and those in the original paper1. One possible cause for the differences between our results and those in the original paper1 is our training method, since we achieve higher F1 scores for each model and stop training when perplexity stops decreasing, which may not be the same criteria Dinan et al. used to determine when to stop train\u2010 ing. It is also possible that in the original paper1, the list of gendered words used to place utterances in bins was a subset of the original gendered word list9, most likely the list of counterfactuals. This could also account for the lower male bias we observed for\nReScience C 8.2 (#13) \u2013 Eaton and Naghavi 2022 10\nthe baseline in our results compared to Dinan et al.\u2019s, however Dinan et al. explicitly stated they used the gendered word list from Zhao et al.9. Evaluating our approach to reproducing the original paper1, one of the strengths of our approach is that we ran all code on Google Colaboratory with one GPU, a free resource, in a reasonable amount of time. However, Google Colaboratory imposes GPU limitations and as a result, we could not use the same batch size as that in the original paper1, although we achieve higher F1 scores than those in the original paper1.\n5.1 What was easy\nWhen reproducing the original paper1, implementing counterfactual data augmenta\u2010 tion and bias controlled training and combining all three bias mitigation techniques was easy. Specifically, counterfactual data augmentation and bias controlled training were well\u2010described in the original paper1 and the list of counterfactuals needed for counterfactual data augmentation was provided by Dinan et al. in an easy\u2010to\u2010use for\u2010 mat. Combining all three bias mitigation techniques was also an easy part of reproduc\u2010 ing the original paper1, as we simply needed to apply the same techniques used when implementing each bias mitigation method individually.\n5.2 What was difficult The only difficulty we encountered, albeit minor, was learning how to use ParlAI, which was necessary in order to use the same model as that in the original paper1. However, after reading through the ParlAI documentation and experimenting with the ParlAI Google Colaboratory tutorial4, we understood how to use ParlAI to fine\u2010tune the model, pre\u2010trained on Reddit conversations2, for the datasets we created.\nReScience C 8.2 (#13) \u2013 Eaton and Naghavi 2022 11\n5.3 Recommendations for reproducibility\nOverall, reproducing the original paper1 was fairly straightforward, butwe dohave three recommendations to further improve reproducibility. The first is more clearly indicat\u2010 ing what model, pre\u2010trained on Reddit conversations, is used, because the source of the model is not provided in the original paper1, only that the model is based on the implementation by Miller et al.3, who introduce ParlAI in that paper. The second rec\u2010 ommendation is to specify the hyperparameters used when fine\u2010tuning each model, as these were not provided in the original paper1. The last recommendation is to describe the stopping condition for fine\u2010tuning the models. We stopped training when perplex\u2010 ity stopped improving, but this resulted in higher F1 scores for the models than those achieved in the original paper1.\n5.4 Communication with original authors\nWe communicatedwith Emily Dinan, one of the authors of the original paper1, who clar\u2010 ified what model, pre\u2010trained on Reddit conversations, was used in the original paper1 and provided us with the command to download the model as well as the hyperparam\u2010 eter settings for training the models."}, {"heading": "Appendix", "text": "5.5 Generated Text Statistics for F0M0 Bin\n5.6 Generated Text Statistics for F+M0 Bin\nReScience C 8.2 (#13) \u2013 Eaton and Naghavi 2022 13\n5.7 Generated Text Statistics for F0M+ Bin\n5.8 Generated Text Statistics for F+M+ Bin\nReScience C 8.2 (#13) \u2013 Eaton and Naghavi 2022 14\n5.9 Distribution of Generated Responses across Bins for each Model\nReScience C 8.2 (#13) \u2013 Eaton and Naghavi 2022 15"}], "title": "[Re] Reproduction and Extension of \u201dQueens are Powerful too: Mitigating Gender Bias in Dialogue Generation\u201d", "year": 2022}