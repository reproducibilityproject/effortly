{"abstractText": "Weperformall experiments using themodel architectures, hyperparameters anddatasets as used in the original work. We also extend the experiments to a new dataset. We fur\u2010 ther contribute a reimplementation of the work in PyTorch Lightning to provide a mod\u2010 ular framework for future research into this area. All experiments are performed on Nvidia GTX 1080 GPUs. Our logs and checkpoints are made available for download via our code repository.", "authors": [{"affiliations": [], "name": "Karolina Drabent"}, {"affiliations": [], "name": "Stefan Wijnja"}, {"affiliations": [], "name": "Thijs Sluijter"}, {"affiliations": [], "name": "Konrad Bereda"}, {"affiliations": [], "name": "Koustuv Sinha"}, {"affiliations": [], "name": "Sharath Chandra Raparthy"}], "id": "SP:c00908cbfe5e91345fb78b39f9ab3a61993014f2", "references": [{"authors": ["W. Gao", "S. Guo", "T. Zhang", "H. Qiu", "Y. Wen", "Y. Liu"], "title": "Privacy-preserving collaborative learning with automatic transformation search.", "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition", "year": 2021}, {"authors": ["Q. Yang", "Y. Liu", "T. Chen", "Y. Tong"], "title": "Federated machine learning: Concept and applications.", "venue": "ACM Transactions on Intelligent Systems and Technology (TIST)", "year": 2019}, {"authors": ["S. Guo", "T. Zhang", "X. Xie", "L. Ma", "T. Xiang", "Y. Liu"], "title": "Towards byzantine-resilient learning in decentralized systems.", "year": 2002}, {"authors": ["L. Melis", "C. Song", "E. De Cristofaro", "V. Shmatikov"], "title": "Exploiting unintended feature leakage in collaborative learning.", "venue": "IEEE Symposium on Security and Privacy (SP). IEEE", "year": 2019}, {"authors": ["J. Kang", "Z. Xiong", "D. Niyato", "Y. Zou", "Y. Zhang", "andM. Guizani"], "title": "Reliable Federated Learning forMobile Networks.", "venue": "CoRR abs/1910.06837", "year": 2019}, {"authors": ["S. Niknam", "H.S. Dhillon", "J.H. Reed"], "title": "Federated learning for wireless communications: Motivation, opportunities, and challenges.", "venue": "IEEE Communications Magazine", "year": 2020}, {"authors": ["T.S. Brisimi", "R. Chen", "T. Mela", "A. Olshevsky", "I.C. Paschalidis", "W. Shi"], "title": "Federated learning of predictive models from federated electronic health records.", "venue": "In: International journal of medical informatics", "year": 2018}, {"authors": ["J. Geiping", "H. Bauermeister", "H. Dr\u00f6ge", "M. Moeller"], "title": "Inverting Gradients\u2013How easy is it to break privacy in federated learning?", "year": 2003}, {"authors": ["B. Zhao", "K.R. Mopuri", "H. Bilen"], "title": "idlg: Improved deep leakage from gradients.", "venue": "arXiv preprint arXiv:2001.02610", "year": 2020}, {"authors": ["L. Zhu", "Z. Liu", "S. Han"], "title": "Deep Leakage from Gradients.", "venue": "Neural Information Processing Systems", "year": 2019}, {"authors": ["A. Hor\u00e9", "D. Ziou"], "title": "Image Quality Metrics: PSNR vs. SSIM.", "venue": "20th International Conference on Pattern Recognition", "year": 2010}, {"authors": ["S.M. Stigler"], "title": "Francis Galton\u2019s Account of the Invention of Correlation.", "venue": "Statistical Science", "year": 1989}, {"authors": ["E.D. Cubuk", "B. Zoph", "D. Mane", "V. Vasudevan", "Q.V. Le"], "title": "Autoaugment: Learning augmentation strategies from data.", "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition", "year": 2019}, {"authors": ["A. Paszke"], "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library.", "venue": "Advances in Neural Information Processing Systems 32. Curran Associates, Inc.,", "year": 2019}, {"authors": ["A. Krizhevsky", "G. Hinton"], "title": "Learning multiple layers of features from tiny images.", "year": 2009}, {"authors": ["H. Xiao", "K. Rasul", "R. Vollgraf"], "title": "Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms. 2017", "year": 2017}, {"authors": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei-Fei"], "title": "ImageNet: A Large-Scale Hierarchical Image Database.", "year": 2009}, {"authors": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "title": "Deep residual learning for image recognition.", "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition", "year": 2016}, {"authors": ["W. Wei", "L. Liu", "M. Loper", "K.H. Chow", "M.E. Gursoy", "S. Truex", "Y. Wu"], "title": "A Framework for Evaluating Gradient Leakage Attacks in Federated Learning.", "venue": "CoRR abs/2004.10397", "year": 2020}], "sections": [{"text": "R E S C I E N C E C Replication / ML Reproducibility Challenge 2021\n[Re] Replication study of \u201dPrivacy-preserving"}, {"heading": "Collaborative Learning\u201d", "text": "Karolina Drabent1,2, ID , Stefan Wijnja1,2, ID , Thijs Sluijter1,2, ID , and Konrad Bereda1,2, ID 1Equal contributions \u2013 2University of Amsterdam, Amsterdam, the Netherlands\nEdited by Koustuv Sinha,\nSharath Chandra Raparthy\nReviewed by Anonymous Reviewers\nReceived 04 February 2022\nPublished 23 May 2022\nDOI 10.5281/zenodo.6574647\n1 Reproducibility Summary\n1.1 Scope of Reproducibility\nWe replicate Gao et al.1, which proposes an automatic search algorithm to find privacy\u2010 preserving transformation policies to protect against gradient reconstruction attacks in a collaborative learning setting. All the main claims made by the authors were tested. We also extend the original experiments to a new dataset, and contribute a PyTorch Lightning framework to aid in future work.\n1.2 Methodology Weperformall experiments using themodel architectures, hyperparameters anddatasets as used in the original work. We also extend the experiments to a new dataset. We fur\u2010 ther contribute a reimplementation of the work in PyTorch Lightning to provide a mod\u2010 ular framework for future research into this area. All experiments are performed on Nvidia GTX 1080 GPUs. Our logs and checkpoints are made available for download via our code repository.\n1.3 Results Overall we find the original results to be reproducible; transformation policies found using Gao et al.1\u2019s method can defend against gradient reconstruction attacks, and these transformations have negligible impact on training efficiency andmodel accuracy. How\u2010 everwe do not observe the reported correlation between the proposed privacy\u2010scoreSpri and reconstruction PSNR.We also find that the degree of protection differs greatly from image to image, with poor protection in the worst case.\n1.4 What was easy The original paper was clearly written and the general idea was easy to follow. There was a codebase available in PyTorch and some of the reported experiments were repro\u2010 ducible using this code.\nCopyright \u00a9 2022 K. Drabent et al., released under a Creative Commons Attribution 4.0 International license. Correspondence should be addressed to Stefan Wijnja (stefan@stfwn.com) The authors have declared that no competing interests exist. Code is available at https://github.com/stfwn/ats-privacy-replication \u2013 DOI 10.5281/zenodo.6508136. \u2013 SWH swh:1:dir:1b0c9cb880eedcbdfb56c51afc8ed74ba437e14b. Open peer review is available at https://openreview.net/forum?id=SY84JTG73CK.\nReScience C 8.2 (#11) \u2013 Drabent et al. 2022 1\n1.5 What was difficult The available codebase was not clearly structured and needed non\u2010trivial work to run some of the experiments reported on in the paper. Therewere otherwise undocumented details in the code that had a large impact on experiment outcomes.\nCommunication with original authors\nGao et al.1 were contacted about multiple issues regarding implementation details and notation clarifications. The authors were very receptive to our questions, and most of these were resolved swiftly and constructively.\nReScience C 8.2 (#11) \u2013 Drabent et al. 2022 2\n2 Introduction\nCollaborative learning systems enable multiple clients to jointly train a machine learn\u2010 ingmodel. Each client locally holds a split of the training data, which they use to locally compute gradients [2] [3] [4]. These local gradients are then shared among all users to update the parameters of the shared model. This removes the need for any individual client to share potentially sensitive data, while still enabling all clients to benefit from a model trained on a larger dataset than they themselves own. This is an important quality in any field where data confidentiality is desired. As such, collaborative learning is used in applications from mobile networks [5] to autonomous driving [6] and health care [7]. However, it has been shown that training images may be recovered from the gradients that are shared to the network [8][9][10]. In general, these reconstruction attacks mask as harmless peers to obtain a shared model state and gradient from the victim, initialize a random input image and subsequently optimize this input such that themodel gradient closely matches the victim\u2019s gradient. The end result is an approximation of the victim\u2019s input image, breaking confidentiality and invalidating the core principles of this style of collaborative learning. Gao et al.1 propose a novel approach to mitigate the threat from reconstruction attacks by augmenting the local training data of the user before calculating the gradients [1]. The augmentation is aimed at making the reconstruction attack prohibitively difficult. The authors develop an automatic search algorithm to find the optimal transformation policies to augment the data and propose two novel metrics, Spri and Sacc, to increase the efficiency of this search. In this reproducibility report, we evaluate the main claims made by the authors of [1] by reproducing their techinques and experiments. Moreover, we assess the availability of hyperparameters and other information needed for reproducibility, and we discuss the usability of the provided codebase. We also extend the experimental setup towards a new dataset and contribute a new, PyTorch Lightning\u2010based framework to enable future work.\n3 Scope of reproducibility\nThe original paper [1] proposes using data augmentation to make gradient reconstruc\u2010 tion attacks in a collaborative learning setting prohibitively difficult. To find transfor\u2010 mation policies that achieve this goal, an automatic search algorithm is developed. Ad\u2010 ditionally, to make the proposed algorithm computationally feasible, the authors devise two novel metrics described in Section 4.2. We split these contributions into the follow\u2010 ing 7 claims and refer to them throughout this report.\n\u2022 Claim 1: By augmenting training samples with carefully\u2010selected transformation policies, reconstruction attacks become infeasible. \u2022 Claim 2: The proposed search algorithm can find effective and general policies \u2013 policies that are able to defeat multiple variants of reconstruction attacks. \u2022 Claim 3: The found policies are highly transferable; good policies searched for one dataset are also suitable for another. \u2022 Claim 4: The found policies have negligible impact on training efficiency. \u2022 Claim 5: In general, a good policy is made up of transformations that distort the details of the training samples, while maintaining the semantic information. \u2022 Claim 6: The five transformations that work best are horizontal shifting (9), brightness (9), brightness (6), contrast (7) and contrast (6). Here, the number inside the brackets represents the intensity of the applied transformation. \u2022 Claim 7: Spri is a good measure of privacy; it is linearly correlated to Peak Signal\u2010 to\u2010Noise Ratio (PSNR) [11] with a Pearson Coefficient [12] of 0.697.\nReScience C 8.2 (#11) \u2013 Drabent et al. 2022 3\nIn [1], each of these claims is accompanied by one or more experiments, the results of which are reported in various tables and figures. In this reproducibility study we rerun the experiments and reproduce their tables and figures, with the addition that we report standard deviations across several experiments. In Section 6, wepresent our results side\u2010 by\u2010sidewith the original work. Then, in Section 7, we discuss the reproducibility of each experiment and evaluate the validity of the claims.\nIn addition to testing the above claims from the original paper, we present two exten\u2010 sions. Both of these extensions are aimed at testing the transferability of the searched policies as claimed in Claim 3.\n\u2022 Extension 1: Using the policies searched on one dataset and applying them to a new dataset can make reconstruction attacks against this new dataset infeasible \u2022 Extension 2: Since good policies share the same general qualities, as claimed by Claim 5, the five best transformations from Claim 6 are the same when using a different dataset.\nAgain, we show the results for these extension in Section 6, and relate them to the orig\u2010 inal claims, experiments and results in Section 7.\n4 Finding privacy-preserving transformation policies\nTheoriginal paper proposes an automatic search algorithm forfindingprivacy\u2010preserving transformation policies. To better understand this contribution, we describe what con\u2010 stitutes a transformation policy and how good policies are found within a reasonable time.\n4.1 Transformation policies Transformations or augmentations have been widely used to improve model perfor\u2010 mance and generalizability in deep learning. In [1], transformations from AutoAug\u2010 ment [13] are repurposed to protect sensitive training data from reconstruction attacks. The library contains 50 different transformations, including rotation, crop, shift, inver\u2010 sion, brightness, and contrast. A transformation policy is a combination of k such trans\u2010 formations applied sequentially to each of the training samples. In [1], k = 3 is chosen and the policies are denoted by the indices of the transformations within the AutoAug\u2010 ment library. It should be noted that while augmented samples are usually added to the training set, here the augmented version replace the originals. Therefore consistently applying the best policy to the data would risk a distribution shift in the dataset. Therefore, the au\u2010 thors propose the hybrid strategy, where a random policy from 3 candidate policies is used in order to preserve the input distribution [1].\n4.2 Reducing the search space To find candidate policies, it is necessary to determine their effect on both privacy and accuracy. The transformations must be applied to training data, and a model must be trained. Because fully training a model is computationally expensive, the authors propose two metrics that serve as a proxy for the privacy preservation and accuracy of the fully trained model: a privacy score (Spri) and an accuracy score (Sacc). Low Spri means the model has high privacy preservation potential, and high Sacc means the model achieves good accuracy with the applied transformation policies. These metrics produce results onmodel that are trainedwith only 10%of the data for only 25%training\nReScience C 8.2 (#11) \u2013 Drabent et al. 2022 4\niterations, reducing the search space and making the policy search feasible in a reason\u2010 able time. Further details about the definition of Spri and Sacc can be found in sections 4.2 and 4.3 of [1].\n5 Experimental setup and code\nTo verify the claims made by the authors of [1], we reproduce their experiments. These experiments roughly fall into four categories: evaluating the effectiveness of the searched policies against reconstruction attacks, testing the transferability of the searched poli\u2010 cies on different datasets and models, checking the impact on model efficiency, and studying the semantics behind the different transformations. Multiple models must be trained on augmented and un\u2010augmented data for all these categories. For the attacks, the approach from [8] is applied. Section 6 provides a detailed description of the exper\u2010 iments and shows the results. To reproduce the experiments performed by the authors, we used their existing code\u2010 base1, which is implemented in PyTorch [14]. We reimplemented the code in PyTorch Lightning2, which leverages the interface advantages of the Lightning framework to make running experiments, logging results and extending the work more intuitive. It is publicly available at github.com/stfwn/ats-privacy-replication.\n5.1 Datasets\nThe experiments in [1] are performed on two datasets, CIFAR\u20101003 [15], and Fashion\u2010 MNIST4 [16]. CIFAR\u2010100 contains 60, 000 color images of size 32 \u00d7 32, from 100 classes. The test set is used as the validation set, consistent with the authors\u2019 codebase. On the other hand, the Fashion\u2010MNIST dataset contains 70, 000 grey\u2010scale images of 28\u00d728 res\u2010 olution from 10 classes. Again the test set is used as a validation set. We run experiments on one additional dataset in our extensions \u2010 Tiny ImageNet2005 [17]. It contains 120,000, 64\u00d764 RGB images of 200 different classes. However, a tiny version of the dataset is also introduced in the original paper for policy\u2010search purposes. It is is a different dataset and this version contains 10% of the original samples, using the same distribution. It\u2019s later used to train the models for the evaluation of Spri and Sacc in the search algorithm.\n5.2 Model descriptions We use the following models:\n\u2022 ResNet20\u20104, a variation of ResNet20 [18] that has four times the number of chan\u2010 nels also used in [8]. The total number of parameters is 4.4M. \u2022 ConvNet [8] \u2013 an 8\u2010layer Convolutional Neural Network, with batch normalization and a ReLU layer after each convolution layer. For this model the total number of parameters is 3.7M.\nThe original codebase uses the implementation of both models from the repository6 of [8]. Our models are exact reimplemations in Pytorch Lightning.\n1https://github.com/gaow0007/ATSPrivacy 2https://github.com/PyTorchLightning/pytorch-lightning 3https://www.cs.toronto.edu/~kriz/cifar.html 4https://github.com/zalandoresearch/fashion-mnist 5http://cs231n.stanford.edu/tiny-imagenet-200.zip 6https://github.com/JonasGeiping/invertinggradients\nReScience C 8.2 (#11) \u2013 Drabent et al. 2022 5\n5.3 Hyperparameters For the policy search, we set Cmax = 1500 and max policies equal to 10. The batch size was 128, and the number of transforms in policy was 3. For training, the batch size was also 128 and the number of epochs was 60 (see Section 5.4). To obtain a semi\u2010trained network, we used a subset of 10% of the training dataset. Attacks are performed on the first six images of the test set, and we used the configu\u2010 rations as described in the original paper. An exception is the experiment that led to Figure 2, where the original configuration was unclear.\n5.4 Computational requirements We ran our experiments using an Nvidia GeForce GTX 1080 GPU. The policy search took approximately 10 hours. The training of one model took approximately 2h 40min using the original approach. However, training for 60 epochs achieves the same accuracy in only 50 minutes by reducing the learning rate sooner, thereby skipping long periods of stagnation. Performing one reconstruction attack in 2500 iterations took approximately 5 minutes. Measuring the correlation between Spri and PSNR therefore took 8.5 hours (including policy search).\n6 Experiments and results\n6.1 Results reproducing original paper Experiment 1 A reconstruction attack on 100 images from the CIFAR\u2010100 validation set is performed with and without a searched transformation policy applied. We doc\u2010 ument the optimization process of the attack in terms of GradSim. The model used is ResNet20, trained on the tiny dataset for 50 epochs. The results of this experiment are shown in Figure 1a, which shows a very similar result to the original paper shown in 1b. In addition to the original figure, we show the standard deviation over the 100 images, since GradSim can differ significantly from image to image. When taking the average of multiple runs, it can be seen that the privacy\u2010aware transform does indeed make the GradSim convergence more difficult.\nExperiment 2 A visual comparison between reconstructed images with and without a searched transformation policy applied is performed for both ResNet20 and ConvNet on images from CIFAR\u2010100 and Fashion\u2010MNIST. The optimizer used in the attack is Adam+Cosine. The images, the resulting reconstructions, and their PSNR values are shown in the left half of Figure 3. The results from the original paper are shown at the right side of Figure 3. As can be seen, the images used and PSNR values reported are different. This is due to the fact that it was too expensive to identify the exact same im\u2010 ages and PSNR values differ quite severely depending on the image used. However, for\nReScience C 8.2 (#11) \u2013 Drabent et al. 2022 6\nall 12 images, we observe a less pronounced visual effect of the transformation policy as well as a smaller gap in PSNR values between the reconstructions with and without the policies applied. This implicates that the effect shown in the original paper is not as severe for all images, although the images we selected may be particularly easy to reconstruct.\nExperiment 3 To gain further insight into the effectiveness of the different policies, we report the qualitative and quantitative results of Adam+Cosine attacks and model accu\u2010 racy for the datasets and models in Figure 3. The results are calculated over 6 images as performing the experiment is very expensive and number wasn\u2019t stated in the paper. The policies considered and the results are listed in Table 1.\nReScience C 8.2 (#11) \u2013 Drabent et al. 2022 7\nTable 1 shows similar patterns to the original paper, where the searched policies have low PSNR values compared to not using transformations. We do observe that PSNR values have a relatively high standard deviation, and during our experiments, we found that the policies do not form a good defense for some images. This problem will be further discussed in Section 7.\nExperiment 4 Thedefensive qualities of the searched transformationpolicies are bench\u2010 marked against existing defenses from the literature [10] [19] under the Adam+Cosine attack. The results are shown in Table 2. Although the exact values differ slightly, the overall results are similar to the original paper, where all the existing defenses perform worse than the hybrid strategy.\nExperiment 5 This experiment concerns Claim 2. Because policies should be general, they are tested against various attack configurations. For this, we again use 6 images from the test set and perform the different attacks on the images without the transfor\u2010 mation policies applied and with the hybrid strategy transformation policies applied. The results are shown in Table 3. As can be seen from the table, the hybrid strategy works well against all configurations of the reconstruction attack. This is in line with the results from the original paper.\nExperiment 6 This experiment concerns the transferability of Claim 3. To test this, the policies searched on CIFAR\u2010100 are applied to Fashion\u2010MNIST using both ResNet20 and ConvNet. Reconstruction attacks are performed with the Adam+Cosine attack. The\nReScience C 8.2 (#11) \u2013 Drabent et al. 2022 8\nresulting PSNR values and accuracies are listed in Table 5. The results differ from the original. It can be seen that the transformation policies are not effective here.\nExperiment 7 The following experiment is aimed at Claim 4. The authors state that applying the search policies has a negligible impact on training efficiency. We trained ResNet20 with the searched policies applied and documented the loss and accuracy con\u2010 vergence to test this. From Figure 4 it can be seen that indeed applying transformations has almost zero impact on the training efficiency. It is also noteworthy to observe that the training curves are almost identical compared with the results from the original work.\nReScience C 8.2 (#11) \u2013 Drabent et al. 2022 9\nExperiment 8 Claim 5 states that good transformation policies obfuscate details in the training samples butmaintain high\u2010order semantic information. As such, attackers will have trouble reconstructing high frequency information. We test this by comparing the attacker\u2010defender gradient similarity during an attack of models trained with the searched policy, a random policy, and no policy applied. From Figure 5, it can be seen that in shallow layers, the gradients differ significantly, whereas in deep layers, the gra\u2010 dients are very similar. This implies that the transformations do indeed have the desired effect and is in line with the results from the original paper.\nExperiment 9 InClaim6 the authors report their 5 top transformations. We testwhether we can find the same ones by calculating the privacy score on the dataset for each indi\u2010 vidual augmentation and show the results in Figure 6a and 6b. Out of the best 5 trans\u2010 formations reported in the original paper we found 4 overlapping ones.\nExperiment 10 The final experiment reproducing the results from the original paper is aimed at Claim 7. The authors claim that their privacy\u2010score Spri is linearly corre\u2010\nReScience C 8.2 (#11) \u2013 Drabent et al. 2022 10\nlated with PSNR with a Pearson\u2010coefficient of 0.697. We test this by running attacks and evaluating Spri on the model trained on tiny cifar100 for 50 epochs and found a very dif\u2010 ferent result. As shown in Figure 2 there is hardly any correlation (Pearson\u2010coefficient is 0.123). This might be due to the fact that these 100 transformation policies are selected at random out of 127.550 possible options. This is a striking result nonetheless, which we discuss in\u2010depth in Section 7.\nExtension2 Weadditionally extend the evaluationof the transferability of the searched policies by testing which transformations work best on a different dataset. Since good policies share the same general qualities, as stated in Claim 5, the five best transforma\u2010 tions from Claim 6 are expected to be the same when using a different dataset. For this experiment, we use the Rescaled ImageNet dataset. The resulting transformations are shown in Figure 6c. Out of the 5 best transformations on the Rescaled ImageNet 3were also found on CIFAR\u2010100 in both our results and the results from the original paper. This shows that, indeed, these transformations contain the desired qualities from Claim 6.\n7 Discussion\nOverall the results in [1] are reproducible, with the exception of Figure 2, where a large discrepancy between our result and the original one exists. Nevertheless, augmentation policiesmayworkwell as a defensemechanismagainst reconstruction attacks. Formost images, an attacker using reconstruction attacks is unable to find privacy\u2010sensitive in\u2010 formation. However, the standard deviation of our results is high, and we consider this a valuable metric to contribute. Some images are vulnerable to the attack even with the proposed defense mechanism, and it is as of yet unclear to us which types of images are more vulnerable than others. This issue must be examined further in future research\nReScience C 8.2 (#11) \u2013 Drabent et al. 2022 11\nto make the approach widely applicable in real\u2010world use\u2010cases where private data is at stake in order to provide predictable privacy guarantees. Additionally, we made observations in the codebase that, to the best of our knowledge, were not reported in the paper or any other accompanying documentation. The first observation was that the loss function that was used included a factor of 0.5. This is not a fundamental flaw during the training phase, as it simply results in smaller gradients and therefore leads to a reduced effective learning rate. However, during the reconstruction attacks, the loss used by the attacker was not multiplied by this factor. This means the attacker in practice uses a different loss function from the one used to generate the gradient that it is attempting to match. This may therefore make recon\u2010 struction more difficult. In our correspondence, Gao et al.1 acknowledged this as a bug and we fixed this in our experiments. The second observation was that two other undocumented augmentations were added in all experiments, namely a random crop and random horizontal flip. Without these, the accuracy of our models decreased by over 10%.\n7.1 What was easy The explanation of the general idea, identified problems and proposed solution of the paper was clearly put and easy to follow. The codebase contained a README with in\u2010 structions on how to run some of the paper\u2019s experiments, and these instructions could be followed without significant problems. Some of the experiments in the original work could be reproduced by running the code as provided.\n7.2 What was difficult The most challenging part about reproduction was the unclear description of experi\u2010 ments in the paper and limited clarity in the codebase. Code in the repository was un\u2010 commented, used many global variables and many layers of indirection. Many chunks of code were not used. Some experimental settings and metrics were not implemented, and some experiment configurations led to fatal errors. It was unclear which steps were originally followed to obtain Figure 2. Despite the au\u2010 thors\u2019 helpful comment on which model was used, we were not able to reproduce the correlation, potentially due to randomness in a vast search space and a relatively lim\u2010 ited sample size. Furthermore, the paper does not state how many images were used to produce the PSNR values in the tables. Finally, undocumented augmentations were added in some but not all settings, which was cause for some delay until this was found to be the cause for a 10% accuracy gap with the authors\u2019 results.\n7.3 Communication with original authors We contacted the authors about multiple clarifications regarding implementation de\u2010 tails and notation in the paper. The authors responded promptly and answered most of our questions in the first round of contact. Firstly, regarding our reproduction of Figure 2, the correspondence was helpful but did not lead to a reproduction."}], "title": "[Re] Replication study of \u201dPrivacy-preserving Collaborative Learning\u201d", "year": 2022}