{"abstractText": "This report contains a set of experiments that seek to reproduce the claims of two recent works related to keypoint estimation, one specific to 6DoF object pose estimation, and the other presenting a generic architectural improvement for keypoint estimation but demonstrated in human pose estimation. More specifically, in the backpropagatable PnP [1], the authors claim that incorporating geometric optimization in a deep-learning pipeline and predicting an object s\u0313 pose in an end-to-end manner yields improved performance. On the other hand, HigherHRNet [2] introduces a novel heatmap aggregation method that allows for scale-aware pose estimations, offering higher keypoint localization accuracy for small scale objects.", "authors": [{"affiliations": [], "name": "Georgios Albanis"}, {"affiliations": [], "name": "Nikolaos Zioulis"}, {"affiliations": [], "name": "Anargyros Chatzitofis"}, {"affiliations": [], "name": "Anastasios Dimou"}, {"affiliations": [], "name": "Dimitrios Zarpalas"}, {"affiliations": [], "name": "Petros Daras"}, {"affiliations": [], "name": "Koustuv Sinha"}, {"affiliations": [], "name": "Jessica Zosa Forde"}], "id": "SP:3f0c1d698486ebe1aecbbefd42ace373cfb49de7", "references": [{"authors": ["B. Chen", "A. Parra", "J. Cao", "N. Li", "T.-J. Chin"], "title": "End-to-End Learnable Geometric Vision by Backpropagating PnP Optimization.", "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition", "year": 2020}, {"authors": ["B. Cheng", "B. Xiao", "J. Wang", "H. Shi", "T.S. Huang", "L. Zhang"], "title": "HigherHRNet: Scale-Aware Representation Learning for Bottom-Up Human Pose Estimation.", "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition", "year": 2020}, {"authors": ["G. Albanis", "N. Zioulis", "A. Dimou", "D. Zarpalas", "P. Daras"], "title": "DronePose: Photorealistic UAV-Assistant Dataset Synthesis for 3D Pose Estimation via a Smooth Silhouette Loss.", "venue": "European Conference on Computer Vision. Springer", "year": 2020}, {"authors": ["T. Sattler", "Q. Zhou", "M. Pollefeys", "L. Leal-Taixe"], "title": "Understanding the limitations of cnn-based absolute camera pose regression.", "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition", "year": 2019}, {"authors": ["K. Sun", "B. Xiao", "D. Liu", "J. Wang"], "title": "Deep high-resolution representation learning for human pose estimation.", "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition", "year": 2019}, {"authors": ["A. Newell", "K. Yang", "J. Deng"], "title": "Stacked hourglass networks for human pose estimation.", "venue": "European conference on computer vision. Springer", "year": 2016}, {"authors": ["V. Lepetit", "F. Moreno-Noguer", "P. Fua"], "title": "Epnp: An accurate o (n) solution to the pnp problem.", "venue": "In: International journal of computer vision", "year": 2009}, {"authors": ["N. Ravi", "J. Reizenstein", "D. Novotny", "T. Gordon", "W.-Y. Lo", "J. Johnson", "G. Gkioxari"], "title": "Accelerating 3D Deep Learning with PyTorch3D.", "venue": "Albanis et al", "year": 2020}], "sections": [{"text": "R E S C I E N C E C"}, {"heading": "Replication / ML Reproducibility Challenge 2020", "text": "[Re] On end-to-end 6DoF object pose estimation and robustness to object scale\nGeorgios Albanis1, ID , Nikolaos Zioulis1, ID , Anargyros Chatzitofis1, ID , Anastasios Dimou1, ID , Dimitrios Zarpalas1, ID , and Petros Daras1, ID 1Centre for Research and Technology Hellas, Thessaloniki, Greece\nEdited by Koustuv Sinha, Jessica Zosa Forde\nReviewed by Anonymous Reviewers\nReceived 29 January 2021\nPublished 27 May 2021\nDOI 10.5281/zenodo.4833219"}, {"heading": "Reproducibility Summary", "text": ""}, {"heading": "Scope of Reproducibility", "text": "This report contains a set of experiments that seek to reproduce the claims of two recent works related to keypoint estimation, one specific to 6DoF object pose estimation, and the other presenting a generic architectural improvement for keypoint estimation but demonstrated in human pose estimation. More specifically, in the backpropagatable PnP [1], the authors claim that incorporating geometric optimization in a deep-learning pipeline and predicting an object s\u0313 pose in an end-to-end manner yields improved performance. On the other hand, HigherHRNet [2] introduces a novel heatmap aggregation method that allows for scale-aware pose estimations, offering higher keypoint localization accuracy for small scale objects."}, {"heading": "Methodology", "text": "We used the publicly provided code where available, adapting it to fit into amodel development kit to facilitate our experiments. We used a dataset fit for validating both claims simultaneously, and designed a set of experiments based on the published methodologies, but also went beyond seeking to validate the higher level concepts. Our experiments were conducted on a Nvidia 2080 12 GB GPU with an average training time of 14 hours."}, {"heading": "Results", "text": "We reproduce the claims of both papers by conducting several experiments in the UAVA dataset [3]. The integration of a differentiable geometric module within an keypointbased object pose estimation model improved its performance in metrics. We additionally verify that this is the case for other differentiable PnP implementations (i.e. EPnP). Further, our results indicate that indeed HigherHRNet improves keypoint localisation performance on small scale objects.\nCopyright \u00a9 2021 G. Albanis et al., released under a Creative Commons Attribution 4.0 International license. Correspondence should be addressed to Georgios Albanis (galbanis@iti.gr) The authors have declared that no competing interests exist. Code is available at https://github.com/tzole1155/EndToEndObjectPose. \u2013 SWH swh:1:dir:5e754c06b0dac970857b31733766fb9a268edee6. Data is available at https://vcl3d.github.io/UAVA/ \u2013 DOI 10.5281/zenodo.3994337. Open peer review is available at https://openreview.net/forum?id=PCpGvUrwfQB.\nReScience C 7.2 (#2) \u2013 Albanis et al. 2021 1"}, {"heading": "What was easy", "text": "Both papers provided publicly available implementations. In addition, many different variations were also found online. Finally, the papers themselves were very clearly written, offering insights on various important details."}, {"heading": "What was difficult", "text": "The main issue that required more effort was identifying the appropriate weights for BPnP [1] in order to balance the different optimization objectives. As expected, this varies for the context that it is applied (task, dataset) and the values presented in the paper did not work in our case. Sub-optimal selection of weights leads to convergence issues.\nCommunication with original authors We communicated with the authors of [1] through GitHub, and we would like to thank them as they provided a fast and detailed response. Furthermore, their responsiveness to past issues had already provided a nice knowledge base regarding reproduction.\nReScience C 7.2 (#2) \u2013 Albanis et al. 2021 2\n1 Introduction\nObject pose estimation seeks to determines the 3D position and orientation of an object in camera-centred coordinates. During the last years, two main directions have been emerged for data-driven 6DoF object pose estimation; direct pose regressionwhich predict pose in an end-to-end manner, and indirect that learns the surrogate task of keypoint localisation and then solves a Perspective-n-Point (PnP) problem to estimate the resulting pose. Even though it has been shown [4] that the latter methods better approach the problem, there are still open challenges that need to be solved. One issue is the splitting between the actual task at hand, and the surrogate task that they learn. The other has to do with the spatial nature of keypoint localisation and smaller scale objects. Recently, two works have been presented that seek to address these issues, BPnP [1] and HigherHRNet [2]. In this work, we seek to reproduce and verify their claims in a task that is relevant for both of these works, drone pose estimation. While BPnP s\u0313 relation has to dowith the task at hand, HigherHRNet is also relevant because commodity drones are usually small form objects, and when flying around the further distance themselves from the operator, effectively reducing their scale in the cameras\u0313 image domain.\n2 Scope of reproducibility\nConsequently, we opt for reproducing the claims of both of these two relevant papers addressing the aforementioned challenges. In more details, the authors of BPnP [1] propose a novel differentiable module which calculates the derivatives of a PnP solver through implicit differentiation, enabling the backpropagation of its gradients to the network parameters, and as such allowing for end-to-end optimization and learning. On the other hand, the authors of HigherHRNet [2] focus on improving the 2d landmarks\u02bc localization performance for smaller-scale humans by proposing a novel multi-scale supervision scheme for training and a heatmap aggregation module for inference. The main claims of both papers can be summarised below:\n\u2022 BPnP: An end-to-end trainable pipeline for object pose estimation, can achieve greater accuracy by combing the reprojection losses (Table 3).\n\u2022 HigherHRNet: Anovelmethod for learning scale-aware representations usinghighresolution feature pyramids, eventually achieving greater results for small scale objects1 (Table 4).\n3 Methodology\nWe implemented our experiments by re-using the publicly available implementation for BPnP, and implementing HigherHRNet after styding the paper, the original publicly available implementation, as well as other implementations. In both cases we integrated the code base in a modular framework that facilitates reproducible experiments [5], which generally required slight modifications of the original code provided by the authors to fit its requirements. The overall methodology for our experiments is depicted in Figure 1. On the left, a traditional monocular heatmap-based keypoint localisation pipeline is presented, whereas on the right, the BPnP required components are illustrated. BPnP: BPnP focuses on the Pose Retrieval stage, and following [1] we trained our model under the 3 different schemes used in the original work as well:\n1We apply the proposed module in the object pose estimation task, while authors originally demonstrated it for the human-pose estimation task, but its concept still applies in our case as well.\nReScience C 7.2 (#2) \u2013 Albanis et al. 2021 3\n\u2022 heatmap loss (lh),\n\u2022 mixture loss lm = lh + \u03b2 \u2217 lproj ,\n\u2022 and pose loss lp = lreg + lproj ,\nwhere lproj = \u2225\u03c0(z|y,K)\u2212x\u2217\u222522 and lreg = \u2225x\u2212\u03c0(z|y,K)\u222522. Also, \u03c0 is the projection function employing the predicted pose(y) from the PnP solver, the corresponding object s\u0313 3D points z and K the camera intrinsic matrix. Apart from these experiments presented also in the original paper, we conducted an extra set of experiments that aimed at validation the concept of end-to-end 6DoF pose estimation via differentiable PnP. We used another openly available differentiable PnP implementation, and additionally, also tested the faster counterpart of BPnP. We present results across many established object pose estimation metrics, as well as computational performance metrics for all the aforementioned experiments. HigherHRNet: On the other hand, for HigherHRNet we focused on the Heatmap regression part by using different models for the decoder part of the architecture, with details following in Section 3.1. All the code and its documentation are submitted and published along with this report.\n3.1 Model descriptions The following models were used as our backbone for regressing the heatmaps and the corresponding coordinate spatial distributions (the decoder part in Figure 1):\n\u2022 HRNet [6] with feature maps of width 48 and 3 stages. The 2nd, and 3rd, stages contain 1, 4 exchange blocks, respectively, and each exchange block contains 4 residual units.\n\u2022 HigherHRNet [2] with feature maps of width 48 and 3 stages. The 2nd, and 3rd, stages contain 1, 4 exchange blocks, respectively, and each exchange block contains 4 residual units.\n\u2022 Stacked Hourglass [7] with depth 2 and feature maps of width 128\nFollowing the heatmap predictions, we apply a decoding operation (i.e. center of mass specifically) in order to extract the keypoints from the heatmaps, which are later driven to the PnP algorithm for retrieving the 6D pose. In addition, we integrated the EPnP [8] algorithm, using the available implementation in Pytorch3D [9] instead of BPnP to assess whether other \u2013 similar in concept \u2013 implementations can verify the claim and quantify the differences between these approaches. As\nReScience C 7.2 (#2) \u2013 Albanis et al. 2021 4\na side-note, it should be mentioned that our models\u02bc configuration slightly differs from the ones described in the original works in order to comply with the working resolution of the dataset we used.\n3.2 Datasets\nUAVA \u2014 As aforementioned, our experiments were conducted on a dataset that allowed for the validation of both works simultaneously. This also better helps in deducing whether the claims are reproducible as the context (task or dataset) can vary. We used theUAVAdataset2 for object pose estimation. UAVA targets human-robot cooperativeUnmanned Aerial Vehicle(UAV) applications and offers two different dronemodels, namely DJI M2ED3 and Ryze Tello4. The UAVA dataset provides the 3D models of both drones accompanied by ground-truth annotations such as 3D bounding boxes, 6D pose, and at the same time multi-modal data. More importantly, the difference in size between the two drone models allows for the validation of scale-invariant pose estimation.\nPreprocessing \u2014Weprocessed the original dataset in order to keeponly the sampleswhere all the 2D keypoints are within the image, given that BPnP relies on softly approximating the coordinate, and that would fail in the case of out of field-of-view keypoints. However, we shouldmention that we did not apply any other filtering (i.e. visibility of all the keypoints, boundary cases, etc.).\n3.3 Hyperparameters We train all the models for 44 epochs and select the best performing model for testing. We used the Adam optimizer with a learning rate of 1e\u2212 4, betas of values 0.9 and 0.999 and no weight decay, and a seed value of 1989 for ensuring reproducibility. Albeit, we experimented with different losses (i.e. KL, MSE) for lh, we found that L1 loss works the best, offering the best results and faster convergence. This could be attributed to the different resolution of the heatmaps grid (in our case is lower) as well as the different configuration of the heatmap decoder model (we used 3 stages instead of 4). It is worth mentioning that we also tried a bigger heatmap resolution (e.g. 160 \u00d7 120) although we decided to conduct our final experiments in the lower resolution for two main reasons. First, most heatmaps regression decoders used in the literaturemake their prediction in the 1/4 of the original image, and second, this higher heatmap resolution would enforce us to further reduce the depth of the decodermodel. Specifically, for BPnPwe set \u03b2 value to 1e \u2212 5 after conducting a greedy heuristic search, with values ranging from 0.001 to 1e\u2212 9, as the proposed value for \u03b2 coefficient, did not work for our case. The selection of a non-appropriate \u03b2 coefficient value can lead to stability issues as noted in Section 5.2.\n3.4 Experimental setup and code As mentioned above, we integrated the authors\u02bc code (BPnP) or our own reimplementations (HigherHRNet) in [5] which is a PyTorch framework for modular and reproducible workflows5. Each model is implemented in a configuration file that defines the different components (optimizer, datasets, model architecture, pre-/post-processing graphs, etc.) and logs all hyperparameters. For each experiment we report the standardmetrics below:\n2https://vcl3d.github.io/UAVA/ 3https://www.dji.com/gr/mavic-2-enterprise 4https://www.ryzerobotics.com/tello 5www.github.com/ai-in-motion/moai\nReScience C 7.2 (#2) \u2013 Albanis et al. 2021 5\nNPE: is the magnitude (L2-norm) of difference between the ground-truth and estimated position vectors from the origin of the camera reference frame to that of the drone body frame, normalised with ground-truth vector. AD: is the angular distance between the predicted, rotationmatrix, and ground-truth,or in other words, the magnitude of the rotation that aligns the drone body frame with the camera reference frame. ACC: considers an estimated pose to be correct if its rotation error is within k\u25e6 and the translation error is below k cm. ADD: is the average distance metric to compute the averaged distance between points transformed using the estimated pose and the ground truth pose. Eventually, a pose estimation is considered to be correct if the computed average distance is within k% of the model diagonal. Proj: is the mean distance between 2D keypoints projected with the estimated pose and those projected with ground truth pose. An estimated pose is considered correct if this distance is within a threshold k.\n3.5 Computational requirements Table 1 showcases the total duration of each experiment (with a 24 batch size) as well as some other useful statistics such as the mean duration time for a forward pass, a backward pass, an optimizer step, as well as the total test duration with batch size 1. It is clear, that the introduction of the differentiable PnP modules in the training procedure increases the total training time significantly, as the backward and step operation require more time. We ran our experiments on a machine with the specifications presented in Table 2.\n4 Results\nOur results support the claims presented by both authors in [1] and [2] respectively. As is demonstrated in Table 3, the model trained with lp achieved better results in most of themetrics for both dronemodels. Similarly, Table 4 indicates that HigherHRNet yields better results for the small-scale drone inmost of themetrics, although its performance for the bigger M2ED drone is worse compared to the standard HRNet model.\n4.1 Results reproducing original papers\nBPnP \u2014With these experiments we show that the addition of a differentiable PnP module improves the performance in object pose estimation task. We provide qualitative results in Figure 3. It is worth highlighting that training with lp does not restrict the shape of the distribution the way that it is constrained when relying on heatmap supervision (i.e. Gaussian distribution approximation). Instead, the model freely localizes the keypoints, which results in more focused predictions. This is illustrated in Figure 2 where qualitative results display the heatmaps on top of the color images."}, {"heading": "Drone NPE\u2193 AD\u2193 ACC2\u2191 ACC5\u2191 ADD2\u2191 ADD5\u2191 Proj2\u2191 Proj5\u2191", "text": "HigherHRNet \u2014 These experiments showcase that the addition of the aggregation module improves the keypoint localization performance when targeting smaller-scale objects. Specifically, the HigherHRNet architecture gives better results in most of the metrics for the small form drone (Tello). On the other hand though, this is not the case for the larger drone, where the HigherHRNet performance is slightly worse than the standard HRNet s\u0313 one.\ndifferentiable PnP algorithm (i.e. EPnP) and the results are demonstrated in Table 5. We also provide extra experiments of a BPnP implementation inwhich the calculation of the higher-order derivatives is ignored from the coefficient s\u0313 graph as presented in Table 6.\nBPnP vs EPnP \u2014 For this experiment we utilised the same backbone (i.e. HRNet) but we changed the BPnP module with the EPnP. We followed the exact same training procedure, hyperparameters, as well as the same loss lm. Results are summarized in Table 5. It is evident that EPnP and BPnP offers comparable results in most of the metrics.\nBPnPfaster \u2014 Authors in [1] provided an alternative method for calculating the gradients through the PnP layer, which essentially is the samemethod as the original, although ignoring the higher-order derivatives from the coefficients graph. Therefore, we provide results using this faster BPnP method in Table 6, comparing the two different versions, as well as their training times in Table 7. It seems that the original version outmatches the faster one, albeit there is no significant performance drop. On the other hand, Table 7 indicates how the second implementation justifies its name. So, it is in users\u02bc flu-\nReScience C 7.2 (#2) \u2013 Albanis et al. 2021 8"}, {"heading": "Drone NPE\u2193 AD\u2193 ACC2\u2191 ACC5\u2191 ADD2\u2191 ADD5\u2191 Proj2\u2191 Proj5\u2191", "text": "ency whether they need to sacrifice gradient accuracy and some performance drop in exchange for efficient training times."}, {"heading": "Drone NPE\u2193 AD\u2193 ACC2\u2191 ACC5\u2191 ADD2\u2191 ADD5\u2191 Proj2\u2191 Proj5\u2191", "text": "After conducting several experiments on the UAVA dataset, the central claims of [1] and [2] stand true; as they both outperform other methods. Particularly, for validating BPnP we conducted the same experiments as the original paper, and further, we compare it with another differentiable PnP method (i.e. EPnP). The inclusion of 2D-3D geometry constraints through differentiable geometric optimization, improves the performance. Extending the experiments of the original paper, we compare another implementation of the BPnPmodule which ignores the high order derivatives from the coefficient graph. This module achieves comparable results as its counterpart apart it is much faster. It is worth noting, that both BPnP, and EPnP are quite time-consuming as demonstrated in Table 1. Finally, we study the performance of the HigherHRNet [1] in a very challenging small scale object. Indeed, the performance of the proposed heatmap aggregation module achieves better results when compared with other well-established methods.\n5.1 What was easy Implementing most of the code was straightforward as authors of both papers provide source code. GitHub issues were another source of retrieving information, clarifying parts of the papers when needed. Additionally, both of the original papers are quite complete, well-written making it easy to follow.\nReScience C 7.2 (#2) \u2013 Albanis et al. 2021 9\n5.2 What was difficult Ourmajor difficulty was related to finding the appropriate value for balancing the terms of mixture loss lm, aka the \u03b2 value. Even though, authors in [1] provided the value that they used for their experiments this did not work for us, as this is a case specific parameter. It is worth noting that a non-appropriate selection of the balancing term can lead to convergence issues and negative results. Even though, not related with the code of both of the papers, we feel that it would be constitutive to mention that we faced the same difficulties when trying to incorporate EPnP in our workflow.\n5.3 Communication with original authors Authors of [1] did not specify the configuration of the used network in the pose estimation task, nor the hyperparameters. Thus, we contacted them through GitHub where they provided a detailed answer, available now to the research community. We did not contact HigherHRNet authors [2] as the online implementations and the text and figures in their paper were a good enough guide to understand and implement it."}, {"heading": "Acknowledgements", "text": "This researchhas been supportedby theEuropeanCommission fundedprogramFASTER, under H2020 Grant Agreement 833507.\nReScience C 7.2 (#2) \u2013 Albanis et al. 2021 10"}, {"heading": "1. B. Chen, A. Parra, J. Cao, N. Li, and T.-J. Chin. \u201cEnd-to-End Learnable Geometric Vision by Backpropagating PnP", "text": "Optimization.\u201d In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020, pp. 8100\u20138109. 2. B. Cheng, B. Xiao, J. Wang, H. Shi, T. S. Huang, and L. Zhang. \u201cHigherHRNet: Scale-Aware Representation Learning for Bottom-Up Human Pose Estimation.\u201d In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020, pp. 5386\u20135395. 3. G. Albanis, N. Zioulis, A. Dimou, D. Zarpalas, and P. Daras. \u201cDronePose: Photorealistic UAV-Assistant Dataset Synthesis for 3D Pose Estimation via a Smooth Silhouette Loss.\u201d In: European Conference on Computer Vision. Springer. 2020, pp. 663\u2013681. 4. T. Sattler, Q. Zhou, M. Pollefeys, and L. Leal-Taixe. \u201cUnderstanding the limitations of cnn-based absolute camera pose regression.\u201d In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019, pp. 3302\u20133312. 5. moai: Accelerating modern data-driven workflows. https://github.com/ai-in-motion/moai. 2021. 6. K. Sun, B. Xiao, D. Liu, and J. Wang. \u201cDeep high-resolution representation learning for human pose estimation.\u201d\nIn: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019, pp. 5693\u2013 5703. 7. A. Newell, K. Yang, and J. Deng. \u201cStacked hourglass networks for human pose estimation.\u201d In: European conference on computer vision. Springer. 2016, pp. 483\u2013499. 8. V. Lepetit, F. Moreno-Noguer, and P. Fua. \u201cEpnp: An accurate o (n) solution to the pnp problem.\u201d In: International journal of computer vision 81.2 (2009), p. 155. 9. N. Ravi, J. Reizenstein, D. Novotny, T. Gordon, W.-Y. Lo, J. Johnson, and G. Gkioxari. \u201cAccelerating 3D Deep Learning with PyTorch3D.\u201d In: arXiv:2007.08501 (2020).\nReScience C 7.2 (#2) \u2013 Albanis et al. 2021 11"}], "title": "[Re] On end-to-end 6DoF object pose estimation and robustness to object scale", "year": 2021}