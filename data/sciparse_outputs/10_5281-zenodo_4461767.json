{"abstractText": "Memory is the ability to store and retrieve information. We can distinguish procedural memory and declarativememory. Proceduralmemory is a type ofmemory that does not require conscious recall and is mostly related to motor tasks, while declarative memory is the ability of a conscious recall of information. Declarative memory can be itself divided into subcategories: semantic and episodic memory. Episodic memory stores past experiences and their emotional associations, while semantic memory stores and recalls facts independent of the context [1]. Memory, and especially semantic memory, can be tested in several ways. In an associa\u2010 tive learning task, two stimuli are mapped together e.g. two words. The subject is then presentedwith one element of the stimuli pair and has to recall the corresponding other \u2014 that is, to recall from a partial cue. Another test to assess memory is with free recall tasks. In this type of task, a subject is presented with a set of items to memorize. Later, the subject is asked to recall as many items as possible [2]. Previous literature has shown that recalling memory items in the absence of cues is a difficult task: subjects usually fail to recall more than short lists of items in a free recall task [3]. However, according to the Search of Associative Memory (SAM) model, associ\u2010 ations between memory items influence memory recall even in the absence of partial cues. From a neuroscience perspective, this could be explained by the overlaps between neuronal representations of memories [4, 5]. Recanatesi et al. [6] present a model of memory retrieval based on a Hopfield model for associative learning, with network dynamics that reflect associations between items due to semantic similarities. Indeed, transitions occur due to the activation of populations of neurons encoding for a memory item. This sequential activation of neuronal ensembles forms stable states at different domain regions of a periodic function, which provides inhibition to the net\u2010 work. Network dynamics are also compatible with empirical observations about free recall previously described [6]. In the present work, we proceed to replicate the model as presented by Recanatesi et al. [6]. During our replication efforts, we discover several errors in parameters and collab\u2010 orate with the original work authors to provide a successful replication and correct the original article.", "authors": [{"affiliations": [], "name": "Carlos de la Torre-Ortiz"}, {"affiliations": [], "name": "Aur\u00e9lien Nioche"}, {"affiliations": [], "name": "Olivia Guest"}], "id": "SP:10fc4919ff0c3327ba1015a0b7b0d319e8506a39", "references": [{"authors": ["L.R. Squire"], "title": "Memory and Brain Systems: 1969\u20132009.", "venue": "J Neurosci", "year": 2009}, {"authors": ["E. Tulving", "F.I.M. Craik"], "title": "The Oxford Handbook of Memory", "year": 2000}, {"authors": ["B.B.M. Jr."], "title": "The immediate retention of unrelated words.", "venue": "J Exp Psychol", "year": 1960}, {"authors": ["J.G.W. Raaijmakers", "R.M. Shiffrin"], "title": "SAM: A Theory of Probabilistic Search of Associative Memory.", "venue": "In: Psychol. Learn. Motiv", "year": 1981}, {"authors": ["S. Romani", "I. Pinkoviezky", "A. Rubin", "M. Tsodyks"], "title": "Scaling Laws of Associative Memory Retrieval.", "venue": "Neural Computation", "year": 2013}, {"authors": ["S. Recanatesi", "M. Katkov", "S. Romani", "andM. Tsodyks"], "title": "Neural Network Model of Memory Retrieval.", "venue": "Frontiers in Computational Neuroscience", "year": 2015}, {"authors": ["J.J. Hopfield"], "title": "Neural networks and physical systemswith emergent collective computational abilities.", "venue": "In:Proc Natl Acad Sci", "year": 1982}, {"authors": ["D.J. Amit"], "title": "Modelling Brain Function: The World of Attractor Neural Networks", "year": 1992}, {"authors": ["D.O. Hebb"], "title": "The Organization of Behavior; A Neuropsychological Theory", "venue": "New York: Wiley,", "year": 1949}], "sections": [{"text": "R E S C I E N C E C Replication / Computational Neuroscience\n[Re] Neural Network Model of Memory Retrieval\nCarlos de la Torre-Ortiz1,2, ID and Aur\u00e9lien Nioche2, ID 1University of Helsinki, Department of Computer Science, Helsinki, Finland \u2013 2Aalto University, Department of Communications and Networking, Helsinki, Finland\nEdited by Olivia Guest ID\nReviewed by Sebastian Bobadilla-Suarez ID James Roach\nReceived 08 November 2019\nPublished 25 January 2021\nDOI 10.5281/zenodo.4461767\n1 Introduction\nMemory is the ability to store and retrieve information. We can distinguish procedural memory and declarativememory. Proceduralmemory is a type ofmemory that does not require conscious recall and is mostly related to motor tasks, while declarative memory is the ability of a conscious recall of information. Declarative memory can be itself divided into subcategories: semantic and episodic memory. Episodic memory stores past experiences and their emotional associations, while semantic memory stores and recalls facts independent of the context [1]. Memory, and especially semantic memory, can be tested in several ways. In an associa\u2010 tive learning task, two stimuli are mapped together e.g. two words. The subject is then presentedwith one element of the stimuli pair and has to recall the corresponding other \u2014 that is, to recall from a partial cue. Another test to assess memory is with free recall tasks. In this type of task, a subject is presented with a set of items to memorize. Later, the subject is asked to recall as many items as possible [2]. Previous literature has shown that recalling memory items in the absence of cues is a difficult task: subjects usually fail to recall more than short lists of items in a free recall task [3]. However, according to the Search of Associative Memory (SAM) model, associ\u2010 ations between memory items influence memory recall even in the absence of partial cues. From a neuroscience perspective, this could be explained by the overlaps between neuronal representations of memories [4, 5]. Recanatesi et al. [6] present a model of memory retrieval based on a Hopfield model for associative learning, with network dynamics that reflect associations between items due to semantic similarities. Indeed, transitions occur due to the activation of populations of neurons encoding for a memory item. This sequential activation of neuronal ensembles forms stable states at different domain regions of a periodic function, which provides inhibition to the net\u2010 work. Network dynamics are also compatible with empirical observations about free recall previously described [6]. In the present work, we proceed to replicate the model as presented by Recanatesi et al. [6]. During our replication efforts, we discover several errors in parameters and collab\u2010 orate with the original work authors to provide a successful replication and correct the original article.\nCopyright \u00a9 2021 C.D.L. Torre-Ortiz and A. Nioche, released under a Creative Commons Attribution 4.0 International license. Correspondence should be addressed to Carlos de la Torre-Ortiz (carlos.delatorreortiz@helsinki.fi) The authors have declared that no competing interests exist. Code is available at https://github.com/c-torre/replication-recanatesi-2015. \u2013 SWH swh:1:dir:0b541aeb4707ecedfbbcdd85adfd0100d748cc03. Open peer review is available at https://github.com/ReScience/submissions/issues/10.\nReScience C 6.3 (#8) \u2013 Torre-Ortiz and Nioche 2021 1\n2 Background\nHopfield [7] proposed a model in which memory storage and retrieval emerge as prop\u2010 erties of the collective behavior of its units, or neurons. This connectionist model is capable of recovering a previously presented pattern or patterns from partial cues, be\u2010 ing able to complete the missing information. Hopfieldnetworks behave asfixed\u2010point attractor networks as their internal state evolves towards a stable single state or fixed point. This is given by their energy function. These types of systems have been used as models of associative memory [8]. In the classic model as described by Hopfield [7], neurons are binary units: the activa\u2010 tion state of each neuron can be either firing or not (on or off). The activity of each unit asynchronously changes in a discrete time scale. As in other connectionist systems, the strength of connections between nodes is de\u2010 scribed by its weight matrix. Weights are only updated upon network initialization and depend on the patterns presented to all network units. The weight matrix takes the shape of a square, symmetric matrix in which all the values in the main diagonal are always zero. All neurons are connected to each other. Also, every neuron is both an input and output node for memory pattern presentation and retrieval. To compose the weight matrix, each node is updated according to a local incremental learning rule, re\u2010 lated to Hebbian learning. Hebb\u2019s rule states that neurons that fire together when a certain pattern is present strengthen the connections between them [9]. In the work by Recantesi et al. [6], modifications to the original Hopfield model have been introduced, with new properties of memory retrieval: the model was adapted to induce transitions between attractor states (recalled memories).\n2.1 Neuron Dynamics\nCurrent \u2014 The original paper described the dynamics of neuron \u03bdi as the change of its current ci with time:\n\u03c4 c\u0307i(t) = \u2212ci(t) + N\u2211 j=1 rj(t) \u00b7Wi,j + \u03bei(t) (1)\nwhere:\n\u03c4 \u2208 R is the decay time, c \u2208 R is the synaptic current, N \u2208 N is the network number of neurons, W is the weight matrix, r \u2208 R is the firing rates, \u03be \u2208 R is the Gaussian noise.\nThe former equation can be discretized using the Euler method:\nci(t+ 1) = \u2212ci(t) + dt\n\u03c4 \u2212ci(t) + N\u2211 j=1 rj(t) \u00b7Wi,j + \u03bei(t)\u221a dt  (2) where: dt \u2208 R is the integration time step\nFiring Rates \u2014 Firing rates r of each neuron are calculated by the gain function g(c), a step function with sublinear behavior or a value of zero:\ng(c) = { (c+ \u03b8)\u03b3 if (c+ \u03b8) > 0 0 if (c+ \u03b8) \u2264 0\n(3)\nReScience C 6.3 (#8) \u2013 Torre-Ortiz and Nioche 2021 2\nwhere:\n\u03b8 \u2208 R is the gain function threshold, \u03b3 \u2208 R<1 is the gain function exponent.\nMemory Patterns \u2014 Every memory is represented by a binary vector or pattern p of sizeN . Each element of this vector corresponds to the state s of each neuron \u03bd for that memory pattern. The value of this state is 0 if the neuron does not encode for that pattern and 1 if it does. The different states are stored in aM \u00d7N matrix, with shape:\n \u03bd1 \u03bd2 \u00b7 \u00b7 \u00b7 \u03bdn p1 s 1 1 s 1 2 \u00b7 \u00b7 \u00b7 s1n p2 1 0 \u00b7 \u00b7 \u00b7 1 ... ... ... . . . ... pm s m 1 s m 2 \u00b7 \u00b7 \u00b7 smn  Inhibition \u2014 The network is subjected to periodic inhibition driven by a sine wave \u03d5(t), with the form:\n\u03d5max \u2212 \u03d5min 2\n[ 1 + sin ( 2\u03c0t+ \u03c0\n2\n)] (4)\nwhere:\n\u03d5min \u2208 R is the minimum inhibition hyperparameter, \u03d5max \u2208 R is the maximum inhibition hyperparameter,\nWeights \u2014 Each neuron in the network is fully connected to all the other neurons. This gives a N \u00d7N weight matrixW i,j representing the strength of connection or weight w between neurons \u03bdi and \u03bdj :\n \u03bd1 \u03bd2 \u00b7 \u00b7 \u00b7 \u03bdn \u03bd1 w1,1 w1,2 \u00b7 \u00b7 \u00b7 w1,n \u03bd2 w2,1 w2,2 \u00b7 \u00b7 \u00b7 w2,n ... ... ... . . . ... \u03bdm wm,1 wm,2 \u00b7 \u00b7 \u00b7 wm,n  To calculate the weight matrix, the following Hebbian rule is used:\nWi,j = \u03ba\nN [ M\u2211 p=1 (spi \u2212 f)(s p j \u2212 f)\u2212 \u03d5(t) ] (5)\nwhere:\n\u03ba \u2208 R\u2a7e0 is the excitation, M \u2208 N is the number of memories, s \u2208 Z[0,1] is the neuron state for a memory, p is the memory pattern, f \u2208 R is the sparsity, \u03d5 \u2208 R is the oscillatory inhibition.\nReScience C 6.3 (#8) \u2013 Torre-Ortiz and Nioche 2021 3\nTo account for short term associations to the previous and next memories as in the SAM model, a new termW \u2217i,j is added to the original weight matrix:\nWSAMi,j = Wi,j +W \u2217 i,j = Wi,j +\n\u03ba\nN\n[ \u03baf\nM\u22121\u2211 p=1 spi \u00b7 s p+1 j + \u03bab M\u2211 p=2 spi \u00b7 s p\u22121 j\n] (6)\nwhere:\n\u03baf \u2208 R is the forward contiguity, \u03bab \u2208 R is the backward contiguity.\nNoise \u2014 Each neuron is subjected to Gaussian noise \u03be, following the probability density function:\np(z) = 1\n\u03c3 \u221a 2\u03c0\ne\u2212 (z\u2212\u00b5)2 2\u03c32 (7)\nwhere:\n\u00b5 \u2208 R is the noise mean, \u03c3 \u2208 R\u2a7e0 is the noise standard deviation.\n2.2 Population Dynamics Simulating the network with the original parameters is very computationally expensive, with the computation time depending primarily on the number of neurons. The system can be simplified, reducing the number of simulated units. All neurons that present the same activation state for the different memories will be considered that belong to the same population \u03c0. Moreover, all these neurons have an identical weight matrixW i,j1, which will be the weight matrix of the population.\nCurrent \u2014 A new term S\u03c0 is introduced in the calculation:\nS\u03c0 = N\u03c0 N\n(8)\nwhere:\nN\u03c0 \u2208 N is the number of neurons in a population.\nThe change of current of population \u03c0i with time is:\n\u03c4 c\u0307i(t) = \u2212ci(t) + U\u2211\nj=1\nrj(t) \u00b7Wi,j \u00b7 S\u03c0 + \u03bei(t) (9)\nwhere:\nU \u2208 N is the number of unique populations.\n1ij notation refers to the simulated unit, which is from now on a population of neurons instead of the single neuron.\nReScience C 6.3 (#8) \u2013 Torre-Ortiz and Nioche 2021 4\nAfter discretizing by Euler:\nci(t+ 1) = \u2212ci(t) + dt\n\u03c4 \u2212ci(t) + U\u2211 j=1 rj(t) \u00b7Wi,j \u00b7 S\u03c0 + \u03bei(t)\u221a dt  (10) Firing Rates \u2014 Firing rates are calculated with the gain function as before.\nMemory Patterns \u2014 The M \u00d7 N matrix containing s states is now a M \u00d7 U matrix. This secondmatrix contains fewer elements than the originalmatrix, allowing for faster com\u2010 putation.\nInhibition \u2014 Inhibition calculation remains unchanged for the simulation with popula\u2010 tions.\nWeights \u2014 The population weight matrixW i,j for neuron populations, withU\u00d7U shape:\nWi,j = \u03ba\nN [ M\u2211 p=1 (upi \u2212 f)(u p j \u2212 f)\u2212 \u03d5(t) ] (11)\nWSAMi,j = Wi,j +W \u2217 i,j = Wi,j +\n\u03ba\nN\n[ \u03baf\nM\u22121\u2211 p=1 upi \u00b7 u p+1 j + \u03bab M\u2211 p=2 upi \u00b7 u p\u22121 j\n] (12)\nwhere:\nu \u2208 Z[0,1] is the activity state of a population.\nNoise \u2014Unmodulated Gaussian noise \u03be is computed as before.\n2.3 Simulation Simulation is carried away with population\u2010level conditions (subsection 2.2). Calcula\u2010 tions are then based on aM \u00d7U matrix instead of a much largerM \u00d7N matrix. Compu\u2010 tation time now scales withM instead ofN but for very large values ofN , which would also increase the number of populations. Recanatesi et al. [6] estimated this method to be 99.9% faster than simulating individual neurons. Firing rates are initialized at rini for populations encoding a randomly chosen memory pattern. Currents are set at r 1 \u03b3\nini. All weights are also defined at this stage. Values of noise and inhibition change per time step. Neuron currents and firing rates are then calculated at each time step as well. A memory is considered recalled if the average firing rate of all encoding neurons is above rrecall. The network is said to recall a certain memory p if the former condition is ever fulfilled for that memory during the simulation time. Table 1 summarizes all hyperparameters used in the simulation.\nReScience C 6.3 (#8) \u2013 Torre-Ortiz and Nioche 2021 5\n2.4 Recall Analysis The network was simulated first at a smaller scale and for one trial to observe detailed dynamics. It was then scaled to computer clusters, allowing to reach a total of 10,000 simulations for each condition needed for memory recall analysis. Each network is sim\u2010 ulated for a total of 450 time cycles. Several metrics are computed to assess the recall performance of the model. In particu\u2010 lar, inter\u2010retrieval time (IRT) is calculated as the number of time cycles until the recall of a new memory item. Other performance metrics such as memory size intersections or the average total recalls are also analyzed.\n2.5 Computational Tools Replication was carried out using Python 3.8.5 with packages NumPy 1.19.1, pandas 1.0.5, matplotlib 3.3.0, SciPy 1.5.2, and tqdm 4.48.0 on Parabola GNU/Linux\u2010libre and CentOS GNU/Linux.\n3 Results\n3.1 Model Simulation Current Dynamics During periods of minimum inhibition, a set of populations dis\u2010 plays a positive current corresponding to one memory. Meanwhile, the remaining pop\u2010 ulations have negative current, meaning other memories are not being recalled. At inhibition maxima, transitions between attractors may happen, with a new set of neuron populations firing at the next inhibition minimum. Transitions are observed when current values near inhibition minima, where values are close enough for the noise to drive changes between limit cycles (Figure 1).\nFiring Rates Currents are subjected to a step function to calculate firing rates. The curve shape of the latter is then closely related to the positive domain of current values over time. Firing rates above rrecall indicate that amemorywas recalled. Differentmem\u2010 ories are recalled at times corresponding to minimum values of inhibition. Transitions may happen between memory items or attractors in periods of minimum inhibition (Figure 2).\nInhibition The sine wave function provides the network with oscillatory inhibition necessary for its dynamics. Values have to be adequately scaled to induce the appro\u2010 priate network behavior of memory recall and transitions between attractors (Figure 3).\nWeights Weights show the strength of the connection between elements ij of the ma\u2010 trix. In the model, three different weight matrices are presented, accounting for the regular connectivity between neuron populations, but also considering item contiguity or associations betweenWeight values change according to the parameters of excitation, forward, and backward contiguity (Figure 4).\nNoise Uncorrelated Gaussian noise is calculated for each population of neurons. The range of values is critical for the network to be successfully simulated, observing the transition between attractors (Figure 5).\nReScience C 6.3 (#8) \u2013 Torre-Ortiz and Nioche 2021 6\nReScience C 6.3 (#8) \u2013 Torre-Ortiz and Nioche 2021 7\nA Attractors\nReScience C 6.3 (#8) \u2013 Torre-Ortiz and Nioche 2021 8\nA Oscillation\nReScience C 6.3 (#8) \u2013 Torre-Ortiz and Nioche 2021 9\nReScience C 6.3 (#8) \u2013 Torre-Ortiz and Nioche 2021 10\nReScience C 6.3 (#8) \u2013 Torre-Ortiz and Nioche 2021 11\n3.2 Recall Analysis Temporal Properties of Recall Recall of newmemories progressively slows down with time, while still possible even at later time cycles, even observing a sharp increase in recalls in the last iterations. Most transitions occur after one time step (IRT = 0), while there is some variability in the distribution. As time passes, the average IRT is likely to decrease due to these rapid memory transitions (Figure 6).\nProbability of Recall The frequency of recall monotonically increases with memory size, asmore overlaps between largememories are expected. As timepasses, the average IRT is likely to decrease due to these rapid memory transitions (Figure 7). Differences in the number of points of the figure compared to the original article are likely due to binning and not due to a change in dynamics.\nMemory Transitions More similar memories are expected to be recalled more often. This effect is observed as most transitions occur between the most similar memories. There is also a higher transition rate between memories with a lower intersection size between them. This leads to fast transitions, or lower IRT values for more similar mem\u2010 ories (Figure 8).\nRecall Performance and Parameters Average total number of memories recalled by 100 networks for 100 different values of forward contiguity and noise variance (Figure 9). Networks recall more words on average withmonotonically increasing with the value of noise variance \u03c32 until saturation. The performance also increases along with forward contiguity \u03baf until saturation, followed by a decrease in recalls. At this point, the con\u2010 tiguity term likely overcomes noise as the drive for memory transitions. An additional evaluation focuses on lower forward contiguity values \u03baf . This supports that the dynam\u2010 ics at the lowest values of \u03baf in the previous figure are due to its relationship with \u03bab, and not entirely due to randomness. A drop in performance is seen as values get closer to backward contiguity \u03bab = 850, rising again afterward.\nReScience C 6.3 (#8) \u2013 Torre-Ortiz and Nioche 2021 12\nReScience C 6.3 (#8) \u2013 Torre-Ortiz and Nioche 2021 13\nReScience C 6.3 (#8) \u2013 Torre-Ortiz and Nioche 2021 14\nReScience C 6.3 (#8) \u2013 Torre-Ortiz and Nioche 2021 15\nReScience C 6.3 (#8) \u2013 Torre-Ortiz and Nioche 2021 16\n4 Discussion\nRecanatesi et al. [6] present a neural network model of long\u2010term memory free recall. In this model, inhibitory oscillations drive network dynamics. Noise and memory item contiguity can change the active attractor. We were not able to replicate the model with the conditions of the original article. The original authors acknowledged several errors in their manuscript, making replication unlikely. Fortunately, collaboration with the original authors enabled to reach success\u2010 ful replication. Most changes involve a normalization in equation terms, leading to changes of several orders of magnitude. An error in the original article scaling both contiguity parameters corresponding to equations 6 and 12 in the reference paper. In the corrected version, a previously missing pre\u2010factor provides correct normalization as reported in equations 6 and 12. Besides, the base values of several hyperparameters needed to be corrected as reflected in Table 1. The following parameters were changed: \u03b3, \u03ba, \u03baf , and \u03bab. Parameters of the original article allowed to replicate of retrieval dynamics, but could not replicate the recall analysis. After applying the corrections in coordination with the original authors, we do not ob\u2010 serve important differences with the original article, reporting a full replication of the original results.\n5 Conclusions\nIn this work, we successfully reproduced the results of the memory model simulation reported by [6]. This was possible by modifying the equations of the original article. Changes scale parameters ormodify their base values to correct the errors in the original manuscript in coordination with the original authors. As in the reference research, we show that oscillating inhibition, together with noise and item contiguity, induce the transition of recall of different memories in a Hopfield model of memory retrieval."}, {"heading": "Acknowledgments", "text": "We acknowledge the computational resources provided by the Aalto Science\u2010IT project and University of Helsinki Finnish Grid and Cloud Infrastructure (persistent identifier urn:nbn:fi:research\u2010infras\u20102016072533).\nReScience C 6.3 (#8) \u2013 Torre-Ortiz and Nioche 2021 17\nReScience C 6.3 (#8) \u2013 Torre-Ortiz and Nioche 2021 18"}], "title": "[Re] Neural Network Model of Memory Retrieval", "year": 2021}