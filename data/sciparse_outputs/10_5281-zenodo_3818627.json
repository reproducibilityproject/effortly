{"abstractText": "DOI 10.5281/zenodo.3818627 One of the challenges in machine learning research is to ensure that the presented and published results are sound and reliable. Reproducibility, which is obtaining similar results as presented in a paper or talk, using the same code and data (when available), is a necessary step to verify the reliability of research findings. Reproducibility is also an important step to promote open and accessible research, thereby allowing the scientific community to quickly integrate new findings and convert ideas to practice. Reproducibility also promotes the use of robust experimental workflows, which potentially reduce unintentional errors. In 2019, the Neural Information Processing Systems conference, the premier international conference for research in machine learning, introduced a reproducibility program [1], designed to improve the standards across the community for how we conduct, communicate, and evaluate machine learning research. One of the components in the program consisted of a community-wide reproducibility challenge on the accepted papers. In this special issue of the ReScience C Journal, we present the top peer-reviewed submissions of the challenge, namely 2019 NeurIPS Reproducibility Challenge.", "authors": [{"affiliations": [], "name": "Koustuv Sinha"}, {"affiliations": [], "name": "Joelle Pineau"}, {"affiliations": [], "name": "Jessica Forde"}, {"affiliations": [], "name": "Rosemary Nan Ke"}, {"affiliations": [], "name": "Hugo Larochelle"}], "id": "SP:45f52a9055f7942a4299cc9f51049a187a846327", "references": [{"authors": ["J. Pineau", "P. Vincent-Lamarre", "K. Sinha", "V. Larivi\u00e8re", "A. Beygelzimer", "F. d\u2019Alch\u00e9-Buc", "E. Fox", "H. Larochelle"], "title": "Improving Reproducibility in Machine Learning Research (A Report from the NeurIPS 2019 Reproducibility Program).", "year": 2020}, {"authors": ["H.E. Plesser"], "title": "Reproducibility vs. replicability: a brief history of a confused terminology.", "venue": "Frontiers in neuroinformatics", "year": 2018}, {"authors": ["M.A. Heroux", "L. Barba", "M. Parashar", "V. Stodden", "M. Taufer"], "title": "Toward a Compatible Reproducibility Taxonomy for Computational and Computing Sciences", "venue": "Tech. rep. Sandia National Lab.(SNL-NM),", "year": 2018}, {"authors": ["Sinha"], "title": "Sciences,Medicine, et al.Reproducibility and replicability in science", "venue": "E. National Academies", "year": 2020}], "sections": [{"text": "R E S C I E N C E C"}, {"heading": "Editorial", "text": "NeurIPS 2019 Reproducibility Challenge\nKoustuv Sinha1,2,3, ID , Joelle Pineau1,2,3, Jessica Forde4, Rosemary Nan Ke2,5, and Hugo Larochelle6 1McGill University, Montreal, Canada \u2013 2Montreal Institute of Learning Algorithms (Mila), Montreal, Canada \u2013 3Facebook AI Research, Montreal, Canada \u2013 4Brown University, USA \u2013 5Polytechnic University, Montreal, Canada \u2013 6Google Brain, Montreal, Canada\nEdited by Nicolas Rougier\nReceived 20 March 2020\nPublished 21 May 2020\nDOI 10.5281/zenodo.3818627\nOne of the challenges in machine learning research is to ensure that the presented and published results are sound and reliable. Reproducibility, which is obtaining similar results as presented in a paper or talk, using the same code and data (when available), is a necessary step to verify the reliability of research findings. Reproducibility is also an important step to promote open and accessible research, thereby allowing the scientific community to quickly integrate new findings and convert ideas to practice. Reproducibility also promotes the use of robust experimental workflows, which potentially reduce unintentional errors. In 2019, the Neural Information Processing Systems conference, the premier international conference for research in machine learning, introduced a reproducibility program [1], designed to improve the standards across the community for how we conduct, communicate, and evaluate machine learning research. One of the components in the program consisted of a community-wide reproducibility challenge on the accepted papers. In this special issue of the ReScience C Journal, we present the top peer-reviewed submissions of the challenge, namely 2019 NeurIPS Reproducibility Challenge.\n1 The Challenge\nThe goal of this challenge was to investigate the reproducibility of empirical results submitted to the 2019 edition of Neural Information Processing Systems (NeurIPS) conference. Unlike our previous editions (2018 ICLR, 2019 ICLR), in this challenge, we only focused on accepted papers at the conference. The primary target audience of the challengewas early career researchers fromuniversities, however, we received participation from the industry as well. The main objective of this challenge was to provide independent verification of the empirical claims in accepted NeurIPS papers and to leave a public trace of the findings from this secondary analysis. We provide a comparative analysis of participation of this challenge as compared to the previous editions in Table 1. A total of 173 papers were claimed for reproduction, which is a 92% increase since the last edition. We had participants from 73 institutions (63 universities and 10 industries) from around the world. Institutions with the most participants came from 3 continents and includeMcGill University, Canada, KTH in Sweden, Brown University in the US and IIT Roorkee in India. In those cases (and several others), a high participation rate occurred when a professor at the university used this challenge as a final course project. In this special issue, we present the top 10 peer-reviewed reports, selected from 84 submissions.\nCopyright \u00a9 2020 K. Sinha et al., released under a Creative Commons Attribution 4.0 International license. Correspondence should be addressed to Koustuv Sinha (koustuv.sinha@mail.mcgill.ca) The authors have declared that no competing interests exist.\nReScience C 6.2 (#11) \u2013 Sinha et al. 2020 1\n2 Baselines, Ablations and Replications\nReplication of a computational study typically means running the same code, using the same input data, and then checking if the results are the same or at least \u201cclose enough\u201d by some degree of numerical approximations 1. This is most easily achieved when the exact code and data to replicate the experiments are provided. To this end, the organizers of the 2019 NeurIPS conference instated a code submission policy for the accepted papers this year. While it wasn\u02bct mandatory, the policy was to encourage authors to submit their code by providing enough flexibility on the timing of submission. This resulted in 74.4% of papers being associated with their code, which was less than 50% in the 2018 NeurIPS conference. From the very beginning of the challenge, wemade these codebases available to participants and offered three tracks to choose from.\n1. Baselines Track - Sometimes it is not feasible to reproduce all the experiments in a paper: factors such as private datasets, extensive training time, the requirement of non-standard compute infrastructure can all limit reproducibility. It is also sometimes the case that baseline methods reported in the papers are not properly implemented, or hyper-parameter search is not done with sufficient care, leading to a poor comparison of alternative methods. Thus we provided an option to the challenge participants to perform a rigorous analysis on the baselines by reimplementing them wherever necessary. Reproducing the baselines can further add to the technical contributions of a paper, and therefore was encouraged in this challenge.\n2. Ablations Track - Since we had almost 75% of accepted papers accompanied with their code, we provided a track which only focuses on the released code. Participants are encouraged to use the authors\u02bc code and perform rigorous ablation experiments by modifying the model and hyperparameter choices, to gain extra insights from the reported methods of the paper and add value to their understanding.\n3. Replications Track - A higher bar of reproducibility is to replicate the experiments explained in the paper from scratch without having to refer to the original codebase. This is helpful in detecting anomalies in the presentation of the ideas in a paper, and it sheds light on the aspects of the implementation that could affect the final results. This is far by the most difficult track, and the implementation results directly add the most value to the understanding of the original paper, often leading to continued discussions with the authors.\n3 Platform and Medium\nIn this edition of the Reproducibility Challenge, we were fortunate enough to have big support from OpenReview 2 and the Program Chairs of NeurIPS 2019. All NeurIPS 2019\n1We have used the definition of \u201cReplication\u201d in the NeurIPS 2019 Reproducibility Challenge following the old definition adopted by ACM [2]. Recent changes to this definition has been proposed by [3] and [4], which states the above definition is suitable for the term \u201cReproduction\u201d instead.\n2https://openreview.net/group?id=NeurIPS.cc/2019/Reproducibility_Challenge\nReScience C 6.2 (#11) \u2013 Sinha et al. 2020 2\naccepted papers were hosted by OpenReview, which facilitated online discussions for the larger research community who were unable to be present physically at the conference in Vancouver in December 2019. OpenReview built a unique platform for the Reproducibility Challenge, which featured the accepted papers as well as allowed challenge participants to claim a paper to work on, and later submit their reports based on their claim. Once submitted, all reproducibility reports underwent an extensive review cycle by a large set of reviewers of the NeurIPS 2019 conference. Due to the transparent review process of OpenReview, many reproducibility reports attracted comments from the original authors, which in turn helped the overall reviewing pipeline. Finally, we selected 10 high-quality reports from 84 submissions to be published in this journal, ReScience C, which is a perfect platform for publication of reproducibility efforts of various computational fields of science.\n4 Relationship with Authors\nAuthors of research papers have much to gain from this challenge as the participants. Using OpenReview, we encouraged participants to clarify various nuances of the implementation of the paper with the original authors. Due to the dual nature of our OpenReview platform, challenge participants could easily communicate with the authors who themselves received notifications from the comments arising in the forum associated with their papers. During the review of the Reproducibility reports (in preparation for this special issue), these communications were also taken into consideration by the reviewers in judging the quality of the report.\n5 Computing Resources\nIn this challenge, we partnered with CodeOcean 3 for providing free cloud computing credits to select teams. CodeOcean is an online web-based platform for reproducible computational science, which is a shareable Docker container living in the cloud. Participants were able to leverage the free compute resources from CodeOcean to run their experiments. CodeOcean provided prompt and necessary support enabling participants to resolve implementation issues to request additional resources to support their experiments.\n6 Content\nIn this special issue, we present the top 10 peer-reviewed reports of the 2019 Reproducibility Challenge. These reports were selected after critical reviews from our reviewers, and consist of reproducibility efforts over broad coverage of topics in Machine Learning, including optimization, initialization, generativemodeling, transfer learning, and reinforcement learning. We are hosting all of the accepted reports in OpenReview for the community to read and add to their understanding of the original NeurIPS 2019 paper.\n7 Conclusion\nReproducibility in machine learning has recently garnered a considerable amount of attention and momentum thanks to key efforts by top researchers. Conferences such as ICLR, AAAI, ICML have organized dedicated workshops on the topic. The premier\n3https://codeocean.com/\nReScience C 6.2 (#11) \u2013 Sinha et al. 2020 3\nconference in the field, NeurIPS, has undertaken a reproducibility program this year which consisted of three components: a code submission policy, the inclusion of the Machine Learning Reproducibility checklist as part of the paper submission process, and this challenge. We hope our endeavor will similarly spur more efforts in reproducing existing ideas and papers, and in turn promote open, accessible and soundmachine learning research.\n8 Acknowledgements\nWe thank the NeurIPS board and the NeurIPS 2019 general chair (Hanna Wallach) and program chairs (Hugo Larochelle, Alina Beygelzimer, Florence dA\u0313lch\u00e9-Buc, Emily Fox) for the unfailing support of this initiative. We thank the many authors who submitted their work to NeurIPS 2019 and communicated with the challenge participants. We thank the program committee (Zhenyu (Sherry) Xue) of NeurIPS 2019 for providing us data and statistics of the papers accepted in the NeurIPS 2019 conference which helped us in building the portal. We thank the OpenReview team (in particular Andrew McCallum, Pam Mandler, Melisa Bok, Michael Spector, and Mohit Uniyal) who provided extensive support from day one to build and host the dual-purpose portal, and to host the results of the reproducibility challenge. We thank CodeOcean (Xu Fei) for supporting our challenge by providing cloud compute resources. Finally, we thank the several participants of the reproducibility challenge who dedicated time and effort to verify results thatwere not their own, to help strengthen our understanding ofmachine learning, and the types of problems we can solve today.\n9 Reviewers\nIn this iteration of the Reproducibility Challenge, we were fortunate enough to attract a large base of reviewers having prior experience in reviewing in large Machine Learning conferences such as NeurIPS, ICML, ICLR, etc. Many thanks to all our reviewers, we acknowledge their hard efforts who spent their precious time to critically review the reports. We hope that our reviewer base will keep supporting us in this endeavor in the future.\nAbhinav Agrawal\nAdria Garriga-Alonso\nAmbrish Rawat\nAndreas Ruttor\nAndreea Gane\nAndrew Drozdov\nAndrew Jaegle\nAndrew Ross\nAngus Galloway\nAntti Koskela\nArna Ghosh\nAustin Brockmeier\nAwa Dieng\nBryan Gibson\nCagri Coltekin\nChao Qin\nCharbel Sakr\nChen Tessler\nCheng Ju\nChuan Li\nDagmar Kainmueller\nDamian Roqueiro\nDavid Arbour\nDavid Krueger\nDi He\nDmitriy Serdyuk\nDong Gong\nDong Yin\nDonghyeon Cho\nDu Tran\nDylan Hadfield-Menell\nElaheh Raisi\nEmmanuel Bengio\nErfan Sadeqi Azer\nEric Crawford\nEric Jang\nErin Conlon\nErin Grant\nErnest Ryu\nFang Liu\nFang Zhao\nFelix Gimeno\nF. Mart\u00ednez-Plumed\nF. Poursabzi-Sangdeh\nGabriel Synnaeve\nGang Wang\nGavin Weiguang Ding\nReScience C 6.2 (#11) \u2013 Sinha et al. 2020 4\nGeorg Martius\nGeorgios Leontidis\nGianfranco Doretto\nHaiqin Yang\nHaitian Sun\nHanna Suominen\nHao He\nHei Law\nHidekazu Oiwa\nHong Ge\nHongyi Wang\nHua Wang\nHuaibo Huang\nHuimin Ma\nHuitong Qiu\nHuziel Sauceda\nJ. Hernandez-Garcia\nJaeho Lee\nJake Bruce\nJesse Dodge\nJessica Forde\nJi Lin\nJiahui Yu\nJiakai Zhang\nJiangwen Sun\nJing Wang\nJinghui Chen\nJitong Chen\nJoan Puigcerver\nJoel Lehman\nJoelle Pineau\nJohn Wieting\nJonathan Hunt\nJosh Roy\nKai Han\nKanika Madan\nKatherine Lee\nKhimya Khetarpal\nKonstantin Mishchenko\nLeo Lahti\nLevent Sagun\nLi cheng\nLi Li\nLi Shen\nLijun Wu\nLinh Tran\nLiping Liu\nLluis Castrejon\nLovedeep Gondara\nMalik Altakrori\nManeesh Singh\nManoj Acharya\nM\u00e5ns Magnusson\nMarlos C. Machado\nMartin Klissarov\nMassimiliano Mancini\nMathew Monfort\nMatthew Schlegel\nMatthias Gall\u00e9\nMaxime Wabartha\nMaxwell Collins\nMelanie F. Pradier\nMichal Drozdzal\nMike Chrzanowski\nMingkui Tan\nMingrui Liu\nMinjia Zhang\nMirco Musolesi\nNan Ke\nNesreen Ahmed\nNikolaos Vasiloglou\nOlga Isupova\nOlivier Delalleau\nOlivier Koch\nPablo Robles-Granda\nPascal Lamblin\nPatrick Philipp\nPaul Tylkin\nPeixian Chen\nPeter Henderson\nPraveen Narayanan\nPrithvijit Chattopadhyay\nQihang Lin\nRazieh Nabi\nRazvan Pascanu\nReinhold Scherer\nRitambhara Singh\nRobert Vandermeulen\nRoy Schwartz\nRyan Lowe\nSadid A. Hasan\nSamuel Albanie\nSandhya Prabhakaran\nSara Hooker\nScott Fujimoto\nSercan Arik\nSergio Valcarcel Macua\nSeungjae Lee\nShagun Sodhani\nShalini Ghosh\nShih-Yang Su\nShivam Patel\nShuai Tang\nShuai Zheng\nShuxin Zheng\nSimon Kornblith\nSohil Shah\nStanislaw Jastrzebski\nStefan Magureanu\nSteffen Udluft\nSwapnil Mishra\nTakashi Ishida\nTakeshi Teshima\nTammo Rukat\nTobias Uelwer\nTzu-Yun Shann\nUthaipon Tantipongpipat\nVenkatadheeraj Pichapati\nV\u00edctor Campos\nVincent Francois-Lavet\nVincent Lepetit\nVolker Fischer\nWenhao Yu\nWenxiao Wang\nWenxuan Wu\nWesley Maddox\nXavier Bouthillier\nReScience C 6.2 (#11) \u2013 Sinha et al. 2020 5\nXiang Yu\nXiang Zhang\nXiangliang Zhang\nXiangru Lian\nXin GUO\nXin Lu\nXinggang Wang\nXingrui Yu\nXingyu Liu\nYash Goyal\nYingyezhe Jin\nYoonho Lee\nYufei Han\nYuji Matsumoto\nYuntian Deng\nZhangjie Cao\nZhourong Chen"}, {"heading": "1. J. Pineau, P. Vincent-Lamarre, K. Sinha, V. Larivi\u00e8re, A. Beygelzimer, F. d\u2019Alch\u00e9-Buc, E. Fox, and H. Larochelle. \u201cImproving Reproducibility in Machine Learning Research (A Report from the NeurIPS 2019 Reproducibility", "text": "Program).\u201d In: arXiv preprint arXiv:2003.12206 (2020). 2. H. E. Plesser. \u201cReproducibility vs. replicability: a brief history of a confused terminology.\u201d In: Frontiers in neuroinformatics 11 (2018), p. 76. 3. M. A. Heroux, L. Barba, M. Parashar, V. Stodden, and M. Taufer. Toward a Compatible Reproducibility Taxonomy for Computational and Computing Sciences. Tech. rep. Sandia National Lab.(SNL-NM), Albuquerque, NM (United States), 2018. 4. E. National Academies of Sciences,Medicine, et al.Reproducibility and replicability in science. National Academies Press, 2019.\nReScience C 6.2 (#11) \u2013 Sinha et al. 2020 6"}], "title": "NeurIPS 2019 Reproducibility Challenge", "year": 2020}