{"abstractText": "In this study, we performed some ablations on the main model developed in the paperUnsupervised Representation Learning in Atari [1] as part of the 2019 NeurIPS Reproducibility Challenge. In this paper, Anand et. al introduce a new learningmethod called SpatioTemporal DeepInfoMax (STDIM), which is an unsupervisedmethod that aims at learning state representations by maximizing particular forms of mutual information between a series of observations. Our work focuses on recreating a subset of their results, along with hyperparameter tuning, slightly altering the STDIM learning objective, and altering the receptive field of the encoder model that Anand et. al introduce in their article. We also suggest directions for further expanding the STDIM method. Our results also suggest that creating an ensemble model would allow for further boosting of the effectiveness of this model.", "authors": [{"affiliations": [], "name": "Gabriel Alacchi"}, {"affiliations": [], "name": "Guillaume Lam"}, {"affiliations": [], "name": "Carl Perreault-Lafleur"}, {"affiliations": [], "name": "Koustuv Sinha"}], "id": "SP:7c9d01ecd301b6e0462e892069695e56e9166b05", "references": [{"authors": ["A. Anand", "E. Racah", "S. Ozair", "Y. Bengio", "M.-A. C\u00f4t\u00e9", "andR. D"], "title": "Hjelm.UnsupervisedState Representation Learning in Atari. 2019", "venue": "ReScience C", "year": 2020}, {"authors": ["G. Alain", "Y. Bengio"], "title": "Understanding intermediate layers using linear classifier probes", "year": 2017}, {"authors": ["A. van den Oord", "Y. Li", "O. Vinyals"], "title": "Representation Learning with Contrastive Predictive Coding", "year": 2018}, {"authors": ["R.D. Hjelm", "A. Fedorov", "S. Lavoie-Marchildon", "K. Grewal", "P. Bachman", "A. Trischler", "Y. Bengio"], "title": "Learning deep representations by mutual information estimation and maximization", "year": 2019}, {"authors": ["M.G. Bellemare", "Y. Naddaf", "J. Veness", "M. Bowling"], "title": "The arcade learning environment: An evaluation platform for general agents", "year": 2013}], "sections": [{"text": "Edited by Koustuv Sinha ID\nReviewed by Anonymous Reviewers\nReceived 15 February 2020\nPublished 21 May 2020\nDOI 10.5281/zenodo.3818625\n1 Introduction\nIn recent years, reinforcement learning has become an active field of research in the learning algorithms field. It consists of buildingmodels which learn how to act and perform tasks in an environment based on past experience. The training of the artificial agents is the main challenge in the production of such algorithms. State representation learning consists of learning to capture the variation in the environment generated by the agent s\u0313 actions. Hence, state representation learning is crucial for building efficient and powerful agents. The studied article focuses on learning such representations without supervision from rewards. In the article, Anand et. al introduce a new method that learns state representations, called SpatioTemporal DeepInfoMax (STDIM). Furthermore, they also introduce a new benchmark based on 22 Atari 2600 games where it becomes possible to evaluate representations based on how well they capture the ground truth state variables of the game. This allows them to compare STDIM to other state-ofthe-art representation learning methods.\nSection 2 summarizes the studied article to demonstrate the various components our work builds upon. Afterwards, a small subset of the results obtained in the article was reproduced. The ablations performed on STDIM are then summarized. We first experimented with tweaking of the various hyper parameters of themodel. This study demonstrates that overall, the default parameters used by the authors of the article are well chosen. The second ablation realized was changing the details of the loss function of STDIM. Our final ablation was changing the encoder to modify the receptive field size that it observes on the input image. The obtained results validate the choices made by the authors and show the robustness of their STDIM method.\nCopyright \u00a9 2020 G. Alacchi, G. Lam and C. Perreault-Lafleur, released under a Creative Commons Attribution 4.0 International license. Correspondence should be addressed to Gabriel Alacchi (gabriel.alacchi@mail.mcgill.ca) The authors have declared that no competing interests exist. Code is available at https://github.com/GabrielAlacchi/atari-ari-extensions. \u2013 SWH swh:1:dir:9f3337a643ab37a0898e0f0acb9ccf29a0428108. Open peer review is available at https://openreview.net/forum?id=GAY8Xw9Qzs.\nReScience C 6.2 (#1) \u2013 Alacchi, Lam and Perreault-Lafleur 2020 1\n2 Summary of the Article\n2.1 Objective and Setup The setup is fairly simple, an agent interacts with the environment and realizes some observations. The objective of the article for the agent is to learn the representations of the observations such that high-level features in the environment are captured in an unsupervised setting. As shown in Figure 1, the aim is to identify the locations of variables such as the player character, enemies, and various of items of interest. It is desired that the model ignores immutable low-level details like the background, which are useless to an agent s\u0313 ability to process the environment.\nIn order to evaluate a representation learning method after it has been trained, the weights are frozen and then the performance is evaluated with a linear probing task in a supervised manner [2]. It should be noted that the gradients of the supervised task are not propagated through the encoder. In order to perform well, the encoder must be able to learn linearly predictable representations of crucial variables in the game without supervision.\n2.2 The SpatioTemporal DeepInfoMax (STDIM)\nThe InfoNCE Objective \u2014 The methods introduced by Anand et. al [1] rely on the InfoNCE mutual information bound [3]. Generally speaking, we have access to a set of samples {(xi, yi)}Mi=1 drawn from a distribution p(x, y). Samples (xi, yi) are considered positive samples since they are drawn from the joint probability distribution. Samples which are not indexed the same, i.e. (xi, yj) for i \u0338= j are considered negative samples since they are drawn independently from the marginal distributions, i.e. Pr(xi, yj) = p(xi)p(yj). The InfoNCE objective is to learn a score function f which maximizes the following estimate which acts as a lower bound of the mutual information betweenX and Y .\nINCE = N\u2211 i=1 log exp f(xi, yi)\nN\u2211 j=1 exp f(xi, yj) (1)\nTomaximize this bound, f must assign positive values to positive samples, and negative values to negative samples.\nThe STDIM Learning Objective \u2014 SpatioTemporal DeepInfoMax (STDIM) is an unsupervised technique for training an encoder network in an unsupervised manner. It builds on the\nReScience C 6.2 (#1) \u2013 Alacchi, Lam and Perreault-Lafleur 2020 2\nidea of DeepInfoMax (DIM, [4]) bymaximizing themutual information (MI) between the representation of consecutive frames by the neural network encoder.\nAn agent interacts with an environment and observes a set of frames {x1, x2, ..., xn} which generally can be any information which represents the state of the game, but in practice are images from the simulated Atari environment. The InfoNCE objective is derived frommini-batches of consecutive observations {(xt, xt+1)i}Bi=1 given by the agent s\u0313 interactions with the environment. Positive samples are defined as pairs of consecutive frames (xt, xt+1), and negative samples as pairs of non-consecutive frames (xt, xt\u22c6)with t\u22c6 \u0338= t+ 1.\nSTDIM simultaneously trains two InfoNCE objectives, the global-local objective (GL) and the local-local objective (LL). GL maximizes the MI between the entire frame xt with small patches of the frame observation at time t + 1. The representations of the small image patches are chosen to be the result of applying the hidden activation functions of a particular layer of the convolutional encoder to the full observation. The layer is picked appropriately to ensure that the hidden activation functions only have a limited receptive field corresponding to 1/16th of the size of the full observations. LLmaximizes the MI between the local feature at time t with the corresponding local feature at time t+ 1. The loss function derived from equation 1 is given by:\nL = LGL + LLL = M\u2211\nm=1 N\u2211 n=1 \u2212 log exp gm,n(xt, xt+1)\u2211 x\u22c6t exp gm,n(xt, x\u22c6t ) + log exp fm,n(xt, xt+1)\u2211 x\u22c6t exp fm,n(xt, x\u22c6t )  (2) where x\u22c6t denotes a non-consecutive observation to xt, and gm,n and fm,n are the score functions for the Global-Local and Local-Local objectives respectively. Following the work of van den Oord [3] on InfoNCE, STDIM uses a bilinear score function for both of these objectives:\ngm,n(xt, xt\u2032) = \u03d5(xt) T \u00b7Wg \u00b7 \u03d5m,n(xt\u2032) and fm,n(xt, xt\u2032) = \u03d5m,n(xt)T \u00b7Wl \u00b7 \u03d5m,n(xt\u2032) (3)\nwhere \u03d5 is the output of the encoder, \u03d5m,n is the local feature vector produced by an intermediate convolution layer of the encoder at the (m,n) spatial location, andWg,Wl are learned weights. When training an encoder using these objectives, Wg and Wl are learned to find themaximum for each InfoNCE bound. At run-time these score function weights are discarded since the final objective of this technique is to learn the encoder \u03d5 which creates linearly interpretable representations of the important variables in the game.\nThe base encoder architecture used for all models in the article is shown in Figure 2.\nReScience C 6.2 (#1) \u2013 Alacchi, Lam and Perreault-Lafleur 2020 3\n2.3 Linear Probing The evaluation of a representation is made possible by using the benchmark based on Atari 2600 games created by the authors of the article, and based on the Arcade Learning Environment (ALE, [5]). The benchmark they created is able to obtain the locations of the variables of interest of a game, which are stored in the RAM of the Atari machine. Each variable is encoded using an 8-bit memory location, and thus there are 256 distinct values possible for each variable. Since there are 22 games, and each game has many variables, the variables were classified into 5 categories:\n\u2022 Agent localization: x and\\or y-coordinates of the agent\n\u2022 Small objects localization: x and\\or y-coordinates of small objects like balls, or missiles\n\u2022 Other localization: the coordinates of enemies or important objects (keys, food) on the board\n\u2022 Score/Clock/Lives/Display: the variables that keep track of the state of the game\n\u2022 Miscellaneous: objects specific to a game\nTo measure the linear interpretability of the encoder \u03d5 s\u0313 representation, a linear classifier called a probe is trained in a supervised manner independently from the representation to predict the value of these variables of interests from the representation. Each variable requires a single probe, and each is a single layer 256-way linear classifier with no activation function which operates on the output layer of the encoder which also has a size of 256 as seen in Figure 2. The F1 scores of this classifier are averaged across each of the 5 categories for each of the 22 games, and the probe F1 scores are averaged across all games for each category are reported.\n3 Setup\nThanks to the effort of the authors, all of the code associated with the article was available on GitHub. Due to being associated with a laboratory, the authors had access to huge computational power: they used multiple P100 and V100 GPUs, and each of their machine had 8 cores to collect data in parallel. We only had access to Google Colaboratory, and 3 virtual machines with one Tesla K80 GPU each on Google Cloud Platform. Collecting samples, the training of an encoder, and its evaluation using linear probing takes around 10 hours, and in some cases over a day when training the ablation discussed in Section 5.3. For this reason, it was not feasible to evaluate our ablations on all 22 games. Only a subset of 1-5 games were chosen per modification implemented. As for collecting the data, a random agent was selected to play the picked games. Finally, Weights and Biases was used to keep track of our multiple trained models and visualize results.\n4 Reproduction of Results\nBy having the author s\u0313 code in our hands, we were able to reproduce their results for STDIM using random agent data collection. We reproduced the results achieved in the article by testing STDIMon the 22 games using the default parameters. Below is detailed our reproduction results. Another method of data collection that Anand et. al consider is using a pretrained agentwith \u03f5-greedy exploration added in [1]. The authors presented results for both data collection, but to save computation we only reproduced results for a random agent.\nReScience C 6.2 (#1) \u2013 Alacchi, Lam and Perreault-Lafleur 2020 4\n5 Ablations\n5.1 Parameter tuning Our first ablation was to tune multiple parameters of the base encoder model. We trained the encoder on the same frames from the Boxing game for different learning rates and then evaluated the representation with the linear probing. The average F1 scores of the variables in the category Score/Clock were greater than 0.95 on every learning rate. Hence, we did not focus on analyzing this part. The most interesting results we obtain was looking at the average F1 scores of the variables in the agent localization category.\nIn Figure 3, we see that the \u03b1 = 0.3 learning rate used in the article is the optimal value for the agent localization variables, but not for the score/clock variables. This suggests that an ensemble of encoders can be trained to predict a particular category of variables, and this would allow for an optimal learning rate for each category of variables instead of having a global learning rate applied on a single encoder. However, this idea implies\nReScience C 6.2 (#1) \u2013 Alacchi, Lam and Perreault-Lafleur 2020 5\ntraining five encoders instead of one.\nAfter this tuning process, we explored some different parameters of the encoder. Every training was realised with the same learning rate \u03b1 = 0.3.\nIn Figure 4, comparing (a) and (b) allows us to confirm that applying grayscaling to the frames does not impact the results. The results obtained with (a), (c) and (d) show that the default size of 100000 for the number of frames the model is trained on was well chosen. Indeed, taking less frames leads to worse results and taking more frames leads to very similar results. This parameter is important because in addition to affecting the accuracy of the encoders, they greatly impact the training time: (d) took around 15hours to train as opposed to 10 hours for (a).\n5.2 Modifying the Loss Function For our second ablation we modify and test the STDIM loss function by weighting the global-local and local-local objectives by a factor p \u2208 [0, 1]:\nL = p \u00b7 LGL + (1\u2212 p) \u00b7 LLL (4)\nBy varying p, we re-weight the importance of minimizing either objective s\u0313 loss, and by setting p = 0.5 they will be weighted equally, and thus the loss is equivalent to the standard STDIM loss. This ablation is similar to an ablation by Anand et. al which contrasts the global output of the encoder at both time steps, hence creating a new global-global objective based on InfoNCE [1]. Moreover, their codebase includes code which implements the p = 1 case of this loss, however this ablation wasn\u02bct explicitly mentioned in their article.\nReScience C 6.2 (#1) \u2013 Alacchi, Lam and Perreault-Lafleur 2020 6\nAs our initial hypothesis would suggest, the loss seemed to have general trend of being smaller as p does so as well for, as seen in Figure 5. All p value losses converge at roughly the same rate and differ by a constant as expected.\nAs seen in the left of Figure 6, a balanced loss for agent probing is by far the best, with both sides lower, p = 0.5 stands out as the best value. For scoring and miscellaneous variables, this is not as clear. The balanced loss is generally among the top values however, extreme values are able to outperform in certain categories. In the case of score/clock variables, either extreme is capable of achieving higher scores. As for miscellaneous states, on the right of Figure 6, this trend suggests that highly weighting the global local objective exhibits better performance in general.\nUnfortunately, these conclusions are based only on 9 runs and a single game. To get clearer trends, this cycle should be performed multiple times on each p value for more\nReScience C 6.2 (#1) \u2013 Alacchi, Lam and Perreault-Lafleur 2020 7\np values to get an idea for the variation of p. However, we are still quite within our means to come to the conclusion that a different encoder for each category of variables would be most beneficial as each category suggests to follow different trends.\nAn avenue for future research would be to experiment with increasing the number of frames used in the score function. Doing this would require extending the definition of the InfoNCE objective to three variables.\nINCE = N\u2211 i=1 log exp f(xi, xi+1, xi+2)\u2211\nj1 \u2211 j2 exp f(xi, xj1 , xj2) (5)\nIn STDIM, a window of three consecutive frames (xt, xt+1, xt+2)would constitute a positive sample, and the function f would take three frames. However it s\u0313 unclear if this extension would yield linearly interpretable representations as effectively as the bivariate case of InfoNCE as introduced by van den Oord [3].\n5.3 Modifying the Receptive Field of the Base Encoder Our final ablation was to explore the effect of tuning the receptive field size of the encoders when computing the loss in equation 2. Anand et. al use a receptive field size of 1/16 without justification in the paper [1]. The selection of the intermediate layer which defines \u03d5m,n in the bilinear score functions for the STDIM loss is fixed such that the receptive field of that layer is 1/16th of the original image. In our experiments we kept the layer we used fixed, but modified the strides of the base encoder from Figure 2 to achieve receptive field sizes of 1/8 and 1/32. The default strides used in the network are 4, 2, 2, 1, and thus the third convolutional layer has a receptive field size of 1/16. For testing a receptive field size of 1/8we employ the strides 4, 2, 1, 2, thus ensuring the output size is approximately the same barring padding. To accomplish 1/32 we employ the strides 4, 4, 2, 1, and to compensate for greatly reducing the size of the output of the final convolutional layer we change its depth from 64 to 128.\nTable 2 summarizes the results of this ablation. Training an encoder for 1/8 receptive field takes approximately 4 times as long due to there being twice as many spatial locations to compute the loss for in each dimension. Therefore due to compute time constraints we evaluated on a subset of 5 games which contain a significant diversity in variable types. These results validate that 1/16 was a good choice of receptive field by the authors, however 1/8 seems to performbetter for variables in theOther Localization andMiscellaneous categories. The results match closely for both 1/8 and 1/16 receptive field sizes, and thus both seem to be viable options for training encoders. As expected, 1/32 performs poorly as the mutual information estimate is based off large patches of the original image, and thus the encoder focuses on only a few locations in the image.\nReScience C 6.2 (#1) \u2013 Alacchi, Lam and Perreault-Lafleur 2020 8\nAn interesting direction for future work would be to experiment with adding multiple STDIM objectives based on various layers of the encoder, such that the loss hinges on multiple receptive field sizes.\nL = \u2211 \u2113 L(\u2113)GL+L (\u2113) LL = \u2211 \u2113 M\u2211 m=1 N\u2211 n=1 \u2212 log exp g(\u2113)m,n(xt, xt+1)\u2211 x\u22c6t exp g(\u2113)m,n(xt, x\u22c6t ) + log exp f (\u2113)m,n(xt, xt+1)\u2211 x\u22c6t exp f (\u2113)m,n(xt, x\u22c6t )  (6) where each score function g\u2113m,n and f \u2113m,n are defined using bi-linear functions as in equation 3 but \u03d5m,n is based on layer \u2113 of the encoder. This would force the encoder to jointly maximize the mutual information of its representation at each of these receptive field sizes.\n6 Discussion and Conclusion\nThanks to the efforts of the authors in making their code accessible and interpretable, the results obtained in their article were easily reproducible. Wemust point out the fact that noteworthy computational power is necessary for reproducing results and to investigate further. It took 17 days of total compute time to obtain all the results in this report with our limited computational power.\nWe noticed that the authorsmade choices about the architecture of the encoder, namely the receptive field size and the equal weighting of the GL and LL objectives, without explaining some of the theoretical motivations behind them. For instance, the 1/16 receptive field decision took up 1 sentence in the paper, and we believe that decision merits more exploration. To best complement their work, we focused our ablations on ideas that we felt were under explored. Our results showed that the author s\u0313 decisions with regards to those ablations were experimentally sound. However, bearing in mind that most of the final results stated throughout this study come from the evaluation of a single encoder, these don\u02bct fully prove their strength. For instance, it is much more likely to see a meaningful trend in the performance of the encoders as a function of p if we train multiple times over all 22 games and average the performances.\nFor future work, the first thing to do would be to repeat our experiments using a larger sample size in order to reduce variance. Also training an ensemble of different encoders for each category of variables, selecting ablations discussed in Section 5 with respect to the best choice for each category. We would also explore the various modifications to the loss as described in Sections 5.2 and 5.3.\n7 Acknowledgements\nWe would like to thank Ankesh Anand who gave us some hints and paths to explore at the beginning of our project, and was generally supportive by answering all of our questions throughout the process. We also thank our girlfriends who gave us all of their free Google Cloud Platform credits."}], "title": "[Re] Unsupervised Representation Learning in Atari", "year": 2020}