{"abstractText": "This report covers our reproduction effort of the paper \u2018Differentiable Spatial Planning using Transformers\u2019 by Chaplot et al. [1]. In this paper, the problem of spatial path planning in a differentiable way is considered. They show that their proposed method of using Spatial Planning Transformers outperforms prior data\u2010drivenmodels and lever\u2010 ages differentiable structures to learn mapping without a ground truth map simultane\u2010 ously. We verify these claims by reproducing their experiments and testing theirmethod on new data. We also investigate the stability of planning accuracy with maps with in\u2010 creased obstacle complexity. Efforts to investigate and verify the learnings of the Map\u2010 per module were met with failure stemming from a paucity of computational resources and unreachable authors.", "authors": [{"affiliations": [], "name": "Rohit Ranjan"}, {"affiliations": [], "name": "Himadri Bhakta"}, {"affiliations": [], "name": "Animesh Jha"}, {"affiliations": [], "name": "Parv Maheshwari"}, {"affiliations": [], "name": "Koustuv Sinha"}, {"affiliations": [], "name": "Sharath Chandra Raparthy"}], "id": "SP:2619d4e15f4ec3fa8161ba13ce6a4486ea5d9c92", "references": [{"authors": ["D.S. Chaplot", "D. Pathak", "J. Malik"], "title": "Differentiable spatial planning using transformers.", "venue": "In: International Conference on Machine Learning. PMLR", "year": 2021}, {"authors": ["L. Lee", "E. Parisotto", "D.S. Chaplot", "E. Xing", "R. Salakhutdinov"], "title": "Gated path planning networks.", "venue": "In: International Conference on Machine Learning. PMLR", "year": 2018}, {"authors": ["A. Tamar", "Y. Wu", "G. Thomas", "S. Levine", "P. Abbeel"], "title": "Value iteration networks.", "year": 2016}, {"authors": ["M. Savva", "A. Kadian", "O.Maksymets", "Y. Zhao", "E.Wijmans", "B. Jain", "J. Straub", "J. Liu", "V. Koltun", "J. Malik"], "title": "Habitat: A platform for embodied ai research.", "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision", "year": 2019}, {"authors": ["F. Xia", "A.R. Zamir", "Z. He", "A. Sax", "J. Malik", "S. Savarese"], "title": "Gibson env: Real-world perception for embodied agents.", "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "year": 2018}, {"authors": ["N. Ketkar"], "title": "Stochastic gradient descent.", "venue": "Deep learning with Python. Springer,", "year": 2017}], "sections": [{"text": "R E S C I E N C E C Replication / ML Reproducibility Challenge 2021\n[Re] Differentiable Spatial Planning using Transformers\nRohit Ranjan1,2, ID , Himadri Bhakta1,2, ID , Animesh Jha2, ID , and Parv Maheshwari2, ID 1Equal Contributions \u2013 2IIT Kharagpur, India\nEdited by Koustuv Sinha,\nSharath Chandra Raparthy\nReviewed by Anonymous Reviewers\nReceived 04 February 2022\nPublished 23 May 2022\nDOI 10.5281/zenodo.6574693"}, {"heading": "Reproducibility Summary", "text": ""}, {"heading": "Scope of Reproducibility", "text": "This report covers our reproduction effort of the paper \u2018Differentiable Spatial Planning using Transformers\u2019 by Chaplot et al. [1]. In this paper, the problem of spatial path planning in a differentiable way is considered. They show that their proposed method of using Spatial Planning Transformers outperforms prior data\u2010drivenmodels and lever\u2010 ages differentiable structures to learn mapping without a ground truth map simultane\u2010 ously. We verify these claims by reproducing their experiments and testing theirmethod on new data. We also investigate the stability of planning accuracy with maps with in\u2010 creased obstacle complexity. Efforts to investigate and verify the learnings of the Map\u2010 per module were met with failure stemming from a paucity of computational resources and unreachable authors."}, {"heading": "Methodology", "text": "The authors\u2019 source code and datasets are not open\u2010source yet. Hence, we reproduce the original experiments using source code written from scratch. We generate all synthetic datasets ourselves following similar parameters as described in the paper. Training the mapper module required loading our synthetic dataset over 1.6 TB in size, which could not be completed."}, {"heading": "Results", "text": "We reproduced the accuracy for the SPT planner module to within 14.7% of reported value, which, while outperforming the baselines [2] [3] in select cases, fails to support the paper\u2019s conclusion that it outperforms the baselines. However, we achieve a similar drop\u2010off in accuracy in percentage points over different model settings. We suspect that the vagueness in the accuracy metric leads to the absolute difference of 14.7% despite the paper being reproducible. We further improve the reproduced figures by increasing model complexity. The Mapper module\u2019s accuracy could not be tested."}, {"heading": "What was easy", "text": "Model architecture and training details were enough to easily reproduce.\nCopyright \u00a9 2022 R. Ranjan et al., released under a Creative Commons Attribution 4.0 International license. Correspondence should be addressed to Rohit Ranjan (ranjanmail.rohit@gmail.com) The authors have declared that no competing interests exist. Code is available at https://github.com/sirmisscriesalot/Differentiable-Spatial-Planning-using-Transformers \u2013 DOI 10.5281/zenodo.6475614. \u2013 SWH swh:1:dir:6aa6080e642126b1166661d245a4f594a777889b. Open peer review is available at https://openreview.net/forum?id=HFUI1pfQnCF.\nReScience C 8.2 (#34) \u2013 Ranjan et al. 2022 1"}, {"heading": "What was difficult", "text": "We lost significant time in generating all synthetic datasets, especially the dataset for the Mapper module that required us to set up the Habitat Simulator and API [4]. The ImageExtractor API was broken, and workarounds had to be implemented. The final dataset approached 1.6 TB in size, and we could not arrange enough computational re\u2010 sources and expertise to handle the GPU training. Furthermore, the description of the action prediction accuracymetric used is vague and could be one of the possible reasons behind the non\u2010reproducibility of the results.\nCommunication with original authors The authors of the paper could not be reached even after multiple attempts.\nReScience C 8.2 (#34) \u2013 Ranjan et al. 2022 2\n1 Introduction\nIn the original paper [1], the problem of spatial path planning in a differentiable way is considered. The authors show that their proposed method of using Spatial Planning Transformers outperforms prior data\u2010driven models that propagate information locally via convolutional structure in an iterative manner. Their proposed model also allows seamless generalisation to out\u2010of\u2010distributionmaps and goals and simultaneously lever\u2010 ages differentiable structures to learn mapping without a ground truth map.\n2 Scope of reproducibility\nWe seek to investigate the following major claims made in the paper:\n\u2022 Claim 1: Their proposed SPT planner module provides a definite improvement of 7\u201019% over state\u2010of\u2010the\u2010art CNN based planning baselines in average action prediction accuracy.\n\u2022 Claim 2: Their proposed SPT planner module maintains stability in accuracy as complexity increases and the number of obstacles increases.\n\u2022 Claim 3: Their proposed SPT module outperforms classical mapping and planning base\u2010 lines under an end\u2010to\u2010end mapping and planning setting.\n3 Methodology\nThe entire codebase iswritten fromscratch for the SPTmodules and the synthetic dataset generation in Python 3.6. Pytorch Lightning was used for the SPT modules. For dataset generation, similar parameters were used, as mentioned in the paper, to the maximum extent. The vagueness of parameters in terms of obstacle size allowed us to test out a range of obstacle sizes and the accuracy of the model on them. All runs were logged on the WandB platform. The training was done using NVIDIA Tesla T4 and P10 GPUs on Google Colaboratory Pro.\n3.1 Model descriptions Our implementation of the model follows the description provided in the paper taking liberties where details are vague. The input map and the goal map are stacked vertically and then fed into a CNN Encoder. The Encoder has 2 fully connected layers with a kernel size=1 and ReLU activation func\u2010 tion. The first layer increases the number of channels from 2 to 64, while the second layer maintains the number of channels and outputs a 64 channel encoded input. As described in the original paper, Positional encoding is added to the encoded input, which is then reshaped and fed into the Encoder part. Their are five encoder layers, each with nheads = 8, dmodel = 512 and dropout = 0.1. This output is fed into a Decoder made of a fully connected layer. The Decoder gives one output for each cell. The output is then reshaped to regain its original map shape. We carry further investigations on how the number of layers in the CNNEncoder, nheads and layers in the Encoder and embedding size affect the SPT Planner Module. Improve\u2010 ments were gained and are detailed in the Results section.\nReScience C 8.2 (#34) \u2013 Ranjan et al. 2022 3\n3.2 Datasets\nThe SPT Planner Module \u2014We create 3 datasets for the SPT planner module, each with a map size = {15 30 50} and up to 5 randomly generated obstacles. The position of the goal is randomly chosen from a free\u2010space cell. 2 different datasets are generated at map size = 15 with up to 10 and 15 obstacles, respectively. Each of these datasets has 100,000maps for training, 5,000 for validation and 5,000 for testing.\nThe End-to-End Mapper and Planner Module \u2014We further used the Habitat Simulator, and Habitat API [4] to generate 36000 maps for training the end\u2010to\u2010end model. Seventy\u2010two scenes from the Gibson dataset [5] from Stanford is loaded onto the simulator, and 500 maps with a grid cell dimension of 0.5 meters and map size of 15, are rendered from each scene. Ground truths for all datasets were generated using the classical Dijkstra\u2019s algorithm. This dataset is over 1.6 TB andmade it difficult to hand\u2010engineer training on limited GPU resources. All datasets generated and used have been released for open\u2010source and can be found on the project\u2019s github page.\n3.3 Hyperparameters An extensive hyperparameter grid search led us back to the same hyperparameters cited in the paper. The model is trained for 40 epochs with a learning rate decay of 0.9 per epoch, a starting learning rate of 1.0 and a batch size of 20. The model is separately trained for each of the map distributions using mean squared error loss and stochastic gradient descent [6].\nReScience C 8.2 (#34) \u2013 Ranjan et al. 2022 4\n4 Reproducibility Results\nWe reproduced the accuracy for the SPT planner module to within 14.7% of reported value, which, while outperforming the baselines [2] [3] in select cases, fails to support the paper\u2019s conclusion that it outperforms the baselines. However, we achieve a similar drop\u2010off in accuracy in percentage points over different model settings. We suspect that the vagueness in the accuracy metric leads to the absolute difference of 14.7% despite the paper being reproducible. The Mapper module\u2019s accuracy could not be tested.\nFigure 4. Sample output for Navigation Task (left) and Manipulation Task (right) visualised. \u2217 Could not be trained due to lack of enough computational resources.\n5 Further Investigation Results and Discussion\nThe CNN Encoder \u2014 The CNN Encoder takes the map and the goal location as the input and encodes the information into an embedding of size dmodel. This is achieved by a\nReScience C 8.2 (#34) \u2013 Ranjan et al. 2022 5"}, {"heading": "M=15", "text": "multi\u2010layer, fully connected convolutional neural network. The kernel size for the con\u2010 volutions is fixed at 1 to have the Encoder generate the same embedding for all input map cells. The CNN Encoder plays a vital role in distilling the input map and represent\u2010 ing it in the best way possible for the Transformer to act on. Table 2 lists all investigation results on the CNN Encoder parameters. Our experiments reveal that while embedding sizes in a reasonable domain have similar accuracies, a higher embedding size provides more expressive power to the model and provides the best accuracy beating the original SPT parameters. We also see an increase in accuracy with increasing CNN Encoder layers. layers = 8 achieves the best accuracy as well as the best validation loss which shows the increase in expressive power of the encodings.\nObstacle Complexity \u2014 Obstacle complexity refers to the distribution of obstacles in the in\u2010 put map. The paper only cites results on input maps with a normal distribution of up to 5 obstacles. We found it crucial to test the SPT\u2019s spatial awareness and learning capa\u2010 bilities as this complexity is heightened. For this purpose, we created two new datasets with a higher distribution of obstacles. Table 3 lists our investigation results on these datasets. We achieved the best accuracy on the distribution with up to 15 obstacles. However, the best validation loss is achieved with the lowest obstacles setting. This leads us to con\u2010 clude that only looking at accuracy figures might be misleading because an increase in obstacles decreases the number of free spaces and consequently the number of predic\u2010 tions the SPT model has to generate.\nThe Transformer Encoder \u2014 The Transformer Encoder takes input that has been encoded into higher embedding space and has been appended with positional encoding. It is fol\u2010 lowed by a Decoder, a fully connected layer that decodes the embeddings finally given out by the Encoder. The number of multi\u2010attention heads and encoder layers affects the expressive power of the Transformer. We conduct investigations by changing these parameters. Table 4 lists these results. The best accuracy is achieved with nheads = 4 and nlayers = 8. A severe drop in accu\u2010 racy is found with nlayers = 12. This leads us to conclude that while increasing nlayers increases learning capabilities of the SPT Planner module, excessive parameters might not be learnt properly from our dataset of size 100,000. The same reason suffices for an increase in nheads.\nReScience C 8.2 (#34) \u2013 Ranjan et al. 2022 6\nThe Best Model \u2014 The prior discussion points out that increasing the expressive power of the CNN Encoder and increasing the complexity of the Transformer Encoder helps increase the accuracy of the model. We combine all these changes to train our best model. The parameters used are: nlayers = 8, dmodel = 128, nheads = 4 andnlayers = 8. The accuracy achieved is 85.14 with a validation loss of 0.651. These figures beat the reproduced SPT Planner Module by 0.87% and 57.64% respectively.\n6 Discussion\n6.1 What was easy The easiest part of the reproduction effort was getting the Spatial Planning Transformer model up and ready from scratch. The authors\u2019 instructions regarding the layer param\u2010 eters and encoder\u2010decoder structure were abundantly clear. Furthermore, although ini\u2010 tialisation information was missing, the model was robust enough to learn under vari\u2010\nReScience C 8.2 (#34) \u2013 Ranjan et al. 2022 7\nous settings.\n6.2 What was difficult We lost significant time generating all synthetic datasets, especially the dataset for the mapper module that required us to set up the Habitat Simulator and API. The Image\u2010 Extractor API was broken, and workarounds had to be implemented. The final dataset approached 1.6 TB in size, and we could not arrange enough compute resources and expertise to handle the GPU training. The SPT Planner Module could not be trained on the M=50 dataset following the same issue.\n6.3 Reproducibility of results of SPT Planner Module Our results lag thosementioned in the paper by amargin of over 14.7%, whichmakes us believe that the paper is not reproducible in its exact form. However, we achieve a sim\u2010 ilar drop\u2010off in accuracy in percentage points over different model settings. We suspect that the paper is indeed reproducible, but the datasets\u2019 vagueness and accuracy met\u2010 ric lead to the exaggerated absolute difference. The lack of openly available standard datasets in the domain presents a challenge. Different papers have to report results on datasets of their choice using a metric they design themselves. The original paper\u2019s au\u2010 thors also did this with their synthetic datasets and a novel action prediction accuracy metric. Furthermore, these datasets are not open\u2010sourced, and generation parameters in the paper are vague in terms of obstacle complexity and size. Our reproductionwould have led to higher accuracies if the authors had provided the accuracy metric code and datasets. Our experiments with maps of increasing obstacle complexity result in a slight increase in validation loss. This points to a plausible explanation for non\u2010reproducibility. The non\u2010uniformity of dataset\u2010generation guidelines couldhave resulted in obstacles of greater size in our synthetic dataset.\n6.4 Stability of the SPT Planner Module Our results show comprehensively that the SPT Planner Module is stable concerning av\u2010 erage action prediction accuracy for slight changes in obstacle complexity andmodel pa\u2010 rameters ranging from CNN Encoder to the Transformer Encoder. This lays the ground for further research that can apply SPTs tomazes and increasingly complex scenes with\u2010 out considerable loss of accuracy.\n6.5 Communication with original authors The authors of the paper could not be reached even after multiple attempts.\n7 Conclusion\nWehave tried to reproduce the paper to the best of our abilities, following the textual de\u2010 scriptions for source code and dataset generation to themaximum extent. Wewere able to improve the reproduced accuracy and loss of the SPT Planner Module by 0.87% and 57.64%, respectively, by increasing the CNN Encoder depth, embedding size and Trans\u2010 former Encoder complexity. This provides ground for further research into increased complexities models that might draw deeper insights and plan more accurately. We could not train the End\u2010to\u2010EndMapper and Planner Module due to a paucity of com\u2010 putational resources. The results that could not be reproduced are so prohibitively ex\u2010 pensive that only very few can afford it, hence it would be better for the community if subsequent authors to this topic make their code and dataset public.\nReScience C 8.2 (#34) \u2013 Ranjan et al. 2022 8"}], "title": "[Re] Differentiable Spatial Planning using Transformers", "year": 2022}