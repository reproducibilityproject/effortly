{"abstractText": "Based on the intuition that attention in neural networks is what themodel focuses on, attention is now being used as an explanation for a models\u02bc prediction (see Galassi, Lippi, and Torroni1 for a survey). Pruthi et al.2 challenge the usage of attention-based explanation through a series of experiments using classification and sequence-to-sequence (seq2seq) models. They examine the model s\u0313 use of impermissible tokens, which are user-defined tokens that can introduce bias e.g. gendered pronouns. Across multiple datasets, the authors show that with the impermissible tokens removed themodel accuracy drops, implying their usage in prediction. And then by penalising attention paid to the impermissible tokens but keeping them in, they train models that retain full accuracy hence must be using the impermissible tokens, but that does not show attention being paid to the impermissible tokens. As the paper s\u0313 claims have such significant implications for the use of attention-based explanations, we seek to reproduce their results.", "authors": [{"affiliations": [], "name": "Rahel Habacker"}, {"affiliations": [], "name": "Andrew Harrison"}, {"affiliations": [], "name": "Mathias Parisot"}, {"affiliations": [], "name": "Ard Snijders"}, {"affiliations": [], "name": "Koustuv Sinha"}, {"affiliations": [], "name": "Sasha Luccioni"}], "id": "SP:a400da5d8dccbca5daa06d14d32e954cfc39792e", "references": [{"authors": ["A. Galassi", "M. Lippi", "P. Torroni"], "title": "Attention in natural language processing.", "venue": "IEEE Transactions on Neural Networks and Learning Systems", "year": 2020}, {"authors": ["D. Pruthi", "M. Gupta", "B. Dhingra", "G. Neubig", "Z.C. Lipton"], "title": "Learning to Deceive with Attention-Based Explanations.", "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Online: Association for Computational Linguistics,", "year": 2020}, {"authors": ["T. Wolf"], "title": "HuggingFace\u2019s Transformers: State-of-the-art Natural Language Processing.", "venue": "CoRR abs/1910.03771", "year": 2019}, {"authors": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "title": "Neural machine translation by jointly learning to align and translate.", "year": 2014}, {"authors": ["A. Vaswani", "N. Shazeer", "N. Parmar", "J. Uszkoreit", "L. Jones", "A.N. Gomez", "L. Kaiser", "I. Polosukhin"], "title": "Attention is all you need.", "year": 2017}, {"authors": ["J. Devlin", "M.-W. Chang", "K. Lee", "K. Toutanova"], "title": "Bert: Pre-training of deep bidirectional transformers for language understanding.", "year": 2018}, {"authors": ["S. Serrano", "N.A. Smith"], "title": "Is Attention Interpretable?", "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics", "year": 2019}, {"authors": ["S. Jain", "B.C. Wallace"], "title": "Attention is not Explanation.", "year": 2019}, {"authors": ["S. Wiegreffe", "Y. Pinter"], "title": "Attention is not not Explanation.", "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)", "year": 2019}, {"authors": ["A. Graves", "J. Schmidhuber"], "title": "Framewise phoneme classification with bidirectional LSTM networks.", "venue": "In: Proceedings", "year": 2005}, {"authors": ["D. Elliott", "S. Frank", "K. Sima\u2019an", "L. Specia"], "title": "Multi30K: Multilingual English-German Image Descriptions. 2016", "year": 2016}, {"authors": ["C. Dyer", "V. Chahuneau", "N.A. Smith"], "title": "A Simple, Fast, and Effective Reparameterization of IBM Model 2.", "venue": "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies", "year": 2013}, {"authors": ["G. Neubig", "Z.-Y. Dou", "J. Hu", "P. Michel", "D. Pruthi", "X. Wang"], "title": "compare-mt: A Tool for Holistic Comparison of Language Generation Systems.", "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations). Minneapolis, Minnesota: Association for Computational Linguistics,", "year": 2019}, {"authors": ["S. Bird", "E. Klein", "E. Loper"], "title": "Natural Language Processing with Python: Analyzing Text with the Natural Language Toolkit", "venue": "Beijing: O\u2019Reilly,", "year": 2009}], "sections": [{"text": "R E S C I E N C E C"}, {"heading": "Replication / ML Reproducibility Challenge 2020", "text": "[Re] Reproducing Learning to Deceive With"}, {"heading": "Attention-Based Explanations", "text": "Rahel Habacker1, ID , Andrew Harrison1\u201e ID , Mathias Parisot1\u201e ID , and Ard Snijders1\u201e ID 1University of Amsterdam, Amsterdam, The Netherlands\nEdited by Koustuv Sinha, Sasha Luccioni\nReviewed by Anonymous Reviewers\nReceived 29 January 2021\nPublished 27 May 2021\nDOI 10.5281/zenodo.4834146"}, {"heading": "Reproducibility Summary", "text": ""}, {"heading": "Scope of Reproducibility", "text": "Based on the intuition that attention in neural networks is what themodel focuses on, attention is now being used as an explanation for a models\u02bc prediction (see Galassi, Lippi, and Torroni1 for a survey). Pruthi et al.2 challenge the usage of attention-based explanation through a series of experiments using classification and sequence-to-sequence (seq2seq) models. They examine the model s\u0313 use of impermissible tokens, which are user-defined tokens that can introduce bias e.g. gendered pronouns. Across multiple datasets, the authors show that with the impermissible tokens removed themodel accuracy drops, implying their usage in prediction. And then by penalising attention paid to the impermissible tokens but keeping them in, they train models that retain full accuracy hence must be using the impermissible tokens, but that does not show attention being paid to the impermissible tokens. As the paper s\u0313 claims have such significant implications for the use of attention-based explanations, we seek to reproduce their results."}, {"heading": "Methodology", "text": "Using the authors\u02bc code, for classifiers we attempt to reproduce their embedding, BiLSTM, and BERT results across the occupation prediction, gender identify, and SST +wiki datasets. Further, we reimplemented BERT using HuggingFace s\u0313 transformer library [3] with restricted self-attention (information cannot flow between permissible and impermissible tokens). For seq2seq we used the authors\u02bc code to reproduce results across Bigram Flip, Sequence Copy, Sequence Reverse, and English-German (En-De) machine translation datasets. We performed refactoring on the authors\u02bc code aiming toward a more uniformly usable code style as well as porting across to PyTorch Lightning. All experiments were run in approximately 130 GPU hours on a computing cluster with nodes containing Titan RTX GPUs."}, {"heading": "Results", "text": "We reproduced the authors\u02bc results across all models and all available datasets, confirming their findings that attention-based explanations can be manipulated and that mod-\nCopyright \u00a9 2021 R. Habacker et al., released under a Creative Commons Attribution 4.0 International license. Correspondence should be addressed to Rahel Habacker (rahel.habacker@student.uva.nl) The authors have declared that no competing interests exist. Code is available at https://github.com/MatPrst/deceptive-attention-reproduced \u2013 DOI 10.5281/zenodo.4692668.. Data is available at https://github.com/MatPrst/deceptive-attention-reproduced/tree/main/deceptive-attention/src/classification/data \u2013 DOI 10.5281/zenodo.4686793. Open peer review is available at https://openreview.net/forum?id=-rn9m0Gt6AQ.\nReScience C 7.2 (#6) \u2013 Habacker et al. 2021 1\nels can learn to deceive. We also replicated their BERT results using our reimplemented model. There was only one result not as strongly (> 1 S.D.) in their experimental direction."}, {"heading": "What Was Easy", "text": "The authors\u02bc methods were largely well described and easy to follow, and we could quickly produce the first results as their code worked straightaway with minor adjustments. They were also extremely responsive and helpful via email."}, {"heading": "What Was Difficult", "text": "Re-implementing the BERT-based classificationmodel to perform replicability, with further specification details on model architecture, penalty mechanism, and training procedure needed. Also, porting code across to PyTorch Lightning."}, {"heading": "Communication With Original Authors", "text": "There was a continuous email chain with the authors for several weeks during the reproducibility work. They made additional code and datasets available per our requests, along with providing detailed responses and clarifications to our emailed questions. They encouraged the work and we wish to thank them for their time and support.\n1 Introduction\nAttention is a mechanism to automatically learn the relevance of different elements of the input to a model, rather than relying on manual feature engineering, allowing computational learning to focus on important elements [1]. Originally introduced to natural language processing (NLP) for neural machine translation [4] its usage has since expanded. Vaswani et al.5 termed it \u201dall you need\u201d, having removed recurrence and convolutions and relied on attention in their Transformer architecture. Transformers are now in wide use, with Devlin et al.6 s\u0313 Bidirectional Encoder Representations from Transformers (BERT) a commonly used model in NLP. Because neural networks are subsymbolic with knowledge stored numerically, it is challenging to understand their inner workings [1]. With interpretability a growing concern in NLP, there is a body of work on attention-based explanations of neural architectures using visualisation of attention weights [7]. However, there is a rich and ongoing debate about whether attention is an explanation or not [8, 9]. Acknowledging the debate, Pruthi et al.2 whose work we seek to reproduce, examine whether models can learn to deceive, by adding a penalty to the loss function that punishes themodel when attention is paid to impermissible tokens. These tokens are user-defined and may refer inter alia to terms for protected traits such as gender (the pronouns she, her etc.), sexual orientation, or race. Their research indicates that the impermissible tokens are still being used by the model as there is no accuracy drop seen, while there is one when these tokens are instead fully removed. Thus, the model is both able to use the impermissible tokens in learning and inference, but not pay attention to them. Hence bringing into question the validity of using attention in the explanation of a model s\u0313 decision.\n2 Scope of Reproducibility\nThe core finding of the paper is that attention-based explanations of models can be deceptive by, for instance, hiding the model s\u0313 use of gendered pronouns at inference from an auditor. Specifically, the authors show that attention weights can be manipulated\nReScience C 7.2 (#6) \u2013 Habacker et al. 2021 2\nduring training by penalising the allocation of attention to impermissible tokens, without this affecting model performance. Any resulting attention-based explanation might suggest that the model did not rely on impermissible tokens to make its predictions, when in reality the model still uses these but not through the attention mechanism, thereby making the model \u201cdeceptive\u201d. The key findings can be decomposed into the following claims which we are testing in our research:\n1. The attention mass on impermissible tokens can be reduced without significantly affecting the classification accuracy of Embeddings + Attention, BiLSTM + Attention, and BERT + Attention models across several tasks (see Tables 3 and 4).\n2. The attention mass on impermissible tokens can be reduced without significantly affecting the seq2seq performance on translation as measured by BLEU (see Table 7).\n3. The attention mass on impermissible tokens can be reduced without significantly affecting the seq2seq accuracy on synthetic data tasks (see Tables 5 and 6).\nWe chose not to perform the human study, testing whether visualised attention weights coulddeceiveNLP/ML trained andTransformer knowledgeable participants, aswedeemed that the small sample size does not add value to the results.\n3 Methodology\nInitially, we attempted to reproduce the findings from the provided repository without contacting the authors. The authors use three classification models: embeddings with attention; BiLSTM with attention; and a BERT-based model with attention. For the seq2seq tasks, the model is an encoder-decoder architecture. Code for all models (except BERT) was available in the authors\u02bc repository, and aside from minor dependency issues in the environment file, we were able to successfully run experiments to reproduce the results. We re-implemented the BERT-based model to replicate their results, and after the BERT-based code was added to the repository, we also reproduced their results. Lastly, we refactored the existing code-base and ported the PyTorch-based code to PyTorch Lightning.\n3.1 Attention Manipulation To explicitly optimise themodels to learn deceptive attention weights, the authors introduce an auxiliary loss component that penalises the model for attending to impermissible tokens. Impermissible tokens are user-defined from a corpus and are the set of words I that a model should not use during training or inference, as they might introduce bias or other ethical issues. An example is the use of gendered pronouns \u201dher, she, Ms.\u201d whichmight lead amodel to discriminate against a specific gender. The remaining words in the corpus are deemed permissible, and thus constitute the complement set Ic. Assuming an input sequence S = w1, w2, ..., wn of n tokens, the authors proceed to define a binary attention mask vector m of length |S|, with each element denoting the occurrence of an impermissible token:\nmi = { 1, if wi \u2208 I 0 otherwise\nFurthermore, we assume an attention vector \u03b1 \u2208 [0, 1]n which denotes the allocated attention for each token in the input sequence. From this, the authors construct the additive task-agnostic penalty termR, such thatL\u2032 = L+R, whereR captures the extent\nReScience C 7.2 (#6) \u2013 Habacker et al. 2021 3\nto which a model s\u0313 attention layer is penalised for allocating attention to impermissible tokens:\nR = \u2212\u03bb log(1\u2212\u03b1Tm)\nHere, the\u03b1Tm termdenotes the attention allocated to impermissible words. Taking the negative log of the complement of this term then allows us tominimise this quantity via standard gradient descent. Furthermore, \u03bb is a coefficient that is used to control the extent to which impermissible attention allocation is penalised. Following the authors\u02bc methods, we consider values for \u03bb = {0, 0.1, 1.0}. For models featuring multi-head attention (such as BERT), the authors use two different penalty variants. Namely, Rmean optimises the mean of the penalty over the set of all heads H, while Rmax instead only considers the head which allocates the most attention to impermissible tokens:\nRmean = \u2212 \u03bb |H| \u2211 h\u2208H log(1\u2212\u03b1Thm), Rmax = \u2212\u03bb \u00b7min h\u2208H log(1\u2212\u03b1Thm)\nNote that for the multi-head penalties, only the heads from the model s\u0313 last layer are considered, thus in BERT \u03b1 is defined as the attention paid by the [CLS] token to the other tokens.\n3.2 Model Descriptions Embedding + Attention This model serves as a baseline to the other models used for classification. It contains between 2.7M and 6.5M parameters depending on the dataset and consists of a dot-product attention mechanism applied on word embeddings. The resulting attention vector is then passed into a linear classifier followed by a softmax activation. The size of the embedding in the original paper was 128. We use crossentropy loss and the Adam optimizer with a learning rate of 0.001 and no weight decay. Pruthi et al.2 argue that the accuracy of the Embedding and BiLSTM models could have been greatly impacted by the lambda parameter because those models might be under-parameterised for the SST-Wiki dataset. To study this we also train models with embedding sizes of 256 and 512. BiLSTM+Attention Themodel still uses word embeddings and consists of a dot-product attention mechanism applied on the output of a bidirectional LSTM [10]. The resulting attention vector is then passed into a linear classifier followed by a softmax activation. Again, the size of the embedding is 128, but we also train models with embedding sizes of 256 and 512. Its number of parameters is between 2.8M and 6.6M parameters depending on the size of the vocabulary of the given dataset. The BiLSTMwas trained using the same hyper-parameters as the Embedding model above, with the dimension of the hidden state being 64. Transformer Models For the transformer-based architecture, we use BERT [6]. Specifically, we use a pre-trained instance of the uncased BERT base model, which consists of 12 transformer blocks (each with 12 heads) amounting to 109M trainable parameters. We trained each model for 10 epochs, with a batch size of 32. All models were optimised using Adam, with a learning rate of 5e\u22125. Furthermore, we applied dropout with p = 0.3 to improve model generalisation. A sequence classification layer is added on top of the architecture, to adapt the model for sentence classification. Following the authors\u02bc methods, we apply a self-attention mask M to the self-attention probabilities via element-wise multiplication in the models\u02bc forward pass, to avoid information flowing between the sets of impermissible tokens I and permissible tokens Ic. Specifically, M is a binary matrix of size n \u00d7 n, where n denotes the sequence length, and where elements Mp,q are 1 if both tokens wp and wq belong to the same set (either I or Ic), and 0 otherwise. Moreover, the first column of M , which denotes the extent to which all other tokens attend to the [CLS] token, is zero also, as this further restricts the flow of information between tokens from I and Ic via the [CLS] token.\nReScience C 7.2 (#6) \u2013 Habacker et al. 2021 4\nSeq2seq Pruthi et al.2 provide a bidirectional and unidirectional Gated Recurrent Unit (GRU) with dot-product attention respectively for their encoder-decoder model tackling seq2seq tasks. The input is passed through the encoder and decoder, where the final hidden state from the bidirectional GRU fed through a linear layer is the initial hidden state to the decoder. The embedding size was 256 and hidden size 512 for both encoder and decoder. We also use a teacher forcing ratio of 0.5 aswell as the top-1 greedy strategy for decoding output sequences. For baseline experiments, this model is also trained with no attention and uniform attention overall source tokens. It overall contains 8.7M parameters for the synthetic datasets and 48.55M parameters for the Multi30K dataset, which are both described in Section 3.3.\n3.3 Datasets The originalwork features 8 taskswith associated datasets. For the classificationmodels, these are Occupational Prediction, Gender Identity, SST + Wiki, and Reference Letters. For the seq2seq experiments, three synthetic datasets were used; Bigram Flip, Sequence Copy, and Sequence Reverse tasks. Additionally, the Multi30K dataset [11] was used for English to German machine translation (MT). All of the datasets were available in the authors\u02bc repository except for Reference Letters, with the authors citing privacy concerns. Consequently, we were not able to reproduce this experiment. For Occupation Prediction the authors state that downsampling by a factor of 10 was done for minority classes. As it was not clear from the data provided in the repository if downsampling had already been applied, the authors confirmed via email that this was the case. No further pre-processing was required, besides that already present in the authors\u02bc code. Details of the datasets used in our experiments are provided in Table 1.\n3.4 Hyperparameters Except for the \u03bb coefficient (values 0.0, 0.1, and 1.0) as used in the computation of the regularising component R, the original work did not provide details regarding hyperparameters and/or tuning thereof. Upon contacting the authors, we learned that no hyperparameter tuning was performed, as the experimental findings could be achieved with conventional parameters. Therefore, in reproducing their experiments we have used the same standard configurations as the authors.\n3.5 Experimental Setup and Code\nThe code used to reproduce the experiments can be found in this Github repository1. The authors\u02bc negative baseline, the first row of each model in Table 3 of the original paper, was produced by removing the impermissible tokens (anonymising or deleting).\n1https://github.com/MatPrst/FACT\nReScience C 7.2 (#6) \u2013 Habacker et al. 2021 5\nThey show that the performance of the model dropped. This drop was in comparison to the true baseline in row two, which provides the models\u02bc performance when impermissible words are freely used with no manipulation penalty applied i.e. \u03bb = 0.0. The third and fourth rows provide results for adjusting the penalty coefficient to 0.1 and 1.0 respectively. To reproduce the experiments by anonymising or removing the impermissible tokens, we had to look deeper into their script. For the Occupation Prediction and Gender Identity datasets, the authors provided an anonymisation functionality that transformed all pronouns to gender-neutral ones. However, for the SST + Wiki dataset, we had to implement the functionality to remove the SST sentence because it was not present in the scripts. Similarly, we added the functionality to the training script for the provided BERT implementation. The repository contained a bash script to run the experiments with Embedding + Attention and BiLSTM + Attention without removing or anonymising the impermissible tokens. We recreated the classification experiments using the seed values from this script. The training outputs of those experiments were not as presented in the README of the authors\u02bc repository and did not contain a clear attentionmass value. However, the authors clarified that the \u201cattention ratio\u201d measure was used in the paper. From those experimental runs, we could determine the average and standard deviation of the five runs. For the classificationmodels, we chose to largely follow the authors\u02bc experimental setup: we run experiments with the same set of values for the loss coefficient \u03bb \u2208 0, 0.1, 1.0. Furthermore, allmodels are trained for 10 epochs and are evaluated on the development set after each epoch. Here, we measure two metrics; the validation accuracy, and the average attention mass over all examples. The model with \u03bb = 0 serves as the baseline for the \u02bcadversarial\u02bc models with \u03bb = 0.1, 1.0, i.e. themodels that are explicitly optimised to learn deceptive attentionmaps. For the BERT replication, when evaluatingmodels on the test set, we follow the authors\u02bc heuristic, i.e. we select the checkpointwhich iswithin 2% of the baseline test accuracy, and which has the greatest reduction in attentionmass on the validation set. As per the authors\u02bc experimental setup we considered the four sequence-to-sequence tasks: Bigram Flipping, Sequence Copying, and Sequence Reversal are synthetic tasks that work with input-output-mappings with the respective gold alignments considered as impermissible tokens. Themodels are trained on 100K random input sequences with length 32 from a vocabulary of 1000 tokens and validated and tested on 100K unseen random sequences. Machine translation from German to English acts as the fourth task for which gold alignments are not available. Thus, the Fast Align toolkit [12] was used by the authors to align target and source words. In this task, the aligned words are used as impermissible tokens. The seq2seq experimental results in Table 4 of the original paper provide the averaged value over five runs. The different runs and their results were not available in the repository, however, after emailing the authors we were provided with the results for each of the five experimental runs in each condition, along with the seeds used. This allowed us to recreate the experiment using the same seeds, and to determine along with the average, whether the standard deviation between our results also matched. Pertaining to the English to German translations, the BLEU score was used, however, this was not available in the repository. After contacting the authors, we were provided with a link to the BLEU library they had used which is called \u201ccompare-mt\u201d [13]. We had meanwhile used the NLTK implementation [14], presuming it to be the most likely used. Therefore, for translation, we have used two different BLEU implementations: comparemt for reproduction and NLTK for replication.\nReScience C 7.2 (#6) \u2013 Habacker et al. 2021 6\nClassification GPU Hours\nModel Batch size Occupation Pred. Gender Identity SST + Wiki\nEmbedding 1 0.62 0.56 1.1 BiLSTM 1 0.94 1 1.3 BERT 32 3.1 1.5 1.2 BERT(HF) 32 4.8 1.9 2.5\nSeq-to-Seq GPU Hours\nModel Batch size Bigram Flip Seq. Copy Seq. Reverse En-De\nEnc-Dec 128 0.42 0.36 0.34 0.15\nReScience C 7.2 (#6) \u2013 Habacker et al. 2021 8\nReScience C 7.2 (#6) \u2013 Habacker et al. 2021 9\n5 Results Beyond Original Paper\nThe authors state that their seq2seq results in Table 4 of the original paper are based on the average of 5 different seeds. Additional to their workwe have examined the standard deviations alongside the average for all the results, comparing their results and ours. Further, while the authors do not state whether classification results from Table 3 in their original paper are based on the average of 5 seeds, we have again completed 5 runs of the experiments and provided the average.\n5.1 Under-Parameterised Models In the original paper, the classification results for the Embedding and BiLSTM models for the task on SST+Wiki are outliers because, while the attention mass over the impermissible tokens decreases as \u03bb increases, the test accuracy also decreases significantly. The authors speculate that this behavior is due to the models being underparameterised. We investigated this by training Embedding and BiLSTM models with larger embedding dimensions. In particular, we compared embedding sizes of 128 (original size), 256, and 512. The results are presented in Tables 8 and 9. Increasing the dimensionality of the embedding does not seem to prevent the accuracy from dropping for larger values of \u03bb. We speculate that the drop in accuracy is due to the way the impermissible tokens are defined for the SST+Wiki dataset. All the words belonging to the SST sentence are labeled as impermissible and will therefore be penalised by the auxiliary loss component. Because the Wikipedia sentence does not provide useful information for the sentiment prediction, themodel cannot rely on it and the accuracy reduces as the penalty term increases. This behaviour does not occur for the other datasets because only a few impermissible tokens were selected for the experiments, allowing the model to find other proxy tokens carrying information about the respective classification tasks. For example, words such as \u201dlesbian\u201d or gendered names such as \u201dMark\u201d were not la-\nReScience C 7.2 (#6) \u2013 Habacker et al. 2021 10\nbeled as impermissible in the Occupation Prediction dataset. An alternative experiment could have been to define the impermissible tokens of the SST+Wiki dataset as thewords in the SST sentence with the strongest positive or negative sentiment scores.\n6 Discussion\nOur results reproduce Pruthi et al.2 s\u0313 finding that models can learn to deceive. Jain and Wallace8 note that for attention to be an explanation, a different configuration of attention weights for the same piece of text should lead to different predictions. The research which we have reproduced implies that the same accuracy (hence prediction) can be maintained while explicitly changing the configuration of attention weights. The implications are clear; either it is providing further evidence for why attention should not be thought of as an explanation, supporting Serrano and Smith7 s\u0313 findings that attention weights can be largely zeroed out without affecting accuracy. Or, if attention is an explanation, then models can be still be trained to change the attention-based explanation given and deceive algorithmic audit. This research thus provides a new vein of the investigation into the attention-based explanation debate. Examination of the standard deviations showed whether reproduction differences were meaningful for seq2seq tasks. Regarding synthetic data, it showed some variance from the authors\u02bc values for attention mass, but it was more strongly in the experimental direction, thus supporting their findings. For machine translation, one result at \u03bb = 0.1 for attentionmasswas not as strong as their result, but still trending in the experimental direction. While the standard deviation does show that there is some variance inherent in the reduced attentionmasses undermanipulation, it provides still further robustness\nReScience C 7.2 (#6) \u2013 Habacker et al. 2021 11\nto the findings. Finally, we obtain very similar results with our replication of the authors\u02bc BERT-based classification model, for both the mean and max penalties, for the majority of the tasks. The only results which we did not manage to exactly replicate concern the test accuracies for SST+Wiki. However, given that we did manage to reproduce these results with the authors\u02bc implementation of BERT, it is more likely that the differences in test accuracies between the reproduction and replication experiments can be attributed to slight differences in hyperparameter settings between the two - however, further investigation would be required to confirm this. Further to this, we observe somewhat large differences between the attention masses - particularly for values where \u03bb = 0. However, given that similar differences persist also for our reproduction results of BERT, it may be the case that these quantities are simplymore susceptible to stochasticity and/or training dynamics, which would then explain the observed discrepancies with the authors\u02bc findings. Either way, these particular differences do not provide grounds to reject the authors\u02bc hypotheses; we still observe that for most replication runs, the accuracy is not impacted despite substantial reductions in attention masses on the impermissible tokens. To this end, our replication efforts provide further evidence for the authors\u02bc claims that attention-as-explanation can be deceptive.\n6.1 What Was Easy Overall, we could easily understand and follow the authors\u02bc descriptions of their methods in the paper. An example of this is the way seq2seq tasks and the datasets used were described in a short but comprehensive way. The provided bash scripts revealed the basic setup and could almost immediately be used to run the experiments on our infrastructure described in Section 3.6. Therefore, we were quickly able to reproduce the first results. The codebase for seq2seq tasks was easily restructured into functions, instead of keeping the train and evaluation functionality at a global code level. Besides this, applying PEP8 to the codebase was an easy task - another positive is that restructuring did not break the code massively at any point, which in our opinion testifies to an already consistent architecture. The authors\u02bc integration of the BLUE score implementation (compare-mt) was missing. However, we could easily add the NLTK BLEU score implementation into the code. We could further observe robust BLEU score results, like the ones we reproduced and replicated turned out to be not significantly different from those reported by the authors.\n6.2 What Was Difficult To a certain extent, it was achievable to re-implement the BERT-based model using only the information provided in the original work. Nevertheless, in the process of reimplementing, several ambiguities arose that we were initially unable to resolve and which had to be clarified by the authors. For instance, the penalty mechanism used to compute R by default assumed an attention vector \u03b1, while BERT, by default, outputs self-attention matrices. We were initially unsure how to obtain a vector from this matrix; only after contacting the authors, we learned that \u03b1 can be obtained from BERT s\u0313 self-attention matrices by only considering the first row of the matrix (for a given selfattention head), which represents the extent to which the [CLS] token attends to all other tokens in the sentence. The authors also further clarified that they only used the attention output of the last (12th) transformer block of the model, whereas we initially understood from the paper that all layers should be taken into account. Additionally, there was some brief confusion relating to the evaluation procedure. In their papers\u02bc Section 5.1, the authors provide a heuristic to \u201cpick\u201d the best deceptive model. Based on how this procedure was formulated, we initially believed that multiple models were trained for a given epoch sequentially, after which the best model (as evaluated by admissible accuracy and largest reduction in attention mass) was selected for\nReScience C 7.2 (#6) \u2013 Habacker et al. 2021 12\nthe next epoch. However, after corresponding with the authors, we learned that rather, a single model is simply trained for 10 epochs, and only then the selection heuristic is applied manually to determine which model checkpoint will be used for evaluation on the test set. In these respects, replicating the model architecture, penalty mechanism, and training proceduremight have been easier, had the original work beenmore precise and explicit regarding its methods. We also chose to port the code to PyTorch Lightning to make it easier to reproduce the research in the future, but this necessitated changes to data loading, pre-processing, and batching. A specific challenge was PyTorch lightning not yet supporting checkpointing over multiple metrics out-of-the-box, meaning we had to implement the authors\u02bc multimetric heuristic ourselves.\n6.3 Communication With Original Authors Following our initial contact email, the authorsmade themselves readily available, quickly responding to a series of emails over multiple weeks, answering all questions clearly, and providing access to everything we required to reproduce their results. We have also provided the authors with this full report.\nReScience C 7.2 (#6) \u2013 Habacker et al. 2021 13"}], "title": "[Re] Reproducing Learning to Deceive With Attention-Based Explanations", "year": 2021}