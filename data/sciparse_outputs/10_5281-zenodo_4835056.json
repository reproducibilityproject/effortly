{"abstractText": "We have upgraded the original code provided by the authors such that it is compatible with recent versions of popular deep learning frameworks, namely the TensorFlow 2.xandPyTorch 1.7.x-libraries. Furthermore, we have created our own implementation of the algorithm in which we have incorporated additional experiments in order to evaluate the algorithms\u0313 relevance in the scope of different dimensionality reduction techniques and differently structured data. We have performed the same experiments as described in the original paper using both the upgraded version of the code and our own implementation taking the authors\u02bc code and paper as references.", "authors": [{"affiliations": [], "name": "Damiaan J. W. Reijnaers"}, {"affiliations": [], "name": "Dani\u00ebl B. van de Pavert"}, {"affiliations": [], "name": "Giguru Scheuer"}, {"affiliations": [], "name": "Liang Huang"}, {"affiliations": [], "name": "Koustuv Sinha"}, {"affiliations": [], "name": "Jesse Dodge"}], "id": "SP:19fae563287ea563adafd7fc098f8e8e9862915d", "references": [{"authors": ["G. Plumb", "J. Terhorst", "S. Sankararaman", "A. Talwalkar"], "title": "Explaining Groups of Points in Low-Dimensional Representations.", "venue": "Proceedings of the 37th International Conference on Machine Learning", "year": 2020}, {"authors": ["D.E. O\u2019Leary"], "title": "Artificial Intelligence and Big Data.", "venue": "IEEE Intelligent Systems", "year": 2013}, {"authors": ["L. Van Der Maaten", "E. Postma", "J. Van den Herik"], "title": "Dimensionality reduction: a comparative review.", "venue": "J Mach Learn Res", "year": 2009}, {"authors": ["R.A. Mancisidor", "M. Kampffmeyer", "K. Aas", "R. Jenssen"], "title": "Learning latent representations of bank customers with the Variational Autoencoder.", "venue": "Expert Systems with Applications", "year": 2021}, {"authors": ["H. He"], "title": "The State of Machine Learning Frameworks in 2019.", "venue": "The Gradient", "year": 2019}, {"authors": ["J. Ding", "A. Condon", "S.P. Shah"], "title": "Interpretable dimensionality reduction of single cell transcriptome data with deep generative models.", "venue": "en. In: Nature Communications 9.1 (May 2018),", "year": 2002}, {"authors": ["H. Zou", "T. Hastie", "R. Tibshirani"], "title": "Sparse Principal Component Analysis.", "venue": "Journal of Computational and Graphical Statistics", "year": 2006}, {"authors": ["S.T. Roweis", "L.K. Saul"], "title": "Nonlinear Dimensionality Reduction by Locally Linear Embedding.", "venue": "In: Science", "year": 2000}, {"authors": ["L. Cayton"], "title": "Algorithms for manifold learning.", "venue": "Univ. of California at San Diego Tech. Rep", "year": 2005}, {"authors": ["B. Sch\u00f6lkopf", "A. Smola", "K.-R. M\u00fcller"], "title": "Kernel principal component analysis.", "year": 1997}, {"authors": ["K. Shekhar"], "title": "Comprehensive Classification of Retinal Bipolar Neurons by Single-Cell Transcriptomics.", "venue": "eng. In: Cell", "year": 2016}], "sections": [{"text": "R E S C I E N C E C Replication / ML Reproducibility Challenge 2020\n[Re] Explaining Groups of Points in Low-Dimensional"}, {"heading": "Representations", "text": "Damiaan J. W. Reijnaers1, ID , Dani\u00ebl B. van de Pavert1, ID , Giguru Scheuer1, ID , and Liang Huang1, ID 1Faculty of Science, University of Amsterdam, Science Park 904, 1098 XH Amsterdam, the Netherlands\nEdited by Koustuv Sinha, Jesse Dodge\nReviewed by Anonymous Reviewers\nReceived 29 January 2021\nPublished 27 May 2021\nDOI 10.5281/zenodo.4835056"}, {"heading": "Reproducibility Summary", "text": ""}, {"heading": "Scope of Reproducibility", "text": "In this paper we present an analysis and elaboration of [1], in which an algorithm is posed by Plumb et al. for the purpose of finding human-understandable explanations in terms of given explainable features of input data for differences between groups of points occurring in a lower-dimensional representation of that input data."}, {"heading": "Methodology", "text": "We have upgraded the original code provided by the authors such that it is compatible with recent versions of popular deep learning frameworks, namely the TensorFlow 2.x- andPyTorch 1.7.x-libraries. Furthermore, we have created our own implementation of the algorithm in which we have incorporated additional experiments in order to evaluate the algorithms\u0313 relevance in the scope of different dimensionality reduction techniques and differently structured data. We have performed the same experiments as described in the original paper using both the upgraded version of the code and our own implementation taking the authors\u02bc code and paper as references."}, {"heading": "Results", "text": "The results presented in [1] were reproducible, both by using the provided code and our own implementation. Our additional experiments have highlighted several limitations of the explanatory algorithm in question: the algorithm severely relies on the shape and variance of the clusters present in the data (and, if applicable, the method used to label these clusters), and highly non-linear dimensionality reduction algorithms perform worse in terms of explainability."}, {"heading": "What was easy", "text": "The authors have provided an implementation1 that cleanly separates different experiments on different datasets and the core functional methodology. Given a working environment, it is easy to reproduce the experiments performed in [1].\nCopyright \u00a9 2021 D.J.W. Reijnaers et al., released under a Creative Commons Attribution 4.0 International license. Correspondence should be addressed to Damiaan J. W. Reijnaers (info@damiaanreijnaers.nl) The authors have declared that no competing interests exist. Code is available at https://github.com/damiaanr/fact-ai \u2013 DOI 10.5281/zenodo.4686025. \u2013 SWH swh:1:dir:df71f3541f882bfb2474dc177b8eaba31f905d9d. Open peer review is available at https://openreview.net/forum?id=hq3TxQK5cox.\nReScience C 7.2 (#16) \u2013 Reijnaers et al. 2021 1"}, {"heading": "What was difficult", "text": "Minor difficulties were experienced in setting up the required environment for running the code provided by Plumb et al. locally (i.e. trivial changes in the code such as the usage of absolute paths and obtaining external dependencies). Evidently, it was timeconsuming to rewrite all corresponding code, including the architecture for the variational auto-encoder provided by an external package, scvis 0.1.02.\nCommunication with original authors No communication with the original authors was required to reproduce their work.\n1 Introduction\nAs AI models are getting more integrated into applications with economic or social implications, the need for explaining decisions made by (potentially complex) models is increasing. In many AI applications, big datasets form the basis of a decision-making algorithm [2]. As these datasets often involve data of high dimensionality, the dimensionality of data is, in many different applications, often reduced using dimensionality reduction (DR) techniques [3].\nData, and consequently the decisions made by an algorithm that are (in)directly based on that data, often involve some sort of \u02bbgrouping,\u0313 either by utilizing a clustering method or by (manually) pre-defined \u02bbclusters of data.\u0313 The \u02bbgrouping process\u02bc may happen before the dimensionality reduction, forwhich, assumingwell-organized data inwhich the dimensions correspond to explainable \u02bbreal-life\u02bc features of the phenomena measured in the data, the distinguishing characteristics of a certain group becomes relatively explainable as these groups aremarked by boundaries in terms of explainable dimensions. However, this process can also happen after the dimensionality of the data has been reduced, which results in the groups being defined in terms of dimensions in a latent space. Especially when grouping occurs after dimensionality reduction, different clusters in data often play a key role in the decision-making process of an algorithm \u2013 one could, for example, when regarding credit riskmodelling, point out a group in latent spacewhich \u02bbposes a high risk\u02bc when provided a loan [4]. Moreover, such groupings arise naturally under the context of (variational) auto-encoders, which are the architecture of preference for many deep learning applications [1, p. 8].\nThus, considering the above-mentioned need for explainable machine decisions, in combinationwith the increasing use of DR algorithms and the reliance on observed data clusters in abstract latent spaces which are not understandable to the humanmind, it is of great relevance and importance to develop methods to explain differences between groups in the light of the application of a certain dimensionality reduction technique.\nElaborating upon the earlier introduced example of credit risk, one could argue that a reasonable explanation for a machine decision is of the kind: \u201cYour loan was rejected, but if you would have earned e422,86 more per month, your loan would have been accepted.\u201d This type of explanation is a counterfactual explanation \u2013 a decision on the basis of a \u02bbfake,\u0313 counterfactual, \u02bbworld,\u0313 in which featureswould have been shaped differently. The proposed explanatory technique in [1] does exactly this: it reverse-engineers obtained cluster labels in latent space to label corresponding data in original space; then \u02bbtweaks\u02bc\n1GitHub, Explaining Low Dimensional Representations, https://github.com/GDPlumb/ELDR, accessed on January 29th, 2021\n2GitHub, shahcompbio/scvis: Python package for dimension reduction of high-dimensional biological data, https: //github.com/shahcompbio/scvis, accessed on January 22nd, 2021\nReScience C 7.2 (#16) \u2013 Reijnaers et al. 2021 2\nan initial cluster by translating that cluster in original space, so that it is mapped to (approximately) the area of the target cluster in latent space. Since the dimensions in original space correspond to explainable features\u2014the groups\u02bc characteristics\u2014which are comprehensible for a humanmind; a translation that corresponds to merely adding or subtracting values to each of these features can be perfectly explained in \u02bbhuman language.\u0313 Since it is desirable for the explanations to be concise, the translation should be as sparse as possible: the translation should \u02bbtweak\u02bc as few dimensions as possible. The intuition behind this idea is visualized in figure 1.\n2 Scope of reproducibility\nAs follows from the introduction, the authors of [1] opted for a counterfactual, sparse explanation for key differences between (naturally arising) groups. Plumb et al. propose an algorithm that generalizes this idea and attempts to find global differences between all groups, constructed through a composition of simpler explanations and introduced as Transitive Global Translations (TGT). The main tested contributions are that \u201cTGTs provide a global and counterfactual explanation between all groups which is mathematically consistent (i.e. symmetrical and transitive)\u201d and that \u201cTGTs overcome the shortcomings of statistical andmanual interpretation which do not use themodel that learned the low dimensional representation that was used to define the groups in the first place.\u201d\nIn addition to replicating these claims using the provided code, we further evaluated these statements by testing compatibility with other DR methods than the originally used variational auto-encoder and tested the applicability of the algorithm on different datasets with different internal structures. All the latter mentioned experiments were performed by rewriting and implementing the algorithm in PyTorch (taking the authors\u02bc code andpaper as references), whereas the original code iswritten inTensorFlow, motivated by the development of PyTorch in becoming the preferred framework by researchers [5] and the assertion that it offers clearer coding structure3. For the sake of ensuring future reproducibility of the experiments originally presented in [1], as an addition, we have also provided an upgraded version of the original code, without further modifications, to make it compatible with recent versions of TensorFlow.\n3Kirill Dubovikov, PyTorch vs TensorFlow \u2013 spotting the difference, https://towardsdatascience.com/ pytorch-vs-tensorflow-spotting-the-difference-25c75777377b, accessed on January 25th, 2021\nReScience C 7.2 (#16) \u2013 Reijnaers et al. 2021 3\n3 Methodology\nIn accordancewith section 2, for the purpose of reproducing [1], several stepswere taken in order to best cover the scope of the original paper with limited resources. Firstly, the provided artifactswere \u02bbmodernized.\u0313 Concretely \u2013 the code provided by Plumb et al. and the external library scviswere upgraded to be compatible with TensorFlow 2.4.14 and matplotlib 3.3.35. Secondly, the provided code was rewritten from scratch \u2013 both the code complementing the original paper; and the scvis library, to make the computation of the explanations independent from the DR algorithms. Thirdly, we have run our implementation on different DR algorithms\u2014both linear and non-linear (see section 3.1)\u2014and on different datasets (see section 3.3). Both the reproduced version (using the original code) and the rewritten version are further explained in section 3.2. All relevant code complementing [1]\u2014the chunks which we have chosen to rewrite\u2014are explicitly stated in the paper, making the method by Plumb et al. reproducible, even if the code would not have been provided by the authors.\n3.1 Dimensionality reduction algorithms Throughout [1], DR algorithms are only mentioned in the general sense. Only in section 4 the algorithm which Plumb et al. use for their experiments is introduced: a variational auto-encoder (VAE), which is a non-linear family of dimensionality reduction algorithms based on (deep) symmetrical neural networks with a bottleneck in themiddle layers which form the latent representation of the input data. The implemented VAE is based on the architecture proposed by [6].\nThe presented explanatory method should work for any DR algorithm while effectively treating the algorithm as a \u02bbblack box.\u0313 Apart from testing this hypothesis, it is relevant to compare \u02bbexplanatory performance\u02bc on different DR algorithms as different algorithms suit different types of data. Furthermore, an explanation based solely on translations could possibly perform worse in situations wherein data is transformed in a non-linear manner, especially when methods inherently non-linearly transform (and warp) the input space. Therefore, as an addition to simply reproducing the implementation, experiments were done with several commonly known DR algorithms listed below.\nLinear methods \u2014We have opted for two different linear DR algorithms: truncated SVD (TSVD) and sparse PCA (SPCA) [7]. Both TSVD and SPCA reduce the dimensionality in a linear fashion. However, a key difference between both algorithms is that SPCA centers the data before computing the decomposition, where TSVD does not. As a result, TSVD handles sparse data more efficiently. The objective of SPCA is to find the sparse components that best reconstruct the data. While standard PCA, in most cases, extracts components using dense expressions, these are often hard to interpret. However, the sparse vectors extracted by sparse PCA naturally match the latent components, which increases explainability.\nNon-linear methods \u2014 The implemented VAE is based on a Gaussian distribution, which results in a probabilistic generative model that preserves both local and global neighbor structures in the data [6]. As the VAE is solely used for encoding a constant latent space, themodel is trained on the entire dataset. We further incorporate the use of three additional non-linear methods: kernel PCA (KPCA), locally linear embedding (LLE) [8] and isomaps. The latter twomentioned techniques are examples ofmanifold learning [9].\n4TensorFlow, Automatically upgrade code to TensorFlow 2, https://www.tensorflow.org/guide/upgrade, accessed on January 22nd, 2021\n5Matplotlib, API Changes, https://matplotlib.org/3.3.3/api/api_changes.html, accessed on January 22nd, 2021\nReScience C 7.2 (#16) \u2013 Reijnaers et al. 2021 4\nTo achieve a non-linear tranformation, KPCA [10] extends standard PCA through the use of kernels. These kernels effectively mimic a complex function that projects the input data on a higher dimensional space in which, consequently, a lower-dimensional subspace is found in which the data is represented, resulting in a more efficient lowdimensional latent representation. The advantage is that KPCA is able to identify clusterswhich are not linear separable. For our experiments, we found that a sigmoid kernel provided the best performance (measured by the metrics proposed in section 3.2) when the latent space resulting from the KPCA was used to find TGTs.\nFinally, manifold learning techniques are implemented to analyse whether TGTs can still perform on a higher-dimensional embedding of a low-dimensional manifold, especially for datasets of which its data might not lay on an underlying low-dimensional manifold. Isomaps can be seen as an extension of KPCA. Isomaps present a lowerdimensional space while maintaining distances between all points, while LLE aims to map to a lower-dimensional space while maintaining the distance in local neighborhoods. While Isomap could be seen as an extension of KPCA, LLE can be understood as a combination of PCAs run on local neighborhoods.\nAs shown in section 4.1, different latent spaces (resulting from applying different processes for the purpose of reducing dimensions) seem to perform better in terms of explainability, which is further discussed in section 5.\n3.2 Model descriptions The original paper aimed to find an explanation for the key differences between a pair of groups in latent space where the explanation is expressed in terms of the original dimensions. This explanation is represented by a counterfactual translation of one of the groups (in the pair) in original space: \u201cwhat if all points in group A, thus \u2200x \u2208 A, x \u2208 Rd, would have been translated, so that \u2200x \u2208 A, \u03b4 \u2208 Rd, x\u2032 = x + \u03b4? Would group A have been roughly the same as group B after the groups have beenmapped to latent space, so that \u2200x \u2208 A,\u2200y \u2208 B, x \u2208 Rd, y \u2208 Rm, r : Rd \u2192 Rm, r(x\u2032) \u2248 r(y)?\u201d.\nAs both the original space (Rd) and latent space (Rm) are Euclidean spaces, translations between any pair of groups within a larger number of groups (> 2) can be constructed by composing two translations (a vector addition in this context) or negating a translation (in this context equivalent to vector-scalarmultiplicationwith \u03bb = \u22121). Using these operations and a reference group, explanations (\u03b4) can be obtained for every possible pair of groups. The intuition behind this idea is illustrated in figure 2. Points in figure 2 do not directly correspond to the groups for which we want to find differences, since these locations will be approximated using sparse translations (see figure 1). Moreover, this linearity enables the possibility to measure the difference between the points in the two groups in latent space using the l2-norm of the squared differences. At the same time, it is also possible tomeasure the sparsity of \u03b4 using l1-regularization, which directly relates to the comprehensibility of the explanation posed by \u03b4.\nIn order to obtain adequate components for the \u03b4-vector between a group and the reference group, all \u03b4-vectors can be initialized to zero and optimized by adjusting the components of \u03b4 corresponding to two randomly chosen groups in the negative direction of the gradient of a loss function which accounts for the above-mentioned constraints. This loss function is formulated in equation 9 of the original paper. While Plumb et al. decide to incorporate the calculation of the gradient as an \u02bbextension\u02bc of the scvis package, whichwe have reproduced using the provided code, we have decided to analytically compute the gradient using SciPy 1.6.0 (using the optimize.approx_fprimemethod). This implementation choice is in line with our aim to rewrite all code from scratch to obtain a \u02bbstand-alone\u02bc division between the explanatory algorithm (which is\nReScience C 7.2 (#16) \u2013 Reijnaers et al. 2021 5\nthe main idea of the original paper) and various \u02bbblack-box\u02bc dimensionality reduction algorithms, including the variational auto-encoder (the only algorithm Plumb et al. experiment with), as introduced in the beginning of this section.\nThe model s\u0313 performance is measured by two separate, but related (see section 4.1), metrics defined by [1, p. 3]: correctness, whichmeasures the degree to which projected points are actually in the vicinity of points they should map to (which they, in a sense, should \u02bbimitate\u02bc) and coverage, which measures the degree to which projected points cover the target group. Both are defined in equation 3\u20134.\n3.3 Datasets In the upgraded-code model, we decided to reproduce the explanations on all provided datasets, including the synthetic and corrupted versions. The datasets used in the experiments in the original paper are the Heart Disease, Boston Housing, and Iris datasets; and the single-cell RNA dataset [11]. Our from-scratch model has incorporated all provided datasets, except for the latter, due to lack of computation power. However, experiments were\nrun on three additional UCI datasets: Seeds, Wine and Glass.6 The additional data allow us to understand how varying underlying structures in data influence explainability using different DR algorithms. Not all datasets can be reduced by all techniques: all three added datasets do not yield eigencomponents for a sigmoidal Kernel-PCA procedure.\n3.4 Hyperparameters A constraint on the resulting explanations is that theymust be sparse. The sparsity level is formally defined by k, representing the number of dimensions in the original space (with \u02bbreal-life\u02bc features) used as the main components of the translation that forms the explanation. The hyperparameter k is enforced after the learning process by truncating the \u03b4-vector. To enforce sparsity during the learning process, the \u03b4-vector is being l1-regularized by a term \u03bb, as follows from [1, p. 5] and was inspired by the field of compressed sensing. The resulting equation, which is minimized, is stated in equation 1. As further explained in section 3.5, different values for \u03bb are tested for optimization.\nloss(\u03b4) = ||r(x\u0304initial + \u03b4)\u2212 r\u0304target||22 + \u03bb||\u03b4||1 (1)\nFollowing [1, p. 6], in equation 2 a similarity measure is defined to capture the degree of which the features of a k1-sparse explanation correspond to k1 dimensions of a k2-sparse explanation where k2 > k1.\nsimilarity(e1, e2) =\n\u2211 |e1[i]|1[e2[i] \u0338= 0]\n||e1||1 (2)\nLastly, hyperparameter \u03f5 defines a threshold for the correctness and coverage metrics, as defined in equation 3\u20134. Although Plumb et al. do hint on the type of search they have performed to find values for \u03f5, it is not clearly expanded upon [1, p. 4]. For the purpose of analysing reproducibility, wehave adopted the same values for \u03f5 (whichwere not present\n6UCI Machine Learning Repository, https://archive.ics.uci.edu/ml/datasets/{heart+disease,iris,seeds,Wine,glass+ identification} and https://archive.ics.uci.edu/ml/machine-learning-databases/housing/, accessed on January 29th, 2021; note the set notation in the latter section of the URL.\nReScience C 7.2 (#16) \u2013 Reijnaers et al. 2021 6\nin the paper) for the same datasets. For all new data, we have introduced similar static values for \u03f5. Plumb et al. do, however, propose an evaluationmethod for \u03f5, although it is again notmentioned in the paper. The authors take themean, minimum andmaximum of the diagonal values of the matrix of the correctness measures for all clusters (similar to the matrices shown in figure 4). These values can be misleading, as larger values for \u03f5 would inherently score high on correctness. Therefore, we have constrained \u03f5 \u2208 [0, 2] and dynamically scale the latent spaces to force the data to be spread out (see section 3.5). We evaluated all values for \u03f5 and outline the means in table 1.\ncorrectness(t) = 1 |Xinitial| \u2211\nx\u2208Xtarget\n1[\u2203x\u2032 \u2208 Xtarget|||r(t(x))\u2212 r(x\u2032)||22 \u2264 \u03f5] (3)\ncoverage(t) = 1 |Xtarget| \u2211\nx\u2208Xinitial\n1[\u2203x\u2032 \u2208 Xinitial|||r(x)\u2212 r(t(x\u2032))||22 \u2264 \u03f5] (4)\n3.5 Experimental setup and computational requirements We have reproduced the experiments in [1] by using the upgraded TensorFlow-code. Additionally, we have run our from-scratch code on three additional datasets and six other dimensionality reduction algorithms. All experiments were run for five trials for eleven different values of \u03bb (evenly spaced between 0 and 5), for which the best set of \u03b4-vectors is chosen for all values of k (k is evenly spaced between 1 and the number of dimensions d in the original space, with a step size of 1 for d \u2264 5, and 2 otherwise).\nAs pointed out in the previous section, an inadequately high value for \u03f5 poses a problem if the data in latent space is of low variance (especially if variance < \u03f5), as the performance measures, introduced in section 3.2 and shown in equations 3\u20134, would unjustly report very high scores. We have solved this problem by rescaling the data in latent space so that a variance of 10 is preserved (which amounts to a standard deviation of \u2248 3.16). We have deliberately chosen not to utilize a method for removing outliers prior to defining the factor with which to rescale the data in latent space, as to preserve \u02bbthe spirit of the data,\u0313 especially since the internal structure of some datasets contain outliers which potentially correspond to groups with a significantly different character, as opposed to outliers resulting from noisy measurements. We further discuss our choice in section 5. However, as we failed to preserve an adequate amount of variance when performing TSVD on the Glass-dataset using this method, we manually scaled the data in this latent space, for this particular dataset, with an additional factor of 20.\nIn the from-scratch implementation, K-means is used for clustering, while for the replicated experiments in the original notebooks, following the code provided by the authors, clusters were manually selected.\nReScience C 7.2 (#16) \u2013 Reijnaers et al. 2021 7\nExperimental meta-results are shown in table 2. The original code, the \u02bcmodernized\u02bc version of the original code and the code for the from-scratch implementation can be found on our GitHub repository [12].\n4 Results\nUsing our upgraded TensorFlow-code and the provided pre-trained VAE models, the obtained results are identical to those provided by the paper s\u0313 codebase for all datasets and methods.7 After re-training all VAE models and relearning all explanations, using the same upgraded TensorFlow-code, we obtain very similar results.8 The results are not identical, as the models learn slightly different representations, in which the man-\nually selected clusters also differ. This indicates that the used cluster generation techniques influence themodel s\u0313 ability to explain based on a translation. An example arises with the Iris dataset: Plumb et al. yield 0.833 coverage, while we get only 0.66. This difference caused by a different organization of clusters propagates further into the results for the corrupted data. Except for minor differences arising from this same issue, we successfully reproduced the results for all other datasets.\nWhen running the Housing, Iris and Heart datasets on the from-scratch implementation of the explanation algorithm using our implementation of scvis, we obtain either similar (see figure 3) or even better results than Plumb et al. do. This again indicates that the method for dimensionality reduction does impact the results.\nFor different datasets and different dimensionality reduction algorithms that map the datasets to different latent spaces, we have encountered the same problem related to the inability of explaining pairs of groups of which the clusters have a different standard deviation, as shown in the original paper in figures 4-7 [1, p. 4]. This problem occurs, among others, in figure 5f in appendix A (and the corresponding measures in figure 6f).\n7To compare, open the .ipynb in all folders (except \u02bbcode,\u0313 \u02bbscvis,\u0313 \u02bbMiscFigures,\u0313 and \u02bbIntegrated-Gradientsmaster\u02bc) on both repositories, and look at the resulting plots: https://github.com/GDPlumb/ELDR and https://github. com/damiaanr/fact-ai/tree/main/ELDR-TF2.x_(pre_trained_models)\n8Now, compare the plots with https://github.com/damiaanr/fact-ai/tree/main/ELDR-TF2.x_(newly_trained_models)\nReScience C 7.2 (#16) \u2013 Reijnaers et al. 2021 8\n4.1 Additional results not present in the original paper As [1, p. 6] pointed out, the correctness and coverage metrics are closely related. If the translations between arbitrary groups are symmetrical, the former translation\u2014which is the technical representation of the explanation\u2014is the negative of the latter (and viceversa). Thus, when applying different linear dimensionality reduction algorithms, both metrics will exactly equal each other, as follows from the results shown below in table 3, and in figure 4 (note that the color map plot for correctness is the \u02bbtranspose\u02bc of the plot for coverage, and vice-versa) and figure 6 (along all different values for k, the graphs conveying correctness and coverage are on top of each other for all linear algorithms).\n0 2 4 Target Group\n0\n1\n2\n3\n4\n5\nIn iti al Gr ou\np\nCorrectness - 0.908\n0 2 4 Target Group\nCoverage - 0.908\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFigure 4. Metrics for Housing on PCA, k = 5\nAn additional observation is that topologybased dimensionality reduction techniques are less suitable for data compression when the compressed data consequently needs to be explained using translation-based counterfactual explanations. Locally linear embedding and isomapping are methods based on manifold mapping: a one- or two-manifold is embedded in two-dimensional space, and points of a higher-dimensional space are mapped onto this manifold. This often yields problem-\natic data structures. For instance, when data is represented using a line (a one-manifold) and embedded in two-dimensional space in which the one-dimensional line is twirled. Since we are only considering translations (and, e.g., no rotations), themethod is unable to adequately explain differences between clusters present onmanifolds in latent space.\nFurthermore, we observed that certain datasets yield constant results when varying the sparsity-constraint. In this case, the compression from high-dimensional to twodimensional space was (nearly) lossless \u2013 a single component suffices to explain the difference between any group. This phenomenon is shown in figure 6e in appendix A, indicating that the chemical composition of wine actually depends on only one latent variable: when using PCA, 99.98% of the variance is explained by only two components (of which 99.81% by the first component) out of 13 input dimensions. Note that the VAE seems to learn an approximation of LLE space (see figure 5e).\nAll results for all introduced performancemeasures, datasets and dimensionality reduction algorithms, are shown in table 3 below and illustrated in figures 5\u20137 in appendix A.\nReScience C 7.2 (#16) \u2013 Reijnaers et al. 2021 9\n5 Discussion\nWe have been able to successfully reproduce and upgrade the code provided by Plumb et al., which is the implementation of the technique presented in [1]. We were able to reproduce results by running the pre-trained models; after re-training these models; and by rewriting the algorithm from scratch.\nThe performance depends on themapping to latent space. A limitation of the algorithm is the lack of variable freedom: only translations can be used to explain differences between groups. By utilizing different DRmethods, we have shown that not all algorithms produce latent spaces in which translations suffice for an explanation between clusters. Especially manifold-based algorithms require a more sophisticated type of explanation using, for instance, rotation and scaling.\nA possible solution would be to generate an explanation in terms of a matrix (so that x\u2032 = Mx + \u03b4 instead of x\u2032 = x + \u03b4). However, explanations using translations are directly interpretable for humans as the dimensions of the tested datasets correspond to explainable features. Considering, for example, rotations, would come at the cost of explainability. However, forcing M to be diagonal (which leads to multiplying all data features by the corresponding diagonal factors) might yield proper explanations (of the type \u201cIf your monthly income would have been twice as big...\u201d).\nFurthermore, by using different datasets, we have shown that some DR techniques do a better job in terms of explainability between clusters. Data might be structured in different ways and produce different types of clusters. It directly follows from our results that the explanation method heavily relies on the generated clusters, and more importantly, its shapes and variance in latent space, which is another limitation.\nIn section 3.5, we opted for an approach in which the latent spaces are dynamically scaled for the purpose of achieving a certain amount of variance in the data, as to obey the fixed hyperparameter \u03f5which was introduced in section 3.4. A more robust method would be to dynamically define \u03f5 based on the variance in the latent space depending on the dataset and, potentially, proper handling of its outliers. We view the lack of expansion on this hyperparameter in the original paper as a (minor) limitation.\nAs VAEs are sampling points in latent space from a distribution, an identical point x in the input spacewhich is repeatedlymapped to latent spacewill yield differentmappings. It would be interesting to further investigate to what extent explanatory algorithms can \u02bbhandle\u02bc this variance \u2013 to what extent such algorithms could find a stable and unchanging explanation in an ever-changing counterfactual world.\n5.1 What was easy After having asserted the dependencies needed to run the code provided by the authors on their GitHub repository, replicating the experiments performed in the paper was easy; the code was cleanly written, and it was easy to understand the architectural choices which the authors\u02bc made in their model.\n5.2 What was difficult Although the code required for computing the explanations was separated from the implementations in other folders in which it is applied on the different datasets, Plumb et al. decided to perform experiments using a VAE and intertwined all code for generating the explanations with the scvis-library. This caused the explanatory model to rely\nReScience C 7.2 (#16) \u2013 Reijnaers et al. 2021 10\non TensorFlow, while the explanation method itself does not require any deep learning. It is preferred to dissect the model into independent components, considering that the explanatory model should work with other DR algorithms, including those which do not require neural pipelines. Because of the authors\u02bc choice to use a VAE, the entire repository relied on TensorFlow. Mainly for this reason, we rewrote everything from scratch, as to provide the code for the dimensionality reduction methods on a \u02bbstandalone\u02bc basis. As we were aiming to reproduce the results provided by the original paper, we also had to rewrite the external scvis-library, which provided the VAE architecture, in PyTorch (for the sake of keeping clean, separated and object-oriented code). This was very time-consuming as both frameworks are fundamentally different (dynamic vs static graph definition). The process of rewriting took approximately two weeks.\nLastly, the results for the Bipolar dataset could only be replicated with the provided model configuration file. Although we have re-trained the model using the upgraded originally provided code, we have chosen to exclude this dataset for the additional experiments due to lack of computation power and time.\n5.3 Communication with original authors A short interaction occurred to gain more insight into the required external libraries as these were not listed in any documentation. Although the authors responded quickly, the issue had already been solved in the meantime."}, {"heading": "Appendices", "text": "A Additional latent spaces and corresponding measures\nReScience C 7.2 (#16) \u2013 Reijnaers et al. 2021 12\nReScience C 7.2 (#16) \u2013 Reijnaers et al. 2021 13\nReScience C 7.2 (#16) \u2013 Reijnaers et al. 2021 14"}], "title": "[Re] Explaining Groups of Points in Low-Dimensional Representations", "year": 2021}