{"abstractText": "This report covers our reproduction of the paper \u2018Thompson Sampling for Bandits with Clustered Arms\u2019 by Carlsson et al. (IJCAI 2021) [1]. The authors propose a new set of al\u2010 gorithms for the stochasticmulti\u2010armed bandit problem (and its contextual variant with linear expected rewards) in settings when the arms are clustered. They show both theo\u2010 retically and empirically that exploiting the cluster structure significantly improves the obtained regret over the traditional assumption with non\u2010clustered arms. Furthermore, they compare the proposed algorithms to previously proposed and well\u2010known bench\u2010 marks for the bandit problem. We aim to reproduce just the empirical evaluations.", "authors": [{"affiliations": [], "name": "Andra\u017e De Luisa"}, {"affiliations": [], "name": "Koustuv Sinha"}, {"affiliations": [], "name": "Sharath Chandra Raparthy"}], "id": "SP:a3f2c8393699d287185db6a575b716d6f722636f", "references": [{"authors": ["E. Carlsson", "D. Dubhashi", "F.D. Johansson"], "title": "Thompson Sampling for Bandits with Clustered Arms.", "venue": "Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21. Main Track. International Joint Conferences on Artificial Intelligence Organization,", "year": 2021}, {"authors": ["R R Core Team"], "title": "A Language and Environment for Statistical Computing", "venue": "R Foundation for Statistical Computing. Vienna,", "year": 2021}, {"authors": ["H. Robbins"], "title": "Some aspects of the sequential design of experiments.", "venue": "Bulletin of the American Mathematics Society", "year": 1952}, {"authors": ["W.R. Thompson"], "title": "On The Likelihood That One Unknown Probability Exceeds Another In View Of The Evidence Of Two Samples.", "venue": "(Dec", "year": 1933}, {"authors": ["H. Wickham"], "title": "ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York, 2016", "venue": "URL: https://ggplot2. tidyverse.org", "year": 2016}, {"authors": ["A. Genz", "F. Bretz", "T. Miwa", "X. Mi", "F. Leisch", "F. Scheipl", "T. Hothorn"], "title": "mvtnorm: Multivariate Normal and t Distributions. R package version 1.1-3", "year": 2021}, {"authors": ["P. Auer", "N. Cesa-Bianchi", "P. Fischer"], "title": "Finite-time Analysis of the Multiarmed Bandit Problem.", "venue": "Machine Learning", "year": 2002}, {"authors": ["S. Pandey", "D. Chakrabarti", "D. Agarwal"], "title": "Multi-armed bandit problems with dependent arms.", "year": 2007}, {"authors": ["D. Bouneffouf", "S. Parthasarathy", "H. Samulowitz", "M. Wistuba"], "title": "Optimal Exploitation of Clustering and History Information in Multi-armed Bandit.", "venue": "Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19. International Joint Conferences on Artificial Intelligence Organization,", "year": 2019}, {"authors": ["T. Zhao", "M. Li", "M. Poloczek"], "title": "Fast Reconfigurable Antenna State Selection with Hierarchical Thompson Sampling.", "venue": "IEEE International Conference on Communications (ICC)", "year": 2019}, {"authors": ["L. Kocsis", "C. Szepesv\u00e1ri"], "title": "Bandit Based Monte-Carlo Planning.", "year": 2006}, {"authors": ["S. Agrawal", "N. Goyal"], "title": "Thompson Sampling for Contextual Bandits with Linear Payoffs.", "venue": "Proceedings of the 30th International Conference on Machine Learning", "year": 2013}, {"authors": ["L. Li", "W. Chu", "J. Langford", "R. Schapire"], "title": "A Contextual-Bandit Approach to Personalized News Article Recommendation.", "venue": "Computing Research Repository - CORR (Feb", "year": 2010}], "sections": [{"text": "R E S C I E N C E C Replication / ML Reproducibility Challenge 2021\n[Re] Thompson Sampling for Bandits with Clustered Arms\nAndra\u017e De Luisa1, ID 1University of Ljubljana, Faculty of Computer and Information Science, Ve\u010dna pot 113, 1000 Ljubljana, SLO\nEdited by Koustuv Sinha,\nSharath Chandra Raparthy\nReviewed by Anonymous Reviewers\nReceived 04 February 2022\nPublished 23 May 2022\nDOI 10.5281/zenodo.6574671"}, {"heading": "Reproducibility Summary", "text": ""}, {"heading": "Scope of Reproducibility", "text": "This report covers our reproduction of the paper \u2018Thompson Sampling for Bandits with Clustered Arms\u2019 by Carlsson et al. (IJCAI 2021) [1]. The authors propose a new set of al\u2010 gorithms for the stochasticmulti\u2010armed bandit problem (and its contextual variant with linear expected rewards) in settings when the arms are clustered. They show both theo\u2010 retically and empirically that exploiting the cluster structure significantly improves the obtained regret over the traditional assumption with non\u2010clustered arms. Furthermore, they compare the proposed algorithms to previously proposed and well\u2010known bench\u2010 marks for the bandit problem. We aim to reproduce just the empirical evaluations."}, {"heading": "Methodology", "text": "Given that no code was provided alongside the original paper (and neither for any of the benchmarks used for comparison), we implement everything from scratch. We write our code in R [2], the well\u2010known programming language for statistical computing, and use some of its basic libraries. We run the experiments on a laptopwith a dual\u2010core Intel i7 processor and 8 GB of RAM. We don\u2019t use a GPU."}, {"heading": "Results", "text": "There are no exact numbers in the original paper to reproduce, rather the main claims are supported with some visualisations. With this in mind, our reproduction confirms the advantage provided by clustering over the assumption of independent arms, as well as the newly proposed algorithms outperforming the referenced benchmarks. We re\u2010 peat all the experimentswithmultiple seeds to obtain robust estimates of the algorithms\u2019 performance and reduce the risk of drawing any conclusions out of results obtained by chance."}, {"heading": "What was easy", "text": "The authors have included in the paper all the necessary details to reimplement their proposed algorithms, recreate the synthetic datasets, and reproduce the experiments for the first part, i.e. the traditional multi\u2010armed bandits setting.\nCopyright \u00a9 2022 A. De Luisa, released under a Creative Commons Attribution 4.0 International license. Correspondence should be addressed to Andra\u017e De Luisa (ad9366@student.uni-lj.si) The authors have declared that no competing interests exist. Code is available at https://github.com/andrazdeluisa/reproducibility_challenge \u2013 DOI 10.5281/zenodo.6498328. \u2013 SWH swh:1:dir:d1d7fa93e952cf14154d5415f253b6507af22833. Open peer review is available at https://openreview.net/forum?id=r5LS3fmh0t.\nReScience C 8.2 (#23) \u2013 De Luisa 2022 1"}, {"heading": "What was difficult", "text": "It is much harder to reimplement some of the referenced benchmarks. The main rea\u2010 sons for these struggles are the inconsistent nomenclature, important details missing in the referenced papers, and some of the compared benchmarks being originally de\u2010 signed to run in a different setting. Furthermore, some additional research into the field of contextual bandits is needed to reproduce the second part of the experiments.\nCommunication with original authors There has been no communication neither with the authors of the original article nor with any authors of the referenced papers.\n1 Introduction\nThemulti\u2010armed bandit is a classical reinforcement learning problem, formally defined for the first time already by Robbins in 1952 [3], focused on the exploration versus ex\u2010 ploitation trade\u2010off. Its name is derived from gambling: imagine a gambler sitting in front of a row of slot machines, deciding which arm to pull next hoping to maximise the prize money (to minimise his loss, actually). The same name is used for whatever problem involves a learner and a fixed set ofN actions to repeatedly choose from, with each action returning a stochastic reward. The learner aims to maximise its obtained reward in a finite number of steps T . He does therefore repeatedly face the dilemma of whether to exploit what he believes to be themost convenient action, or to explore other actions, hoping to find an even better one. In the beginning the learnermight have been provided with some additional info about the available actions or not, it depends on the setting of the problem. Anyway, he constantly updates his knowledge about the actions based on the obtained rewards. An interesting and useful generalisation is the contextual bandit. At each iteration the learner gets an additional context vector which he can use, in addition to the past re\u2010 wards, to choose his next action. His goal over time is to get enough information about how the context vectors and rewards are related, to be able to predict the best arm by looking at feature vectors. Both mentioned types of multi\u2010armed bandits are analysed also in the paper we aim to reproduce, Thompson Sampling for Bandits with Clustered Arms by Carlsson et al. [1]. The problems defined in the article have an additional, defining characteristic: the arms are clustered (both in the analysed classical and contextual bandits). The leading idea is to understand howmuch can the obtained reward be optimised if the learner can leverage the additional knowledge on the relations between arms. Themulti\u2010armed bandit problem (with itsmultiple variants) can be successfully applied to some common usecases. Let\u2019s take a recommender system on an e\u2010commerce web\u2010 site as an example to better explain all the mentioned settings. When a user visits the homepage of the website, an agent in the background has to decide which product to show him first, hoping he will eventually buy it (thus obtaining a reward). Without any additional knowledge about the user, this example represents a classical multi\u2010armed bandit. However, the items on sale are not completely unrelated between them. If they can be grouped in meaningful clusters, for which we expect similar selling success, the problem translates to a multi\u2010armed bandit with clustered arms. These clusters might even be hierarchically structured (e.g. the items are clustered into electronics and clothes, with the latter further divided into sports and elegant clothing). Furthermore, if some data about the user is also available (i.e. a context vector), we talk about a contextual multi\u2010armed problem, with the context influencing the obtained reward (since users\u2019 preferences vary).\nReScience C 8.2 (#23) \u2013 De Luisa 2022 2\n2 Scope of reproducibility\nThe main goal of the original paper is \u201cto show, both theoretically and empirically, how exploiting a given cluster structure can significantly improve the regret and computational cost compared to using standard Thompson sampling\u201d [1]. To achieve it, new algorithms based on a multi\u2010level Thompson sampling scheme [4] are proposed. These algorithms are de\u2010 signed to solve the stochastic multi\u2010armed bandit with clustered arms (MABC) problem and its contextual variant (CBC) with linear expected rewards (linearly dependent on the given context vector). Under some specific assumptions (mainly strong dominance of the best cluster and Bernoulli distributed arm rewards), theoretical bounds on the regret are provided, with the dependence on the number of armsN removed in favor of dependence on the properties of the selected clustering. The algorithms are then tested on some specifically constructed datasets that meet the assumptions, as well as com\u2010 pared with other recently proposed algorithms that solve the MABC and CBC problems even in settings where the theoretical assumptions are violated. The results indicate the theoretical guarantees hold true and the proposed algorithms are at least comparable with the evaluated baselines (and most often outperforming them). In this work, we aim to fully reproduce all the experiments described in the paper. We divide the claims we focus our work on into three subsections for clearness.\n1. ClassicalMABC settingwith clusterings thatmeet the required theoretical assump\u2010 tions\n\u2022 In flat clusterings, the proposed algorithm (TSC) outperforms the baseline (Thompson sampling \u2013 TS).\n\u2022 In hierarchical clusterings, the proposed algorithm (HTS) outperforms the baseline (TS).\n\u2022 Regret does only depend on the clustering quality, not on the number of arms N .\n2. Classical MABC setting with clustering that violates the defined assumptions\n\u2022 The proposed algorithms (TSC, HTS) still perform better than TS (in both flat and hierarchical settings).\n\u2022 TSC and HTS are at least comparable (and most often better) than other, re\u2010 cently proposed algorithms that also solve the MABC problem.\n3. Contextual bandits variant (CBC)\n\u2022 The proposed algorithm LinTSC outperforms the baseline LinTS. \u2022 LinTSC is at least comparable and most often better than other, recently pro\u2010 posed algorithms that also solve the CBC problem.\n3 Methodology\nGiven that no code was provided alongside the original paper (and neither with any of the articles where the benchmarks used for comparison are described), we implement everything from scratch 1. Additional details about the proposed algorithms and how we implement them follow in the next section 4.\n1The code required to reproduce all the experiments is available on the GitHub repository https://github.com/ andrazdeluisa/reproducibility_challenge\nReScience C 8.2 (#23) \u2013 De Luisa 2022 3\n3.1 Datasets We don\u2019t use any externally provided dataset, all the experiments are run with synthet\u2010 ically generated data, following the instructions in the paper. For each experiment we prepare a separate dataset. The multi\u2010armed bandit is a special case of a reinforcement learning problem, and as such doesn\u2019t require any split into training or test data, the agents learn and take actions simultaneously.\n3.2 Computational requirements We write our code exclusively in R [2], the well\u2010known programming language for sta\u2010 tistical computing, and use some of its basic libraries for data manipulation and visu\u2010 alisation (ggplot2 [5], stats [2], mvtnorm [6]). We run the experiments on a laptop with a dual\u2010core Intel i7 6th generation processor and 8 GB of RAM. We don\u2019t use a GPU. The computational resources are very limited, but the nature of the problem doesn\u2019t require huge processing, thus allowing us to smoothly run all the required experiments. Some slow down is observed in the runs with a higher number of arms and actions, but we still manage to evaluate the models over multiple (up to 100) seeds in a couple of hours, which is crucial to get accurate estimations of their performance.\n4 Experiments\nAs described in [1], a standard multi\u2010armed bandit (MAB) problem is defined with a set of N arms A, a finite number of steps T and reward functions rt(at) which depend on the played arm at at the timestep t, but might as well depend on the timestep t itself. In MABC and CBC problems, the arms are additionally divided into (flat/disjoint or hierar\u2010 chical) clusters. Rewards are drawn from some distribution rt \u223c Dat , with an unknown mean EDat [rt] = \u00b5at . The goal of the learner is to maximise the expected cumulative reward over a sequence of T time steps or, equivalently, to minimise its expected cu\u2010 mulative regret E[Rt] w.r.t. the optimal arm at\u2217 = argmaxat\u2208A\u00b5at\u2200t \u2264 T (the cumula\u2010 tive regret represents how much reward did the learner lose due to not always playing the best arm 2). Rewards might be drawn from arbitrarily chosen distributions, but to simplify the derivation and proof of theoretical bounds for the cumulative regret, only Bernoulli and uniformly distributed rewards are used in the original paper. Since the reward functions differ in different MAB settings, we provide additional details about them in the following sections.\n4.1 Classical MABC In the experiments with classical MABC problems, all the rewards are drawn from a Bernoulli distribution rt(at) \u223c Bernoulli(\u03b8at). The parameters \u03b8a are defined in ad\u2010 vance (but not known to the learner) and constant for each arm a, therefore the cumu\u2010 lative regret can be defined as R = \u2211T t=1 \u03b8a\u2217 \u2212 rt(at), where \u03b8a\u2217 = maxa\u2208A\u03b8a is the\nexpected reward for playing the best arm. The arms are divided into clusters based on their \u03b8 value. These clusters might be disjoint (each arm gets assigned to exactly one cluster) or hierarchical (each arm gets assigned to exactly one leaf in a clustering tree). Not all algorithms can solve both types of the MABC problem. The baseline algorithm that solves a MAB problem is Thompson sampling (TS) [4], first designed by Thompson in 1933 (much before the MAB problem was even formalised). It doesn\u2019t take into account any clustering information (thus being able to solve all the proposed MABC settings). The main idea behind it is to select which arm to play at the\n2Notice that the regret might be (and often is) negative at single timesteps when positive rewards are ob\u2010 served.\nReScience C 8.2 (#23) \u2013 De Luisa 2022 4\ncurrent step probabilistically, i.e. with respect to the current belief about the arms re\u2010 ward distributions. The learner starts with assigning uninformative Beta(1, 1) priors over expected rewards \u03b8a \u2208 [0, 1] to each arm a (the Beta distribution was chosen due to its conjugate characteristic). Then, at each step, it takes a random sample from the Beta(St(a), Ft(a)) distribution for each arm, and plays (greedily) the arm with the high\u2010 est sampled expected reward. The posterior belief in that arm\u2019s true \u03b8 value is then up\u2010 dated according to the observed reward r: St+1(a) = St(a)+r, Ft+1(a) = Ft(a)+(1\u2212r). Since the learner doesn\u2019t get any information about the arms that were not played, the other posteriors are not updated. The newly proposed algorithm that exploits the disjoint clustering structure (TSC [1]) is based on the exact same idea as TS, but adds an additional level to the selection of the arm to play. Instead of sampling from the arms\u2019 priors directly, it keeps prior beliefs for the clusters too and samples from themfirst. When the cluster with the highest sampled expected reward is selected using TS, the procedure is repeated for the arms within that cluster. Then the posteriors (both for the selected cluster and played arm) are updated according to the observed reward. The proposed algorithm for theMABC problemwith hierarchically clustered arms (HTS [1]) is a natural extension of TSC: it applies the Thompson sampling at each node to select in which subtree to search for the arm to play. TSC is basically a HTS on a tree with depth 2, while TS is a HTS on a tree with depth 1 (a single node with N leaves). The assumptions on the clustering structures required for the theoretical regret bounds to hold are quite tight, assuming strong dominance (and hierarchical strong dominance) between the clusters. This actually means that every arm from the optimal cluster must have a higher expected reward than any other arm from the other clusters (in the hier\u2010 archical structure, this condition is applied at each node level). The authors, therefore, provide precise instructions for the construction of synthetic datasets on which the al\u2010 gorithms are tested. To build a strongly dominant disjoint clustering structure on which to test the proposed algorithms, we need to define the following hyperparameters:\n\u2022 the number of arms N ,\n\u2022 the number of clustersK,\n\u2022 the size of the optimal cluster A\u2217,\n\u2022 the width of the optimal cluster w\u2217,\n\u2022 the distance of the other clusters to the optimal one d,\n\u2022 and the number of timesteps T .\nThe arms are divided into clusters randomly. The probabilities assigned to the arms in the optimal cluster are sampled uniformly from U(0.6 \u2212 w\u2217, 0.6), while those in the other clusters are sampled from U(0.5 \u2212 w\u2217 \u2212 d, 0.6 \u2212 w\u2217 \u2212 d). In all the clusters, two arms get assigned the upper and lower bound of the interval their values were sampled from (e.g. the highest expected reward is always 0.6). In the Results section 5, we show how those hyperparameters influence the obtained cumulative regret. The hierarchical datasets are built in a completely different way (and require fewer hy\u2010 perparameters). The probabilities assigned to the arms are sampled uniformly from U(0.1, 0.8) and then recursively sorted and merged into a balanced binary tree that meets the hierarchical strong dominance assumptions (i.e. at each node, the top half of the arms gets assigned to a subnode and the bottom half to the other). Other than the number of arms N and timesteps T , the defining hyperparameter is the number of levels L 3.\n3Notice that a single\u2010level tree represents a MAB, and a two\u2010level one a MABC problem.\nReScience C 8.2 (#23) \u2013 De Luisa 2022 5\nClassical MABC with violated assumptions \u2014 The strong dominance condition is difficult to meet in real\u2010life scenarios, therefore we evaluate the performance of the proposed al\u2010 gorithm also on datasets where this assumption is not met, and compare them to other well\u2010knownalgorithms that solve theMABCproblem. Wegenerate the synthetic datasets in a completely different way: we assign a parameter xa \u223c U(0, 1) to each arm, group them intoK clusters using the K\u2010means algorithm and then convert their parameters to probabilities with \u03b8a = f(xa), where f(x) = 12 (sin (13x) sin (27x) + 1). The function is smooth, therefore arms in the same cluster have similar expected rewards, but its peri\u2010 odicity ensures there are no strongly dominant clusters. For the hierarchical structure, we repeat the same process at each level. The proposed algorithms based on Thompson sampling are compared to the following ones:\n\u2022 The UCB1 [7] (Upper Confidence Bound) algorithm solves the MAB problem (ig\u2010 nores the clustering structure). It is based on a deterministic policy, which at each step selects the arm that maximises the expression r\u0304a + cp \u221a 2lnn na\n, where r\u0304a is the average reward obtained from arm a so far, na the number of times a was played and n the total number of plays. Each arm needs to be played once at the begin\u2010 ning for initialisation.\n\u2022 The UCBC algorithm (designed as TLP \u2013 Two Level Policy \u2013 by Pandey et al. [8], named UCBC in [9]) is an extension of UCB1 to clusters. It uses a two\u2010level selec\u2010 tion schema, with first selecting the best cluster with respect to the UCB1 formula, and then playing the best arm from the cluster. A policy on how to represent the clusters need to be chosen. Since the authors of [1] don\u2019t mention which one they use, we choose to implement the MAX policy (which [8] states to perform best). With the MAX policy, each cluster is represented by its best arm (other proposed policies are MEAN and PMAX).\n\u2022 The TSMax algorithm (named HTS when first proposed in [10], renamed to avoid misunderstandings) is extremely similar to the TSC. The only difference is that the clusters\u2019 posterior beliefs are defined as the posterior of the current best arm inside the cluster.\n\u2022 The UCT algorithm (Upper Confidence Bound for Trees [11]) is an extension of the UCB1 algorithm to hierarchical clustering structures. It applies theUCB1 selection procedure recursively at each node and selects the most promising one until a single arm is selected.\n4.2 Contextual CBC In the experiments with CBC problems, the expected values of the rewards are linearly dependent on the context vector xt \u2208 Rd and arm parameters \u03b8a \u2208 Rd: E[rt(a)|xt] = xTt \u03b8a. The parameters \u03b8a are defined in advance (but not known to the learner) and re\u2010 main constant throughout the experiment, while a different context xt is observed at each timestep t \u2264 T . The rewards are uniformly distributed rt(at) \u223c U(0, 2xTt \u03b8at). The cumulative regret is defined as \u2211T t=1 x T t \u03b8a\u2217t \u2212 rt(at), where \u03b8a\u2217t = argmaxa\u2208A x T t \u03b8a is\nthe best arm for the given context (the best arm is not always the same). The arms are grouped into clusters based on their \u03b8a parameter vector. We use only disjoint cluster\u2010 ings in our experiments. A baseline algorithm, derived from Thompson sampling, that solves the CBCwith linear expected rewards is the LinTS (first mentioned by Agrawal et al. [12]). It\u2019s similar to TS in the MABC setting: at every step it samples from the prior distributions of the arms\u2019 parameters, plays the arm with the highest sampled expected reward and it doesn\u2019t use any clustering information. The learner startswith uninformative standardmultivariate\nReScience C 8.2 (#23) \u2013 De Luisa 2022 6\nnormalN(0, I) priors for parameter \u03b8a distributions (the Gaussian distribution was cho\u2010 sen due to its conjugate characteristic). At each step it samples fromN(xTt \u00b5a, xTt B\u22121a xt) for each arm, and plays (greedily) the arm with the highest sampled expected reward. The posterior belief in that arm\u2019s \u03b8c value is then updated according to the observed context and obtained reward: Bat = Bat + xtxTt , fat = fat + rxt, \u00b5at = B\u22121at fat . The posteriors for the other arms are not updated. The newly proposed algorithm that exploits the disjoint clustering structure (LinTSC [1]) is based on the same idea as LinTS, but adds an additional level to the selection of which arm to play (it keeps prior beliefs also for each cluster, and updates them according to the obtained rewards). LinTSC relates to LinTS in the same way as TSC relates to TS. The proposed algorithms based on Thompson sampling are compared to the following ones:\n\u2022 The LinUCB (Linear Upper Confidence Bound [13]) algorithm solves the CBC prob\u2010 lem (it ignores the clustering structure). The arm selection procedure is inspired by its MABC counterpart: the learner plays the arm that maximises the expres\u2010 sion: xTt \u03b8a \u03b1 \u221a xTt Baxt. It basically applies online ridge regression to estimate the\nparameters. The values for \u03b8a and Ba are computed and updated in the same way as in LinTS.\n\u2022 The LinUCBC (Linear Upper Confidence Bound for Clusters [9]) algorithm is an extension of LinUCB to clustered set of arms. It is based on the LinUCB algorithm, but adds another level to the arm selection procedure: it first selects which cluster and then which arm to play next.\nThere are no strict assumptions that the synthetic datasets for CBC problems should meet. Contextual data is generated in the same way as in [9]: we have N arms and K clusters, each arm j is uniformly randomly assigned to a cluster i. For each cluster we sample a centroid \u03b8ci \u223c N(0, I5) and assign the parameters to its arms as follows: \u03b8j = \u03b8 c i + \u03f5\u03bd, \u03bd \u223c N(0, I5). We control the expected diameter of a cluster by varying \u03f5. We generate the context at each timestep as is described in [9], sampling them from a multivariate standard normal distribution.\n5 Results\nThere are no exact numbers in the original paper to reproduce, rather the main claims are supported with visualisations. With this in mind, our reproduction confirms the advantage provided by clustering over the assumption of independent arms, as well as the newly proposed algorithms outperforming the referenced benchmarks. We repeat all the experiments with multiple seeds to obtain robust estimates of the algorithms\u2019 performance and reduce the risk of drawing any conclusions out of results obtained by chance. We show the obtained results in Figures 1 and 2 (the estimates are obtained with evaluations over multiple \u2013 25 to 100 \u2013 random seeds). We provide details on single reproduced claims (as defined in Section 2) in the following subsections.\n5.1 Classical MABC In Figure 1 we show all the obtained results from the experiments within the classical MABC problem setting (as described in the previous section), with plots 1a \u2010 1e present\u2010 ing the disjoint and plot 1f the hierarchical clustering. The results clearly show that taking into account the clustering structure of the arms significantly increases perfor\u2010 mance (i.e. lowers the cumulative regret). Our results are perfectly in line with those reported in the original paper. In each one of the plots we show how a dataset\u2019s hyper\u2010 parameter affects the learner\u2019s performance:\nReScience C 8.2 (#23) \u2013 De Luisa 2022 7\n0\n200\n400\n600\n0.01 0.05 0.1 0.15 0.2 0.25 0.3 d\nC um\nul at\niv e\nre gr\net\nTS TSC\n(a) w* = 0.1, N = 100, A* = K = 10.\n0\n200\n400\n600\n0 0.1 0.2 0.3 w*\nC um\nul at\niv e\nre gr\net\nTS TSC\n(b) d = 0.1, N = 100, A* = K = 10.\n0\n200\n400\n600\n10 30 50 70 90 110 130 150 N\nC um\nul at\niv e\nre gr\net\nTS TSC\n(c) w* = d = 0.1, A* = K = \u230a \u221a N\u230b.\n0\n200\n400\n600\n2 5 10 15 20 25 K\nC um\nul at\niv e\nre gr\net\nTS TSC\n(d) w* = d = 0.1, N = 100, A* = 10.\n0\n200\n400\n600\n10 20 30 40 50 A*\nC um\nul at\niv e\nre gr\net\nTS TSC\n(e) w* = d = 0.1, N = 100, K = 10.\n0\n250\n500\n750\n1000\n0 1 2 3 log2(N) L\nC um\nul at\niv e\nre gr\net\nHTS N = 50 HTS N = 100 HTS N = 1000 HTS N = 5000\n(f) Dependence on the number of levels L.\npaper. Since TSMax does also exploits clustering and just slightly differs from TSC, we believe the authorsmust havemade somemistakes in its implementation (they report a worse performance than UCBC).\n\u2022 Figure 2b: TSC clearly stands out from the others in term of performance when we significantly increase the number of arms N . Again we observe better TSMax performance than reported.\n\u2022 Figure 2c: UCT algorithm performmuch worse than HTS (note that TSC is an HTS with L = 1). The obtained regrets are in line with the original paper, but we ob\u2010 serve higher uncertainty in our estimations (although we repeated each experi\u2010 ment the same number of times and show the errorbars in the same way \u2013 \u00b1 1 standard deviation).\n5.3 Contextual CBC In Figures 2d \u2010 2f we show the results of our experiments with the contextual bandits algorithms. The single plots present the evaluations on datasets generated with differ\u2010 ent hyperparameters, however they all have the same shape, hence we can analyse all of them together. We can see that in the CBC problem, the algorithms that leverage the clustering information heavily outperform the others, while there is no significant dif\u2010 ference between UCB\u2010 and TS\u2010based methods. The authors of the original paper here claim that LinTSC slightly outperforms LinUCBC, but fromour results we definitely can\u2019t draw the same conclusion.\n6 Discussion\nWith our work we are able to successfully reproduce the results obtained by the authors of the original paper. As explained in the previous section, some of our results differ slightly from those reported in the paper mostly with respect to variance of the esti\u2010\nReScience C 8.2 (#23) \u2013 De Luisa 2022 9\nmates, but they still support the main claims. However, we have to point out a couple of potential issues that we identified in their work. First of all, Upper Confidence Bound algorithms rely on the initialisation step during which each arm should be played once. If we look again at Figures 2b or 2c, we see that those learners spent a significant amount of the allocated time just playing each arm one by one. Furthermore, for t < 1000, the presented numbers are just the result of playing the first N arms, therefore determined by their ordering. Some more concerns arise when dealing with the CBC problem. In the original paper, each arm has its own parameter vector \u03b8a that we want to learn from the obtained re\u2010 wards and given contexts xt. They mimic the same setting as in Bouneffouf et al. [9]. However, the other two algorithms (LinTS and LinUCB) are designed for a different set\u2010 ting, where a different context is given for each arm at every timestep. Furthermore, per [9], the expected reward should be linearly dependent on the context and played arm\u2019s feature vector, but the reward itself should lie inside [0, 1]. This is clearly in contrast with our CBC setting, where both negative and larger rewards are possible (and actually really common too). We can\u2019t expect dot products of normally sampled vectors to always fulfill these conditions.\n6.1 What was easy The authors have included in the paper all the necessary details to reimplement their proposed algorithms, recreate the synthetic datasets, and reproduce the experiments for the first part, i.e. the classical MABC setting.\n6.2 What was difficult It is much harder to reimplement some of the referenced benchmarks. The main rea\u2010 sons for these struggles are the inconsistent nomenclature (the referencedpaper presents multiple algorithmswhichwere designedwith a different name than the one used in the reproduced paper), important details missing in the referenced papers, no code avail\u2010 able whatsoever and some of the compared benchmarks being originally designed to run in a different setting (especially true for CBC problems). Furthermore, some addi\u2010 tional research into the field of contextual bandits is needed to reproduce the CBC part of the experiments, since due to all the inconsistencies between the different papers, we had a hard time understanding how are contextual bandits supposed to work.\n6.3 Communication with original authors There has been no communication neither with the authors of the original article nor with any authors of the referenced papers."}], "title": "[Re] Thompson Sampling for Bandits with Clustered Arms", "year": 2022}