{"abstractText": "DOI 10.5281/zenodo.4242972 Abstract This article endeavours to reproduce the experimental study of B. B\u00fceler, A. Enge, K. Fukuda: Exact Volume Computation for Polytopes: A Practical Study, 2000, in which a variety of algorithms for volume computation are applied to a variety of different higher-dimensional polytopes. The original software is used on a modern machine to redo the computations. It turns out that due to Moore\u2019s law, running times go down, but the original conclusions are still valid.", "authors": [{"affiliations": [], "name": "Andreas Enge"}, {"affiliations": [], "name": "Thomas Arildsen"}], "id": "SP:9ca4fdd6b191a607de6dc3ea5ef928154dc3fa24", "references": [{"authors": ["B. B\u00fceler", "A. Enge", "K. Fukuda"], "title": "Exact Volume Computation for Polytopes: A Practical Study.", "venue": "DMV Seminar. Basel: Birkha\u0308user Verlag,", "year": 2000}, {"authors": ["B. B\u00fceler", "A. Enge. vinci"], "title": "Distributed under GPL v2+", "venue": "URL: http ://doi", "year": 2003}, {"authors": ["L. Court\u00e8s"], "title": "Re] Storage Tradeoffs in a Collaborative Backup Service for Mobile Devices.", "venue": "ReScience C", "year": 2020}], "sections": [{"text": "Edited by Thomas Arildsen ID\nReviewed by Dmitrii Pasechnik ID\nReceived 10 April 2020\nPublished 04 December 2020\nDOI 10.5281/zenodo.4242972\nDedicated to Komei Fukuda, who taught me the values of free software and scientific ethics\n1 The original study\n1.1 Volume computation for polytopes A convex polytope of dimension d can be given in two forms: by its vertex or V -representation as the convex hull of n points v1, . . . , vn \u2208 Rd; or by its halfspace orH-representation as the bounded intersection of m halfspaces given by inequalities aix \u2a7d bi with ai \u2208 Rd and bi \u2208 R, i = 1, . . . ,m, or in matrix notation Ax \u2a7d b with A \u2208 Rm\u00d7d and b \u2208 Rm. When both are available, this is called the double description, which also provides the incidence information which point vj lies on the boundary of which halfspace given by the equality aix = bi. The elements of the (unique) minimal V -representation are called vertices; the boundaries {aix = bi} of the (unique unless the polytope is contained in a strict affine subspace) minimalH-representation are called facets. Algorithmically these representations are not equivalent, since the size of one may be exponential in the size of the other. For instance, the d-dimensional hypercube can be given by m = 2d inequalities \u22121 \u2a7d xj \u2a7d 1 for j = 1, . . . , d, but it has 2d vertices x with xj \u2208 {\u22121, 1} for j = 1, . . . , d. There is a notion of duality, which \u201cswaps\u201d vertices and facets. So the dual of a hypercube, a cross polytope, has the exponential number 2d of facets and the linear number 2d of vertices. The volume of a polytope is understood with respect to the standard measure of Rd; for instance, the hypercube has volume 2d. Algorithms for exact volume computation generally decompose the polytope into smaller ones, for which a volume formula is available. These can be simplices, a generalisation of 2-dimensional triangles or 3-dimensional tetrahedra, given by d + 1 facets or d + 1 vertices v0, . . . , vd, which are the intersections of any subset of d facets; the volume of such a simplex is given by 1d! |det(v1 \u2212 v0, . . . , vd \u2212 v0)|. In the case of a decomposition into simplices, one speaks of a triangulation. For instance, by triangulating the facets and augmenting each of the d \u2212 1-dimensional simplices by an interior point of the polytope, one obtains a boundary triangulation. Alternatively, there are signed decompositions into simplices, usually containing also points exterior to the polytope, such that by an inclusion\u2013exclusion principle every point in the interior of every simplex is counted exactly once. Other methods decompose the polytope into \u201cpyramids\u201d, the convex hull of a facet and an additional vertex, with volume 1d times the height times the d \u2212 1-dimensional volume of a facet. All these algorithms eventually recurse over the dimension, so for general polytopes they end up having an exponential complexity in the dimension.\nCopyright \u00a9 2020 A. Enge, released under a Creative Commons Attribution 4.0 International license. Correspondence should be addressed to Andreas Enge (andreas.enge@inria.fr) The authors have declared that no competing interests exist. Code is available at https://doi.org/10.5281/zenodo.4293820 \u2013 DOI 10.5281/zenodo.4293820.. Data is available at https://doi.org/10.5281/zenodo.4293875 \u2013 DOI 10.5281/zenodo.4293875. Open peer review is available at https://github.com/ReScience/submissions/issues/27.\nReScience C 6.1 (#17) \u2013 Enge 2020 1\n1.2 Experimental results\nThe subject of [1] was to implement all algorithmswe could find in the literature and to compare their suitability for different classes of polytopes. We also suggested practical improvements to some algorithms and developed a new one. Table 1 from [1, \u00a75.3] summarises our computations; it records for each algorithm (the columns) and each polytope (the rows, sorted by decreasing ration/m) the running time in seconds, aswell as reasons why the computation may not have succeeded. The last two columns give the timings for switching from one representation to the other. The rough conclusion was that the \u201cupper right\u201d and \u201clower left\u201d corners of the volume computation part correspond to good combinations. For detailed descriptions of the algorithms and the polytopes, see the original article [1].\n2 Software installation\nThe goal of this article, written in the context of the \u201cTen Years Reproducibility Challenge\u201d1, is to try and reproduce the results of [1], by running the old code on a current machine and comparing results. In fact, [1] is my first scientific article, written not ten, but more than twenty years ago. The first challenge is obviously to locate the old source code (and example data files) and to compile it with modern tools.\n1https://rescience.github.io/ten-years/\nReScience C 6.1 (#17) \u2013 Enge 2020 2\n2.1 Fast track The volume computation software written during the project, called vinci, was published under the GPL and is still available on my web page [2], which also hosts the polytope data files. It is written in ANSI C, so I expected no problems redoing the computations, and indeed there were none: It is enough to download and uncompress the vinci source code and the polytope files, to type make and to run the generated binary. This could be the end of \u00a72. However, it turns out that the version on the web is not the one used for carrying out the computations of [1]. For instance, the algorithm of column \u201cDel\u201d in Table 1, deemed numerically too unstable, has been removed before publication of the source code. So tomeet the reproducibility challenge, I have attempted to get back as closely as possible to the historical situation when writing [1].\n2.2 Historically informed performance\nSoftware \u2014 The historical instrument, an HP 7000/735-99 server, has long gone; so the challenge is to run the historical software on a modern machine. First of all, where is the historical source code? The article [1] was submitted in January 1998, revised in August 1998, and published in a book with conference proceedings in 2000. Only the latest released version 1.0.5 of vinci is available on the web [2]. It contains aChangeLog file that dates it to 2003, and gives 2001 as the release date for 1.0.1 (no date for release 1 is mentioned), which is clearly after writing the article. On my private hard disk I have kept an mbox file containing mails with the results of the batch submission system of the time, including the outputs of the vinci runs. In these, the software identifies itself as \u201cVINCI - Version gamma as of\u201d with three different dates, \u201c27.09.1997\u201d, \u201c18.10.1997\u201d and \u201c11.10.1997\u201d. On the same disk, there is indeed a file vinci_gamma.tar.bz2; uncompressing it produces files with time stamp October 12, 1997, a rather close match. Moreover, it turns out that the software identifies itself with the date \u201c12.10.1997\u201d defined in vinci.h. In the following, I document precisely the steps taken to compile this source code as well as all auxiliary software.\n$ wget https://zenodo.org/record/4293820/files/vinci_gamma.tar.bz2 $ tar xvf vinci_gamma.tar.bz2 $ cd vinci_gamma $ make vinci.c:750:1: warning: return type defaults to \u2019int\u2019 [-Wimplicit-int]\n750 | main (int argc, char *argv []) | ^~~~\nIndeed, the return type int is missing in front of main, and by default the current gcc version 9.2.0 complains about it.\n$ ./vinci square\nprints 4, the volume of the hypercube in dimension 2, the files for which are included in the vinci tarball. So far, so good; the twenty years old software compiles and seems to run without problem. According to the article, we need the auxiliary software packages lrs and qhull; they are used for computing a boundary triangulation (column \u201cBnd\u201d of Table 1) or a Delaunay triangulation (column \u201cDel\u201d), respectively. The two binaries are compiled separately, put into the vinci directory and called from vinci via the C library system call; communication is done through input and output files. The article [1] provides URLs for the different programs, but the exact versions of the time were not recorded. The current website of lrs2 lists as the oldest available versionlrslib-0.4.0 of December 11, 2000, which is after our experiments. So the historically informed performance needs to compromise and rely on a slightly newer lrs version than available at the time.\n$ wget http://cgm.cs.mcgill.ca/~avis/C/lrslib/archive/lrslib-040.tar.gz $ tar xvf lrslib-040.tar.gz\n2http://cgm.cs.mcgill.ca/ avis/C/lrslib/archive/\nReScience C 6.1 (#17) \u2013 Enge 2020 3\n$ cd lrslib-040/ $ make buffer.c:69:5: error: conflicting types for \u2019getline\u2019\n69 | int getline(void) | ^~~~~~~\nIn file included from buffer.c:6: /home/enge/.guix-profile/include/stdio.h:616:18: note:\nprevious declaration of \u2019getline\u2019 was here 616 | extern __ssize_t getline (char **__restrict __lineptr,\nThe function getline is declared in buffer.c and used only there, and it clashes with a function of the same name in the C library glibc-2.29. Such a function is not part of the C standard, but a GNU extension3; running git blame on its source4 shows that it has been around since at least 1995. So while the makefile of lrs hardcodes gcc as the C compiler, the binary was probably linked at the time against the C library of the HP system without that extension. The function getline is renamed to mygetline, and its three occurrences (declaration, definition and usage) are replaced in the file by the following commands:\n$ wget https://zenodo.org/record/4293820/files/lrs-getline.patch $ patch < lrs-getline.patch\nwhere the content of the file lrs-getline.patch is given in Table 2.\nThen compilation with make goes through. (It turns out that reproducing a reproducibility experiment is a tricky affair: Between writing the first version of this article and revising it a few months later, compilation of lrs actually stopped working; gcc-10.2.0 complains about a lacking inclusion of stdlib.h and then about duplicate definitions, whereas gcc-9.3.0 emits warnings, but does compile the project nevertheless.) So build lrs (with a suitable compiler version) and copy the binary into the correct directory:\n3Search for \u201cgetline\u201d in the documentation at https://archive.softwareheritage.org/browse/content/sha1_git:c48e3e692f6f4a9c9dfd8e51ebb1ecf18e756e28/raw/?filename= stdio.texi\n4https://sourceware.org/git/?p=glibc.git;a=blob;f=stdio-common/getline.c;h=9b1641f23e1ae527de54a206a7e7b3f49bea d0f3;hb=HEAD\nReScience C 6.1 (#17) \u2013 Enge 2020 4\n$ make $ cp lrs ../ $ cd ../\n(The current lrs version lrslib-070 has also renamed the internal function; so here I have simply redebugged a problem that was already solved in a later version.) The qhull website5 distributes code from 2019 and qhull-1.0 from 1993 according to the time stamps of the files of the tarball and their copyright notices. So it is either too old or too new. The git repository of the project provides some more information:\n$ git clone https://github.com/qhull/qhull.git qhull-git $ cd qhull-git $ git tag -l\nThe last command prints tags ranging from v3.0 to v7.3.2, and then from 2002.1 to 2019.1. The file src/Changes.txt states that version 3.0 was released on February 11, 2001; at the time, we probably used version 2.4 of 1997, but not having its source code, I again compromised in my historically informed performance by using release 3.0\n$ git checkout v3.0 $ cd src $ make $ cp qhull ../../ $ cd ../../\nNow the vinci_gamma directory contains the three binaries vinci, lrs and qhull. The latter two can be tested on the square via\n$ export PATH=.:$PATH $ ./vinci square -m lrs ... Using \u2019lrs\u2019 for computing a boundary triangulation. ... Volume: 4\n$ ./vinci square -m qhull ... Running qhull with the options d i Q0 Qz. precision problems\n2 coplanar half ridges in output 1 coplanar horizon facets for new vertices\nOutput file of qhull opened. 2 pseudo-simplices to be computed. ... Volume: 4.000000000000e+02\nSo it looks like we are operational for vinci. Table 1 also contains timings for switching between the V - and the H-representations; the \u201ceasy\u201d direction (from fewer vertices/facets to many facets/vertices) is computed by lrs, already installed above; the \u201chard\u201d direction is computed by the additional external software pd (\u201cprimal-dual\u201d), which is no more available at the web page given in the old paper. However, it can be found using a quick web search:\n$ wget http://www.cs.unb.ca/~bremner/software/pd/pd.tar.gz $ mkdir pd-build $ cd pd-build $ tar xvf ../pd.tar.gz\nThe time stamp of the extracted files isMay 14, 2013, but themain C file gives as date November 20, 1997. Depending on the example (here, rv_10_12.ine), the computation may fail with an error message such as\n5http://www.qhull.org/\nReScience C 6.1 (#17) \u2013 Enge 2020 5\nMulint overflow for a*b a=...b=... BASE: 10000 DIGITS: 250 Maximum integer is BASE^(DIGITS-1)-1 Increase BASE and/or DIGITS and recompile!\nFrom the code, it turns out we are in 32-bit mode; but larger constants are hard-coded for 64-bit arithmetic. To switch, one may add -DB64 to CFLAGS in makefile as follows:\n$ wget https://zenodo.org/record/4293820/files/primal-dual-64bit.patch $ patch < primal-dual-64bit.patch\nwhere the content of primal-dual-64bit.patch is reproduced in Table 3. Then compile and copy the binary to the correct directory:\nWith the patch applied, BASE becomes 109 and DIGITS becomes 112; so with or without the 64-bit patch, the maximal precision is about 1000 decimal digits, which is apparently not enough. Interestingly, the culprit is the input file (discussed in more detail in \u00a72.2.2), which specifies a precision with the directive digits 1000. As a solution, one may multiply the number of digits in the file by 2 until the computation passes; this is necessary only for the \u201chard\u201d direction using pd. For recomputing the corresponding entries of the table, the precision is thus increased as follows: For rh_8_*.ext and rv_8_*.ine, from 500 to 1000 digits; for rh_10_*.ext, from 500 to 2000 digits; and for rv_10_*.ine, from 1000 to 2000 digits.\nInput files \u2014 It turns out that between the versions gamma and 1.0.5 of vinci, the requirements for the polytope input files changed. For algorithms working with the double description, both require the V - representation in a .ext-file (for \u201cextreme points\u201d) and the H-representation in a .ine-file (for \u201cinequalities\u201d). But the older version also requires a .icd-file for the incidence information, while the newer one recomputes it on the fly from the other two files. The polytopes on the web [2] lack the .icdfiles. These could be recreated using code from vinci-1.0.5, but luckily it turned out that I had also kept copied of the historical polytope files on my hard disk.\n(Re-)debugging, or reverse debugging \u2014 The -m parameter of vinci specifies the method to use for volume computation; if it is not set, it uses the default -m hhv (for the gamma version, renamed to -m hot in release 1.0.5), corresponding to the column labelled \u201cHOT\u201d in Table 1. As seen above, this works on the square, but it fails already on a 4-dimensional hypercube with a segmentation fault. Using a gdb backtrace and comparing the offending function tri_ortho between the versions gamma and 1.0.5 reveals that indeed a null pointer is dereferenced; the bug is fixed by applying a patch as follows:\n$ wget https://zenodo.org/record/4293820/files/vinci-dummy.patch $ patch < vinci-dummy.patch\nwhere the content of vinci-dummy.patch is reproduced in Table 4.\nReScience C 6.1 (#17) \u2013 Enge 2020 6\nMaybe the code worked at the time on the HPmachine since, as its name indicates, the variable dummy is indeed not used after the call to tree_out; it is only required for a different volume computation method. In 1997, we displayed timing information with two decimals; given Moore\u2019s law, this is not enough anymore, and the following patch switches to four decimals: $ wget https://zenodo.org/record/4293820/files/vinci-print.patch $ patch < vinci-print.patch\nwhere the content of vinci-print.patch is shown in Table 5. diff -u vinci_gamma.old/vinci.c vinci_gamma/vinci.c --- vinci_gamma.old/vinci.c 2020-09-16 17:31:16.512294710 +0200 +++ vinci_gamma/vinci.c 2020-09-16 17:35:24.744301773 +0200 @@ -968,7 +968,7 @@\nLawrence\u2019s method, a signed decomposition algorithm (corresponding to the columns \u201cLnd\u201d and \u201cLd\u201d in Table 1) uses a random hyperplane. In the original code, we tried to make it reproducible between different runs by calling srand with the fixed seed 4, then using rand to create several int values. However, the random numbers are not the same due to different values of RAND_MAX: 231 \u2212 1 on my modern 64-bit system, probably 215 \u2212 1 on the old 32-bit HP system (the old logs indeed show 15-bit randomnumbers, which is consistent with this hypothesis). This implies that the simplex volumes added and subtracted during the algorithm are not the same any more. Vinci prints the random numbers, and it would be possible to replace the calls to rand by using the fixed list of numbers of the time; given the numerical instability of the signed decomposition, where substractions may result in huge cancellations, I considered it was not worth the effort. It turns out that an additional tiny patch is needed to reproduce the results for one of the volume computation algorithms. Lasserre\u2019s method requires to choose a pivot in a matrix; by default vinci uses a defensive partial pivoting strategy, choosing the largest pivot in absolute value, while the column \u201crL\u201d of Table 1 was computed with an early abort strategy, choosing the first pivot larger than the value 0.01 of MIN_PIVOT if possible. There is no command line parameter for selecting the behaviour, so a trivial patch needs to be applied to vinci.h: $ wget https://zenodo.org/record/4293820/files/vinci-pivoting.patch $ patch < vinci-pivoting.patch\nReScience C 6.1 (#17) \u2013 Enge 2020 7\nthe content of which is reproduced in Table 6.\nThis one-character change has an important influence on the running time, while still producing correct results; I initially forgot it, and to give an example, the running time of about 6s for rL on rh-10-25 in Table 8 rises to about 29s with partial pivoting. This is due to our strategy of dynamic programming in Lasserre\u2019s algorithm, where partial pivotingmakes it less likely that volumes of subfacesmay be retrieved from memory, as explained right above \u00a75.1 in [1]. Pivoting also plays a role when computing a simplex volume as a determinant; but the article [1] mentions it only in the context of Lasserre\u2019s algorithm and not for triangulations. Indeed, a quick experiment with CH on rh-10-20 shows no noticeable difference in the running time. And looking at the output of the batch jobs used to produce Table 1 shows that partial pivoting is used for other algorithms than Lasserre\u2019s. After all these little patches, it is time to build the vinci executable again:\n$ make\n2.3 Modern times\nFor good measure and to go with modern tools, I have also included vinci release 1.0.5 into GNU Guix6, a modern, functional (in the sense of functional programming) GNU/Linux distribution with exact dependency tracking and the aim of easing reproducibility7. The full build recipe, added in git commit b457f3cc16, is shown in Table 7. Besides some self-explaining metadata, the core content of the record are the fields build-system and arguments. To start with, the build system is chosen as gnu-build-system, which essentially runs\n./configure && make && make check && make install\nOf these four phases, only the second one,make, is actually kept. The check phase is disabled by the line #:test? #f. Installation is done \u201cby hand\u201d, copying only the binaryvinci into the output directory. The package depends on lrs, which is called lrslib in GNU Guix, via theinputs field. The configure phase is replaced by a phase in which the actual location of the lrs binary is coded into the vinci.h header file, instead of expecting it in the current directory. GNU Guix has mechanisms for \u201cgoing back in time\u201d; the commands\n$ guix pull --commit=b457f3cc16 $ guix environment --ad-hoc vinci\ndowngrade the Guix version on a machine to the moment where vinci-1.0.5 was added to it and start an ephemeral environment with the vinci version of this commit. Unfortunately, guix pull modifies the user environment (otherwise said, it pollutes the environment with state, which is nonfunctional). Functional time travel is enabled by guix time-travel, which essentially combines the two previous commands into one without modifying state:\n$ guix time-machine --commit=b457f3cc16 -- environment --ad-hoc vinci 6https://guix.gnu.org/ 7See Konrad Hinsens\u0313 blog post at https://guix.gnu.org/blog/2020/reproducible-computations-with-guix/\nReScience C 6.1 (#17) \u2013 Enge 2020 8\nReScience C 6.1 (#17) \u2013 Enge 2020 9\nOf course it is not possible to travel to an alternative past: When the software versions of \u00a72.2 were written, Guix did not yet exist, so there is no past in which they would have been available in Guix. Tomake it possible to carry out the historically informed performance of \u00a72.2 with modern tools, I added the older software versions to an additional \u201cchannel\u201d of Guix, guix-past8, which was introduced in the context of another article in this Ten Years Reproducibility Challenge [3, \u00a72.3]. Channels are a way of defining \u201coverlays\u201d on top of the \u201cofficial\u201d Guix distribution. Using them requires a declaration in a configuration file. For instance, the guix-past channel is registered by putting exactly the following lines into $HOME/.config/guix/channels.scm:\n(cons (channel (name \u2019guix-past) (url \u201dhttps://gitlab.inria.fr/guix-hpc/guix-past\u201d) (introduction (make-channel-introduction \u201d0c119db2ea86a389769f4d2b9c6f5c41c027e336\u201d (openpgp-fingerprint \u201d3CE4 6455 8A84 FDC6 9DB4 0CFB 090B 1199 3D9A EBB5\u201d))))\n%default-channels)\nThen\n$ guix pull\nupdates GNU Guix (from %default-channels) to its latest version, and adds the guix-past channel. Finally\n$ guix environment --ad-hoc vinci@0.97.10.12 \\ lrslib@4.0 primal-dual@0.97.11.20\ncreates an ephemeral environment in which the vinci, lrs and pd versions referenced in \u00a72.2 are available.\n3 Reproduced results\nIn line with the Ten Years Reproducibility Challenge, I have used the historically informed approach described in \u00a72.2 to recompute Table 1. According to [1], themachine used at the timewas anHP7000/73599 workstation with about 500MB of RAM. We did not record more detail; a web search reveals documentation about an HP 9000 (not 7000) 735/99 computing cluster9, introduced in 1992, with a 32-bit PA-7100 CPU clocked at 99 MHz and a 30W power consumption. I now performed all computations on my laptop with an Intel Core i5-6300U CPU running at 2.4 GHz (with a boost mode up to 3.0 GHz) and a thermal design power of 15W10 with 16GB of main memory. The results are recorded in Table 8. As in [1], I used a MIN_PIVOT of 0.01 for Lasserre\u2019s algorithm of column \u201crL\u201d and partial pivoting for all other methods, cf. the discussion at the end of \u00a72.2.3 on reverse debugging. At first glance, the table looks quite similar to the old one: Volumes that were computed incorrectly are still incorrect; intractable computations are (mostly) still intractable; and while all running times are lower, the conclusions of [1] about which algorithms behave well or poorly on which classes of polytopes are confirmed. Unlike in [1], all computations with Lawrence\u2019s formula in the degenerate case (column \u201cLd\u201d) fail. This may be due to bad interfacing with the used version of lrs, which is not the same as in [1]; but since already there we concluded that Lawrence\u2019s method is numerically unstable and should thus be avoided (at least in the way we implemented it, with a random hyperplane), I did not find it worthwhile to debug, and also did not try to recompute the examples that already failed in 1997. A few entries in the table deserve special comment, since the computations can now be carried out on a machine with more memory. These are cube-10 with lrs (column \u201cBnd\u201d) and qhull (column \u201cDel\u201d); the\n8https://gitlab.inria.fr/guix-hpc/guix-past 9https://www.openpa.net/systems/hp-9000_735_755.html, https://www.hpmuseum.net/display_item.php?hw=431 10https://ark.intel.com/content/www/us/en/ark/products/88190/intel-core-i5-6300u-processor-3m-cache-up-to-3-00-ghz.h\ntml\nReScience C 6.1 (#17) \u2013 Enge 2020 10\nlatter requires about 3GB of memory. Our implementation of Lasserre\u2019s algorithm (column \u201crL\u201d) uses a dynamic programming style approach to store and retrieve volumes of lower-dimensional faces, starting at dimension d\u2212 2, and going down to dimension 2 (since edges, of dimension 1, are always simplices, their length is easy to recompute and does not warrant wasting memory). For the complete cut polytope on five vertices, ccp-5 of dimension 10, we needed in [1] to limit storage to six levels, from dimension 8 down to 3, while now we can also store 2-dimensional volumes. The same holds for the metric polytope Fm-6 of dimension 15, where instead of storing face values of nine levels we can store all twelve levels from dimension 13 down to 2, using a bit less than 5GB of memory. For a closer comparison, it is interesting to consider the factor by which the current computations are faster than in 1997. This is recorded in Table 9 for the combinations of polytopes and algorithms where this makes sense, that is, those that are computed correctly and with a running time that could be measured to two significant digits in both tables and, for Lasserre\u2019s method, with the same level of storage. Again, all numbers are rounded to two significant digits. Since numbers are divided that were already rounded to two significant digits, there is a total rounding error of up to 11%, but this should be enough for drawing some general conclusions. The disparity between the numbers, ranging from 37 to 450, is somewhat surprising; in the following, I try to provide some explanations. The results in the first column labelled \u201cBnd\u201d are entirely computed by lrs. The discussion on BASE and DIGITS at the end of \u00a72.2.1 shows that the software relies on its own implementation ofmultiprecision integers using machine integers; so it is reasonable to assume that the quite homogeneous factors in Table 9, between 100 and 140, measure the relative performance of integer arithmetic on the two machines. This is consistent with the upper half of column \u201cH\u2192V \u201d, from cube-9 to Fm-6, which is\nReScience C 6.1 (#17) \u2013 Enge 2020 11\nalso computed by lrs, while I have no explanation for the factors 340 and 450 in the lower half of column \u201cV\u2192H\u201d, from ccp-5 to cross-9, again obtained by lrs. The remaining results in the columns \u201cH\u2192 V\u201d and \u201cV\u2192H\u201d are computed by pd with essentially the same multiprecision arithmetic, with a more spread variation of the factors; I do not know enough of the algorithm and implementation to venture an explanation. The column \u201cDel\u201d is obtained by a mixture of a call to qhull for computing the Delaunay triangulation, and the computation of the resulting simplex volumes in vinci; both use double precision, so one might conclude that the rather homogeneous numbersmeasure the relative performance of floating point operations between the two machines. This is, however, also the case for \u201cLnd\u201d, with a somewhat larger gain on themodernmachine. Lasserre\u2019s algorithm of column \u201crL\u201d alsomakes heavy use of floating point arithmetic, not only for the volume computation formula, but also for the linear algebra when projecting faces onto affine subspaces. Additionally, it stores partial results for later reuse in a balanced search tree. So the higher factor in the \u201crL\u201d columnmight be explained by a relatively better memory performance on the modern machine. However, also the algorithms behind the columns \u201cCH\u201d and \u201cHOT\u201d store partial results in a tree in memory, and there the factors are rather small. On the other hand, for the \u201ccombinatorial part\u201d of going down by one dimension, they do not use floating point operations for projections, but symbolic intersection of faces with facets, both given by an ordered array of vertices specified by their labels of int type. So the relative performance of this operation may explain the consistently smaller factors in the two columns, except for the outlier rv-8-30. This polytope with 30 facets has by far the largest number of vertices, 4482, of all considered examples (cf. Table 1). So the combinatorial structure of the polytopes appears to not only have an influence on the relative performance of the different algorithms, but also on the performance of the same algorithm on different processors: While the factors do vary a lot over the complete table, they are quite similar inside a \u201cblock\u201d of one algorithm applied to a fixed\nReScience C 6.1 (#17) \u2013 Enge 2020 12\nclass of polytopes. Admittedly, these attempts at explanation are more speculation than proof; for a scientifically sounder comparison, one would have to carry out more detailed experiments, which would require access to the old machine. All factors in Table 9 are (much) larger than the quotient of the nominal clocking speeds of the processors of about 24. Moreover, the energy efficiency of the computations has probably increased a lot: The thermal design power has been halved from the processor in the workstation to that in the laptop (while the figures for the complete system are unknown), and nevertheless the speed of the computations has gone up by a factor of about 100. This should be attributable to the miniaturisation of transistors as captured by Moore\u2019s law. Since more than 22 years have passed between the computations, even a conservative estimate of two years per \u201cMoore cycle\u201d leads to eleven cycles and a factor of 211 \u2248 2000. Notice that this is very far from the factors recorded in Table 9, which is consistent with the general observation that Moore\u2019s law does not apply to the performance of serial code anymore, but that the increased number of transistors is used to place more computing cores onto the same die. So in order to profit from more powerful computers, code needs to be parallelised. It looks at first as if this would not be too difficult for volume computation: By recursing over the dimension, the algorithms visit the polytope faces and perform a tree traversal. So different branches of the tree may be assigned to different processors. Since not all branches have the same size, a task based approach may prove to be fruitful. However, by storing and reusing partial results, the faster algorithms exploit the fact that the face lattice is not a tree, but a directed acyclic graph with cycles if directions are removed: Every edge is contained in at least two faces of dimension 2, for instance. In a fast parallel implementation, to profit from this structure would require additional communication between the computing cores.\n4 Conclusion\nReproducing the results of Table 1 has been quite easy, due to the following observations. All used software is free and readily available on the Internet. The task would have been easier if the software and example polytope files could have been published electronically alongside the article; or if all software projects (including my own\u2026) had archived all releases on their websites11. Nowadays, developing source code in a publicly accessible, version controlled repository could also ease the task of finding a specific release in the future, assuming the technology (git, right now) remains sufficiently stable and in use or provides an upgrade path (as is available from subversion to git, for instance). Secondly, all used software is implemented in standard C with a simple makefile. The choice of the C language seems to be optimal for reproducibility due to its ubiquity; compilers have been available for all platforms, notably gcc of the GNU project: The oldest version still available on the GNU ftp server12 is release 1.3.5 from 1989. The build system of all used software is also very simple; again, GNU make is available on all platforms. (In the worst case, the C files could be compiled \u201cby hand\u201d and the resulting object files could all be linked together into an executable, the presence of makefile in vinci is a mere convenience.) Once the software had been compiled, redoing the computations has not posed any problem either (on the contrary, the availability of more powerful machines has considerably reduced the waiting time!). However, one of the algorithms (\u201crL\u201d) has required patching a header file to select an option; it would have been easier if this had been realised by a command line option, prominently advertised next to the table of results. I am pleased to see that while not all computations have been sped up by the exact same factor, ourmain observations of the time remain valid: The ordering of the algorithms by performance on the different polytope classes is still the same, making it possible to automatically select the best algorithm for a given polytope. Moreover the chosen example polytopes are still meaningful; even twenty years later, it is not possible to go much further due to the exponential complexity of the algorithms (and most likely of the problem itself). So this is an interesting case in which algorithms may remain relevant over a long period of time.\n11This problem is addressed by the Software Heritage project, https://www.softwareheritage.org/. 12https://ftp.gnu.org/old-gnu/gcc/\nReScience C 6.1 (#17) \u2013 Enge 2020 13"}], "title": "[Re] Volume computation for polytopes: Vingt ans apre\u0300s", "year": 2020}