{"abstractText": "The image classification experiments on CIFAR\u201010, CIFAR\u2010100 and ImageNet are repro\u2010 duced to within 0.29%, 0.18% and 0.25% of reported values respectively. The language modeling experiments produce an average deviation of 0.22%,while the generativemod\u2010 eling experiments onWGAN,WGAN\u2010GP and SN\u2010GAN are replicated towithin 2.2%, 1.8% and 0.33% of value reported in the original paper. We perform ablation studies for change of dataset in language modeling and for effect of weight decay on ImageNet. We also perform analysis of generalization ability of op\u2010 timizers and of training stability of GANs. All of the results largely support the claims made in the paper [1].", "authors": [{"affiliations": [], "name": "Anirudh Buvanesh"}, {"affiliations": [], "name": "Madhur Panwar"}, {"affiliations": [], "name": "Koustuv Sinha"}, {"affiliations": [], "name": "Sharath Chandra Raparthy"}, {"affiliations": [], "name": "Juntang Zhuang"}], "id": "SP:97c28f5429b89ea9c82d5607ec046c4e01b7ba8f", "references": [{"authors": ["J. Zhuang", "T. Tang", "Y. Ding", "S.C. Tatikonda", "N. Dvornek", "X. Papademetris", "J. Duncan"], "title": "AdaBelief Optimizer: Adapting Stepsizes by the Belief in Observed Gradients.", "venue": "en. In: Advances in Neural Information Processing Systems", "year": 2020}, {"authors": ["H. Robbins", "S. Monro"], "title": "A stochastic approximationmethod.", "venue": "Annals of Mathematical Statistics", "year": 1951}, {"authors": ["I. Sutskever", "J. Martens", "G. Dahl", "G. Hinton"], "title": "On the importance of initialization and momentum in deep learning.", "venue": "Proceedings of the 30th International Conference on Machine Learning", "year": 2013}, {"authors": ["Y.E. NESTEROV"], "title": "A method for solving the convex programming problem with convergence rate O(1/k2).", "venue": "In: Dokl. Akad. Nauk SSSR", "year": 1983}, {"authors": ["D.P. Kingma", "J. Ba"], "title": "Adam: A Method for Stochastic Optimization. 2017", "year": 2017}, {"authors": ["L. Liu", "H. Jiang", "P. He", "W. Chen", "X. Liu", "J. Gao", "J. Han"], "title": "On the Variance of the Adaptive Learning Rate and Beyond.", "venue": "[cs, stat] (Apr", "year": 2020}, {"authors": ["I. Loshchilov", "F. Hutter"], "title": "Decoupled Weight Decay Regularization", "year": 2019}, {"authors": ["A. Graves"], "title": "Generating Sequences With Recurrent Neural Networks", "year": 2014}, {"authors": ["M. Zaheer", "S.J. Reddi", "D. Sachan", "S. Kale", "S. Kumar"], "title": "Adaptive Methods for Nonconvex Optimization.", "venue": "Proceedings of the 32nd International Conference on Neural Information Processing Systems. NIPS\u201918. Montre\u0301al, Canada: Curran Associates Inc.,", "year": 2018}, {"authors": ["L. Luo", "Y. Xiong", "Y. Liu", "X. Sun"], "title": "Adaptive Gradient Methods with Dynamic Bound of Learning Rate", "year": 2019}, {"authors": ["L. Balles", "P. Hennig"], "title": "Dissecting Adam: The Sign, Magnitude and Variance of Stochastic Gradients", "year": 2020}, {"authors": ["J. Bernstein", "A. Vahdat", "Y. Yue", "M.-Y. Liu"], "title": "On the distance between two neural networks and the stability of learning.", "venue": "Neural Information Processing Systems", "year": 2020}, {"authors": ["X. Ma"], "title": "Apollo: An Adaptive Parameter-wise Diagonal Quasi-Newton Method for Nonconvex Stochastic Optimization", "year": 2021}, {"authors": ["I.J. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"], "title": "arXiv:1406.2661 [stat.ML", "venue": "Generative Adversarial Networks", "year": 2014}, {"authors": ["K. Simonyan", "A. Zisserman"], "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition", "year": 2015}, {"authors": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "title": "Deep Residual Learning for Image Recognition.", "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "year": 2016}, {"authors": ["G. Huang", "Z. Liu", "L. Van Der Maaten", "K.Q. Weinberger"], "title": "Densely Connected Convolutional Networks.", "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "year": 2017}, {"authors": ["M.P. Marcus", "M.A. Marcinkiewicz", "B. Santorini"], "title": "Building a Large Annotated Corpus of English: The Penn Treebank.", "venue": "In: Comput. Linguist", "year": 1993}, {"authors": ["S. Merity", "C. Xiong", "J. Bradbury", "R. Socher"], "title": "Pointer Sentinel Mixture Models.", "venue": "CoRR abs/1609.07843", "year": 2016}, {"authors": ["S. Hochreiter", "J. Schmidhuber"], "title": "Long Short-Term Memory.", "venue": "Neural Computation", "year": 1997}, {"authors": ["M. Arjovsky", "S. Chintala", "L. Bottou.Wasserstein GAN"], "title": "arXiv:1701.07875 [stat.ML", "year": 2017}, {"authors": ["I. Gulrajani", "F. Ahmed", "M. Arjovsky", "V. Dumoulin", "A. Courville"], "title": "Improved Training of Wasserstein GANs", "year": 2017}, {"authors": ["T. Miyato", "T. Kataoka", "M. Koyama", "Y. Yoshida"], "title": "Spectral Normalization for Generative Adversarial Networks.", "year": 2018}, {"authors": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A. Graves", "I. Antonoglou", "D. Wierstra", "M. Riedmiller"], "title": "Playing Atari with Deep Reinforcement Learning", "year": 2013}, {"authors": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei-Fei"], "title": "Imagenet: A large-scale hierarchical image database.", "venue": "IEEE conference on computer vision and pattern recognition. Ieee", "year": 2009}, {"authors": ["J. Chen", "D. Zhou", "Y. Tang", "Z. Yang", "Y. Cao", "Q. Gu"], "title": "Closing the Generalization Gap of Adaptive Gradient Methods in Training Deep Neural Networks.", "year": 2020}, {"authors": ["H. Mittal", "K. Pandey", "Y. Kant"], "title": "ICLR Reproducibility Challenge Report (Padam : Closing The Generalization Gap Of Adaptive Gradient Methods in Training Deep Neural Networks)", "year": 2019}], "sections": [{"text": "R E S C I E N C E C Replication / ML Reproducibility Challenge 2021\n[Re] AdaBelief Optimizer: Adapting Stepsizes by the"}, {"heading": "Belief in Observed Gradients", "text": "Anirudh Buvanesh1,2, ID and Madhur Panwar1,2, ID 1Birla Institute of Technology and Science, Pilani (BITS Pilani), Pilani, India \u2013 2Equal contribution\nEdited by Koustuv Sinha,\nSharath Chandra Raparthy\nReviewed by Anonymous Reviewers\nReceived 04 February 2022\nPublished 23 May 2022\nDOI 10.5281/zenodo.6574643"}, {"heading": "Reproducibility Summary", "text": ""}, {"heading": "Scope of Reproducibility", "text": "The proposed optimizer: AdaBelief, claims to achieve three goals: fast convergence as in adaptive methods, good generalization as in SGD, and training stability. We perform experiments to validate the claims of the paper [1]."}, {"heading": "Methodology", "text": "To validate these claims, we reproduce experiments on Image Classificationwith CIFAR\u2010 10, CIFAR\u2010100 and ImageNet datasets, on Language Modeling with Penn Treebank, and on Generative Modeling with WGAN, WGAN\u2010GP and SN\u2010GAN architectures. We use the code provided by the author1. All experiments were performed on 8 NVIDIA V100 GPUs and took about 1096 GPU hours in total."}, {"heading": "Results", "text": "The image classification experiments on CIFAR\u201010, CIFAR\u2010100 and ImageNet are repro\u2010 duced to within 0.29%, 0.18% and 0.25% of reported values respectively. The language modeling experiments produce an average deviation of 0.22%,while the generativemod\u2010 eling experiments onWGAN,WGAN\u2010GP and SN\u2010GAN are replicated towithin 2.2%, 1.8% and 0.33% of value reported in the original paper. We perform ablation studies for change of dataset in language modeling and for effect of weight decay on ImageNet. We also perform analysis of generalization ability of op\u2010 timizers and of training stability of GANs. All of the results largely support the claims made in the paper [1]."}, {"heading": "What was easy", "text": "The authors provide implementation formost of the experiments presented in the paper. Well documented code and lucid paper helped understand the experiments clearly.\n1https://github.com/juntang\u2010zhuang/Adabelief\u2010Optimizer\nCopyright \u00a9 2022 A. Buvanesh and M. Panwar, released under a Creative Commons Attribution 4.0 International license. Correspondence should be addressed to Anirudh Buvanesh (anirudhb1102@gmail.com) The authors have declared that no competing interests exist. Code is available at https://github.com/anirudhb11/Adabelief-Optimizer-RC. \u2013 SWH swh:1:dir:53eeebe14e9d02d912fc3c58c375b5095e8db941. Open peer review is available at https://openreview.net/forum?id=B9gDnMmn0t.\nReScience C 8.2 (#9) \u2013 Buvanesh and Panwar 2022 1"}, {"heading": "What was difficult", "text": "The challenging aspects in our study were: (1) Grid search for optimal hyperparameters (HP) in caseswhereHPwere not provided or results did notmatch, (2) time and resource intensive experiments like ImageNet ( \u223c 22 hrs.) and SN\u2010GAN (\u223c 15 hrs.), (3) writing code to evaluate claims of the AdaBelief paper.\nCommunication with original authors We communicated with the author of the original paper, Juntang Zhuang, on multiple occasions for doubts related to hyperparameters and code, to which he promptly replied and helped us.\nReScience C 8.2 (#9) \u2013 Buvanesh and Panwar 2022 2\n1 Introduction\nOptimization is at the heart of machine learning. Training of neural networks aims to find the optimal solution (deepest valley on the loss surface) using gradient descent. The variation in method to traverse the loss landscape gives rise to different optimizers. Dis\u2010 covering different optimizers is an active area of research in machine learning. In this report, we reproduce and add on to the experimental analysis of an optimizer, AdaBelief [1], introduced in 2020 at NeurIPS conference. The proposed optimizer, AdaBelief, claims to outperform its counterparts on various real world deep learning tasks. As a part of the ML Reproducibility Challenge, we repli\u2010 cate all the experiments mentioned in the AdaBelief paper [1], comparing it with other optimizers, and also perform additional experiments to investigate the efficacy of Ad\u2010 aBelief.\n2 Details of Optimizers\nOptimizers are of two types: (1) accelerated Stochastic GradientDescent (SGD) family [2] that includes SGDwithmomentum [3] & Nestrov Accelerated Gradient (NAG) [4], and (2) adaptivemethods like Adam [5], RAdam [6], AdamW[7], RMSProp [8], Yogi [9], AdaBound [10], AdaBelief [1], MSVAG [11], Fromage [12], Apollo [13]. SGD [2] family uses the same learning rate for all parameters, whereas, adaptive meth\u2010 ods update their parameters as a function of gradients. While this has shown success in faster convergence due to a more streamlined trajectory, it has raised questions re\u2010 garding the generalization ability of adaptive methods. RMSProp [8] builds over SGD by penalizing updates in directions that have high gradients. The intuition behind this is to prevent drastic updates in particular directions. It does so by damping themagnitude of update by factor of exponential moving average (EMA) computed for squares of gra\u2010 dients. Adam [5] improves over RMSProp by introducing a momentum term that helps prevent over\u2010damping of step size as in case of RMSProp. RAdam [6] seeks to tackle the convergence problem of Adam by proposing to use a small learning rate during initial stages of training when variance is high, while AdamW [7] and MSVAG [11] address the generalization problem in Adam. AdamW does this by introducing a weight decay regu\u2010 larization term and MSVAG decomposes Adam as a sign update and magnitude scaling. Yogi [9] considers the effect of mini\u2010batch size and proposes an update equation that has shown to outperform Adam with very little hyperparameter tuning. AdaBelief [1] amplifies (or dampens) its updates by a factor proportional to the \u2019belief\u2019 in observed gradient i.e. square of difference between the observed gradient and EMA of the gra\u2010 dient. AdaBound [10] bridges the gap between SGD family and Adaptive methods by making use of an update that smoothly transitions from Adam to SGD. Fromage [12] takes a different path to optimization \u2010 it accounts for the network structure by loop\u2010 ing in weight matrices into the update equation. Apollo [13] takes a step forward from the aforementioned first order optimizers by approximating the Hessian via a diagonal matrix, keeping computations in\u2010line with first\u2010order schemes.\n3 Scope of reproducibility\nAdaBelief [1] claims to performs better than existing optimizers. To evaluate the validity of its claims, we investigate the following target questions:\n\u2022 Does AdaBelief perform better in comparison to other optimizers on real world tasks of image classification, language modeling, generative modeling and rein\u2010 forcement learning?\nReScience C 8.2 (#9) \u2013 Buvanesh and Panwar 2022 3\nS. No. Task Dataset Setup Rep. Status Our Contribution No. of Exp. GPU HPR Total GPU hours 1. Image\nClassification\nCIFAR\u2010 10 VGG, RN, DN\n\u2713 Exp. on Apollo [13]; bias\u2010variance anal. 30 2.5 75\n2. CIFAR\u2010 100 VGG, RN, DN\n\u2713 Exp. on Apollo [13]; bias\u2010variance anal. 30 2.5 75\n3. ImageNet ResNet18 \u2713 Analysis of weight decay 3 22 66 4. Language\nModeling PTB, WT2\nLSTM (1 layer)\n\u2713 Fromage LRS; WT2 11 1.33 14.63\n5. LSTM (2 layer)\n\u2713 AdamW & RAdam LRS; WT2 11 2.5 27.5\n6. LSTM (3 layer)\n\u2713 AdamW & RAdam LRS; WT2 11 3.75 41.25\n7. Generative Modeling CIFAR\u2010 10 WGAN \u2713 N/A (only reproduced paper\u2019s [1] exp.) 70 0.89 53.55 8. WGAN\u2010GP \u2713 N/A (only reproduced paper\u2019s [1] exp.) 70 1 66.5 9. SN\u2010GAN \u2713 HP search; training stablity anal. 45 15 675 10. Reinforcement\nLearning N/A Space\nInvaders (Atari)\n\u2713 Beyond AdaBelief paper [1] 2 1 2\ntrain\u2010test split of 50, 000 : 10, 000. (b) CIFAR\u2010100: It is same as CIFAR\u201010 but the images are grouped into 100 classes (600 images per class). (c) ImageNet [25]: We use ILSVRC 2012 dataset6 which consists of \u223c 1.35M images of size 256\u00d7 256 split into 1000 classes. Train\u2010val\u2010test split is 1, 281, 167 : 50, 000 : 100, 000. As part of pre\u2010processing we remove mis\u2010labelled data7 (d) Penn Treebank8 (PTB) [18]: The train\u2010val\u2010test split of tokens is 887, 521 : 70, 390 : 78, 669. (e) WikiText\u20102 (WT2) [19]: It is a subset of WikiText\u2010103, features a larger vocabulary and retains the punctuation, original case and numbers which are omitted in PTB dataset. We ran experiments on WT29 using the train\u2010val\u2010test token split of 2, 045, 059 : 213, 119 : 240, 498.\n4.3 Hyperparameters In this section we mention the HP used by optimizers in our experiments. Optimal values of commonly used HP are listed in Table 2. Below we mention the source of these values and details of HP search. For most experiments, we use the optimizer\u2010specific HP as mentioned in the original repository5 since searching the HP for all experiments is computationally infeasible. However, the repository does not mention the HP for SN\u2010GAN & Fromage, and the men\u2010 tioned HP for 2\u2010 and 3\u2010layer LSTM models for AdamW & RAdam resulted in large devi\u2010 ation. So, we perform learning rate (LR) search for Fromage and 2\u2010 & 3\u2010layer AdamW and RAdam over the interval [10\u22123, 10\u22122] (5 values). For SN\u2010GAN, we search \u03b21 (3 values in [0.4, 0.9]) and \u03f5 (3 values in [10\u221212, 10\u22126]). For Reinforcement Learning, we use LR of 10\u22124 and \u03f5 = 10\u221210 for AdaBelief and Adam, as mentioned on the RL repository4. Now we list the HP which are specific to each optimizer. The LR decays to 1/10th of its value at 150th epoch for image classification onCIFAR\u201010 andCIFAR\u2010100, and at epoch 70 & 80on ImageNet. AdaBelief usesweight_decouple=False, fixed_decay=False, rectify=False for all the experiments and weight_decouple=True on ImageNet. SGDusesmomentum=0.9, andApollouseswarmup=200, weight_decay_type=\u2019L2\u2019 for image classification on CIFAR\u201010 and CIFAR\u2010100. AdaBound uses final_lr=30 on PTB and final_lr=0.01 with GAN experiments.\n4.4 Computational requirements We run experiments on a Portable Batch System (PBS) managed cluster. We used 8 NVIDIA V100 GPUs and 384 GB RAM. All experiments except ImageNet use a single GPU.\n6ImageNet dataset (Kaggle) 7Blacklisted images (GitHub) 8Penn Treebank Dataset 9WikiText\u20102 dataset\nReScience C 8.2 (#9) \u2013 Buvanesh and Panwar 2022 5\nGPU runtime of all experiments are listed in table 1.\n5 Experiments and Results\n5.1 Experiments reproducing original paper To evaluate the performance of AdaBelief and to validate the aforesaid claims, we per\u2010 form experiments on various tasks like Image Classification, Language Modeling, Gen\u2010 erativeModeling, Reinforcement Learning and compare our results with those stated in the paper [1]. HP details can be found in Table 2.\nImage classification \u2014We run experiments on CIFAR\u201010 and CIFAR\u2010100 using VGG11 [15], Resnet34 [16] and DenseNet121 [17] architectures, performimg 3 independent runs on 9 optimizers10. Additionally, we perform experiments using Apollo optimizer [13], that has claimed to outperform AdaBelief on CIFAR datasets with ResNet110 architecture. Fig. 1 plots test accuracy results. Plots for train accuracies are reported in Fig. 9. All the obtained results agree with those reported in the AdaBelief paper [1]. To assess the performance on large scale datasets, we ran experiments on ImageNet [25]. We follow a similar setting as the author and run experiments on AdaBelief [1] and MSVAG [11] and report results for remaining optimizers from literature (Table 3). The top\u20101 accuracy lags by 0.32% and 0.18% respectively in case of AdaBelief and MSVAG. Other optimizers from literature use weight decay of 10\u22124 while the author performs experiments on AdaBelief using a value of 10\u22122. We analyse the effect of weight decay in section 6.2.\nLanguageModeling \u2014We ran experiments on Penn Treebank (PTB) dataset [18] using 1,2,3\u2010 layer LSTMmodels. We report test perplexities (ppl) (Fig. 6) for 3 independent runs on 9 optimizers10. Plots for train ppl are reported in Fig. 5. For Fromage, the author does not provide HP, hence we use grid search to find the optimal LR = 10\u22122. In case of 2 layer LSTM using AdamW & RAdam, we find that an LR = 10\u22123 gives a ppl of 73.78 & 74.05, while LR = 10\u22122 gives a ppl of 93.61 & 90.49 respectively. The author reports a ppl\u223c 73, \u223c 73.5 at LR = 10\u22122. Similarly, in 3\u2010layer LSTM, LR = 10\u22123 for AdamW and RAdam works better than LR = 10\u22122. PTB is a small dataset, so, we additionally experiment on WikiText\u20102 (section 6.1) for Adam and AdaBelief (top performers in case of PTB) on the setting reported here11.\nGenerative Modeling \u2014We run experiments on WGAN [21], WGAN\u2010GP [22] & SN\u2010GAN [23]. SN\u2010GAN makes use of a ResNet generator with spectral normalization in the discrimi\u2010 nator and is trained for 100,000 steps. Five independent runs on 9 optimizers12 are per\u2010 formed. We also perform these experiments using the Padam [27] optimizer on WGAN andWGAN\u2010GP. FID values for SN\u2010GAN and Padam (Table 4, 5). Fig. 4 shows the variation in FID during training, giving an idea of stability and convergence of different optimiz\u2010 ers. Boxplots of FID values corresponding tomultiple runs onWGAN andWGAN\u2010GP are\n10SGD, Adam, AdamW, AdaBelief, Yogi, MSVAG, RAdam, Fromage, AdaBound 11https://github.com/salesforce/awd\u2010lstm\u2010lm 12SGD, Adam, RMSProp, AdaBelief, Yogi, MSVAG, RAdam, Fromage, AdaBound\nReScience C 8.2 (#9) \u2013 Buvanesh and Panwar 2022 6\nshown in Fig. 3. Collages of generated images for all optimizers are reported in Fig. 11, 12, 13. (a) SN\u2010GAN: In case of Fromage [12] and MSVAG [11], we obtain \u223c4 and \u223c8 worse FID than what is reported, while for AdaBound [10] we obtain a \u223c40 better FID. We suspect the reason for this large deviation to be a difference in HP value being used. Since we performed a HP search for SN\u2010GAN, our HP (Table 2) are optimal. The results of re\u2010 maining optimizers were comparable to what was reported in the paper. (b) WGAN:We observe that AdaBelief outperforms other optimizers with a median FID of \u223c80 which agrees with reported value. We observe a significantly worse FID with Fromage. (c) WGAN\u2010GP: AdaBelief and AdaBound achieve comparable results\u223c67 FID which are bet\u2010 ter than the other optimizers. Fromage shows similar deviation like in WGAN. With Padam, we find that for both WGAN and WGAN\u2010GP, increasing the partial (p) i.e. mov\u2010 ing from SGD towards Adam, decreases the FID. The FIDs obtained are found to agree with or are marginally better than what was stated in the paper.\n5.2 Experiments beyond original paper\nRL toy \u2014 To investigate the efficacy of AdaBelief in use cases beyond text and images we train an agent to play Space Invaders (Atari Game). We report Q value and reward func\u2010 tion for Adam and AdaBelief in Fig. 14, 15. We compare our results with author\u2019s results from here13 and find that both results agree.\nImage Classification on CIFAR-10 and CIFAR-100 using Apollo \u2014 Apollo [13] is another optimizer that claims to achieve better convergence speed and generalization than SGD and vari\u2010\n13https://github.com/juntang\u2010zhuang/rainbow\u2010adabelief\nReScience C 8.2 (#9) \u2013 Buvanesh and Panwar 2022 7\nants of Adam. To investigate this, we experiment with Apollo on CIFAR\u201010 and CIFAR\u2010 100. Fig. 9, 10 show the train, test accuracies on VGG11, ResNet34 and DenseNet121 for the 3 independent runs. AdaBelief outperformsApollo in all settings exceptDenseNet121 on CIFAR\u2010100. It can also be seen that as we move from a simpler (VGG11) to a complex architecture (DenseNet121) the gap between Apollo and AdaBelief reduces. We made use of official implementation of Apollo in our experiments14.\nEvaluating GAN training stability \u2014 To assess stability of AdaBelief while training GANs, we look into difference between SN\u2010GAN\u2019s generator and discriminator training losses on CIFAR\u201010. We do this for AdaBelief, Adam and RMSProp (since they have top\u20102 FID scores on SN\u2010GAN) in the adaptive family, and with SGD for a comparison. Fig. 16 plots the generator and discriminator training losses. We observe that the adaptive methods aremore stable than SGD andwithin the adaptive family the order of stability frommost stable to least stable varies as RMSProp, AdaBelief, Adam.\nEvaluating generalization ability \u2014 To evaluate AdaBelief\u2019s ability to generalize, we analyze the bias and variance of image classificationmodels trained using SGD, Adam, AdaBelief and Apollo optimizers on CIFAR\u201010 and CIFAR\u2010100. We use the method outlined here [28] for bias\u2010variance analysis. For each optimizer, we note its train and test accuracy (Fig. 1) corresponding to the epoch with best test accuracy (acc), and compute their dif\u2010 ference. This data is stated as 3\u2010tuples in Table 6. Lower training acc denotes high bias and vice\u2010versa. The difference between the train and test acc is a measure of variance. Based on Table 6, we observe that AdaBelief models have the least bias on all configu\u2010 rations, while they have 2nd, 3rd or 4th lowest variance. SGD has the least variance on most configurations (highlighted in red), but their bias is high (mostly ranked 3rd or 4th in low bias)."}, {"heading": "Evaluating convergence speed \u2014", "text": "Definition 5.1 (Epoch of Convergence (EC)). Letmk denote the metric (acc or ppl) at kth epoch. EC is then defined as the smallest epoch x such that |my\u2212mx| < \u03b4 \u2200 y \u2208 [x, x+w], where w and \u03b4 are chosen as 15 and 0.05 respectively. In other words, EC is the smallest\n14https://github.com/XuezheMax/apollo\nReScience C 8.2 (#9) \u2013 Buvanesh and Panwar 2022 8\nepoch for which there exists at least w(= 15) epochs to its right with accuracies (or perplexities) within a fixed tolerance \u03b4(= 0.05). If such x cannot be found, the said optimizer is said to have failed to converge (FTC).\nTo address the claim on convergence ability different optimizers (section 3) we make use of Def. 5.1. We perform the analysis for Image Classification and Language Model\u2010 ing (section 5.1) experiments. We smoothen the accuracy (or perplexity) curves for all optimizers by finding the exponential moving average (EMA) with a smoothing factor \u03b2 = 0.7. Analyzing the computed ECs yield that the convergence speed of AdaBelief is comparable to other members of Adaptive family for experiments performed on CIFAR datasets (Fig. 1). For LanguageModeling experiments, we find that Adam and AdaBelief show similar convergence trends but considerably lag behind in comparison to RAdam, AdamW and Fromage (Fig. 6) that are unaffected by learning rate decay which takes place at 100th epoch. For exact EC values refer Table 7.\n6 Ablation studies\n6.1 WikiText-2 on LSTM To study the performance change due to a larger dataset, we ran Language Modeling experiments on WikiText\u20102 [19] using AdaBelief and Adam optimizers with 1, 2, 3 layer LSTMmodels. Fig. 7, 8 show train and test perplexity for 3 independent runs. It can be seen that the performance of Adam and AdaBelief is comparable on 1 and 2 layer LSTM models, while in the 3 layer case AdaBelief outperforms Adam by \u223c 5 ppl.\nReScience C 8.2 (#9) \u2013 Buvanesh and Panwar 2022 9\n6.2 Effect of weight decay on ImageNet\nThe paper [1] uses a weight decay of 10\u22122 while experimenting with AdaBelief on Ima\u2010 geNet. However, the results for other optimizers are from the literature that typically use a (smaller) weight decay of 10\u22124. To evaluate the effect of weight decay, we exper\u2010 iment with AdaBelief using weight decay = 10\u22124 and find \u223c 2% drop in top\u20101 accuracy. So, it may be interesting to see the effect of weight decay on other optimizers.\n7 Discussion\nWe now summarize the validity of claims from section 3: (a) Results in section 5.1 show that AdaBelief outperforms other optimizers in most use cases. (b) From section 5.2.5, we find that the convergence speed of AdaBelief is largely in line with adaptive meth\u2010 ods. (c) Based on the analysis in section 5.2.4, we infer that AdaBelief generalizes well, which is evident by its models having lowest bias and relatively low variance. However, it does not uniformly outperform SGD. Therefore,we fail to completely validate the abil\u2010 ity of AdaBelief generalizing as well as SGD. (d) Even though in section 5.2.3, the least difference between generator and discriminator loss is in case of RMSProp , AdaBelief does outperform other members of the adaptive family. It defeats SGD by a significant margin. Thus, we find that AdaBelief has stability comparable to adaptive methods in complex settings like GANs. What was easy The authors provide implementation for most of the experiments pre\u2010 sented in the paper. Well documented code and lucid paper helped understand the experiments clearly. What was difficult While hyperparameters (HP) of some experiments were absent (sec\u2010 tion 5.1.3), some had discrepancies (section 5.1.2). We had to perform grid search for these cases. Training SN\u2010GAN and ImageNet was a resource intensive process which increased the computational burden (Table 1). Formulating the analysis to evaluate the claims of the paper was also challenging 5.2. Communication with original authors We are thankful to the author Juntang Zhuang. He helped us with the implementation and HP details for various experiments. We con\u2010 firmed the HP for WGAN, SN\u2010GAN, and LSTM experiments. We also clarified the source of Penn Treebank dataset and blacklisting of images in ImageNet. Recommendations for reproducibility Given the time and resource constraints, we per\u2010 formed only a basic analysis of bias\u2010variance trade\u2010off to evaluate the generalization\nReScience C 8.2 (#9) \u2013 Buvanesh and Panwar 2022 10\nability of AdaBelief. A more advanced analysis might help in revealing the exact weak\u2010 ness of AdaBelief models in terms of ability to generalize. Based on our experiments, ablation studies and analysis, we find that AdaBelief is a promising optimizer combining the best of both worlds \u2010 accelerated and adaptive gra\u2010 dient methods."}, {"heading": "Appendices", "text": "A Experiments on language modeling\nA.1 Penn Treebank dataset We ran experiments using LSTM [20] models on Penn Treebank dataset [18] and plot train perplexities (Fig. 5) and test perplexities (Fig. 6) for 3 independent runs.\nA.2 WikiText-2 dataset We perform experiments on WikiText\u20102 dataset [19] using LSTM models with Adam [5] and AdaBelief [1] as optimizers. Train perplexities (Fig. 7) and test perplexities (Fig. 8) are reported for 3 independent runs.\nReScience C 8.2 (#9) \u2013 Buvanesh and Panwar 2022 13\n(a) VGG11 on Cifar10 (b) Resnet34 on Cifar10 (c) Densenet121 on Cifar10\n(d) VGG11 on Cifar100 (e) Resnet34 on Cifar100 (f) Densenet121 on Cifar100\nReScience C 8.2 (#9) \u2013 Buvanesh and Panwar 2022 16\nReScience C 8.2 (#9) \u2013 Buvanesh and Panwar 2022 17\nReScience C 8.2 (#9) \u2013 Buvanesh and Panwar 2022 18\nF Convergence Analysis\nF.1 Cifar10, Cifar100, LSTM To understand convergence abilites of different optimizers we make use of Def. 5.1. Table 7 shows the convergence epoch for the different optimizer for experiments per\u2010 formed onCifar10, Cifar100 usingVGG11, ResNet34, DenseNet as backbones and onPTB dataset trained using LSTMs.\nReScience C 8.2 (#9) \u2013 Buvanesh and Panwar 2022 19"}], "title": "[Re] AdaBelief Optimizer: Adapting Stepsizes by the Belief in Observed Gradients", "year": 2022}