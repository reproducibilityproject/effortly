{"abstractText": "The claims of the paper [1] are threefold: (1) Summers and Dinneen1 made the surpris\u2010 ing yet intriguing discovery that all sources of nondeterminism exhibit a similar degree of variability in themodel performance of a neural network throughout the training pro\u2010 cess. (2) To explain this fact, they have identified model instability during training as the key factor contributing to this phenomenon. (3) They have also proposed two ap\u2010 proaches (Accelerated Ensembling [2] and Test\u2010Time Data Augmentation [3]) to mitigate the impact on run\u2010to\u2010run variability without incurring additional training costs. In the paper [1], the experiments were performed on two types of datasets (image classification and language modelling). However, due to the intensive training and time required for each experiment, we will only consider image classification for testing all three claims.", "authors": [{"affiliations": [], "name": "Waqas Ahmed"}, {"affiliations": [], "name": "Sheeba Samuel"}, {"affiliations": [], "name": "Koustuv Sinha"}, {"affiliations": [], "name": "Sharath Chandra Raparthy"}], "id": "SP:325dbb07259592e237b33e4c4a0e4cf482b7a743", "references": [{"authors": ["C. Summers", "M.J. Dinneen"], "title": "Nondeterminism and Instability in Neural Network Optimization.", "venue": "Proceedings of the 38th International Conference on Machine Learning,", "year": 2021}, {"authors": ["Y.Wen", "D. Tran", "J. Ba"], "title": "Batchensemble: an alternative approach to efficient ensemble and lifelong learning.", "venue": "arXiv preprint arXiv:2002.06715", "year": 2020}, {"authors": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "title": "Going deeper with convolutions.", "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition", "year": 2015}, {"authors": ["D. Zhuang", "X. Zhang", "S.L. Song", "S. Hooker"], "title": "Randomness In Neural Network Training: Characterizing The Impact of Tooling.", "year": 2021}, {"authors": ["M. Hartley", "T.S. Olsson"], "title": "dtoolai: Reproducibility for deep learning.", "year": 2020}, {"authors": ["F. Renard", "S. Guedria", "N. De Palma", "andN. Vuillerme"], "title": "Variability and reproducibility in deep learning formedical image segmentation.", "venue": "Scientific Reports", "year": 2020}, {"authors": ["L. Heumos", "P. Ehmele", "K. Menden", "L.K. Cuellar", "E. Miller", "S. Lemke", "G. Gabernet", "S. Nahnsen"], "title": "mlf-core: a framework for deterministic machine learning.", "venue": "arXiv preprint arXiv:2104.07651", "year": 2021}, {"authors": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "title": "Deep Residual Learning for Image Recognition.", "venue": "IEEE Conference on Computer Vision and Pattern Recognition,", "year": 2016}, {"authors": ["A. Krizhevsky", "G. Hinton"], "title": "Learning multiple layers of features from tiny images.", "year": 2009}, {"authors": ["A. Paszke", "S. Gross", "F. Massa", "A. Lerer", "J. Bradbury", "G. Chanan", "T. Killeen", "Z. Lin", "N. Gimelshein", "L. Antiga"], "title": "Pytorch: An imperative style, high-performance deep learning library.", "venue": "Advances in neural information processing systems", "year": 2019}, {"authors": ["I. Loshchilov", "F. Hutter"], "title": "Sgdr: Stochastic gradient descent with warm restarts.", "year": 2016}, {"authors": ["S. Kornblith", "M. Norouzi", "H. Lee", "G. Hinton"], "title": "Similarity of neural network representations revisited.", "venue": "In: International Conference on Machine Learning. PMLR", "year": 2019}], "sections": [{"text": "R E S C I E N C E C Replication / ML Reproducibility Challenge 2021\n[Re] Nondeterminism and Instability in Neural Network"}, {"heading": "Optimization", "text": "Waqas Ahmed1, ID and Sheeba Samuel1, ID 1Friedrich Schiller University, Jena, Germany\nEdited by Koustuv Sinha,\nSharath Chandra Raparthy\nReviewed by Anonymous Reviewers\nReceived 04 February 2022\nPublished 23 May 2022\nDOI 10.5281/zenodo.6574623"}, {"heading": "Reproducibility Summary", "text": ""}, {"heading": "Scope of Reproducibility", "text": "The claims of the paper [1] are threefold: (1) Summers and Dinneen1 made the surpris\u2010 ing yet intriguing discovery that all sources of nondeterminism exhibit a similar degree of variability in themodel performance of a neural network throughout the training pro\u2010 cess. (2) To explain this fact, they have identified model instability during training as the key factor contributing to this phenomenon. (3) They have also proposed two ap\u2010 proaches (Accelerated Ensembling [2] and Test\u2010Time Data Augmentation [3]) to mitigate the impact on run\u2010to\u2010run variability without incurring additional training costs. In the paper [1], the experiments were performed on two types of datasets (image classification and language modelling). However, due to the intensive training and time required for each experiment, we will only consider image classification for testing all three claims."}, {"heading": "Methodology", "text": "Our approach to investigating the claimsmade in the paper [1] can be divided into three parts: (1) Replication: we used the publicly available code and adapted it to our ex\u2010 perimental environment with some modifications to replicate the results; (2) Ablation study: we tried to use different parameters, reducing the total implementation time to less than half compared to the original study, while keeping the central claim intact; (3) Generalization: we studied the authors\u2019 claim on a much more complex dataset and architecture to gain insights on the reproducibility of the conclusion. All experiments necessarily required extensive training, with a single experiment alone requiring 490 hours of 2 Nvidia Tesla V100 16GB (i.e., 700 trained models)."}, {"heading": "Results", "text": "With our tests and the obtained results, we confirm that all individual and combined sources of nondeterminism have similar effects on model variability and that instabil\u2010 ity in neural network optimization is the main reason for this phenomenon. However, our results show some discrepancies in the reduction of variability by test\u2010time data aug\u2010 mentation (TTA) and accelerated ensembling (claim 3 above). Like the original study, we show that these approaches successfully reduce variability, but the degree of reduction\nCopyright \u00a9 2022 W. Ahmed and S. Samuel, released under a Creative Commons Attribution 4.0 International license. Correspondence should be addressed to Waqas Ahmed (ahmed.waqas@uni-jena.de) The authors have declared that no competing interests exist. Code is available at https://github.com/wi25hoy/MLRC21_Nondeterminism. \u2013 SWH swh:1:dir:75ebcdb9cee4f17c5440b6ca9fd2c6901a929aea. Open peer review is available at https://openreview.net/forum?id=BNefkaG73At.\nReScience C 8.2 (#1) \u2013 Ahmed and Samuel 2022 1\nis reported as 61%, whereas our study reports 51% as the highest value. Despite some small differences, the third claim remains and we support it."}, {"heading": "What was easy", "text": "The authors havemade the source code publicly available in theGitLab repository. Even without extensive documentation, reimplementation of the experiments was straight\u2010 forward and required little effort. Moreover, the paper\u2019s clearly presented details signif\u2010 icantly reduced the effort required to set up the experimental configurations. The use of regular neural network training and widely used datasets was the icing on the cake to follow the implementation. This allowed us to explore other new aspects of themethod."}, {"heading": "What was difficult", "text": "Although the implementation was easy to comprehend and intuitive with the resources provided, the validation of some baselines proved to be computationally intensive and time\u2010consuming, requiringmultiple runs. In particular, the variability analysis required training 100models each for 500 epochs to verify the role of a single source of nondeter\u2010 minism. Nevertheless, we managed to maintain the original settings, but we could not run multiple iterations to gain more confidence in the results.\nCommunication with original authors At the beginning of our reproducibility study, we contacted the original authors once. The basic questions about the experimental settings were answered and the foundation for the rest of our experiments was laid. In addition, we also referred to their post and answers available on the OpenReview portal.\nReScience C 8.2 (#1) \u2013 Ahmed and Samuel 2022 2\n1 Introduction\nIn the pursuit of reproducibility in Deep Learning, a key criterion is the elimination of sources of nondeterminism in model optimization. Random initialization of weights is considered the main source of nondeterminism [4], but other sources such as random shuffling of training data [5], random data augmentation [6], and even GPU libraries such as cuDNN also contribute [7] . These random parameters tend to initialize with a random value every time we train the model, even if we use the same source code. On the one hand, such randomization helps to achieve sound performance, but on the other hand, it leads to run\u2010to\u2010run variability. This causes difficulties in verifying and improving baselines. To have complete experimental control, a better understanding of these random components is required, which is why each independent model is trained multiple times as a standard practice. While this can solve the problem, it is extremely costly in terms of computational resources and time. The authors\u2019 original work focused on quantifying the independent effect of each source of nondeterminism on model training. All different sources of nondeterminism were found to have similar effects on model variability. They also created an experimental protocol that used standard evaluation measures of model diversity and variability to capturemodel behaviour better. While developing a basicmechanismof understanding, they also discovered model instability as a major cause of run\u2010to\u2010run variability. To support this finding, experiments were conducted on image classification and language modeling datasets. In the end, two solutionswere proposed to reduce variabilitywithout additional costs.\n2 Scope of reproducibility\n1) First, we have attempted to replicate all three of the paper\u2019s claims:\n\u2022 Claim 1: All sources of nondeterminism have similar effects on model diversity and variability.\nThis claim seems to be a surprising discovery, as it could pave the way for researchers to improve the algorithm as a whole to reduce the effects of model variability, rather than focusing on each source of nondeterminism separately. In this reproducibility report, all sources of nondeterminism were tested individually and also in combination with other sources for ResNet\u201014 [8] on the CIFAR\u201010 dataset [9].\n\u2022 Claim 2: The key driver of this phenomenon \u201cAll sources of non\u2010determinism have similar effects on model diversity and variability\u201d is the instability of model opti\u2010 mization.\nModel optimization is said to have instability where small changes to the initial parame\u2010 ters lead to large changes to the final parameter values. Simply put, changing the initial\u2010 ization of a single weight by the smallest possible amount of 10\u221210 has the same effect as initializing all weights with completely random values. This study shows that any source of nondeterminism is susceptible to a change in weights by at least 10\u221210 and therefore produces the same amount of variability. This also illuminates the discovery of [4], which shows that removing a single source of nondeterminism is not sufficient to improve the stability of the training.\n\u2022 Claim 3: Accelerated ensembling and TTA are two possible solutions to reduce model variability without additional training costs (e.g., time).\nAs mentioned earlier, the standard practice to counter model variability is to train mod\u2010 els multiple times, which costs additional computational resources. This claim attracts\nReScience C 8.2 (#1) \u2013 Ahmed and Samuel 2022 3\nour attention because it could change current practices and facilitate the reproducibil\u2010 ity of experiments in the context of Deep Learning by promoting deterministic training without additional costs.\n2) Although the main objective of our study is to reproduce the main claims, we did not limit ourselves to these only. We have also conducted a series of experiments that go beyond the paper and follow two lines of investigation:\n\u2022 Ablation study: During the reproducibility study, one of the main difficulties we faced was the excessive amount of time required to conduct a single experiment. In Section 4.2, we attempted to address this issue by recommending changes to the default settings supported by our experimental results, while keeping the main claims intact.\n\u2022 Generalization to larger architectures and datasets: Because we were able to re\u2010 duce the experimental time, we tested the authors\u2019 claim on a larger architecture and dataset to verify that the claim still holds in general.\n3 Methodology\n3.1 Code The publicly available source code was provided by the original authors in the corre\u2010 sponding GitHub repository . It was written using the Pytorch [10] and NumPy libraries with Python 3.7.5. We used the same code and made some adjustments, such as setting thewidth of the output terminal and downloading the datasetsmanually. With thesemi\u2010 nor changes, we were able to run the code according to our experimental environment. The code provides a basic structure that allows numerous architectures to be used with few changes. However, additional code must be added for architectures larger than ResNet\u201018 [8]. The code also has command line functionality that allows the user to con\u2010 figure seed values and hyperparameter settings for a specific task. Although the code covers the entire implementation with the exact experimental settings described in the paper, a portion of the code is missing to visualize the results.\n3.2 Model descriptions We initially chose the ResNet\u201014 model because it was frequently used for image clas\u2010 sification experiments in the original study along with the CIFAR\u201010 datasets. Due to the large number of trained models required for a single experiment, we did not have time to work with other models except ResNet\u201034, which we created ourselves to test the generalization of the claim over larger model architectures. Figure 1 shows the ba\u2010 sic residual block of a ResNet architecture, consisting of two 3x3 convolutional layers followed by batch normalization before activation. On the same basis, Figure 3 shows the modular architecture of ResNet\u201014 and ResNet\u201034 [8] without the first 7x7 convolu\u2010 tional layer and the final fully\u2010connected layer. The blocks are shown in parentheses along with the number of output channels, with the multiplier indicating the number of residual blocks in that module.\n3.3 Datasets To investigate the effects of nondeterminism on image classification, the authors used CIFAR\u201010 as the primary training dataset to train the ResNet\u201014 model. Because the ar\u2010 chitectures used in the original study were smaller, the use of CIFAR\u2010100 was not seen. In comparison, we additionally used the CIFAR\u2010100 dataset to train and test the larger\nReScience C 8.2 (#1) \u2013 Ahmed and Samuel 2022 4\narchitecture (ResNet\u201034) in our extended study. It is worth noting that the authors used a runtime code to download the datasets using the torchvision library. Although this would have been the preferred choice, we encountered some issues due to the external access limitations of our experimental environment. Thus, we manually downloaded the dataset and specified the path, whichworkedwell for us. We used the authors\u2019 proto\u2010 col for the training and testing portions with 50k samples and 10k samples of CIFAR\u201010, respectively. No validation set was used. Table 1 shows the datasets used in our experi\u2010 mental tests.\n3.4 Hyperparameters In training the models, all standard parameters were used as given in the paper to more closely approximate the original approach. While not all values were mentioned in the paper, they could be easily found in the code itself. Allmodelswere trainedwith a cosine learning rate decay [11] with a maximum learning rate of 0.40 and 500 epochs. We also\nReScience C 8.2 (#1) \u2013 Ahmed and Samuel 2022 5\nused the first three epochs for warm\u2010up with linear learning rate, as was the case in the original study. Throughout the training, the SGD optimizer was used with a batch size of 512, a momentum of 0.9, and a weight decay of 5.10\u22124.\n3.5 Evaluation for the effects of nondeterminism To understand the impact of each source of nondeterminism, the original study devel\u2010 oped a protocol related to performance variability and model diversity representation.\nPerformance variability: All random sources are controlled by seeding values. To test a single source, all other sources are assigned the constant deterministic seeding value of 1, except for the source that is under observation and assumes different seeding val\u2010 ues from 1 to the total number of training runs. For the sources that cannot be seeded, such as cuDNN, the range is limited to 0 to 1, with 0 and 1 indicating the random and deterministic values, respectively. In the original settings, the total number of training runs is set to 100, so for each source of nondeterminism, 100 models can be trained. If we assume 4 different sources, each can be represented as S1, S2, S3, and S4, where S denotes the seed values. For example, S1 denotes the seed for random parameter ini\u2010 tialization, S2 for training data shuffling, S3 for data augmentation, and S4 for cuDNN. If we set (S1 = 1, S2 = 1, S3 = R,S4 = 1), whereR denotes the range from 1 to the total number of training runs, we would analyse the effect of data augmentation as a random source. If we want to analyse the effects of multiple sources simultaneously, we would also assign the R values to other sources. Finally, to work with performance variability, we consider the standard deviation of accuracy and cross entropy across all 100 trained models for each source.\nRepresentation diversity: In addition to performance variability, the authors also con\u2010 sidered the representation of the trained models. This allows us to determine the differ\u2010 ence in the representation of the trained models even when their performance variabil\u2010 ity is the same. In doing so, they used four different metrics that we followed. Of these, we did not find an implementation for the Centered Kernel Alignment (CKA) evaluation metric [12], which is considered the most advanced evaluation metric for determining similarities between models. The first metric is the simplest, which uses the average disagreement between pairs of 100 models. Second, they used the average correlation between the models\u2019 predictions. Finally, they examined the change in performance when two models are ensembled from the same source of nondeterminism.\n3.6 Extended experiments With the extended experiments, we have tried to eliminate the difficulties we encoun\u2010 tered in replicating the experiments to verify the paper\u2019s claim. One of our main con\u2010 cerns was the time required for each experiment. Therefore, we performed a series of experiments beyond the original work to satisfy the claims without being computation\u2010 ally intensive.\nModels v/s model variability: In this experiment, we examine the actual number of models required to test the variability analysis for each source of nondeterminism,which can reflect the same conclusion as the original settings while reducing the overall com\u2010 putational cost. Due to time constraints, we chose to work with only two sources. We varied the number of models for each source and observed the results using the same evaluation measures.\nReScience C 8.2 (#1) \u2013 Ahmed and Samuel 2022 6\nEpochs v/s model variability: Another factor contributing to the long training time is the use of a large number of epochs. In this experiment, we investigate the effects of a different number of epochs on the variability of the model, and therefore try to obtain similar results with a smaller number of epochs.\n3.7 Experimental setup and computational requirements To achieve similar results as in the original study, we strictly follow the same experi\u2010 mental environment and use Pytorch as frameworkwith Python = 3.7.5, NumPy = 1.17.4, Torch = 1.3.1 and Torchvision = 0.4.2. All experiments were performed on the HPC clus\u2010 ter ARA using the SLURMworkload manager at Friedrich Schiller University Jena. This system consists of multicore nodes for high computational performance and therefore offers a variety of GPU systems that can be used. According to our needs, we chose to workwith 2NVIDIATeslaV100GPUs equippedwith 24 core Intel CPUand 128GBofRAM. Table 2 shows the number of experiments performed and the time needed for them. It is worth mentioning that the evaluation part of all experiments involves the technique of TTA, which must also be performed on a GPU, so the time needed for the evaluation part is also added. The first three experiments belong to each of the three claims and the others are part of our extended study, which is not included in the original studies.\n4 Results\nIn this sectionwe present our experimental results by replicating all three claims and go\u2010 ing beyond the paper [1]. First, we start with the core replication by following exactly in the footsteps of the original authors and producing all three experiments for each claim. Second, we ran threemore experiments that help us obtain the same resultsmuch faster and generalize the first two claims across different datasets and architectures.\n4.1 Core replication results Effects of nondeterminism sources: Table 3 shows the result of our replication study for claim 1 with some minor differences as reported in [1]. In addition to all sources of nondeterminism, we performed additional deterministic training to verify that we have complete control over all sources and no other effects of randomness are observed during the training process unless otherwise noted. We trained all 100 models for each source, with 100 different seed values, as in the original work. We obtained almost the same results when we analysed the effect of each source separately. However, when the combination of multiple sources was tested, we found only a few anomalies (marked with red colour in Table 3), which ultimately appear to be negligible. Therefore, we support the claim that all sources of nondeterminism have similar effects on model variability and diversity.\nReScience C 8.2 (#1) \u2013 Ahmed and Samuel 2022 7\nThe effect of instability: Table 4 shows the result of the second claim. Note that the second claim states that instability in neural network optimization is the key factor for similar effects of nondeterminism sources on model variability. To observe the effects of instability, deterministic training was performed with a small change of 1 bit (5 \u00d7 10\u221210) in a single randomweight for 100models. Our results show that this 1\u2010bit change generates about as much variability as any other source of nondeterminism. Therefore, this claim can be considered confirmed by our experiment.\nReduction of variability with proposedmethods: In Table 6 we show the results of our replication study for the third claim. Unlike the other two claims, our study this time shows numerous differences in the results compared to the original study. The TTA and accelerated ensembling methods were used to reduce model variability, and the values were compared to the result obtained by combining all sources of nondeterminism pre\u2010 sented as a \u201dSingle Model\u201d. First, we found a computational error in the percentage of variability reduction in the original paper itself, which was taken as the average value for all 5 metrics compared to the single model. This measure would allow us to see the effectiveness of these two probable solutions. Although the mathematical formula is not given in the study, it seems intuitive to work with average values. Therefore, we performed a calculation of the baseline averages and found differences in the overall reduction percentages. For all reduction percentages, the values on paper appear to be about 20% higher than the calculated values. This is highlighted in red, as shown in Table 5. Second, in addition to the calculation error, we found other minor anomalies that ac\u2010 count for more than 10% change, as shown in Table 6 (highlighted in red). Including all minor differences, we have again shown that TTA and accelerated ensembling can be used to reduce variability. However, the highest possible percent reduction was re\u2010 duced from 61% to 51% compared to the original study. When compared to the recal\u2010 culated values, a slight increase in the percentage is observed. Moreover, the different types of TTA alone seem to cause an equal reduction in performance variability, while the change is mainly visible in model diversity. Thus, the overall variability is reduced. Despite these differences, the third claim remains and we therefore support it..\n4.2 Additional results not present in the original paper In replicating all three claims, we faced the major problem of the time required to pro\u2010 duce the results. While it is understandable that the nature of the problem requires\nReScience C 8.2 (#1) \u2013 Ahmed and Samuel 2022 8\nmultiple model training, it is still not clear to us why the author used 100 trained mod\u2010 els with 500 epochs each as default settings for each source of nondeterminism. Since these two parameters play an important role in the time required for an experiment, we decided to explore this area with the goal of reducing the training time while maintain\u2010 ing all the claims. This will allow the scientific community to test the claims on larger architectures and datasets with less time consumption.\nNo. of models v/s Variability: In this experiment, we used the original settings, except for the number of trained models considered for the variability analyses. By changing the number of trained models for nondeterminism, we observed the change in model variability. Due to lack of time, we experiment with only 3 different sources of nonde\u2010 terminism. We found that the result is not significantly different from the number of models, except for the error bars associated with standard deviation of accuracy and cross entropy. While all important metrics remain the same, it can be observed that these error bars decrease as the total number of models increases, as can be seen in Figure 3. All sources tested in Table 7 show the same trend.\nReScience C 8.2 (#1) \u2013 Ahmed and Samuel 2022 9\nEpochs v/s Variability: Since we found that a smaller number of trained models is less likely to affect the result in terms of model variability, we kept the number of trained models constant at 25. In addition to the original settings, we changed the number of epochs from 100 to 500 to see its impact on model variability. It can be seen that chang\u2010 ing the epochs does not affect the performance variability, but the model diversity. The values for pairwise correlation and change in two models ensembling indicate greater model diversity as the number of epochs increases. The same trend can be observed in both sources, as shown in Table 8.\nGeneralization: In this section, we examine the first two claims of the paper on a larger scale in terms of architecture and dataset. So far, these claims have shown no difference in results and have only been tested with ResNet\u201018 representing the largest architec\u2010 ture in [1]. For this reason, we went a step further and conducted experiments to test the generalization of nondeterminism and instability to CIFAR\u2010100 (dataset) and ResNet\u2010 34 (model architecture). We conducted the experiment to obtain the 25 trained models with 200 epochs each for the sources of nondeterminism listed in Table 9. We obtained an average accuracy of 63%for the CIFAR\u2010100 test dataset, however the goal is to observe the changes in model variability and its metrics. We have found that the two different sources of nondeterminism produce roughly the same variability. The relative variabil\u2010 ity of instability with \u201dRandom Bit Change\u201d also shows a similar result. However, the significant change in values of thesemetrics are observed about three times higher than the experiments performed with CIFAR\u201010 and ResNet\u201014. Even though this difference is due to the lower accuracy of the test results, the two main claims still hold.\nReScience C 8.2 (#1) \u2013 Ahmed and Samuel 2022 10\n5 Discussion\nOur results in section 4 fully support the first two assertions regarding the effects of \u201dnon\u2010determinism\u201d and its identified cause \u201dinstability\u201d. To gain sufficient confidence in the result, we also tested these claims on larger architectures and datasets, which also confirms the results of the study. But, when conducting experiments with accel\u2010 erated ensembling and TTA as a solution to reduce variability, some differences were found. Our results show that both approaches can reduce variability. However, the ex\u2010 tent to which they reduce variability is presented higher in the original study and lacks concrete numbers. In addition, we did not find in the paper a mathematical formu\u2010 lation for the average percentage of variability reduction that could have avoided this discrepancy. However, this is not sufficient to refute the claim. Therefore, we also sup\u2010 port the third claim. Moreover, the discovery that all sources of nondeterminism have similar effects on model variability is novel in itself and opens many interesting areas of research toward reproducibility of deep neural networks.\nStrengths and weaknesses: One of our strengths in the reproducibility study was that we stuck to the original implementation by using the publicly provided code and were able to create the experimental environment with exact hardware and software speci\u2010 fications. This allowed us to obtain similar results that confirmed the paper\u2019s claims. Another strength of our work was to perform some additional experiments that helped us reduce the overall computation time, allowing experiments with a larger architec\u2010 ture and dataset to be completed on time. The weakness of our approach is that in the limited time available for the reproducibility study, we could not test the claims about different combinations of hyperparameters, since 100 of trainedmodelsmust be seeded for each experiment, whereas training a single model takes about 40minutes."}], "title": "[Re] Nondeterminism and Instability in Neural Network Optimization", "year": 2022}