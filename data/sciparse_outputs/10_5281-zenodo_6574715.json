{"abstractText": "The original author\u2019s code contained the code necessary to train both GNNs and ex\u2010 plainer models from scratch. However, some alterations made by us were necessary to be able to use it. To validate the authors\u2019 claims, the trained RCExplainer model is compared with other explainer models in terms of fidelity, robustness and efficiency. We extended the work by investigating the generalisation to the image domain and ver\u2010 ified the authors\u2019 implementation.", "authors": [{"affiliations": [], "name": "Romana Isabelle Wilschut"}, {"affiliations": [], "name": "Thomas Paul Alexandre Wiggers"}, {"affiliations": [], "name": "Roman Sebastiaan Oort"}, {"affiliations": [], "name": "Thomas Arthur van Orden"}, {"affiliations": [], "name": "Koustuv Sinha"}, {"affiliations": [], "name": "Sharath Chandra Raparthy"}], "id": "SP:453962f8a8b6f79ffdacb89e853da4842be6b1b0", "references": [{"authors": ["M. Bajaj", "L. Chu", "Z.Y. Xue", "J. Pei", "L. Wang", "P.C.-H. Lam", "Y. Zhang"], "title": "Robust Counterfactual Explanations on Graph Neural Networks.", "venue": "Advances in Neural Information Processing Systems", "year": 2021}, {"authors": ["T.N. Kipf", "M. Welling"], "title": "Semi-Supervised Classification with Graph Convolutional Networks.", "venue": "CoRR abs/1609.02907", "year": 2016}, {"authors": ["J. Zhou", "G. Cui", "S. Hu", "Z. Zhang", "C. Yang", "Z. Liu", "L. Wang", "C. Li", "M. Sun"], "title": "Graph neural networks: A review of methods and applications.", "venue": "In: AI Open", "year": 2020}, {"authors": ["P. Velickovic", "G. Cucurull", "A. Casanova", "A. Romero", "P. Li\u00f2", "Y. Bengio"], "title": "Graph Attention Networks.", "venue": "International Conference on Learning Representations,", "year": 2018}, {"authors": ["Z. Ying", "D. Bourgeois", "J. You", "M. Zitnik", "J. Leskovec"], "title": "GNNExplainer: Generating Explanations for Graph Neural Networks.", "venue": "Advances in Neural Information Processing Systems", "year": 2019}, {"authors": ["P.E. Pope", "S. Kolouri", "M. Rostami", "C.E. Martin", "H. Hoffmann"], "title": "Explainability Methods for Graph Convolutional Neural Networks.", "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition", "year": 2019}, {"authors": ["D. Luo", "W. Cheng", "D. Xu", "W. Yu", "B. Zong", "H. Chen", "X. Zhang"], "title": "Parameterized Explainer for GraphNeural Network", "year": 2020}, {"authors": ["J. Adebayo", "J. Gilmer", "M. Muelly", "I. Goodfellow", "M. Hardt", "B. Kim"], "title": "Sanity Checks for Saliency Maps.", "venue": "Advances in Neural Information Processing Systems", "year": 2018}, {"authors": ["G.F. Montufar", "R. Pascanu", "K. Cho", "Y. Bengio"], "title": "On the Number of Linear Regions of Deep Neural Networks.", "venue": "Advances in Neural Information Processing Systems", "year": 2014}, {"authors": ["J. Kazius", "R. McGuire", "R. Bursi"], "title": "Derivation and Validation of Toxicophores for Mutagenicity Prediction.", "venue": "Journal of Medicinal Chemistry", "year": 2005}, {"authors": ["N. Wale", "I. Watson", "G. Karypis"], "title": "Comparison of Descriptor Spaces for Chemical Compound Retrieval and Classification.", "venue": "In: Knowl. Inf. Syst", "year": 2008}, {"authors": ["F. Monti", "D. Boscaini", "J. Masci", "E. Rodol\u00e0", "J. Svoboda", "M.M. Bronstein"], "title": "Geometric Deep Learning on Graphs and Manifolds Using Mixture Model CNNs.", "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "year": 2017}, {"authors": ["Y. LeCun", "C. Cortes", "C. Burges. \u201cMNIST handwritten digit database.\u201d In"], "title": "ATT Labs [Online", "venue": "Available: http://yann.lecun.com/exdb/mnist 2", "year": 2010}, {"authors": ["Ren", "Malik"], "title": "Learning a classification model for segmentation.", "venue": "Proceedings Ninth IEEE International Conference on Computer Vision", "year": 2003}, {"authors": ["V.P. Dwivedi", "C.K. Joshi", "T. Laurent", "Y. Bengio", "X. Bresson"], "title": "Benchmarking Graph Neural Networks", "venue": "Wilschut et al. 2022 11 [Re] Robust Counterfactual Explanations on Graph Neural Networks", "year": 2020}], "sections": [{"text": "R E S C I E N C E C Replication / ML Reproducibility Challenge 2021\n[Re] Robust Counterfactual Explanations on Graph"}, {"heading": "Neural Networks", "text": "Romana Isabelle Wilschut1,2, ID , Thomas Paul Alexandre Wiggers1,2, ID , Roman Sebastiaan Oort1,2, ID , and Thomas Arthur van Orden1,2, ID 1University of Amsterdam, Amsterdam, Noord-Holland, NL \u2013 2Equal contribution\nEdited by Koustuv Sinha,\nSharath Chandra Raparthy\nReviewed by Anonymous Reviewers\nReceived 04 February 2022\nPublished 23 May 2022\nDOI 10.5281/zenodo.6574715\n1 Reproducibility Summary"}, {"heading": "Scope of Reproducibility", "text": "The aim of this paper is to reproduce the claimsmade in the paper Robust Counterfactual Explanations on Graph Neural Networks [1]. The authors claim to have developed a novel method for explaining Graph Neural Networks (GNNs) which outperforms the existing explainer methods in three different ways, by being (1) more counterfactual, (2) more robust to noise and (3) efficient in terms of time."}, {"heading": "Methodology", "text": "The original author\u2019s code contained the code necessary to train both GNNs and ex\u2010 plainer models from scratch. However, some alterations made by us were necessary to be able to use it. To validate the authors\u2019 claims, the trained RCExplainer model is compared with other explainer models in terms of fidelity, robustness and efficiency. We extended the work by investigating the generalisation to the image domain and ver\u2010 ified the authors\u2019 implementation."}, {"heading": "Results", "text": "For the validation of the original paper, we compare the pre\u2010trained model and the re\u2010 trained model to the results reported in the original paper. The retrained RCExplainer outperformed the other methods on fidelity and robustness, which corresponds with the results of the original authors. The measured efficiency of the method also corre\u2010 sponds to the original result. To extend the paper, this comparison is also performed using a train\u2010test split, which showed no significant difference. The implementation of the metric is investigated and concerns are raised. Finally, the method generalises well to MNISTSuperpixels in terms of fidelity, but lacks in robustness."}, {"heading": "What was easy", "text": "The original paper described their metrics for comparing multiple explainer models clearly, which made it easier to reproduce. Moreover, a codebase was available which included a pre\u2010trained explainer model and files for training the other models. Because\nCopyright \u00a9 2022 R.I. Wilschut et al., released under a Creative Commons Attribution 4.0 International license. Correspondence should be addressed to Romana Isabelle Wilschut (romana.wilschut@student.uva.nl) The authors have declared that no competing interests exist. Code is available at https://github.com/RomanOort/FACTAI. \u2013 SWH swh:1:dir:5744faf1da0fd9b520c8aa72a0608b78f0a91e7a. Open peer review is available at https://openreview.net/forum?id=HWNgihGX20Y.\nReScience C 8.2 (#45) \u2013 Wilschut et al. 2022 1\nof this, we could easily find the reason for differences between our results and those of the paper."}, {"heading": "What was difficult", "text": "The most difficult part of the reproduction study was determining the functionality of the provided codebase. The original authors did provide a general README file that in\u2010 cluded instructions for all code parts. However, using these provided instructions, we were not able to run this code without changes. As the provided codebase was very ex\u2010 tensive, it was difficult to understand and determine how the different modules worked together.\nCommunication with original authors We found it not necessary to contact the original authors for this reproduction study.\nReScience C 8.2 (#45) \u2013 Wilschut et al. 2022 2\n2 Introduction\nGraphNeural Networks (GNNs) [2] are a recent development in the field of deep learning, aiming to exploit structural information by representing the input data as graphs. By passingmessages along the nodes of the input graphs, these networks can use the struc\u2010 tured nature of these graphs to reason on them. This allows GNNs to achieve ground\u2010 breaking results in a variety of fields such as the modelling of physics systems or molec\u2010 ular analysis [3]. However, GNNs are similar to conventional neural networks (NNs) and can therefore similarly be considered a black box. Hence, they do not always provide a sufficient explanation for their outcome. Nevertheless, such an explanation might be useful in some applications. An explanation, as presented in [1], is simply a subset of edges of the input graph. The authors of [1], to whom we will refer as the original authors from this point on, consider an explanation to be counterfactual if the prediction on the input graph changes significantly when the edges in the explanation are removed from the input graph. Several methods to explain the reasoning of GNNs have already been proposed [4, 5, 6, 7]. However, these models fall short in that their generated explanations are neither counterfactualnor robust to noise. These features are important for amodel because they make the explanations concise, easy to understand for humans and more trustworthy [1]. The original authors propose the RCExplainer model [1], which meets both criteria, and claim it is capable of outperforming existing explainer models, on the task of graph classification, while also being at least as time\u2010efficient.\n3 Scope of reproducibility\nWith this paper, we aim to validate the original authors\u2019 claims, their experimental setup, and investigate the application of theirmethod to another domain. Our code1 is publicly available and builds upon the code2 of [1]. The original authors tested the RCExplainer model on three different datasets, however, due to long training times, we employed only one of these three. This reproduction paper aims to validate the following claims as made by the original authors:\n\u2022 The RCExplainer model produces superior counterfactual explanations in com\u2010 parison to previous methods based on fidelity scores for all levels of sparsity.\n\u2022 The RCExplainer model is more robust to noise than competitive methods based on ROC AUC score.\n\u2022 The RCExplainermodel is at least as efficient in terms of inference time as existing explainer models.\nMoreover, we conduct a set of additional experiments to inspect the following exten\u2010 sions to the original paper:\n\u2022 Split the dataset into a proper train test split, that is no overlap between those sets, for training the explainer model and validating the effect on its performance in terms of fidelity and ROC AUC scores.\n\u2022 Apply the RCExplainermethod to the task of image classification using theMNIST\u2010 Superpixels dataset.\n\u2022 Calculate the ROC AUC scores in two additional ways. 1Our source code is located at https://github.com/RomanOort/FACTAI. 2Theoriginal authors\u2019 code is available at https://marketplace.huaweicloud.com/markets/aihub/notebook/detail/?id=e41f63d3\u2010\ne346\u20104891\u2010bf6a\u201040e64b4a3278.\nReScience C 8.2 (#45) \u2013 Wilschut et al. 2022 3\nThe next section will discuss the method of [1] in more detail and introduce our addi\u2010 tional experiments. Section 5 reports the results to validate the original authors\u2019 claims as well as the results of our extensions. Finally, Section 6 reflects on our work and con\u2010 cludes that we were able to partly reproduce the original paper.\n4 Methodology\n4.1 Model description The original authors propose a method consisting of two steps. First, the common de\u2010 cision logic of a GNN is extracted based on a set of linear decision boundaries (LDBs). This set comes from aGNN that is trained for graph classification. Second, the explainer model, based on the set of LDBs, which is a simple neural network, is trained to generate counterfactual explanations.\nGraph neural network The graph neural network, denoted by \u03d5, is trained to classify input graphs. This model consists of an arbitrary number of graph convolutional layers, which produce an embedding vector, and a fully connected head. This head predicts the class probabilities from the embeddings.\nExplanation network The explanation model, denoted by \u03d5\u03b8, is trained using the em\u2010 bedding vectors as produced by the GNN. The network consists of two linear layers with ReLU activations.\nLinear Decision Boundaries \u2014 The architecture of the classification GNN, \u03d5, can be divided into two distinct parts: the graph convolutional layers, denoted by \u03d5gc, and the fully connected layers, denoted by \u03d5fc. The RCExplainer model proposed by the original authors works by partitioning the output space of the graph convolutional layers into a set of decision regions, one for each class of the dataset. Given that the GNN uses piecewise linear activations on the neurons [8], its decision regions can be modelled by a set of linear decision boundaries (LDBs), the combination of which forms a convex polytope. As the total number of LDBs of a GNN grows exponentially with respect to the number of neurons [9], it is intractable to compute all the LDBs of a model. However, an LDB can be written as a linear equation of the form wT x + b = 0, where the basis w and the bias b can be computed with the following equations:\nw = \u2202 (max1(\u03d5fc(\u03b1))\u2212max2(\u03d5fc(\u03b1)))\n\u2202\u03b1 , (1)\nb = max1(\u03d5fc(\u03b1))\u2212max2(\u03d5fc(\u03b1))\u2212wT\u03b1, (2)\nwhere \u03b1 = \u03d5gc(G), so the embedding of the graph G in the output space of the graph convolutional layers, and the max1 and max2 operations take the highest and second\u2010 highest value of the input respectively. The original authors, therefore, propose to uni\u2010 formly sample a random subset of input graphs and extract their respective LDB, in order to circumvent the complexity of computing all LDBs, giving a subset of decision boundaries H\u0303 \u2282 H. The set of LDBs forming a decision region for a specific class is then chosen to cover the maximum amount of graphs belonging to that class while ensuring that this region covers as few graphs of other classes as possible. The set of LDBs H\u0303c that forms the decision regions of a class c is determined by iteratively applying the following rule:\nh = min h\u2208H\u0303\\H\u0303c g(H\u0303c, c)\u2212 g(H\u0303c \u222a {h}, c) + \u03b5 k(H\u0303c, c)\u2212 k(H\u0303c \u222a {h}, c) , (3)\nReScience C 8.2 (#45) \u2013 Wilschut et al. 2022 4\nwhere g(H\u0303c, c) is the total number of graphs belonging to class c that are covered by the LDBs in H\u0303c, k(H\u0303c, c) is the total number of graphs not belonging to class c that are covered by H\u0303c, and \u03b5 is a small noise term that ensures the best LDB is chosen, even when the numerator equals zero. This rule is applied until H\u0303c covers all graphs of class c, and then repeat this process for every class.\nExplanations \u2014Having extracted a decision region for each class, the original authors use this to generate an explanation S for each graph G, where S consists of a subset of the edges in G. This explanation is generated through the fully connected neural network \u03d5\u03b8, parameterized by \u03b8. This model takes the node embeddings of nodes i and j gener\u2010 ated by \u03d5gc, and returns the probability that an edge between these two nodes is part of G\u2019s explanation. Over all node pairs, this forms the matrix M, where each entry is the probability of the corresponding edge in the adjacency matrix belonging to S, which is then chosen to be the set of all edges with a value greater than 0.5 inM. The goal during training is to train a model such that the prediction of the GNN on the explanation is consistent with the prediction on the original graph, such that \u03d5(S) = \u03d5(G). Furthermore, the original authors want to ensure that removing the edges in S from G changes the prediction on G significantly, such that \u03d5(G\\S) \u0338= \u03d5(G). In order to satisfy these goals, the original authors define the following loss function:\nL(\u03b8) = \u2211 G\u2208D (\u03bbLsame(\u03b8,G) + (1\u2212 \u03bb)Lopp(\u03b8,G) + \u03b2Rsparse(\u03b8,G) + \u00b5Rdiscrete(\u03b8,G)) (4)\nwhereLsame is a term ensuring that the explanation ofGhas the same classification asG itself, Lopp ensures that removing S fromG changesG\u2019s classification, the combination of these terms ensuring that the explanations are counterfactual. Furthermore,Rsparse is a simple L1\u2010regularization overM, ensuring only a small amount of edges is selected to be part of the explanation byminimizing this term, andRdiscrete is a term that pushed the values in M closer to either 0 or 1 to more closely resemble an actual adjacency matrix.\n4.2 Datasets The original paper evaluates the model on three different datasets: Mutagenicity [10], BA\u20102motifs [7], and NCl1 [11]. Due to time constraints, our reproducibility paper only at\u2010 tempts to reproduce the results on theMutagenicity dataset. TheMutagenicity dataset is a binary dataset containing over 4000molecules of different sizes represented as graphs (see Table 1), with a target stating whether these molecules are mutagenic or not. Be\u2010 sides the Mutagenicity dataset, we also employed the MNISTSuperpixels dataset [12], containing 60, 000 graphs, in order to evaluate the RCExplainer model on a task in a dif\u2010 ferent field. These graphs are obtained from the MNISTSuperpixels dataset [13], which contains images of handwritten digits, and are based on the images that are segmented using a superpixel segmentation [14]. This decreases the size of the graphs, by reducing the image from 28\u00d728 pixels to 75 superpixels. Furthermore, where the graph represen\u2010 tation of a standard image would be a regular grid, where each pixel is only connected to its direct neighbours, which is identical for each image, the superpixel representa\u2010 tion introduces irregularity between the different images, as the segmentation of each image is different ensuring each image has a different graph.\n4.3 Experimental setup and code This section is split into two parts: the experiments concerning the validation of the claims made by the original paper\u2019s authors, and the experiments which validate our aforementioned extensions.\nReScience C 8.2 (#45) \u2013 Wilschut et al. 2022 5\nReproducibility \u2014 First, the original authors train a GNN from scratch on the classifica\u2010 tion task. This GNN is then used to obtain the predictions and node embeddings of the input graphs. These embeddings and predictions are used to train the RCExplainer model as described in Section 4.1. Subsequently, the trained RCExplainer model is com\u2010 pared with other explainer models in terms of fidelity, robustness and efficiency (see Section 5.1). Due to long training times, we chose to compare the RCExplainer only to the RCExp\u2010NoLDB [1] and PGExplainer models [7], all trained from scratch on 10 differ\u2010 ent seeds using the hyperparameters mentioned in the original paper. The GNN used as the prediction model is the pre\u2010trained GNN provided alongside the codebase, with 3 graph convolutional layers. Moreover, the original paper uses the entirety of the Mutagenicity dataset for training the GNN, but for training the explainer network only 1742 samples are used. We follow this same setup in our experiments. However, the original authors only mention an 80/10/10% train\u2010val\u2010test split for training the GNN, but no specific split for training the explanation networks. After inspecting the codebase, we observed that the training set is always a subset of the test set and, therefore, it appears that the data used for the evaluation of the RCExplainer is not entirely unseen by the model. Consequently, we decided to also evaluate all models using a train\u2010test split of 80/20%, which is a more common split used in artificial intelligence. The results of the comparison between both splits are discussed in Section 5.2. Furthermore, for evaluating the model based on robustness, the area under the curve (AUC) of a computed receiver operating characteristic curve (ROC) is calculated. In the provided codebase there were some unclear aspects of the AUC computation, which are addressed in Section 5.3.2.\nExtension \u2014 In addition to reproducing the results of the original codebase and the orig\u2010 inal datasets, we applied the method in a different domain to evaluate the method\u2019s ability to generalise to a new domain. Where the original authors employed the Muta\u2010 genicity dataset, which requires a certain level of chemical knowledge in order to inter\u2010 pret the qualitative results. Therefore, we applied the RCExplainer model on the image domain as we expect these qualitative results to be easier to interpret intuitively (see 5). For this purpose, the MNISTSuperpixels dataset [12] is used. This dataset was chosen because of its relative simplicity compared to other vision datasets. In order to apply the RCExplainer model to the MNISTSuperpixels dataset, a GNN was trained from scratch, using 4 graph convolutional layers, with 100 hidden units, fol\u2010 lowed by an embedding layer consisting of 30 units. This increase in model size is nec\u2010 essary to obtain results comparable to state\u2010of\u2010the\u2010art [15]. More details are presented in Appendix 9. We used the hyperparameters as specified in the original paper and trained the model for 600 epochs. For comparison, both an RCExplainer and PGExplainer model have been trained to ex\u2010 plain this GNN. The training uses the default hyperparameters for both models, similar to the comparison in the original paper. Again, following the original paper, we do not make use of a test train split, and evaluation is performed on part of the training set.\nReScience C 8.2 (#45) \u2013 Wilschut et al. 2022 6\n4.4 Computational requirements To run all experiments, that is to say, both the reproduction study and the extension, we made use of 6 computers with varying specifications, but that contain at least one NVIDIA 2080TI GPU. The exact specifications can be found in Appendix 8. Table 2 states the training time in GPU hours per model. The total training time for all models adds up to \u00b1454 hours of GPU runtime.\nFidelity The original authors use fidelity to compare which model produces explana\u2010 tions with the strongest counterfactual characteristics. Fidelity is the amount the pre\u2010 diction confidence decreases when the explanation is removed from the input graph. A higher value indicates stronger counterfactual characteristics. This metric can be sen\u2010 sitive to the sparsity of explanations, which is the percentage of the remaining edges of the input graph after deleting the explanation. The results for this metric can be seen on the right\u2010hand side in Figure 1. Note that the sparsity values are shown from 50% instead of 75%, because of a lack of datapoints for the PGExplainer on the 75\u201080% interval. Figure 1 shows that the RCExplainer has the highest performance of the models, corresponding to the findings of the original authors. However, the performance of the RCExp\u2010NoLDB and PGExplainer in Figure 1 is significantly lower than in the original authors\u2019 paper. As mentioned in Section 4.3.1, we use the hyperparameters as specified in the origi\u2010 nal paper. For comparison, the model was also evaluated using the hyperparameters mentioned in the README file of the codebase, changing the parameters \u00b5, \u03bb and \u03b2 in the loss function. The corresponding results are reported in Appendix 11 and show that changing the hyperparameters significantly affects performance. Therefore, we hy\u2010 pothesise that the hyperparameters are the reason for the performance discrepancies as seen in Figure 1.\nRobustness The robustness of amodel ismeasuredbyhowmuchan explanation changes after noise is added to the input graphs. The graphs are modified by adding random\nReScience C 8.2 (#45) \u2013 Wilschut et al. 2022 7\nnoise to the node features and randomly adding or deleting edges. The produced expla\u2010 nation of each noisy input graph is compared to the ground truth, the k best (top-k) edges of the explanation of the unmodified graph, by computing a ROC curve and computing the AUC of this ROC curve. The higher the AUC score of the model, the more robust it is. Each model is evaluated for different levels of noise, measured in the percentage of nodes and edges that are modified, ranging from 0% to 30%. The results are shown on the right\u2010hand side of Figure 2. It shows that the re\u2010trained RCExplainer performs the best for almost all noise values. This corresponds with the findings in the original paper. However, similar to the fidelity results, the results of the RCExp\u2010NoLDB and PGExplainer are much lower than shown in the original author\u2019s paper. We again hypothesise that this is explained by the hyperparameter tuning, following the same reasoning as in the previous paragraph.\nEfficiency The original authors claim their method is at least as efficient as previous methods, and report a 0.01s\u00b1 0.02 execution time to produce a single explanation. Our experiments show a 0.007s \u00b1 0.0006 execution time. This slight difference is likely due to differences in hardware platform and library versions. So, while unable to compare the performance of the RCExplainer model to other explainers, regarding their time efficiency, wewere able to achieve results in linewith the findings of the original authors on the run time of the RCExplainer model.\nReScience C 8.2 (#45) \u2013 Wilschut et al. 2022 8\n5.2 Results beyond original paper As mentioned in section 4.3.1, the RCExplainer is evaluated using data that has already been encountered during training. Therefore, all models have also been evaluated on fidelity and robustness with a train\u2010test split to see the effect of this experimental setup. Figure 1 and 2 show the results of these evaluations, where the 80/20% split is shown on the left side and the 100/100% on the right side. For both metrics, the figures show no significant differences. This lack of difference is likely because the explainer model is trained to explain the GNN, not the data, and therefore a train\u2010test split does not seem to have a significant influence on the performance for training the explainer models.\n5.3 Extension This section discusses the results of our extensions to the original method. First, the results of the extension to a newdomain are presented in Section 5.3.1. Then, the results of two additional AUC computations are reported in Section 3.\nMNISTSuperpixels \u2014 In order to determine whether the claims of the original authors also extend to other domains, we measured the fidelity performance and noise robustness of the RCExplainer on the MNISTSuperpixels dataset (see Figure 3). To compare these curves, the same evaluation is also performed using the PGExplainer.\nFidelity Figure 3 show that both models achieve high fidelity, especially for sparsity lower than 90%, indicating that both methods saturate the task, achieving near\u2010optimal performance. The explainers have been trained using a 100/100% train\u2010test split following the original paper. While this makes it significantly easier to saturate performance on the test set, as the samples are seen during training, results on other datasets in Section 5.1 show no clear difference between a more conventional train\u2010test split and evaluating on the full set. Therefore, we hypothesise that the explainers still generalise well to this domain. Performing this evaluation with a split of 80/20% is still preferred, but not feasible in this reproduction study due to the long training time of the models. We speculate that the decrease in fidelity for higher sparsity levels is likely not due to the model\u2019s ability to select explanations, but rather because the explanations are smaller as the sparsity level increases. As they become smaller, the counterfactual graph is more similar to the original graph retaining the same prediction. While unable to verify the performance advantage of the RCExplainer over the PGExplainer in this domain, we can verify its ability to generalise to new domains.\nReScience C 8.2 (#45) \u2013 Wilschut et al. 2022 9\nRobustness In contrast to the fidelity performance, the noise robustness shows a clear difference according to Figure 3. This difference could be caused due to an inherent difference in robustness threshold in the MNISTSuperpixel dataset compared to Muta\u2010 genicity. As not every pixel in an image is essential, and even with large parts missing, it is still possible to correctly classify an image. The PGExplainer is more robust to noise, remaining close to the original explanation, evenwith noisy input graphs. However, the performance of the RCExplainer falls short, and the method appears to be less robust to noise in this domain.\nAUC computation \u2014When examining the implementation of the AUC computation we found this was adjusted when compared to the standard definition of the AUC\u2010score, without motivation, leaving us unsure of these adjustments. The AUC\u2010score is used to compare the accuracy of S\u2032 to S, where S\u2032 is produced from noisy input graphs to evalu\u2010 ate robustness to this noise. The explanation problem is formulated as a binary classifi\u2010 cation problem. For this classification, the original authors only consider true positives and false positives when measuring the AUC, discarding the false negatives and giving the metric a positive bias. A false negative could occur when an edge in S is no longer in S\u2032, for example, when S\u2032 covers a different part of the original graph. If the explainer producingS\u2032 is not robust to noise, its AUC score could be incorrectly high if it only produces a subset of the ground truth explanation S. This means, under noisy circumstances, an explainer only has to predict a single correct edge to attain a perfect AUC score, instead of predicting the full ground truth. Therefore, false negatives appear to provide important information. True negatives are also discarded, but while their inclusion is standard practice, they only add information about the size of the graph compared to the explanation. When evaluating robustness, this is not as relevant and mostly reduces the difference between the scores. Hence, we compared the originalmethod and the inclusion of the false negatives, shown in Appendix 7. For the highest noise percentage, this yields an 0.895% AUC score de\u2010 crease. While this means the original method includes a slight positive bias, a bias is also present in the other explanation methods as the same evaluation code is used. Our foremost concern would be the comparison to other papers, where the metric might be implemented differently. We, therefore, chose to retain the original AUC computation method, as the bias is small and we prefer to retain the ability to compare our results to the original paper.\n6 Discussion\nThis paper is a reproduction study of Robust Counterfactual Explanations on Graph Neural Networks [1]. We were partly able to reproduce the original authors\u2019 claims that their model produces more counterfactual explanations, is more robust to noise and is at least as time\u2010efficient. The RCExplainer showed equal results, while the RCExp\u2010NoLDB and PGExplainer differed, which we hypothesise is because of the hyperparameters. For our reproduction paper, we only employed the experiments on the Mutagenicity dataset, and compared it solely to the RCExp\u2010NoLDB and the PGExplainer, due to time constraints. Moreover, the results of the experiments have been obtained for 10 different seeds. Additionally, multiple extensions were performed to validate the experimental setup of the original paper and apply the model to the image domain."}, {"heading": "What was easy and what was difficult", "text": "The original authors provided a codebase that included all code to reproduce the ex\u2010 periments. However, the instructions within this extensive codebase did not perfectly align with the method as proposed in the original paper. Therefore, we had to make\nReScience C 8.2 (#45) \u2013 Wilschut et al. 2022 10\nsome alterations to the code to be able to fully use it and hence mentioning all hyperpa\u2010 rameters in the original paper would improve reproducibility. Moreover, a pre\u2010trained explainermodel was provided, but this only included amodel for one seed, instead of 10 seeds. Furthermore, other explainer methods, to which the original authors compare their method were already implemented as well. Finally, the original paper described their metrics for comparing multiple explainer models clearly, which made it easier to reproduce.\nCommunication with original authors There was no communication with the original authors, as we did not find it necessary in order to reproduce the paper."}, {"heading": "Appendix", "text": "7 AUC comparison\nAs concerns were raised about the specifics of the AUC computation and its effect, the AUC of different approaches are shown in Table 3 for RCExplainer and Table 4 for PG\u2010 Explainer. These scores are computed on the Mutagenicity dataset using the provided pre\u2010trained model for RCExplainer and PGExplainer, trained using the provided script and parameters. The effect is most notable under the highest noise levels, which causes S\u2032 to differ themost from S. The original approach is positively biased for all explainers, but not equally and, therefore, affects the comparison. The effect is small enough that we chose to ignore it to retain the ability to compare to the original paper.\n8 Hardware\nFor the MNISTSuperpixels dataset, we deviated from the GNN architecture used by the original authors, as it had low performance. A high accuracy of the prediction model is important because it validates the counterfactuals produced by the explanation model. A poorly trained prediction model may have arbitrary explanations, even if the explana\u2010 tion model is correctly trained, and therefore does not have meaningful counterfactu\u2010 als. A properly trained explanation model should allow for qualitative evaluation of the method. By increasing the number of layers and hidden dimensions of themodel, the larger GNN achieves a test\u2010set score of 85% accuracy, just short of the test\u2010set score reached in [15]. This is shown in Figure 4. Training for the baseline model was stopped early due to low performance.\nReScience C 8.2 (#45) \u2013 Wilschut et al. 2022 12\n10 MNISTSuperpixels Qualitative Results\nFigure 5 shows the qualitative results of the RCExplainer model on the MNISTSuperpix\u2010 els dataset, using twelve randomly sampled graphs. The nodes overlayed on the images are the centroids of the superpixels of the input images, and the brighter their colour, the higher their probability of being included in the explanation of the model. While the original authors mainly define the explanation to be a set of edges they also provide a definition for an explanation consisting of nodes, which we employed for this visualization. There, a node n \u2208 N has a weight an, defined as follows:\nan = max i\u2208N (Mni), (5)\nwhere M is the matrix generated by the explanation network f\u03b8. This means that the weight of a node corresponds to the probability of the edge with the highest probabil\u2010 ity of belonging to the explanation. Every node with a weight higher than 0.5 is then considered to be part of the explanation of that graph.\nReScience C 8.2 (#45) \u2013 Wilschut et al. 2022 13\n11 Hyperparameter comparison\nReScience C 8.2 (#45) \u2013 Wilschut et al. 2022 14"}], "title": "[Re] Robust Counterfactual Explanations on Graph Neural Networks", "year": 2022}