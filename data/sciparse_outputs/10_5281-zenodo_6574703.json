{"abstractText": "The core finding of the paper is a novel architecture FamNet for handling the few\u2010shot counting task. We examine its implementation in the provided code on GitHub and compare it to the theory in the original paper. The authors also introduce a data set with 147 visual categories FSC\u2010147, which we analyze. We try to reproduce the authors\u2019 results on it and on CARPK data set. Additionally, we test FamNet on a category specific data set JHU\u2010CROWD++. Furthermore, we try to reproduce the ground truth density maps, the code for which is not provided by the authors.", "authors": [{"affiliations": [], "name": "Ma\u0161a Kljun"}, {"affiliations": [], "name": "Matija Ter\u0161ek"}, {"affiliations": [], "name": "Domen Vre\u0161"}, {"affiliations": [], "name": "Koustuv Sinha"}, {"affiliations": [], "name": "Sharath Chandra Raparthy"}], "id": "SP:1cac31d7940417af11fc6e4e511679cfa808b594", "references": [{"authors": ["S. Abousamra", "M. Hoai", "D. Samaras", "C. Chen"], "title": "Localization in the crowd with topological constraints.", "year": 2021}, {"authors": ["L. Boominathan", "S.S. Kruthiventi", "R.V. Babu"], "title": "Crowdnet: A deep convolutional network for dense crowd counting.", "venue": "Proceedings of the 24th ACM international conference on Multimedia", "year": 2016}, {"authors": ["X. Cao", "Z. Wang", "Y. Zhao", "F. Su"], "title": "Scale aggregation network for accurate and efficient crowd counting.", "venue": "Proceedings of the European Conference on Computer Vision (ECCV)", "year": 2018}, {"authors": ["V. Ranjan", "B. Wang", "M. Shah", "M. Hoai"], "title": "Uncertainty estimation and sample selection for crowd counting.", "venue": "Proceedings of the Asian Conference on Computer Vision", "year": 2020}, {"authors": ["C. Zhang", "H. Li", "X. Wang", "X. Yang"], "title": "Cross-scene crowd counting via deep convolutional neural networks.", "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition", "year": 2015}, {"authors": ["C. Asha", "A. Narasimhadhan"], "title": "Vehicle counting for traffic management system using YOLO and correlation filter.", "venue": "IEEE International Conference on Electronics,", "year": 2018}, {"authors": ["Y. Chen", "J. Lu"], "title": "A Multi-Loop Vehicle-Counting Method under Gray Mode and RGB Mode.", "venue": "Applied Sciences", "year": 2021}, {"authors": ["D. Onoro-Rubio", "R.J. L\u00f3pez-Sastre"], "title": "Towards perspective-free object counting with deep learning.", "venue": "European Conference on Computer Vision. Springer", "year": 2016}, {"authors": ["J. Quesada", "P. Rodriguez"], "title": "Automatic vehicle counting method based on principal component pursuit backgroundmodeling.", "venue": "IEEE International conference on image processing (ICIP). IEEE", "year": 2016}, {"authors": ["S. Zhang", "G. Wu", "J.P. Costeira", "J.M. Moura"], "title": "FCN-rLSTM: Deep spatio-temporal neural networks for vehicle counting in city cameras.", "venue": "Proceedings of the IEEE international conference on computer vision", "year": 2017}, {"authors": ["C. Arteta", "V. Lempitsky", "A. Zisserman"], "title": "Counting in the wild.", "venue": "European conference on computer vision. Springer", "year": 2016}, {"authors": ["S. Zhang", "X. Yang", "Y. Wang", "Z. Zhao", "J. Liu", "Y. Liu", "C. Sun", "C. Zhou"], "title": "Automatic fish population counting by machine vision and a hybrid deep neural network model.", "year": 2020}, {"authors": ["V. Ranjan", "U. Sharma", "T. Nguyen", "M. Hoai"], "title": "Learning To Count Everything.", "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition", "year": 2021}, {"authors": ["S. Ren", "K. He", "R. Girshick", "J. Sun"], "title": "Faster R-CNN: Towards real-time object detection with region proposal networks.", "venue": "Advances in neural information processing systems", "year": 2015}, {"authors": ["T.-Y. Lin", "P. Goyal", "R. Girshick", "K. He", "P. Doll\u00e1r"], "title": "Focal loss for dense object detection.", "venue": "Proceedings of the IEEE international conference on computer vision", "year": 2017}, {"authors": ["K. He", "G. Gkioxari", "P. Doll\u00e1r", "R. Girshick"], "title": "Mask R-CNN.", "venue": "Proceedings of the IEEE international conference on computer vision", "year": 2017}, {"authors": ["T.-Y. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C.L. Zitnick"], "title": "Microsoft COCO: Common objects in context.", "venue": "European conference on computer vision. Springer", "year": 2014}, {"authors": ["M.-R. Hsieh", "Y.-L. Lin", "W.H. Hsu"], "title": "Drone-based object counting by spatially regularized regional proposal network.", "venue": "Proceedings of the IEEE international conference on computer vision", "year": 2017}, {"authors": ["V.A. Sindagi", "R. Yasarla", "V.M. Patel"], "title": "Pushing the frontiers of unconstrained crowd counting: New dataset and benchmarkmethod.", "venue": "Proceedings of the IEEE International Conference on Computer Vision", "year": 2019}, {"authors": ["V.A. Sindagi", "R. Yasarla", "V.M. Patel"], "title": "JHU-CROWD++: Large-Scale Crowd Counting Dataset and A Benchmark Method.", "venue": "Technical Report", "year": 2020}], "sections": [{"text": "R E S C I E N C E C Replication / ML Reproducibility Challenge 2021\n[Re] Learning to count everything\nMa\u0161a Kljun1,2, ID , Matija Ter\u0161ek1,2, ID , and Domen Vre\u01611,2, ID 1University of Ljubljana, Faculty of Computer and Information Science, Ve\u010dna pot 113, 1000 Ljubljana \u2013 2Equal contributions\nEdited by Koustuv Sinha,\nSharath Chandra Raparthy\nReviewed by Anonymous Reviewers\nReceived 04 February 2022\nPublished 23 May 2022\nDOI 10.5281/zenodo.6574703\n1 Reproducibility Summary"}, {"heading": "Scope of Reproducibility", "text": "The core finding of the paper is a novel architecture FamNet for handling the few\u2010shot counting task. We examine its implementation in the provided code on GitHub and compare it to the theory in the original paper. The authors also introduce a data set with 147 visual categories FSC\u2010147, which we analyze. We try to reproduce the authors\u2019 results on it and on CARPK data set. Additionally, we test FamNet on a category specific data set JHU\u2010CROWD++. Furthermore, we try to reproduce the ground truth density maps, the code for which is not provided by the authors."}, {"heading": "Methodology", "text": "Weuse the combination of the authors\u2019 and our own code, for partswhere the code is not provided (e.g., generating ground truth density maps, CARPK data set preprocessing). We also modify some parts of the authors\u2019 code so that we can evaluate the model on various data sets. For running the code we used the Quadro RTX 5000 GPU and had a total computation time of approximately 50 GPU hours."}, {"heading": "Results", "text": "We could not reproduce the density maps, but we produced similar density maps by modifying some of the parameters. We exactly reproduced the results on the paper\u2019s data set. We did not get the same results on the CARPK data set and in experiments where implementation details were not provided. However, the differences are within standard error and our results support the claim that the model outperforms the base\u2010 lines."}, {"heading": "What was easy", "text": "Running the pretrained models and the demo app was quite easy, as the authors pro\u2010 vided instructions. It was also easy to reproduce the results on a given data set with a pretrained model.\nCopyright \u00a9 2022 M. Kljun, M. Ter\u0161ek and D. Vre\u0161, released under a Creative Commons Attribution 4.0 International license. Correspondence should be addressed to Domen Vre\u0161 (dv6968@student.uni-lj.si) The authors have declared that no competing interests exist. Code is available at https://github.com/tersekmatija/re-LearningToCountEverything \u2013 DOI 10.5281/zenodo.6508260. \u2013 SWH swh:1:dir:cf19f5a717c777cd1097e938ef4e6bdb735f71c7. Data is available at https://drive.google.com/file/d/1ymDYrGs9DSRicfZbSCDiOu0ikGDh5k6S/view?usp=sharing. Open peer review is available at https://openreview.net/forum?id=HKbgd3zmh0t.\nReScience C 8.2 (#39) \u2013 Kljun, Ter\u0161ek and Vre\u0161 2022 1"}, {"heading": "What was difficult", "text": "It was difficult to verify the ground truth density map generation as the code was not provided and the process was incorrectly described. Obtaining a performant GPU was also quite a challenge and it took quite many emails to finally get one. This also meant that we were unable to reproduce the training of the model.\nCommunication with original authors We contacted the authors three times through issues on GitHub. They were helpful and responsive, but we have not resolved all of the issues.\nReScience C 8.2 (#39) \u2013 Kljun, Ter\u0161ek and Vre\u0161 2022 2\n2 Introduction\nCounting objects in a scene is a task that is very simple and intuitive for humans, how\u2010 ever, the problem arises when there are hundreds, thousands, or even more objects in one scene as the counting becomes difficult or impossible. Yet, sometimes it is bene\u2010 ficial to have a count estimation of such big amounts of objects and that is why many approaches for counting objects have been proposed. These methods can easily outper\u2010 form humans, especially when there are many objects in a scene. Still, the advantage of humans is that we are able to count objects from the majority of visual categories with ease, which is not the case with the current object counting methods. In fact, the counting approaches that have been proposed until now can usually handle only one visual category at the time, and even those categories are mostly limited to a few, most frequently humans [1, 2, 3, 4, 5], vehicles [6, 7, 8, 9, 10], and animals [11, 12]. The reason behind these limitations in the currently proposed approaches is twofold. The major\u2010 ity of counting approaches requires dot annotations for thousands of objects on few thousands of training images. The second reason is that there exists no large enough unconstrained data set, which would allow the development of a method for counting any visual category. Both of these limitations exist as dot annotation and development of a large enough data is a laborious and a costly task. In this reportwe try to reproduce the paper Learning toCount Everything [13], inwhich the authors try to overcome both of the abovementioned limitations. Instead of mimicking the previousworks and treating counting as a fully supervised regression task, they pose counting as a few shot regression task. This approach is generalizable as only an input image with a few exemplars from the same image (that represent the object of interest) is required to achieve generalization to a completely novel visual category class. Second, the authors of this paper also address the lack of data sets with many visual categories as they introduce a data set including more than 6000 images from 147 visual categories.\n3 Scope of reproducibility\nThe authors are interested in counting everything and they achieve that by posing count\u2010 ing as a few\u2010shot regression task. The core finding of the paper is a novel architecture called FamNet that handles a few\u2010shot counting task together with a novel adaptation strategy that adapts the network to any novel visual category at test time, by using only a few exemplar objects from the novel category. Furthermore, the authors introduce a data set containing 147 different visual categories and they show that their method out\u2010 performs other state\u2010of\u2010the\u2010art approaches \u2013 object detectors as well as few\u2010shot count\u2010 ing approaches. We test these key findings from the paper:\n\u2022 FamNet outperforms other few\u2010shot approaches when it comes to object counting.\n\u2022 FamNet performs well even on a category\u2010specific data set.\n\u2022 Increasing the number of exemplars decreases FamNet\u2019s error.\n4 Methodology\nWhere available, we use the authors\u2019 code from GitHub. We modify it so that we can evaluate the model on different data sets. Additionally, we prepare our scripts for gen\u2010 erating ground truth density maps, ablation study, and preprocessing of CARPK data set, as the authors do not provide it.\nReScience C 8.2 (#39) \u2013 Kljun, Ter\u0161ek and Vre\u0161 2022 3\n4.1 Model descriptions FamNet is composed of two main modules \u2013 a multi\u2010scale feature extraction module and a density prediction module. The multi\u2010scale feature extraction module is based on the ImageNet pretrained network, more specifically on the first four blocks from a pretrained ResNet\u201050 backbone. From the code, we find out that the authors use the pre\u2010 trained ResNet\u201050 model from TorchVision. The density prediction module is designed in a way to be agnostic to the visual categories. They achieve this by not feeding the features obtained from the feature extraction module directly. Contrary, they rather use the correlation map between the exemplar features and image features as the in\u2010 put to the density prediction module. We show a visualization of inputs to the density prediction module in Appendix 7. As mentioned, the proposed FamNet can adapt to a new visual category once trained, using only a few exemplars. To understand the novel adaptation loss that is used during test time we first quickly describe the Min\u2010Count and Perturbation losses. Let B denote the set of provided exemplar bounding boxes (bounding boxes denoting examples of the object, that we are counting, given to the network). For each bounding box b \u2208 B, let Zb represent the crop from the density map Z at location b.\nMin-Count Loss \u2014Min\u2010Count Loss is defined as\nLMinCount = \u2211 b\u2208B max(0, 1\u2212 ||Zb||1). (1)\nThe idea behind this loss is that the sumof density valueswithinZb should be at least 1 as the predicted count is a sum of predicted density values, and there is at least one object at the location b. Meaning that if the total value of the density map in the exemplar box is equal to or greater than 1, the loss will not increase for this location, but if the total value of the density map in the exemplar box is smaller than 1, we increase the loss. By inspecting the authors\u2019 code, however, we find out that Min\u2010Count loss is incorrectly implemented. Instead of using the difference between 1 and ||Zb||1, the authors use the squared difference. In notation, the implementation of Min\u2010Count loss in the original implementation is\nL implemented MinCount = \u2211 b\u2208B max(0, (1\u2212 ||Zb||1)2). (2)\nWe address the issue and test the performance of the model for both implementations in Section 5.\nPerturbation Loss \u2014 Perturbation Loss is defined as\nLPer = \u2211 b\u2208B ||Zb \u2212Gh\u00d7w||22, (3)\nwhereGh\u00d7w is a 2D Gaussian window of size h\u00d7w and standard deviation \u03c3G = 8. The authors do not provide the reasoning for the chosen value, so we try different options to investigate its influence. We report our findings in Section 5.2.1. This loss is inspired by the success of tracking algorithms based on correlation filter, where algorithms learn a filter that has the highest response at the location of the bounding box and lower re\u2010 sponse at all perturbed locations. We can look at the density map Z as the correlation response between the exemplars and the image.\nAdaptation loss \u2014 The final loss, called adaptation loss, is defined as a weighted combina\u2010 tion\nLAdapt = \u03bb1LMinCount + \u03bb2LPer, (4)\nReScience C 8.2 (#39) \u2013 Kljun, Ter\u0161ek and Vre\u0161 2022 4\nwhereLMinCount is theMin\u2010Count Loss, LPer is Perturbation Loss and \u03bb1 and \u03bb2 are scalar hyper\u2010parameters. The authors fine\u2010tuned them on validation set, and we use the same values \u03bb1 = 10\u22129 and \u03bb2 = 10\u22124. Note that adaptation loss is only used at test time, and MSE between predicted and ground truth density map over all pixels is used as a loss during training.\n4.2 Data sets\nFSC-147 \u2014 As the majority of the data sets are dedicated to a specific visual category, the authors collected and annotated 6135 images across 147 different visual categories. The average image height is 774 and the average image width is 938 pixels. In each image, all objects are dot\u2010annotated in an approximate center of the object. Furthermore, in a majority of cases (96.26%) three object instances are randomly selected and are addition\u2010 ally annotatedwith axis\u2010aligned bounding boxes denoting exemplar bounding boxes. In some cases four (3.45%), five (0.27%), or six (0.02%) object instances are additionally an\u2010 notated. The data set is divided into train, validation, and test sets in a way that each of these sets does not share any object categories. The train, validation, and test sets consist of 3659, 1286, and 1190 images, respectively. The authors provide two sets of ground truth density maps which are the same in all but two cases (3417.npy and 3477.npy). In the second set of ground truth density maps, the first image appears more blurred, while different objects are counted on the latter image (see Figure 1).\nIn the original paper, the authors compare FamNet to some common object detectors \u2010 Faster R\u2010CNN [14], RetinaNet [15], and Mask R\u2010CNN [16], which were pretrained on COCO data set [17]. Thus they select a subset of FSC\u2010147, which contains categories that also appear in COCO. We manually try to find the intersecting categories and find that 17 categories appear under the same (or similar) name in both data sets. The authors in\u2010 clude all images from FSC\u2010147 from those categories in COCO\u2010Val and COCO\u2010Test splits that they provide, and do not leave out any categories that might appear in FSC\u2010147 and COCO.\nResizing of FSC-147 images before using FamNet \u2014 The authors provide a link to FSC\u2010147 data set in their GitHub repository. However, the images there are already resized as a part of preprocessing before using FamNet. The authors decided to resize all images to a fixed height of 384 pixels. They claim that they adjusted the width of the images in the way that the aspect ratio is preserved. As the authors provide the information about the original dimensions of each image, we checked, whether all processed images are correctly resized to have a height of 384 pixel and if their aspect ratio is truly preserved. We found some cases where the aspect ratio was not preserved. We showed such cases\nReScience C 8.2 (#39) \u2013 Kljun, Ter\u0161ek and Vre\u0161 2022 5\nto the authors, who replied that they did not preserve the aspect ratio for images with original width of less than 384 pixels. However, the provided example with a corrupted aspect ratio did not have a width smaller than 384.\nFSC-147 ground truth density map generation \u2014 The authors do not provide the code for the generation of ground truth density maps, but rather provide the already pre\u2010computed density maps and only describe the process. While this is beneficial, as it saves com\u2010 putation time, it is somewhat questionable, as we do not get a full insight into how the data set was generated, and cannot verify their claims. An issue has been opened on the authors\u2019 GitHub, but they did not provide the code. We implemented our own code as described in the paper. We used Gaussian smoothing with adaptive window size and estimated the size of the objects from distances between dot annotations and their nearest neighbor. We averaged those distances to obtain the size of the Gaussian window sG. The authors claim that they use the sG4 as the standard deviation, however, we could not reproduce the results using this value. We obtained the closest results with sG8 (see Figure 2 for illustrative example). When we asked the authors about the issue, they suggested that large discrepancies might be due to them computing ground truth deviations on larger images, and then downscaling them to the sizes in the data set. However, we still could not reproduce the same results with the suggested approach. This question still remains open and the issue has not been resolved. Our code produces results most similar to the ground truth density maps, though displacement for some points is visible.\nCARPK \u2014 The authors want to check the performance of FamNet on category specific counting task. TheyuseCARPKdata set [18], which contains around 90,000 cars recorded in various parking lots, taken with drones. The data set is already split into train and test set, and for each image ground truths in a form of bounding boxes are provided. In order to convert the data set into a form suitable for the evaluation of FamNet, we had to create the density maps that represent the ground truths, select the bounding boxes that represent the exemplars, and resize the images to have a height of 384 pixels. The authors do not provide any information about the preprocessing of that data set. We first obtained the distribution for a number of exemplars in FSC\u2010147 data set. We then sampled a number of exemplars n from this distribution for each image and ran\u2010 domly chose n bounding boxes that represent exemplars. To obtain the density maps, we represented each car by a Gaussian filter of the size of the provided bounding box. We set the \u03c3 of the filter to h+w16 in order to follow the setting of \u03c3 for FSC\u2010147 data set. However, we did not set the \u03c3 based on the authors\u2019 description in the paper but based on our findings, described in Section 4.2.3.\nReScience C 8.2 (#39) \u2013 Kljun, Ter\u0161ek and Vre\u0161 2022 6\nJHU-CROWD++ \u2014 Aswewant to check how the FamNet performs on a typical crowd count\u2010 ing data set, we extend the authors\u2019 research and test the pretrained model on JHU\u2010 CROWD++ data set [19, 20]. This includes 4327 images collected under a diverse set of conditions (adverse weather, various illumination, varying densities, etc.) and 1.51mil\u2010 lion annotations (dots, approximate bounding boxes, etc.). We again had to do some preprocessing in order to get the data set in the format for evaluation of FamNet. We used the same preprocessing as we did for CARPK data set (see Section 4.2.4).\n4.3 Hyperparameters There are several FamNet hyperparameters that have to be set. The authors set \u03c3G = 8 used in perturbation loss (see Section 4.1.2) without any explanation why. We therefore decided to check how different values of \u03c3G impact the error of the model. To select the best value of \u03c3G we used grid search. We tested every even integer value between 2 and 20 and report our findings in Section 5.2.1. We did not test \u03bb1 and \u03bb2 from adaptation loss (see Section 4.1.3). The authors set their values to 10\u22129 and 10\u22124, respectively. They say that setting them to such small values is necessary, so the adaptation loss has a similar magnitude to the training loss. We also did not test the number of gradient descent steps and the learning rate during the test time adaptation. The authors said that these two values were tuned along with \u03bb1 and \u03bb2 on the validation set.\n4.4 Experimental setup and code To perform our experiments, we used the authors\u2019 and our code. Authors\u2019 code is avail\u2010 able on theirGitHub repository1. Weperformedour experiments by runningfiletest_extended.py, which is an extended version of the authors\u2019 file test.py, with some flags formanipula\u2010 tion of different options. To evaluate different values of\u03c3G, weused scriptchoose_sigma.py, which is run in a similar way as the scripts mentioned before. We also did some prepro\u2010 cessing and testing in our Jupyter notebooks that are self\u2010explanatory to run. Model is evaluated with absolute error (MAE) and root mean squared error (RMSE), de\u2010 fined as:\nMAE = 1\nn n\u2211 i=1 |ci \u2212 c\u0302i|, RMSE = \u221a\u221a\u221a\u221a 1 n n\u2211 i=1 (ci \u2212 c\u0302i)2, (5)\nwhere n denotes the number of instances in test/val set, ci denotes the number of se\u2010 lected objects on i\u2010th image from that set, and c\u0302i denotes the predicted count for that image.\n4.5 Computational requirements All experiments were ran on GPU only (hence we do not report used CPU and RAM). We ran our experiments on a server with Nvidia Quadro RTX 5000 GPU. Each evaluation of FamNet on test or validation set (FSC\u2010147) takes around 2 minutes without the test time adaptation and around 1 hour and 40 minutes with it. The ablation study with number of exemplars takes around 3 hours (the execution times are shorter when the number of exemplars is decreased). Evaluation of FamNet on subset of categories from COCO data set takes around 40 minutes with adaptation. Evaluation of FamNet on CARPK data set (with adaptation) takes around 1 hour and 20 minutes. We spent around 50 GPU hours to run all of our experiments.\n1https://github.com/cvlab-stonybrook/LearningToCountEverything\nReScience C 8.2 (#39) \u2013 Kljun, Ter\u0161ek and Vre\u0161 2022 7\n5 Results\nOur results support the claims of the authors about the quality of their proposed FamNet structure. We managed to reproduce their results exactly (where the code is provided) or up to the point that we can confirm that their model performs as they claim in com\u2010 parison with the other methods\n5.1 Results reproducing original paper Wereproducedmost of the results obtainedwithFamNet onFSC\u2010147 andCARPKdatasets. The only exception is the ablation study with number of exemplars, where our results do not entirely support the authors\u2019 claim.\nEvaluation of FamNet on FSC-147 dataset \u2014We managed to get the same results as the au\u2010 thors when testing FamNet on FSC\u2010147 validation and test set, which supports the claim that FamNet outperforms other tested few\u2010shot approaches. The results are given in Ta\u2010 ble 1 of the original paper. However, we did not test the few\u2010shot methods that FamNet is compared to.\nComparison with object detectors \u2014We tried to reproduce the comparison of FamNet with object detectors, trained on COCO data set. The authors compare FamNet with the de\u2010 tectors on images from FSC\u2010147 data set from categories that overlap in FSC\u2010147 and COCO. We did not manage to reproduce the exact results obtained by the authors (Ta\u2010 ble 2 in their paper), but we get the results that are within the standard error of theirs or, in case of RetinaNet worse than theirs, and still support the claim that FamNet beats listed object detectors. Our results are shown in Table 1. Additionally to the authors, we report the standard error of MAE estimate, which was calculated from standard devia\u2010 tion. We obtained those results using TorchVision models. The authors use Detectron2 models instead, which perform worse in our experiments.\nNumber of exemplars ablation study \u2014We reproduced the experiment, that tested the impact of the number of exemplars on the performance of FamNet. However, our results (see Table 2) do not entirely support the claim of the authors that increasing the number of exemplars improves the performance of the FamNet. We can see that by increasing the number of exemplars from 2 to 3, RMSE increased on both test and val set, while the MAE increased on val set and decreased on train set.\nReScience C 8.2 (#39) \u2013 Kljun, Ter\u0161ek and Vre\u0161 2022 8\nTable 2. The performance of FamNet on FSC\u2010147 data set with respect to the number of exemplars. Columns SE show standard errors of MAE estimates.\nMAE SE RMSE\nMCNN 188.9 / 483.4 CSR\u2010Net 85.9 / 309.2\nFamNet (our results) 256.9 15.0 652.5\nEvaluation on a category-specific data set \u2014 The authors evaluate FamNet on a CARPK data set. We reproduced their results of FamNet trained on FSC\u2010147, but we did not try to reproduce the results for FamNet trained on CARPK. As the authors do not describe the used preprocessing, we did not get exactly the same results as they did. However, our results are close to theirs and still support their claim that FamNet performs well on this category\u2010specific data set. We got MAE 27.9 (with standard error 1.1) and RMSE 36.4, while the authors got MAE 28.8 and RMSE 44.4.\n5.2 Results beyond original paper Additionally, we tested how \u03c3G (Section 4.1.2) and correction of the Min\u2010Count Loss affect the model\u2019s performance, evaluated the model on another category\u2010specific data set, visually inspected the errors of the model and effects of test time adaptation.\nImpact of \u03c3G on the error of the model \u2014 Since the authors do not provide any justification for setting \u03c3G = 8, we test how theMAE of FamNet changeswith different \u03c3G (see Figure 3). We can see that \u03c3G has practically no impact on MAE of FamNet.\nMin-Count Loss correction \u2014 Sincewehavenoticed that the authors\u2019 definition ofMin\u2010Count Loss differs from their implementation (see Section 4.1.1), we tested how it affects the error of the model. Our results did not show any significant difference in MAE and RMSE.\nEvaluation on JHU-CROWD++ \u2014 To test the performance of FamNet on category\u2010specific data set even further, we evaluated it on the JHU\u2010CROWD++ data set (see Section 4.2.5). We use a model trained on FSC\u2010147. The results are shown in Table 3. We can see that FamNet performs worse than baselines. However, this data set is challenging (large number of objects, small bounding boxes) and training the model on that data set with a higher number of exemplars would likely boost the performance.\nReScience C 8.2 (#39) \u2013 Kljun, Ter\u0161ek and Vre\u0161 2022 9\nPerformance of the model without adaptation \u2014 Additionally, we visually inspect the images, where absolute error, normalized by the ground truth count, is the highest or the lowest. Our observations and visualisations are described in Appendix 8.\nEffect of adaptation on model\u2019s predictions \u2014 To inspect the effect of adaptation, we analysed the most positive and negative effects of adaptation on model\u2019s performance. We de\u2010 scribe the results in Appendix 9.\n6 Discussion\nWe tried to reproduce the results from the paper Learning to count everything. We ob\u2010 tained the same results as in the paper for some experiments. For others, our results are still close enough to the papers\u2019. We confirmed that FamNet outperforms other few\u2010shot approaches when it comes to object counting and that FamNet performs well even on a category\u2010specific data set. Our experiments disprove the authors\u2019 claim that increasing the number of exemplars decreases FamNet\u2019s error. We assume that this is due to the fact that we discarded different exemplars than the authors. This might suggest that choosing correct exemplars is more important than choosing more of them.\n6.1 What was easy A demo app with clear instructions helped with the understanding of the model. The model\u2019s architecture was understandable from the code and the paper. It was easy to reproduce the results on FSC\u2010147 with a pretrained model.\n6.2 What was difficult Reproducing the ground truth densitymapswas difficult, as the process in the paper did not lead to the paper\u2019s results, and the code for it was not provided. We did our best to mimic the ground truth density maps. Evaluating other models for which the code was not providedwas challenging, as no parameters were given (e.g., confidence or intersect over union thresholds for object detectors). We struggled obtaining a good enough GPU. Due to lack of time, we were unable to train the model ourselves, and we delegate this to future work.\n6.3 Communication with original authors We contacted the authors three times through issues on GitHub. They were helpful and responsive, but we have not resolved all of the issues."}, {"heading": "Appendix", "text": "7 Input features to density prediction module\nThe authors use a density prediction module that is agnostic to the visual categories. Instead of feeding the features obtained from the feature extraction module directly, they rather use the correlation map between the exemplar features and image features as the input to the density prediction module. In Figure 4 we show an example of an input (correlation features) to the density prediction module.\n8 Images with the best and the worst relative MAE on test set without adaptation\nWe visually inspect the images where absolute error normalized by the ground truth count is the highest or the lowest, and show some of the images in Figure 5. We can see that the algorithm predicts density maps with highest relative count error in cases where he predicts counts for wrong objects. In all three cases defined shapes which confuse the algorithmare present. Algorithmworks the best on images, where the shape of the object it counts is well\u2010defined and differs from the background, or there is no background at all. Adaptation in some cases improves the prediction, while in some cases it makes it worse. Thus, we investigate the affect of adaptation in the next section of appendix.\n9 Effect of adaptation on predictions\nWe inspect on which images the absolute error normalized by the ground truth count is improved or worsened the most. We show examples of those images in Figure 6. We do not observe any special pattern in the shown images. The main reason for a bigger impact of adaptation on those images is that their relative errors were quite high/low\nReScience C 8.2 (#39) \u2013 Kljun, Ter\u0161ek and Vre\u0161 2022 12\nand consequently absolute changes in the prediction had a bigger impact on relative error.\nReScience C 8.2 (#39) \u2013 Kljun, Ter\u0161ek and Vre\u0161 2022 13\nReScience C 8.2 (#39) \u2013 Kljun, Ter\u0161ek and Vre\u0161 2022 14\nReScience C 8.2 (#39) \u2013 Kljun, Ter\u0161ek and Vre\u0161 2022 15"}], "title": "[Re] Learning to count everything", "year": 2022}