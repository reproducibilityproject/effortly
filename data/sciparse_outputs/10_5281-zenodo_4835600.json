{"abstractText": "We verify the outcome of the methodology proposed in the article, which attempts to provide post-hoc causal explanations for black-box classifiers through causal reference. This is achieved by replicating the code step by step, according to the descriptions in the paper. All the claims in the paper have been examined, and we provide additional metric to evaluate the portability, expressive power, algorithmic complexity and the data fidelity of their framework. We have further extended their analyses to consider all benchmark datasets used, confirming results.", "authors": [{"affiliations": [], "name": "Ivo Verhoeven"}, {"affiliations": [], "name": "Xinyi Chen"}, {"affiliations": [], "name": "Qingzhi Hu"}, {"affiliations": [], "name": "Mario Holubar"}], "id": "SP:01d067563b26b18ce867c8dd57bbee0db5ee68dc", "references": [{"authors": ["D.V. Carvalho", "E.M. Pereira", "J.S. Cardoso"], "title": "Machine learning interpretability: A survey on methods and metrics.", "year": 2019}, {"authors": ["M.Moradi andM. Samwald"], "title": "Post-hoc explanation of black-box classifiers using confident itemsets.", "venue": "Expert Systems with Applications", "year": 2021}, {"authors": ["M. O\u2019Shaughnessy", "G. Canal", "M. Connor", "M. Davenport"], "title": "Rozell.Generative causal explanations of blackbox classifiers", "year": 2020}, {"authors": ["D.P. Kingma", "M. Welling"], "title": "Auto-Encoding Variational Bayes", "year": 2014}, {"authors": ["M. Hutson"], "title": "Artificial intelligence faces reproducibility crisis.", "venue": "DOI: 10.1126/science.359.6377.725. eprint: https://science.sciencemag.org/content/359/6377/725.full.pdf. URL: https://science.sciencemag.org/content/359/6377/725. ReScience C", "year": 2018}, {"authors": ["O.E. Gundersen", "S. Kjensmo"], "title": "State of the art: Reproducibility in artificial intelligence.", "venue": "Proceedings of the AAAI Conference on Artificial Intelligence", "year": 2018}, {"authors": ["S. Hochreiter", "J. Schmidhuber"], "title": "Long short-term memory.", "venue": "Neural computation", "year": 1997}, {"authors": ["J. He", "D. Spokoyny", "G. Neubig", "T. Berg-Kirkpatrick"], "title": "Lagging Inference Networks and Posterior Collapse in Variational Autoencoders", "year": 2019}, {"authors": ["Y. LeCun. \u201cThe MNIST database of handwritten digits.\u201d In"], "title": "http://yann", "venue": "lecun. com/exdb/mnist/", "year": 1998}, {"authors": ["H. Xiao", "K. Rasul", "R. Vollgraf"], "title": "Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms.", "year": 2017}, {"authors": ["R. Socher", "A. Perelygin", "J. Wu", "J. Chuang", "C.D. Manning", "A.Y. Ng", "C. Potts"], "title": "Recursive deep models for semantic compositionality over a sentiment treebank.", "venue": "Proceedings of the 2013 conference on empirical methods in natural language processing", "year": 2013}, {"authors": ["Z. Hu", "Z. Yang", "X. Liang", "R. Salakhutdinov", "E.P. Xing"], "title": "Toward controlled generation of text.", "venue": "In: International Conference on Machine Learning. PMLR", "year": 2017}], "sections": [{"text": "R E S C I E N C E C"}, {"heading": "Replication / ML Reproducibility Challenge 2020", "text": "[Re] Replication Study of \u2019Generative causal explanations of black-box classifiers\u2019 Ivo Verhoeven1, ID , Xinyi Chen1, ID , Qingzhi Hu1, ID , and Mario Holubar1, ID 1University of Amsterdam, Amsterdam, the Netherlands\nEdited by Koustuv Sinha\nReviewed by Anonymous Reviewers\nReceived 29 January 2021\nPublished 27 May 2021\nDOI 10.5281/zenodo.4835600"}, {"heading": "Reproducibility Summary", "text": ""}, {"heading": "Scope of Reproducibility", "text": "We verify the outcome of the methodology proposed in the article, which attempts to provide post-hoc causal explanations for black-box classifiers through causal reference. This is achieved by replicating the code step by step, according to the descriptions in the paper. All the claims in the paper have been examined, and we provide additional metric to evaluate the portability, expressive power, algorithmic complexity and the data fidelity of their framework. We have further extended their analyses to consider all benchmark datasets used, confirming results."}, {"heading": "Methodology", "text": "We use the same architecture and (hyper)parameters for replication. However, the code has a different structure and we provide a more efficient implementation for the measure of information flow. In addition, Algorithm 1 in the original paper is not implemented in their repository, so we have also implemented Algorithm 1 ourselves and further extend their framework to another domain (text data), although unsuccessfully. Furthermore, we make a detailed table in the paper to show the time used to produce the results for different experiments reproduced. All models were trained on Nvidia GeForce GTX 1080 GPUs provided by Surfsaras\u0313 LISA cluster computing service at university of Amsterdam1."}, {"heading": "Results", "text": "We reproduced the framework in the original paper and verified the main claims made by the authors in the original paper. However, the GCE model in extension study did not manage to separate causal factors and non-causal factors for a text classifier due to the complexity of fine-tuning the model.\n1This is our course project for themaster course Fairness, Accountability, Confidentiality andTransparency in AI at the University of Amsterdam. Lisa cluster: https://userinfo.surfsara.nl/systems/lisa.\nCopyright \u00a9 2021 I. Verhoeven et al., released under a Creative Commons Attribution 4.0 International license. Correspondence should be addressed to Ivo Verhoeven (ivo.verhoeven@student.uva.nl) The authors have declared that no competing interests exist. Code is available at https://github.com/shin-ee-chen/UvA_FACT_2021. \u2013 SWH swh:1:dir:9d960937f2b11bb67b53bbe4eaa940c14cadf9d1. Open peer review is available at https://openreview.net/forum?id=Vqtf4kZg2j.\nReScience C 7.2 (#23) \u2013 Verhoeven et al. 2021 1"}, {"heading": "What was easy", "text": "The original paper comes with extensive appendices, many of which contain crucial details for implementation and understanding of the intended function. The authors provide code for most of the experiments presented in the paper. Although at the beginning their code repository was not functional, we use it as a reference to re-implement our code. The author also updated their code two weeks after we start our own implementation, whichmade it easy for us to verify the correctness of our re-implementation."}, {"heading": "What was difficult", "text": "The codebase the authors provided was initially unusable, with missing or renamed imports, hardcoded filepaths and an all-around convoluted structure. Additionally, the description of Algorithm 1 is quite vague and no implementation of it was given. Beyond this, computational expense was a serious issue, given the need for inefficient training steps, and re-iterating training several times for hyperparameter search.\nCommunication with original authors This reproducibility study is part of a course on fairness, accountability, confidentiality and transparency in AI. Since it is a course project where we interactedwith other group in the forum, and another group also working with this paper has reached out to the authors about problems with the initial repository, we did not find necessary to do it again.\n1 Introduction\nMachine learning is increasingly used in different applications. The wide-scale spread of these methods places more emphasis on transparent algorithmic decision making, which has the potential tomitigate the potential for disruptive social effects. Yet, despite reliable results of complex black boxes, their internal reasoning and inner workings are not necessarily apparent to end-users or even designers. As a result, not even trained experts can grasp the reasoning behind forecasts. Moreover, modern legislation have necessitated the opportunity challenging these systems, especially in heavily regulated domains, increasing the need for machine learning systems that are (post-hoc) interpretable [1].\nBlack-box artificial intelligence approaches likeDeepNeuralNetworkshave oftenproven to be able to capture complex dependencies within data, and allow for making accurate predictions. However, the actual internal logic used by systems dependent on such approaches is often nebulous or totally unclear. In this paper, we will reproduce the paper which focuses on the explainability aspect of AI.\nExplainable Artificial Intelligence (XAI) refers to systems that seek to clarify how a blackbox AImodel achieves its performance. Post-hoc XAI achieves the desired explainability be generating reasons for decisions after having trained a black-box classifier. This is often achieved by extracting correlations between input features and the eventual forecasts [2]. This paper aims to reproduce such a post-hoc XAI algorithm, capable of providing clear and interpretable explanations for complex black-box classifiers [3]. The central contribution made by [3] is placing explanations in a causal-generative framework. By using a variational auto-encoder (VAE) [4] , low dimensional representations of the data canbe achieved. By further incorporating amutual information loss between the classifier and the latent variables, the latent space is decorrelated into factors that\nReScience C 7.2 (#23) \u2013 Verhoeven et al. 2021 2\nactively influence the classifier s\u0313 prediction, and those that capture superfluous variation in the underlying dataset. The causal-VAE , dubbed a generative causal explainer (GCE) by [3] can be studied and intervened with to provide a powerful framework for inducing explainability.\nThere exists literature suggesting that some studies cannot be replicated [5]. This is valid also for the most respected journals and conferences in AI. Steps must be taken to ensure high trustworthiness of AI algorithms [6]. In the experiments presented here, we re-implement the framework used by [3], extend their analyses to all benchmark datasets discussed and provide an extension to a novel domain. When beginning our process, the officially provided code repository corresponding to the authors\u02bc publication was not capable of running in all discussed scenarios, nor producing results similar to those presented 2. However, our re-implementation has ascertained all results, and been extended to incorporate all analyses, and can report their framework to be reproducible. All code has been made publicly available via a Github repository3.\n2 Scope of Reproducibility\nThe original paper establishes a causal inference framework. While intuitive, this introduces the additional challenge of balancing causal intervention with expressive latent variables. To overcome this hurdle, [3] integrate causal inference into the VAE learning objective. This system consists of two basic components: a way to describe the data distribution, and a method of generating a consistent classification model. In order to get a clear objective of the reproduction, we consider the the main claim(s) of the original paper as:\n\u2022 Claim 1: creating a generative framework that describes a black-box classifier, and a process that achieves this while maintaining latent descriptions that result in high data fidelity and scalability of the overall system\n\u2022 Claim 2: their objective function helps the generativemodel disentangle the latent variables into causal and non-causal factors for the classifier. Their approach is sufficient for any black-box classifier that can provide class gradients and probabilities with respect to the classifier input\n\u2022 Claim 3: the resulting explanations show what is important to the accompanying classifier\n\u2022 Claim 4: the learned representation may also be used to explain counterfactual explanations as it incorporates both generative and causal modelling\nThe standard of reproducibility demands that machine learning methods be routinely evaluated on the verifiability of their results. The following additional metrics (properties) will be used as measurement of reproducibility:\n\u2022 Portability: themodelling domains and types ofmachine learningmodels that can benefit from their framework\n\u2022 Expressive Power: the explanation structure that can be produced by their framework\n\u2022 Algorithmic Complexity: the computational complexity of their algorithm 2Since, the authors have responded to suggestions and rewritten their code base substantially. 3https://github.com/shin-ee-chen/UvA_FACT_2021\nReScience C 7.2 (#23) \u2013 Verhoeven et al. 2021 3\n\u2022 Data Fidelity: the degree of precision of the datawill range from low tohighfidelity. In cases where high-faithfulness data to train the model are not necessary, lowfaithfulness data often may be used. The small amount of usable data will greatly influence the model s\u0313 ability to yield accurate estimates.\n3 Project Description\nAs stated, we evaluate the performance of the method proposed in paper[3] as a causal post-hoc explainer of black-box classifiers by reproducing the code necessary. At the beginning of this project, the code provided by the authors was not in a usable state. Hence, we decided to reimplement the code for the necessary architectures and conduct experiments with the implementation details provided in the paper. By the time we finished our implementation, the project repository had been updated to be able to reproduce the initial claims in the paper. Beyond just re-implementing already existing code, two extensions were considered. First, the paper suggests a technique for selecting theK, L and \u03bb hyperparameters used to train the generative model (Algorithm 1). However, no implementation of this algorithm is present in the authors\u02bc code. To test its validity and because of the tedious and time-consuming nature of manual hyper-parameter search, we implemented the automated algorithm using reasonable assumptions. Second, in an effort to verify the robustness of this method, similar experiments for image classification were also conducted using a textual domain. Compared to the simple image benchmark datasets used in the initial experiments, text classification and generation are considerably more complex. These models will be used to test the scalability of the proposed framework.\n4 Methodology\nThe Generative Causal Explainer (GCE), is at its core a generative model with awareness of a discriminative black-box classifier. For the generative model, this paper exclusively uses the Variational Auto-Encoder (VAE) [4]. While the VAE allows expression of a dataset in terms of a low-dimensional posterior distribution, the addition of the classifier allows for disentangling the proposed latent space into variable subsets that causally influence decisions of the classifier and those that capture superfluous variance. The former subset is denoted \u03b1, whereas the latter is denoted \u03b2, having cardinalitiesK and L respectively. The goal of this modelling framework is to learn a generative mapping g : (\u03b1, \u03b2) \u2192 X that further satisfies the following criteria: p(g(\u03b1, \u03b2)) \u2248 p(X), the factors (\u03b1, \u03b2) are statistically independent and \u03b1 has strong clausal influence on the classifier s\u0313 output Y . The proposed objective function of this framework is thus,\nargmax g\u2208G C(\u03b1, Y ) + \u03bb \u00b7 D(p(g(\u03b1, \u03b2)), p(X)) (1)\nwhere g is theGCEmodel that satisfies the constraints from the set of possible generative models G, C(\u03b1, Y ) is a metric that quantifies the causal influence of \u03b1 on Y and D is a variational lower bound thatmeasures the proximity of p(g(\u03b1, \u03b2)) to p(X). The inclusion of the D(p(g(\u03b1, \u03b2)), p(X)) is necessary to ensure the generated explanations remain in, and ideally closely approximate, the data distribution. While there are several candidates for a causal influence metric, the original authors opted for the information theoretic motivated mutual information (MI). O\u02bcShaughnessy et al. offer several reasons for choosingMI, including its compatibility with deep neural networks, its ability to quantify indirect causal links between the GCE s\u0313 latent space and\nReScience C 7.2 (#23) \u2013 Verhoeven et al. 2021 4\nthe classifier, and its equivalence to \u02bbinformation flow\u02bc in the proposed causal model when considering do-calculus. Thus, in the above provided loss function,\nC(\u03b1, Y ) = I(\u03b1;Y ) = E\u03b1,Y [ log p(\u03b1, Y )\np(\u03b1)p(Y )\n] , (2)\nwhere I(\u03b1;Y ) is the aforementionedMI between the causal factors and the classifier. No closed form solution computation for Eq. 2is provided. Rather, aMonte-Carlo estimator is employed, using data samples drawn from the GCE posterior and their classifications by the accompanying black-box classifier. For detailed explanation of the underlying method, we direct the reader to Appendix D of [3]. Note that this estimation method requires passing drawn samples through both the GCE s\u0313 decoder network and the classifier, and for low variance estimates of I(\u03b1;Y ), numerous estimates are required. As such, estimating this quantity at every training step is computationally expensive, especially when considering the cost of a vanilla VAE.\n4.1 GCE Architectures As VAEs do not explicitly limit the architectures used in the encoder and decoder networks, much like the classifiers in question, they can make use of the same inductive knowledge encoded into the black-box classifiers. Thus, for image classification, given the performance of convolutional neural networks, a similar set of networks can be considered for the GCE. Naturally, while replicating and unspecified, the same architectures as used by O\u02bcShaughnessy et al. are used here as well. The image classification datasets used (see Sec. 5) are benchmark datasets of low complexity. Hence, both the classification and generative models were limited to shallow neural networks. The classifiers consisted of 2 ReLU activated convolutional layers fed into a max-pooling layer before 2 ReLU activated linear classification layers. Drop-out was present prior to either linear layer. In all instances, this sufficed for achieving near perfect accuracy. Both the encoder and decoder used 3 layers of convolution (transposed for the decoder), with additional linear layers for converting the feature maps. Specifics for the models used are given in Table 3. For text classification, the architecture is shown in Figure 4. The core architecture used was the Long Short-Term Memory (LSTM) network [7]. The classifier consisted of a bidirectional LSTM, whose hidden states at every time-step were concatenated and fed into a 3 successive convolution/ReLU/max-pooling blocks, before being projected into classification nodes via a linear layer. The embeddings used came from the 840b token Common Crawl pre-trained 300 dimensional GloVe vectors, limited to the vocabulary present in the SST dataset4. Ultimately, this model achieves 84% accuracy on the binary SST classification task. The encoder and decoder nets used a common VAE generation architecture [4], consisting of single layer LSTM, with embeddings not using pre-initialised weights. The hidden and cell states at the last time-step were used for predicting the input-dependent posterior statistics, with the posterior samples being fed into the decoder as the initial hidden and cell states, while also being appended to every embedding vector. Posterior collapse, a situation where the decoder network essentially ignores the encoder output, proved a serious problem for text generation. To overcome this issue, an aggressive training regiment was used [8]. Here, the encoder network is trained until convergence before updating the decoder network, resulting in stable and informative signals for the decoder network. This method of training is necessary for the first few epochs, but quickly ameliorates the situation and allows for regular VAE training to continue. However, given the sheer computational expense of using aggressive training, combining this withMC-estimation of I(\u03b1;Y )would quickly prove intractable. As such,\n4Available at https://gist.githubusercontent.com/bastings/b094de2813da58056a05e8e7950d4ad1/raw/ 3fbd3976199c2b88de2ae62afc0ecc6f15e6f7ce/glove.840B.300d.sst.txt\nReScience C 7.2 (#23) \u2013 Verhoeven et al. 2021 5\nboth the classifier and generativemodel were first trained disjoint, resulting in a regular VAE text-generator, before attempting to fine-tune into a functioning GCE.\n4.2 Implementation Upon start of this project, the original authors\u02bc official code repositorywasnon-functioning. Hence, all models were implemented from scratch using the descriptions and guidelines provided in the original paper. The only exception to this was the MC-estimation for information flow, although this was further optimised during the project. We exclusively used PyTorch, PyTorch-Text and PyTorch-Lightning for implementation, and all models were trained on Nvidia GeForce GTX 1080 GPUs provided by Surfsaras\u0313 LISA cluster computing service.\n5 Experimental Setup and Code\n5.1 Datasets Experiments using image classifiers are conducted on the traditionalMNISThand-written digits [9] and the newer Fashion MNIST (fMNIST) datasets [10]. The official training set of traditional MNIST was split into training and validation subsets of 50,000 and 10,000 images, respectively. The test set remained the same as the original dataset, composed of 10,000 images. Only the images labelled \u02bb3\u02bc or \u02bb8\u02bc were used to train the binary 3/8 classifier, whereas images labelled \u02bb1 ,\u0313 \u02bb4\u02bc and \u02bb9\u02bc were selected to train the 1/4/9 classifier. For fMNIST, the training set remains the same as the original dataset containing 60,000 images. The test set is divided into a validation set and a test set, containing 6,000 and 4,000 images, respectively. The t-shirt, dress, and coat images, labelled \u02bb0 ,\u0313\u03123 ,\u0313 and \u02bb4 ,\u0313 were used to train the 0/3/4 classifier. Both traditional MNIST and fMNIST were limited to samples with the labels of interest. All images were scaled to size 28 \u00d7 28. In both datasets, the train/validation/test splits was done using the file indices. Experiments conducted using text classification used the Stanford sentiment treebank [11] movie reviews corpus. Here the officially recommended train/validation/test splits were used. Rather than using the 5-class fine-grained classification, only positive and negative reviews were used, with the \u02bbvery-\u02bc classes being converted to their less polar alternatives.\n5.2 Hyperparameters In the reproduction experiments, hyperparameters are set to be the same as the original paper. The lists of hyperparameters of CNN classifier and GCE model can be found in\nReScience C 7.2 (#23) \u2013 Verhoeven et al. 2021 6\nTable 4 and Table 5, see Appendix 8. Beyond just the values provided, however, Algorithm 1 from the original paper was also implemented to conduct a hyperparameter search for K, L and \u03bb. This procedure was not rigorously defined in the original paper, using terms that are left open for interpretation, such as \u201dplateaus\u201d or \u201dapproaches\u201d. Due to this, some assumptions were made in the process:\n\u2022 \u201cPlateauing\u201d was defined as the value in question achieving a local optimum, with the next iteration reversing its trend.\n\u2022 \u201cApproaching the value from step 1\u201d was defined as either coming within a certain percentage threshold of the target value, or being closer to the target value than the next iteration.\n\u2022 D and C were defined as loss values subject to minimization.\nThis technique requires three parameters to be chosen:\n1. \u03be: a factor that dictates how close to theD obtained in step 1 a value must be to be considered as having approached D.\n2. \u03bb0: the value of \u03bb to start with.\n3. \u03ba: the factor by which to increase \u03bb.\nIn our hyperparameter search experiments, we use \u03be = 0.05, \u03bb0 = 10\u22123 and \u03ba = 100.5.\n5.3 Model Training In reproduction experiments, we trained a GCE model to generate explanation factors for image classifier outputs. The CNN classifier was trained for image recognition task using SGD as optimizer. The network architecture is shown in Table 2 and the hyperparamters settings are listed in Table 5. The GCE model is trained to maximize the objective 1 with Adam optimizer. We use the values of K, L and \u03bb suggested in the original paper. Hyperparameter details can be found in Table 4.\n6 Results\n6.1 Results reproducing original paper Using the GCE model described in Section 4.1, explanations for the CNN classifiers trained on MNIST and fMNIST datasets were found. The latent factors \u03b1 and \u03b2 are visualized in Figure 1, showing exactly how g(\u03b1, \u03b2) and the classifier output change as the\nReScience C 7.2 (#23) \u2013 Verhoeven et al. 2021 7\nlatent factors are modified for the 3-8 classifier. One can observe that \u03b11 influences the features that separate the digits 3 and 8 (the classifier s\u0313 output being given by the colour surrounding the digits) while retaining stylistic features unrelated to the classifier such as skew and thickness. By contrast, non-causal factors \u03b2i controls features irrelevant to classifier outputs. As shown in Figure 1(b-d), changing \u03b2i leads to stylistic changes of digits but does not affect classifier predictions. By visualizing high-resolution latent factor sweeps in Figure 2, the GCEmodel can assist a practitioner in identifying important data features for classification results. As shown in the first row from the top in Figure2, the digits \u02bb4\u02bc smoothly transition into \u02bb9\u02bc by completing the loop of the digit \u02bb9\u02bc while the digit stem remains fixed. Finally, the \u02bb9\u02bc digit gradually transitions to a \u02bb1 .\u0313 To verify the causal influence of theGCEon the classifier, the informationflow is studied, along with an ablation study of individual factors on classifier performance. Figure 3(a) shows the information flow from\u03b1 factors to Y is highwhile the information flow from \u03b2 factors to Y is low. For the ablation study, we delete individual data points from the data by fixing individual latent factors in each validation data set to different random values taken from the prior N(0, 1). This decrease in the precision of the classification is seen in Figure 3 (b). Note that modifying aspects influenced by causal factors degrades the accuracy of the classifier substantially, whereas elimination of non-causal aspects only has amarginal effect on the accuracy of the classifier. In the original paper, the ablation study was only implemented for 0/3/4 classifier with FMNIST dataset. In addition, we also plot the information flow and accuracy s\u0313 plot for MNIST dataset (in Figure 9). Our implementation reproduces the results in the original paper and supports the authors\u02bc claim that their method is able to separate causal and non-causal factors of a classifier. The learned latent factors can then be applied to explain classification decisions of a classifier.\nReScience C 7.2 (#23) \u2013 Verhoeven et al. 2021 8\nGiven the results presented above, and their proximity to those presented in the original paper, we tentatively verify all claims presented in Sec. 2. The GCE model produces high-quality examples that seem to align with the classifier s\u0313 internal decisionmaking process. Furthermore, by using interventions in the GCE s\u0313 posterior, information regarding features important to the black-box classifier were made apparent. Such a framework also clearly supported the use of counterfactuals, with alterations in the causal factors seeing the class change, and the stylistic interpretation of the produced examples remaining unaltered. Lastly, the computation of information flow to individual factors and the performed ablation study (now extended to all initial experiment domains), clearly show the success of mutual information in disentangling the GCE s\u0313 latent space into causal and non-causal factors. However, upon implementing GCEs for simple classification models, the scalability of the proposed framework can already be drawn into question. As mentioned in 4.1, the introduction of aMC-estimated quantity likemutual information has significant impact on the computational expense required for training, essentially forcing significantly more passes of data through the decoder and classifier networks for a single weight update. Even for the relatively sparse CNN-basedGCE and classifier, the suggested number of \u03b1 and \u03b2 estimates in conjunction with current implementation, implied 2500 additional forward passes for a single backward pass. All our experiments indicated this being the bottleneck of the modelling pipeline. Future research into optimising this process, for example by ensuring lower variance estimates or mixing of new sample estimates with older generations, could prove valuable in extending the original research. The issues with computational efficiency were strongly exacerbated by the requirement of a more complex generative model for the text domain. In fact, given the use of an al-\nReScience C 7.2 (#23) \u2013 Verhoeven et al. 2021 9\nternative training regiment, incorporating information flow to induce causal disentanglement would have made training until convergence virtually intractable. While the eventual failure to fine-tune from a functioning language generation model to a GCE could be an artefact of the pathologies plaguing text generation using auto-regressive architectures, it also speaks to the potential of portability for this framework. Using a classifier to produce interpretable understanding of the latent space in such a language generation model could prove tremendously interesting, allowing for a causal framework similar to the work of [12]. Furthermore, being able to fine-tune pre-trained VAEs into GCEs would provide the suggested framework far more flexibility, essentially addressing the computational efficiency issues mentioned before.\n7.1 Shortcomings of the original paper The greatest shortcoming found in the original paper is the failure to address the scalability problem of the approach, along with the lack of rigour when describing the hyperparameter selection technique.\n7.2 Reflection: What was easy? What was difficult? It was not trivial to re-implement the proposed method because the specifics and some details required for the implementation do not appear in the paper. However, we still managed to reproduce the results in their paper. In order to extend the algorithm to another domain, code modifications were required. No instruction is given in their original repository on how this could be done, which makes it difficult to extend this framework or apply it to other domains without reading their paper and code in depth. Having access to the (updated) codebase was quite helpful however, as it includes some implementation specifics that are not mentioned in their paper, which we made use of as a source of reference when implementing and debugging our own repository.\n8 Conclusion\nWhile some issues and discrepancies were encountered while re-implementing, ultimately we conclude that the original paper combined with the official repository are enough to validate the claims of [3]. Results were comparable, and indeed led to highquality explanations. However, while the central idea is elegant and is now proven to work, we bring into doubt the extensibility of their approach. Due to the computational expense required, it is likely that the GCEmodels introduced will only function, in their current implementation, for small datasets and simple classifiers. Finally, this project confirms just how difficult it is to make implementations of AI transparent and reproducible."}, {"heading": "APPENDIX", "text": "A. Text Explainer\nFigure 4. Text-VAE architecture.\nThe text classifier used a bi-directional LSTM of 256 units. As mentioned the embeddings had been initialised using pre-trained GLoVe embeddings. All hidden states were then concatenated into a single feature map. Blocks of convolutional layers (32 filters of kernel size 3), ReLU activations and max-pooling (kernel size of 2 and stide of 2) were applied to reduce these feature map sizes. Ultimately all feature maps were projected down into the required number of classes using a linear layer. Adam was used as the optimizer, using an initial learning rate of 1e-3 and decaying this by a factor of 0.85 at every epoch. The text-VAE follows the proposed structure given in [8] as closely as possible. The embeddings used 512 dimensions and the single layer LSTM 1024. The last hidden and cell states were projected down to 32 latent dimensions. Aggressive training was used until the mutual information between the latent variables and the encoder input stabilised (typically 5 epochs). For the inner loop in the aggressive training schedule, 250 iterations were allowed, after which it was assumed the encoder had converged. The\nReScience C 7.2 (#23) \u2013 Verhoeven et al. 2021 11\nKullback-Leibler divergence was weighted using a linear annealing schedule between the first 10 epochs. Fine-tuning was attempted using \u03bb = 1 \u2212 3 and lr = 1e \u2212 3 using the standard GCE training scheme. These hyperparameters empirically proved to induce lowest lowest causal loss after a gridsearch. Ultimately, no model improved it s\u0313 causal loss after more than 3 epochs."}, {"heading": "B. Neural network architectures", "text": "Table 2. Network architecture for CNN classifier\nNetwork architecture for CNN classifier Input (28\u00d728)\nConv2 (32 channels, 3\u00d73 kernels, stride 1, pad 0) ReLU\nConv2 (64 channels, 3\u00d73 kernels, stride 1, pad 0) ReLU MaxPool (2\u00d72 kernel) Dropout (p = 0.5) Linear (128 units)\nReLU Dropout (p = 0.5)\nLinear (M units) Softmax\nTable 3. GCE network architecture used for MNIST and fMNIST experiments\nDataset MNIST MNIST FMNIST\nclasses 3,8 1,4,9 0,3,4 K 1 2 2 L 7 2 4 \u03bb 0.05 0.1 0.05 steps 8000 8000 8000 lr 5\u00d7 10\u22124 5\u00d7 10\u22124 10\u22124 N\u03b1 100 75 100 N\u03b2 25 25 25 batch size 64 64 32\nTable 5. Hyperparameter settings for CNN classifier\nDataset MNIST MNIST FMNIST\nclasses 3,8 1,4,9 0,3,4 lr 0.1 0.1 0.1 mom. 0.5 0.5 0.5 batch size 64 64 64 epochs 20 30 50\nReScience C 7.2 (#23) \u2013 Verhoeven et al. 2021 12"}, {"heading": "C. Additional Reproduction results", "text": "ReScience C 7.2 (#23) \u2013 Verhoeven et al. 2021 13\nReScience C 7.2 (#23) \u2013 Verhoeven et al. 2021 14"}], "title": "[Re] Replication Study of \u2019Generative causal explanations of black-box classifiers\u2019", "year": 2021}