{"abstractText": "The authors of the paper, which we reproduced, introduce a method that is claimed to improve the isotropy (a measure of uniformity) of the space of Contextual Word Repre\u2010 sentations (CWRs), outputted bymodels such as BERT or GPT\u20102. As a result, the method would mitigate the problem of very high correlation between arbitrary embeddings of such models. Additionally, the method is claimed to remove some syntactic informa\u2010 tion embedded in CWRs, resulting in better performance on semantic NLP tasks. To verify these claims, we reproduce all experiments described in the paper.", "authors": [{"affiliations": [], "name": "Benjamin D\u017eubur"}, {"affiliations": [], "name": "Koustuv Sinha"}, {"affiliations": [], "name": "Sharath Chandra Raparthy"}], "id": "SP:a14d319d21519bd3f526914ffdda768e8326b2e8", "references": [{"authors": ["J. Mu", "P. Viswanath"], "title": "All-but-the-Top: Simple and Effective Postprocessing for Word Representations.", "venue": "In: International Conference on Learning Representations", "year": 2018}, {"authors": ["J. Devlin", "M.-W. Chang", "K. Lee", "K. Toutanova"], "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.", "venue": "DOI: 10.18653/v1/N191423. URL: https://aclanthology.org/N19-1423", "year": 2019}, {"authors": ["A. Radford", "J. Wu", "R. Child", "D. Luan", "D. Amodei", "I. Sutskever"], "title": "Language Models are Unsupervised Multitask Learners.", "year": 2019}, {"authors": ["Y. Liu", "M. Ott", "N. Goyal", "J. Du", "M. Joshi", "D. Chen", "O. Levy", "M. Lewis", "L. Zettlemoyer", "V. Stoyanov"], "title": "RoBERTa: A Robustly Optimized BERT", "venue": "Pretraining Approach", "year": 2019}, {"authors": ["J. Gao", "D. He", "X. Tan", "T. Qin", "L. Wang", "T. Liu"], "title": "Representation Degeneration Problem in Training Natural Language Generation Models.", "venue": "CoRR abs/1907.12009", "year": 2019}, {"authors": ["S. Rajaee", "M.T. Pilehvar"], "title": "A Cluster-based Approach for Improving Isotropy in Contextual Embedding Space.", "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers). Online: Association for Computational Linguistics,", "year": 2021}, {"authors": ["A. Wang", "A. Singh", "J. Michael", "F. Hill", "O. Levy", "S. Bowman"], "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding.", "venue": "Proceedings of the 2018 EMNLPWorkshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP. Brussels, Belgium: Association for Computational Linguistics,", "year": 2018}, {"authors": ["A. Wang", "Y. Pruksachatkun", "N. Nangia", "A. Singh", "J. Michael", "F. Hill", "O. Levy", "S.R. Bowman"], "title": "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems.", "venue": "CoRR abs/1905.00537", "year": 2019}, {"authors": ["E. Agirre", "D. Cer", "M. Diab", "A. Gonzalez-Agirre"], "title": "SemEval-2012 Task 6: A Pilot on Semantic Textual Similarity.", "venue": "*SEM", "year": 2012}, {"authors": ["E. Agirre", "D. Cer", "M. Diab", "A. Gonzalez-Agirre", "W. Guo"], "title": "SEM 2013 shared task: Semantic Textual Similarity.", "year": 2013}, {"authors": ["E. Agirre", "C. Banea", "C. Cardie", "D. Cer", "M. Diab", "A. Gonzalez-Agirre", "W. Guo", "R. Mihalcea", "G. Rigau", "J. Wiebe"], "title": "SemEval-2014 Task 10: Multilingual Semantic Textual Similarity.", "venue": "Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval", "year": 2014}, {"authors": ["E. Agirre"], "title": "SemEval-2015 Task 2: Semantic Textual Similarity, English, Spanish and Pilot on Interpretability.", "venue": "Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval", "year": 2015}, {"authors": ["E. Agirre", "C. Banea", "D. Cer", "M. Diab", "A. Gonzalez-Agirre", "R. Mihalcea", "G. Rigau", "J. Wiebe"], "title": "SemEval-2016 Task 1: Semantic Textual Similarity, Monolingual and Cross-Lingual Evaluation.", "venue": "Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016). San Diego, California: Association for Computational Linguistics,", "year": 2016}, {"authors": ["M. Marelli", "S. Menini", "M. Baroni", "L. Bentivogli", "R. Bernardi", "R. Zamparelli"], "title": "A SICK cure for the evaluation of compositional distributional semantic models.", "venue": "LREC", "year": 2014}, {"authors": ["S. Ravfogel", "Y. Elazar", "J. Goldberger", "Y. Goldberg"], "title": "Unsupervised Distillation of Syntactic Information from Contextualized Word Representations.", "venue": "Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP. Online: Association for Computational Linguistics,", "year": 2020}, {"authors": ["G.A. Miller", "C. Leacock", "R. Tengi", "R.T. Bunker"], "title": "A Semantic Concordance.", "venue": "New Jersey, March", "year": 1993}], "sections": [{"text": "R E S C I E N C E C Replication / ML Reproducibility Challenge 2021\n[Re] A Cluster-based Approach for Improving Isotropy in"}, {"heading": "Contextual Embedding Space", "text": ""}, {"heading": "Benjamin D\u017eubur1, ID", "text": "1University of Ljubljana, Ljubljana, Slovenia\nEdited by Koustuv Sinha,\nSharath Chandra Raparthy\nReviewed by Anonymous Reviewers\nReceived 04 February 2022\nPublished 23 May 2022\nDOI 10.5281/zenodo.6574649\n1 Reproducibility Summary"}, {"heading": "Scope of Reproducibility", "text": "The authors of the paper, which we reproduced, introduce a method that is claimed to improve the isotropy (a measure of uniformity) of the space of Contextual Word Repre\u2010 sentations (CWRs), outputted bymodels such as BERT or GPT\u20102. As a result, the method would mitigate the problem of very high correlation between arbitrary embeddings of such models. Additionally, the method is claimed to remove some syntactic informa\u2010 tion embedded in CWRs, resulting in better performance on semantic NLP tasks. To verify these claims, we reproduce all experiments described in the paper."}, {"heading": "Methodology", "text": "We used the authors\u2019 Python implementation of the proposed cluster\u2010based method, which we verified against our own implementation based on the description in the pa\u2010 per. We re\u2010implemented the global method based on the paper fromMu and Viswanath [1], which the cluster\u2010based method was primarily compared with. Additionally, we re\u2010implemented all of the experiments based on descriptions in the paper and our com\u2010 munication with the authors."}, {"heading": "Results", "text": "We found that the cluster\u2010based method does indeed consistently noticeably increase the isotropy of a set of CWRs over the global method. However, when it comes to se\u2010 mantic tasks, we found that the cluster\u2010based method performs better than the global method in some and worse in other tasks, or that the improvements are within margin of error. Additionally, the results of one side experiment, which analyzes the structural information of CWRs, also contradict the authors\u2019 findings for the GPT\u20102 model."}, {"heading": "What was easy", "text": "The described methods were easy to understand and implement, as they rely on PCA and K\u2010Means clustering.\nCopyright \u00a9 2022 B. D\u017eubur, released under a Creative Commons Attribution 4.0 International license. Correspondence should be addressed to Benjamin D\u017eubur (bd5830@student.uni-lj.si) The authors have declared that no competing interests exist. Code is available at https://github.com/Benidzu/isotropy_reproduction. \u2013 SWH swh:1:dir:407d517fb5299301bcfc7f8aa461a4c3bf7c36b0. Open peer review is available at https://openreview.net/forum?id=rxWeB3zQ2CY.\nReScience C 8.2 (#12) \u2013 D\u017eubur 2022 1"}, {"heading": "What was difficult", "text": "There were many ambiguities in the paper: which splits of data were used, the proce\u2010 dures of the experiments were not described in detail, some hyperparameters values were not disclosed. Additionally, running the approach on big datasets was too compu\u2010 tationally expensive. There was an unhandled edge case in the authors\u2019 code, causing the method to fail in rare cases. Some results had to be submitted online, where there is a monthly limit of submissions, causing delays.\nCommunication with original authors Weexchangedmany e\u2010mailswith the authors, whichwere very responsive andhelpful in describing themissing information required for reproduction. In the end, we still could not completely identify the sources of some remaining discrepancies in the results, even after ensuring the data, preprocessing and some other implementation details were the same.\nReScience C 8.2 (#12) \u2013 D\u017eubur 2022 2\n2 Introduction\nEmbeddings from popular contextual NLPmodels such as BERT [2], GPT\u20102 [3], RoBERTa [4], etc. suffer from the so\u2010called representation degeneration problem [5], where the individual tokens\u2019 embeddings form an anisotropic cone\u2010like shape in the embedding space. This means that even unrelated words can have excessively positive correlations. Methods which study and attempt to improve the isotropy (a measure of uniformity) of the space on a global level (e.g. [1]) have been predominantly used so far to tackle this problem. However, due to the clustered structure of these Contextual Word Representa\u2010 tions (CWRs), the authors of the chosen paper [6] propose a local, cluster\u2010basedmethod, which could further improve on the existing global approaches. Apart from further improving isotropy, the method supposedly also removes some local structural and syntactic information within the clusters, improving the CWRs perfor\u2010 mance on semantic tasks.\n3 Scope of reproducibility\nThroughout the paper, the authors use contextual embeddings of three models to sup\u2010 port their claims: BERT, RoBERTa and GPT\u20102. Various datasets are used to generate these contextual embeddings, which are then enhanced with the proposed method, evaulated and used to support claims about the performance of the method. Specifi\u2010 cally, these claims are:\n\u2022 Claim 1: The cluster\u2010based method outperforms the baseline and global method, in all cases in terms of isotropy of CWRs as well as in almost all cases in terms of Spearmancorrelationperformance, on 7 Semantic Textual Similarity (STS) datasets.\n\u2022 Claim 2: A wide and shallow Multi\u2010Layer Perceptron (MLP) performs the best in terms of accuracy on all 6 chosen binary classification tasks from the GLUE [7] and SuperGLUE [8] benchmarks, when trained on BERT emebeddings which were enhanced by the cluster\u2010based approach.\n\u2022 Claim 3: A MLP described as in Claim 2 also converges to an optimum in fewer epochs, when the embeddings are enhanced by the cluster\u2010based approach.\n\u2022 Claim 4: Removing dominant directions from CWRs of punctuations and stop words in sentences with the same syntactic structure (same group) results in fewer nearest neighbors of the CWRs being from the same group, as syntactic informa\u2010 tion is discarded.\n\u2022 Claim 5: The cluster\u2010based approach brings together verbs which have the same meaning (sense) but different tense as seen in the SemCor corpus, by decreasing the average euclidean distance between their CWRs, relative to the distance be\u2010 tween verbs in the same tense but with a different sense.\nIn our reproduction, we verify all the listed claims by reproducing all the related exper\u2010 iments. Claims 1 and 2 are the most important ones as they directly address the perfor\u2010 mance of the cluster\u2010based method, while Claims 3, 4 and 5 are essentially attempted explanations of different side effects of the proposed method. In addition to these claims, the authors analyze the effect of the number of clusters in the K\u2010Means algorithm on isotropy as well as evaluate the layer\u2010wise isotropy of the contextual models. We have also reproduced these, purely statistical experiments for the sake of completeness of our reproduction.\nReScience C 8.2 (#12) \u2013 D\u017eubur 2022 3\n4 Methodology\nThe paper referenced a Github repository 1, in which we found a single Jupyter note\u2010 book with the implementation of the cluster\u2010based method, the isotropy metric, as well as an example of evaluating the isotropy and Spearman correlation performance on the STS\u2010B dataset. We first re\u2010implemented the cluster\u2010based method and verified that it works the same way \u2013 however in the end we used the authors implementation due to its slightly better runtime. There was an unhandled edge case in the original implemen\u2010 tation however \u2013 if fewer embeddings belonged to some cluster than the number of PCs to be removed, the original implementation would result in an out\u2010of\u2010bounds exception. We fixed this by repeating the clustering step until each cluster was sufficiently repre\u2010 sented. The method uses the Scipy library for K\u2010Means clustering and ScikitLearn for PCA. As the global method is simply a special case of the cluster\u2010based method with the num\u2010 ber of clusters k = 1, its re\u2010implementation was trivial. Wedidhowever have to re\u2010implement all of the experiments only from their descriptions in the paper and based on the help we got from our correspondence with the authors. We did not require a GPU for any of our experiments.\n4.1 Model descriptions & hyperparameters For the contextualmodels, we used the Transformers library and the default pre\u2010trained weightswereused (specifically the casings bert-base-uncased, gpt2 and roberta-base). These models all output 768\u2010dimensional embeddings at each of their 12 layers. As reported in the original paper, the hyperparameters of the global and local, cluster\u2010 based approach were set for each model separately, as seen in Table 1. These values were used for all experiments.\nWhen it comes to GLUE and SuperGLUE binary classification tasks, the contextual em\u2010 beddings were used to train a fully\u2010connected MLP. It\u2019s structure remains the same across all tasks, using the hyperparameters communicated to us by the authors. Specifi\u2010 cally, for a single data sample (which is either a sentence or a pair of sentences), we only consider the first 64 tokens\u2019 representations, which we flatten into a vector of length 64\u00d7 768, which represents our input layer. The next layer is a 100\u2010dimensional hidden layer with ReLU activation, followed by the output layer \u2013 a single neuron with sigmoid activation. The MLP is trained using binary cross\u2010entropy loss and uses the Adam opti\u2010 mizer with step size 0.005, for a maximum of 10 epochs. The reported results are based on the model which achieves the best validation set score. For the experiment where we analyze the CWRs of punctuations and stop words, we use the K\u2010nearest\u2010neighbor implementation by ScikitLearn with k = 6, which is exactly the number of possible neighbors from the same structural group (we only use the first CWR of the respective punctuation or stop word in a sentence). We then calculate the relative part of nearest neighbors belonging to the same group for each individual embedding and average the results. Note that each stop word or punctuation type (e.g. comma) is analyzed separately and the search is performed only amongst CWRs of the same type.\n1https://github.com/Sara-Rajaee/clusterbased_isotropy_enhancement/\nReScience C 8.2 (#12) \u2013 D\u017eubur 2022 4\nLastly, for the verb tense experiment, we consider verbswithmultiplemeanings (senses) and in two tenses \u2013 present simple and past simple (e.g. \u201dsay\u201d and \u201dsaid\u201d correspond to the same verb in different tenses by our definition). Then, for each verb, we calculate all possible euclidean distances between representations of same tense and samemeaning, same tense and different meaning, different tense and same meaning. We then finally average across all distances at the lowest level of hierarchy. We repeat the calculation for the representations enhanced by the cluster\u2010based method.\n4.2 Datasets For the main experiment on which Claim 1 in Section 3 is based, 7 Semantic Textual Similarity (STS) datasets were used. The STS\u20102012 to STS\u20102016 [9, 10, 11, 12, 13] as well as STS\u2010B are available at: https://ixa2.si.ehu.eus/stswiki/index.php/Main_Page , while the SICK\u2010 R [14] dataset is available at: https://marcobaroni.org/composes/sick.html. Individual data samples of these datasets are comprised of two sentences, and their semantic similari\u2010 ty/relatedness score, which is a real value on the scale from 0 to 5. In Table 2, the total number of data samples for each dataset after filtering is seen. Note that only the En\u2010 glish test splits were used, as in the original paper. Four of the seven datasets had some badly encoded samples (nomore than 10), whichwe simply discarded, after preliminary testing which showed that they do not noticeably affect the results. The two sentences of each sample were sent through the contextual models separately.\nFor the classification experiment on which Claim 2 in Section 3 is based, a selection of tasks (datasets) from GLUE [7] (https://gluebenchmark.com/) and SuperGLUE [8] (https:// super.gluebenchmark.com/) were used. In some cases, data sampleswere composed of pairs of sentences, while in others, a single sentence was given. In the first case, the pairs of sentences were encoded together, by concatenating their tokens and adding special tokens in the following way: [CLS]<sentence1>[SEP]<sentence2>[SEP]. The embeddings of these special tokens were also considered by the MLP classifier. Note that for the purpose of this experiment, we first merged the train, validation and test splits before applying the global or local enhancement method, as did the authors originally. Due to the big size of SST\u20102 and BoolQ datasets, we had to limit the size of training and/or validation splits by randomsub\u2010sampling. The number of samples for each task are seen in Table 3. We found that 10745 \u00d7 64 was near the maximum number of embeddings that we could affoard to run PCA on, given our hardware. For the punctuation / stop word experiment, the authors provided a dataset based on Ravfogel et al. [15] (available at https://nlp.biu.ac.il/~ravfogs/resources/syntax_distillation/)which consists of 150000 groups of 6 sentences, where sentences from each group have the same syntactic structure but different semantics. For each of the tokens of interest sep\u2010 arately (\u201dthe\u201d, \u201dof\u201d, \u201d,\u201d and \u201d.\u201d), we randomly sampled 200 groups, where each group con\u2010 tained at least one appearance of the token per sentence. For the verb tense experiment, we used the SemCor corpus [16], available at http://web. eecs.umich.edu/~mihalcea/downloads.html#semcor. Out of over 30000 sentences, weused 11838\nReScience C 8.2 (#12) \u2013 D\u017eubur 2022 5\nof them, which contained the verbs wewere interested in. Specifically, these were verbs that appeared in present and past tense and also occurred in at least 2 different senses at least 10 times. The analysis of layer\u2010wise isotropy and the number of clusters in K\u2010Means is done on the STS\u2010B dev split.\n4.3 Experimental setup and code The code of our reproduction is available at https://github.com/Benidzu/isotropy_reproduction. The isotropy measure (as defined in the original paper), Spearman performance (which is just the Spearman coefficient multiplied by 100) and accuracy were the main metrics used to evaluate our experiments. In order to evaluate the uncertainty in some of the main results, we resorted to bootstrap as well estimation of variance across multiple re\u2010runs of procedures containing stochasticity (e.g. initial positions of centroids in K\u2010 Means, initial weights of MLP classifiers).\n4.4 Computational requirements The experiments were reproduced on a sytem with the 8\u2010core, 16\u2010thread Ryzen 3700x processor, 16GB of RAM and RTX3060Ti GPU (which was not explicitly used for any ex\u2010 periment). On a set of 30000 768\u2010dimensional embeddings, the global method ran for 12.5 seconds and the local, cluster\u2010based method for 14 seconds. On a bigger set of 200000 embed\u2010 dings, the global method ran for 98.9 seconds and the local method ran for 79.8 seconds. In addition, the local method requires a lot less memory at once, as it performs PCA for each cluster of embeddings separately. The training of MLP classifiers for the classification experiments required nomore than a minute on average.\n5 Results\nThe reproduced results support some of the claims of the original paper. Specifically, the cluster\u2010based method indeed consistently outperforms the global and baseline in terms of isotropy. However, when it comes to Spearman performance on Semantic Tex\u2010 tual Similarity tasks, the local method performs better than the global method on some datasets andworse on others. Similar is true for the classification tasks, where the differ\u2010 ence in performance is mostly within margin of error. Analyzing verb tense, the Claim 5 from Section 3 is fully supported by our reproduction, while some discrepancies are observed when it comes to Claim 4.\n5.1 Results reproducing original paper\nSemantic Textual Similarity experiment \u2014 In this section we address Claim 1 from Section 3. In Figure 1 we plot the Spearman correlation performance for each method, contextual model and STS dataset. Due to the random nature of K\u2010Means, we repeat the experi\u2010 ment with the local method 5 times. We plot the results for each of the five repetitions individually. Additionally, we report the averages of these five repetitions in Table 4. Compared to the numbers in Table 2 of the original paper, our results are slightly more pessimistic. Embeddings enhanced by the local method perform noticeably better than those, enhanced by the global method, on some datasets and worse on others. There are also many cases where the difference in performance is within margin of error. In Table 5 we report the isotropy values of CWRs from each of the STS datasets, for each contextual model and enhancement method. These results support the original results achieved by the authors, as seen in Table 6 of the original paper.\nReScience C 8.2 (#12) \u2013 D\u017eubur 2022 6\nGLUE & SuperGLUE classification tasks \u2014 In this section we address Claim 2 from Section 3. In Table 6 we report average scores (accuracy / Matthew\u2019s correlation) of the MLP classi\u2010 fier on the test set based on 5 repetitions. Each repetition, we re\u2010ran the corresponding embedding enhancement method and randomly re\u2010initialized and re\u2010trained the MLP, accounting for both sources of variance. It seems that the classifier trained on locally enhanced embeddings achieves the best scores on most of the tasks, however, due to the high uncertainty and small differences betweenmethods, we cannot confidently argue that onemethod is better than the other. Due to this uncertainty, our results do not fully support the original findings as seen in Table 3 in the paper.\nConvergence time \u2014 In this section we address Claim 3 from Section 3. In Figure 2, we plot the per\u2010epoch performance of the MLP for two SuperGLUE tasks on the validation split. Our results support the original claim, as the MLP converges to an optimum in only a few iterations when trained on enhanced embeddings, while the same does not hold for baseline embeddings.\nPunctuation and stop word experiment \u2014 In this section we address Claim 4 from Section 3. In Figure 3, we plot the percentage of nearest neighbors from the same structural (syn\u2010\nReScience C 8.2 (#12) \u2013 D\u017eubur 2022 7\ntactical) group, for baseline and enhanced embeddings. The results line up with the authors\u2019 results (Figure 3 in original paper) for BERT and RoBERTa embeddings, where the removal of dominant directions via the method decreases the percentage of neigh\u2010 bors from the same group. However, this does mostly not hold for GPT\u20102 embeddings in our reproduction.\nVerb tense experiment \u2014 In this section we address Claim 5 from Section 3. In Table 7, we report the results of the corresponding experiment, described in Section 4.1. The results support the claim, as they are very similar to authors\u2019 results in Table 4 of the original paper.\nAdditional isotropy analysis \u2014 In this last section, we report the reproduction results of the additional isotropy analysis of the contextual models\u2019 embeddings. The results, ana\u2010\nReScience C 8.2 (#12) \u2013 D\u017eubur 2022 8\nlyzing the impact of number of clusters in K\u2010Means and the layer\u2010wise isotropy of the contextual models are seen in Tables 8a and 8b respectively. Our results support the original results, as seen in Tables 1 and 5 in the original paper.\nIn general, many of the original authors\u2019 claims are supported by our experimentation. The achieved isotropy scores across the reproduced experiments are similar to the orig\u2010 inal ones, implying that the cluster\u2010based method is working as intended. However, even in situations with seemingly no randomness (extracting baseline embeddings of datasets and evaluating isotropy), we could not perfectly reproduce the original results. This might imply discrepancies on hardware\u2010level computation or due to different ver\u2010 sioning of used libraries (e.g. Transformers). Consequently, this perhaps implies that the local method is not robust enough to such variations, to consistently outperform the global method (e.g. in terms of Spearman coefficient performance on STS tasks), as originally claimed. Similarly, for the classification tasks, after our own re\u2010implementation, we found out that authors used Keras for theMLP classifier, while we used ScikitLearn (albeit with all hyperparameters set equivalently). This was another source of potential discrepancies, but the similar results reflect that this was not a real issue. Amore likely reason for some differences in this experiment might be the fact that, while the authors stated that they re\u2010trained the MLP multiple times before submitting and reporting the results of the best classifier (chosen by validation set performance), we opted for themore robust and\nReScience C 8.2 (#12) \u2013 D\u017eubur 2022 9\nless biased score estimation via averaging across multiple submissions and additionally estimating the errors of our estimates. When it comes to Claims 3 and 5 from Section 3, our results fully support these claims, although again, we are unable to get exactly the same numbers, perhaps due to the reasons listed above or due to minor differences in implementation. Finally, with the punctuation and stop word experiment, we were surprised by the fact that by removing local dominant directions of CWRs from the GPT\u20102 model, we actu\u2010 ally increased the percentage of neighbors from the same structural group. Since the percentage of nearest neighbors with the same syntactical structure was relatively low to begin with in this case (compared to BERT and RoBERTa), we believe the dominant directions carried mostly semantic information, and by removing them, the syntactical information in the embeddings became more dominant.\n6.1 Recommendations for further experimentation Unfortunately, due to various limitations and our budget, we could not afford much ad\u2010 ditional experimentation beyond the scope of the paper. However, during our analysis, we came up with some ideas and experiments, which could be further looked into. We list some of these ideas the following. Firstly, for the GLUE & SuperGLUE classification tasks, the authors first merge train and test splits and then run the embedding enhancement method and then train the MLP. In a practical scenario, where we would like to predict the class for a completely new data sample, repeating this whole process becomes computationally infeasible. Therefore, the following experimental procedure, where the learning step is performed only once (and updated on a less regular basis), could be evaluated and compared to the original one:\n1. Run the cluster\u2010based method on contextual embeddings of the training set. Save the centroids of each cluster in original space as well as its corresponding top prin\u2010 cipal components to be removed.\n2. Train the MLP on the enhanced embeddings.\n3. At prediction time (for test data), extract the contextual embeddings of the new data sample. For each CWR, enhance it by doing the following: assign it to the nearest cluster, based on the saved centroids in step 1, then subtract the centroid and remove the corresponding PCs.\n4. Pass the enhanced embeddings of the data sample to the MLP for prediction.\nOther additional ideas include experimentingwith differentMLP architectures, or some of the remaining GLUE / SuperGLUE tasks, namely COPA, QNLI, QQP, etc. Additionally, using a different clustering algorithm or distance measure could prove to be beneficial.\n6.2 What was easy The explanations of the methods and experiments in the original paper were easy to fol\u2010 low. The cluster\u2010based method relies on K\u2010Means clustering and PCA, both of which we were already familiar with. The code present in the referenced repository was therefore easy to understand.\n6.3 What was difficult Some key implementation details of various experiments and hyperparameters of al\u2010 gorithms were not disclosed in the original paper, making exact re\u2010implementation of the experiments more difficult. Even after receiving the necessary information, there\nReScience C 8.2 (#12) \u2013 D\u017eubur 2022 10\nwere discrepancies in results which could not be attributed to randomness, differences in data, or some differences in implementation (assuming authors used the published code). Due to some big datasets used in some experiments, we had to subsample the num\u2010 ber of data samples to be able to run the described algorithms. Our system would in some cases completely freeze due our CPU usage reaching 100% because of PCA com\u2010 putations. Additionally, extracting embeddings, re\u2010running themethodsmultiple times and performing expensive procedures such as bootstrap took a lot of time. The most time\u2010consuming step by far was estimating the performance and error of our estimates on GLUE and SuperGLUE classification tasks. In order to get test split results, one has to manually submit the predictions through the official website. This was an issue in our case due to the restrictions of submissions \u2013 a team is only allowed to make up to two submissions a day and six per month, which dragged out our collection of results.\n6.4 Communication with original authors Weexchangedmany e\u2010mailswith themain author of the paper, in order to enquire about various hyperparameters and other implementation details of each experiment and to ensure we set up our experiments the same way. The author was quite helpful and re\u2010 sponsive. Unfortunately, we had to accept that some discrepancies between our results would still be present (see Sections 6 and 6.3 for our comments on these discrepancies), after much time spent attempting to reduce them."}], "title": "[Re] A Cluster-based Approach for Improving Isotropy in Contextual Embedding Space", "year": 2022}