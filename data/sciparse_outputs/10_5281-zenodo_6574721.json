{"abstractText": "In the paper the authors propose a new evaluation that respects inter\u2010novel\u2010protein in\u2010 teractions, and also a new method, that significantly outperforms previous PPI meth\u2010 ods, especially under this new evaluation. Therefore we will first inspect if this kind of evaluation is objectively better, and secondly, we will try to reproduce the results of the proposed model in comparison with previous state\u2010of\u2010the\u2010art, PIPR [2].", "authors": [{"affiliations": [], "name": "Ur\u0161a Zrim\u0161ek"}, {"affiliations": [], "name": "Koustuv Sinha"}, {"affiliations": [], "name": "Sharath Chandra Raparthy"}], "id": "SP:d4b6722bb43e5ec8a7f903751883f9d4822e4042", "references": [{"authors": ["G. Lv", "Z. Hu", "Y. Bi", "S. Zhang"], "title": "Learning Unknown from Correlations: Graph Neural Network for Inter-novel-protein Interaction Prediction", "year": 2021}, {"authors": ["M. Chen", "C. Ju", "G. Zhou", "X. Chen", "T. Zhang", "K.-W. Chang", "C. Zaniolo", "W. Wang"], "title": "Multifaceted Protein-Protein Interaction Prediction Based on Siamese Residual RCNN.", "venue": "Bioinformatics", "year": 2019}, {"authors": ["Y. Gui", "R. Wang", "Y. Wei", "X. Wang"], "title": "DNN-PPI: A Large-Scale Prediction of Protein\u2013Protein Interactions Based on Deep Neural Networks.", "venue": "Journal of Biological Systems", "year": 2019}], "sections": [{"text": "R E S C I E N C E C"}, {"heading": "Replication / ML Reproducibility Challenge 2021", "text": "[Re] Learning Unknown from Correlations: Graph Neural"}, {"heading": "Network for Inter-novel-protein Interaction Prediction", "text": "Ur\u0161a Zrim\u0161ek1, ID 1University of Ljubljana, Faculty of Computer and Information Science, Ve\u010dna pot 13, 1000 Ljubljana, SI\nEdited by Koustuv Sinha,\nSharath Chandra Raparthy\nReviewed by Anonymous Reviewers\nReceived 04 February 2022\nPublished 23 May 2022\nDOI 10.5281/zenodo.6574721\n1 Reproducibility Summary\nThis is a report of reproducibility of paper [1], submited to ML Reproducibility Challenge 2021."}, {"heading": "Scope of Reproducibility", "text": "In the paper the authors propose a new evaluation that respects inter\u2010novel\u2010protein in\u2010 teractions, and also a new method, that significantly outperforms previous PPI meth\u2010 ods, especially under this new evaluation. Therefore we will first inspect if this kind of evaluation is objectively better, and secondly, we will try to reproduce the results of the proposed model in comparison with previous state\u2010of\u2010the\u2010art, PIPR [2]."}, {"heading": "Methodology", "text": "For the reproduction we used authors code, slightly changing the pipeline for autom\u2010 atization. We also used PIPR code, where we completely changed the pipeline, to be able to use it on the same datasets as GNN\u2010PPI, but used their function for building the model. The experiments were run on Nvidia Titan X GPU, using around 250 GPU hours altogether."}, {"heading": "Results", "text": "We reproduced the papers results within standard deviations of our repeated experi\u2010 ments. But in some cases, this still means there is a big difference between the perfor\u2010 mances, which is coming from different train\u2010test splits of the newly proposed split\u2010 ting schemes. Even with these discrepancies we still managed to (at least partially) confirm all authors claims. The proposed model GNN\u2010PPI performed better than PIPR overall and for inter\u2010novel\u2010protein interactions, evaluation on their proposed schemes predicted the generalization performance better, and their model is also robust for pre\u2010 dictions for newly discovered proteins \u2013 here our results were surprising, theywere even better when the network was built knowing fewer proteins.\nCopyright \u00a9 2022 U. Zrim\u0161ek, released under a Creative Commons Attribution 4.0 International license. Correspondence should be addressed to Ur\u0161a Zrim\u0161ek (uz2273@student.uni-lj.si) The authors have declared that no competing interests exist. Code is available at https://github.com/zrimseku/Reproducibility-Challenge \u2013 DOI 10.5281/zenodo.6511807. \u2013 SWH swh:1:dir:6eedc394f714587f35840bee0aac3e675bfa6c5a. Open peer review is available at https://openreview.net/forum?id=Hc8GOhfmhRF.\nReScience C 8.2 (#47) \u2013 Zrim\u0161ek 2022 1"}, {"heading": "What was easy", "text": "It was easy to run GNN\u2010PPI code on different datasets and with different parameters, as their repository is nicely organized and the code is clearly structured. It was also easy to understand their idea of the problem, the reasons for new evaluation and the framework of their proposed model."}, {"heading": "What was difficult", "text": "In both models used in this reproduction, the environment setup was harder than ex\u2010 pected. There was no documentation or comments in the code, which made it hard at first to understand it. Some debugging was needed for GNN\u2010PPI and a lot of code changes for PIPR to train well.\nCommunication with original authors We communicated with authors through email. They provided some useful clarifica\u2010 tions of the method and pipeline.\nReScience C 8.2 (#47) \u2013 Zrim\u0161ek 2022 2\n2 Introduction\nThe study ofmulti\u2010type Protein\u2010Protein Interaction (PPI) is fundamental for understand\u2010 ing biological processes from a systematic perspective and revealing disease mecha\u2010 nisms. Existing methods suffer from significant performance degradation when tested on different dataset, that was not used for training (in comparison to only dividing one dataset into train and test set). In this paper, authors investigate the problem and find that it is mainly attributed to the poor performance for inter\u2010novel\u2010protein interaction prediction. However, current evaluations overlook the inter\u2010novel\u2010protein interactions, and thus fail to give an instructive assessment. As a result, theypropose to address theproblem fromboth the evaluation and themethod\u2010 ology. Firstly, they design a new evaluation framework that fully respects the inter\u2010 novel\u2010protein interactions and gives consistent assessment across datasets. Secondly, they propose a graph neural network based method (GNN\u2010PPI), that uses correlations between proteins for better inter\u2010novel\u2010protein interaction prediction. Experimental re\u2010 sults on real\u2010world datasets of different scales demonstrate that GNN\u2010PPI significantly outperforms state\u2010of\u2010the\u2010art PPI predictionmethods, especially for the inter\u2010novel\u2010protein interaction prediction.\n3 Scope of reproducibility\nSince authors propose a new evaluation framework, our first task will be to critically inspect their methodology, that is based on different construction of train and test sets. We need to confirm that this kind of dataset splitting is objectively better and mimics the real world problem that they are trying to solve. Secondly, we will try to reproduce experimental results on real\u2010world datasets of differ\u2010 ent scales, that were shown in the paper. The authors compare their model to a series of baseline algorithms, some Machine Learning based models (Support Vector Machines, Random Forest and Linear Regression) and some Deep Learning basedmodels (PIPR [2], DNN\u2010PPI [3] and DPPI [4]). Since most of the author claims were made on comparisons of GNN\u2010PPI with PIPR, we will try to reproduce those. The main claims of the original paper, that we will test are the following:\n1. GNN\u2010PPI has higher micro\u2010F1 score than PIPR on SHS27k, SHS148k and STRING datasets, using random, BFS or DFS testset construction strategies.\n2. GNN\u2010PPI predicts inter\u2010novel\u2010protein interaction better than PIPR on the same datasets and testset construction strategies as in claim above.\n3. Test performance on trainset\u2010homologous testset (trained and tested on same set: SHS27k or SHS148k) underBFS andDFSpartition schemes reflects theperformance of generalizing knowledge to unseen testset (trained on SHS27k or SHS148k and tested on STRING) better than using random partition scheme \u2013 micro\u2010F1 scores are more similar, when training and testing GNN\u2010PPI on data split in such way.\n4. If we construct the PPI network in GNN\u2010PPI only from the trainset, the micro\u2010F1 score is still better than the one of PIPR. This shows that the trained model is robust to newly discovered proteins and their interactions.\n4 Methodology\nFor the reproduction we used authors code, slightly changing the pipeline for automa\u2010 tization of multiple runs with different seeds. We also used PIPR code, where we com\u2010 pletely changed the pipeline, to be able to use it on the same datasets as GNN\u2010PPI. We\nReScience C 8.2 (#47) \u2013 Zrim\u0161ek 2022 3\nonly used their function build_model, in which we changed last layer activation func\u2010 tion from softmax to sigmoid, because otherwise the model failed in predicting multiple interaction types.\n4.1 Model descriptions In our experiments we used the author\u2019s model, GNN\u2010PPI and the previous state\u2010of\u2010the\u2010 art for multi\u2010type protein interaction prediction, PIPR.\nGNN-PPI \u2014 GNN\u2010PPI is a graph neural network based method that uses correlation be\u2010 tween two protein features to predict multiple types of their interaction. Pairwise inter\u2010 action data are firstly assembled to build the graph, where proteins serve as the nodes and interactions as the edges. The model is developed by constructing an embedding for each protein to obtain predefined features, then processed by Convolution, Pooling, BiGRU and FCmodules (it is called GIN network) to extract protein\u2010independent encod\u2010 ing (PIE) features, which are aggregated by graph convolutions and arrive at protein\u2010 graph encoding (PGE) features. Embeddings are pretrained for each amino\u2010acid and combined for the proteins by their amino\u2010acid sequences. The last is Multi\u2010label PPI prediction. For unknown PPIs, we combine their protein feature encoded by the pre\u2010 vious process with a dot product, and then use a fully connected layer as classifier for multi\u2010label PPI prediction. For optimization the authors use Adam optimizer.\nPIPR \u2014 PIPR employs a Siamese architecture of residual RCNN encoder to better appre\u2010 hend and utilize the mutual influence of two sequences. It uses the same pretrained embeddings as GNN\u2010PPI, which are then send through the RCNN, fromwhich we get se\u2010 quence embedding vectors. This are multiplied with a dot product to form a sequence pair vector. Finally, this sequence pair vector is fed into a multi layer fully connected network with categorical cross\u2010entropy loss function, to predict the multi\u2010label PPI pre\u2010 diction.\n4.2 Datasets We trained and tested this two models on three different databases (and their combina\u2010 tions):\n1. STRING: this database collected, scored, and integrated most publicly available sources of protein\u2010protein interaction information and built a comprehensive and objective global PPI network, including direct (physical) and indirect (functional) interactions. In this paper, we focus on the multi\u2010type classification of PPI by STRING. It divides PPI into 7 types: reaction, binding, post\u2010translational modifi\u2010 cations (ptmod), activation, inhibition, catalysis, and expression. Each pair of in\u2010 teracting proteins contains at least one of them. We use all PPIs of Homo sapiens, which contains 15,335 proteins and 593,397 PPIs.\n2. SHS27k: randomly selected 1690 proteins of Homo sapiens subset of STRING, that have 7624 PPIs between them.\n3. SHS148k: randomly selected 5189 proteins ofHomo sapiens subset of STRING, that have 44488 PPIs between them.\nThe interactions from the datasets were combined into labels, so thatmultiple lines that represent different types of interactions between two proteins are combined into one datapoint, where label is a vector of length 7 (number of interaction types) with ones on the indices of interactions that are present and zeros elsewhere.\nReScience C 8.2 (#47) \u2013 Zrim\u0161ek 2022 4\nThe authors split datasets so that the test set contained 20%of the interactions. Splitting schemes will be described in the experimental setup more thoroughly, as their evalua\u2010 tion approach was one of their main contributions to the field. Whole datasets are avail\u2010 able on the authors GitHub repository, and the combined ones, used for PIPR training can be found on our repository.\n4.3 Hyperparameters For GNN\u2010PPI we used the hyperparameters that authors described in Table 10 in the paper. The only change was setting batch size to 2048 when training on STRING dataset, as that was much faster, and the authors also used that in the code on their repository. For PIPR we did a manual hyperparameter search, to find the parameters where the model works best. We tried different batch sizes, 64, 128, 256, 512, 1024 and 2048; differ\u2010 ent learning rates, 0.1, 0.01, 0.001 and 0.0001; numbers of epochs, 100, 200 and 300; and RMSprop and Adam optimizers. As the training takes a lot of time, we didn\u2019t train the model until the end for all options. With batch sizes, we fully trained it only on smaller datasets and compare the results. We saw that smaller batch size slows the training time a lot, so when using it, we needed to lower the number of epochs. For SHS27k, we saw that it is better to train for 200 epochs with batch size 128, than to do less epochs with batch size 64 (or more with bigger batch size), so we chose this combination. For SHS148k batch size 256 gave better results than 128, and with bigger performance fell. On STRING, the only option was to take batch size 1024, as others were too slow, and with 2048 training also slowed, because we ran out of memory on GPU. Here we only did 100 epochs, because after that, we couldn\u2019t use the GPU anymore. We set optimizer to Adam after it proved better on couple runs on the smallest dataset. Learning rate was set just based on first couple epochs (10\u201020), as we quickly saw that with 0.0001 the train\u2010 ing loss hardly even falls, and that with 0.01 and 0.1 it only falls at the beginning, and after couple of epochs, the model stops learning. So we set it to 0.001 for all datasets.\n4.4 Experimental setup and code One of the main authors claims is that the usual evaluation scheme (randomly spliting the interactions into train and test set) is not correct from the protein interaction point of view. The result of such split is, that the majority of the proteins were already seen during training, and it is much easier to predict the interactions for such proteins, then for some completely new. To show this, they separate the test set into three subsets: XBS denotes interactions where both proteins were already seen during training, XES denotes those where one protein was seen, and XNS those interactions, where both proteins are first seen in test phase. In the random testset construction, the XNS set is almost empty (which can be also seen in Table 3), meaning that the testing is not representative for new proteins. To solve this, authors propose two new strategies, Breath\u2010First Search (BFS) and Depth\u2010 First Search (DFS), where the testset is constructed by firstly selecting the root node and then performing the proposed BFS or DFS strategy to select other nodes for the test set (0.2 of the whole dataset in our experiments). As they more thoroughly explained in the paper, these strategies seem to mimic the real world case, where some new cluster of proteins, that tightly interact with each other, is found (BFS) or we just have some new proteins distributed around the previously known network (DFS). As the authors did in the paper, we then compared the models based on their micro\u2010 averaged F1 score (which is the same as the accuracy giving each sample the same im\u2010 portance). We repeated the experiments 5 times where it wasn\u2019t too computationally expensive, to get the uncertainty and see how reliable our results are. We were not able to do multiple runs on STRING dataset for each setting, as each run was very expen\u2010 sive, but we believe that the uncertainty here would be very small, as we have a very big dataset. The code for all our experiments is available on this repository.\nReScience C 8.2 (#47) \u2013 Zrim\u0161ek 2022 5\nTraining time [min] Testing time [s]\nSHS27k SHS148k STRING SHS27k SHS148k STRING\nGNN\u2010PPI 11 63 1064 0.2 1.2 44 PIPR 36 135 602 5 23 291\n5 Results\nIn this section we will present the results of the experiments that we did to support the claims above. Our results were in some parts far from the authors results, but even so, they support the main claims of the original paper.\n5.1 Comparison of GNN-PPI and PIPR Results in this subsection refer to our first two claims. In Table 3 we can see the com\u2010 parison of the models in question on all three datasets, trained and tested with all three partition schemes. For first two datasets we ran the experiments 5 times, with 5 differ\u2010 ent random seeds, to see how the performance changes on different sets. We can see that especially for SHS27k the standard deviation for bfs and dfs splits is very big, which also explain why our results are at some points quite far from the authors. For STRING, multiple runs were too expensive, so we only trained and tested once. But this should be enough to asses the authors claims, because the dataset is so much larger, that the randomness of the split effects the performance less. The comparison of general performance, that refers to the first claim, can be seen if we look at the micro\u2010F1 score averaged across whole dataset (column average). Except for BFS mode on the smallest dataset, where uncertainty is too big, we can confirm that in all other cases GNN\u2010PPI performs better. The second claim is that GNN\u2010PPI predicts inter\u2010novel\u2010protein interactions better. The subsets we are observing are denoted with bs, standing for both proteins being seen\nReScience C 8.2 (#47) \u2013 Zrim\u0161ek 2022 6\nduring training, es, either of the proteins seen during training and ns, neither of the proteins seen before. So for the second claim, we need to compare the performance on subsets es and ns. The results in the table are in bold where uncertainty is small enough that we can confirm it. We can also observe that even where it is not, mean is never bigger for PIPR.\n5.2 Generalization performance In this subsectionwewill inspect the third claim, that says that with the newly proposed evaluation protocol, we are better assessing the models generalization abilities. To test that, authors trained both models (we will reproduce results for GNN\u2010PPI) on SHS27k and SHS148k datasets and tested them on the bigger STRING dataset. The test sets of both smaller datasets were now used as validation sets during training, to determine themodel fromwhich epoch should be taken as best. The authors then compared these to the results on trainset\u2010homologous testset (here authors don\u2019t use validation during training, as they use that set for testing). The results in Table 4 confirm claim 3. As we can see generalizing accuracy severely drops when using random partition scheme (\u223c 0.2). With newly proposed schemes we get similar, sometimes even better performance when testing on STRING set.\n5.3 Robustness for unknown proteins In Table 5 we have the comparison of different PPI graph construction methods \u2010 GCA means that the PPI graph was constructed from all proteins in the dataset, and GCT means that PPI graph was constructed only form the proteins in the train set. This re\u2010 sults confirm claim 4, but in a different way as those in the original paper. It is true that the performance with GCT construction method is still better than PIPR method, but contrary to authors results, we get even better performance from the GCT construction method than with GCA.\n6 Discussion\nNew evaluation protocol which proves superiority of a newly proposed model could be questionable, because the authors would only select a new evaluation that speaks for their model. But in this case, they also compared themodels by random strategy, which is the previously used evaluation method. The new evaluation also has empirical stud\u2010 ies to support it and is based on the domain knowledge, so we think that the authors proposed a good framework that can be very useful in future research in this field as such performance prediction is more on par with real world situation. When comparing GNN\u2010PPI and PIPR model, we confirm that GNN\u2010PPI has better aver\u2010 age performance than PIPR. We should also observe, that the standard deviation for new partiton scheme BFS on the small dataset is too big for us to make certain claims. But we see that on the bigger dataset, it becomes smaller, so we can rely more on this evaluations. There is a bit less certainty when assessing second claim \u2013 comparing re\u2010 sults in columns XES and XNS from Table 3. First let\u2019s observe that the size of XNS is to small in random partition scheme for all datasets, so we won\u2019t compare models on it. Next, we see that with BFS split scheme, training useful models is much harder than for other two. We need a lot more data for models to perform well, and if we have it, then we can claim that GNN\u2010PPI performs better at least when one of the proteins in the pair is seen in training. For both unseen before, we can\u2019t claim with enough certainty. So let\u2019s say that we partially proved the second claim \u2010 it holds for random and DFS scheme, and for BFS on bigger datasets. There is onemore shortcoming of our approach. We ran out of time to run 200 epochs of training PIPR on STRING dataset. If we had the time,\nReScience C 8.2 (#47) \u2013 Zrim\u0161ek 2022 7\nthe result could be better, and could potentially beat GNN\u2010PPI. We propose this to be checked in further reproduction. Ifwe compare absolute scores ofGNN\u2010PPIwith those that authors described in the paper, they vary quite a bit. This happens because the standard deviations are so big, and the authors might (could be unintentionally) chose the better results of their runs. Their in\u2010depth analysis on test subsets also didn\u2019t include uncertainty, which is a problem, as we can see from our results that show that uncertainty is very big and so their results are not reliable. We confirmed that the new evaluation protocol is much better in assessing the models generalization abilities, which was the main problem of the field that the authors were trying to solve. An important thing to observe here is also that when using any model for prediction of PPI, we can use this evaluation protocol and predict its performance in much more detail. We know how many of the proteins that we are interested in are completely new, and how many were already known when training the model, so we can predict the performance separately for any new set, based on performances on test subsets BS, ES and NS. When comparing the robustness of predictions for unknown proteins, we need to ask ourselves why can we even use the GCA method, that actually uses test data in network construction during training. In most cases that would be a big mistake, but if we look from a protein perspective, this has a practical explanation. We could know which pro\u2010 tein exist, what amino\u2010acids they are built of, so we could put them into the network, but we wouldn\u2019t know anything about their interactions yet. If we are interested into such proteins, than GCA evaluation will tell us more about our models performance. If we want to know how our model will perform for some newly discovered proteins, we need to inspect evaluation using GCT. In Table 5 we can see a big discrepancy with authors results. They were showing that with GCT the performance falls, which would be expected because we have less information, but in our case, the mean results are better with GCT construction. If we look at the standard deviations, we again see that the differences between results of multiple runs are big, so we can\u2019t say with certainty that this results show that GCT is better than GCA. The reason for this could also just be different root node at splitting schemes, and therefore different clusters of unknown proteins. But in any case, it shows that the performance of GNN\u2010PPI with graph con\u2010 struction only from the trainset is much better than the performance of PIPR, which confirms the fourth claim and further proves the usefulness of GNN\u2010PPI. In this reproduction, we left out some parts of the paper that we think should be fur\u2010 ther verified. We only took PIPR model, and didn\u2019t inspect the others for the same tasks. Except for overall performance, also authors didn\u2019t compare themselves with other methods, but we believe that for saying that their model is state\u2010of\u2010the\u2010art in inter\u2010 novel\u2010protein interaction prediction, they should also inspect other models more thor\u2010 oughly. With a lot of trouble with setting up PIPR and long training times, we ran out of time for this additional experimentation. We also just superficially grasped the hy\u2010 perparameter selection for PIPR, and with proper grid search (that would require a lot of computing resources), we could also find better settings for it, or in other case, we could more confidently say that GNN\u2010PPI is better. We also left out the separate results for labels, and ablation study, for which there was no code, but since it could be easily implemented, we suggest it to be done in future. We didn\u2019t do it because of time re\u2010 strictions, and because the authors said in our communications, that it was not the part of original paper, only an addition for requirements of the reviewer, included in arXiv version.\n6.1 What was easy The easiest part of this reproduction was to understand the paper and it\u2019s ideas. The authors motivate and describe the problem in an easy to understand way, supported by expressive graphics. They support the ideas with domain knowledge, which makes\nReScience C 8.2 (#47) \u2013 Zrim\u0161ek 2022 8\nthe paper much more insightful. The algorithm for GNN\u2010PPI is clearly described in the paper, so we believe that it would be possible to put it in code without major problems. But their code is also nicely structured and easy to run with various parameters, so we suggest you use it when in need for their method.\n6.2 What was difficult For GNN\u2010PPI there was also no environment file, and in the environment descriptions, versions of some libraries were left out, so it took some time to set the environment cor\u2010 rectly. You can load the environment file from our repository, to avoid this problem. There was also no documentation of the authors code, so when inspecting the imple\u2010 mentation, it was difficult to recognize use of some variables. There was some minor debugging needed before the code ran smoothly. We couldn\u2019t use the already trained models for any of our tasks. If we are least judging from their names, they were trained only using the train set, so only useful for checking the accuracy on test part of the same dataset. But this can\u2019t be done, because they didn\u2019t upload the split information, so we can\u2019t build this exact test set. If we wanted to use the models on some other dataset, they would need to upload the model with best validation accuracy, as that one would probably be better. This were all minor problems compared to the difficulty of running the PIPR model. Their environment file was not useful, and the versions they described on repository were not compatible with each other, which meant that we spent a lot of time finding out which libraries we should load to run their code. The code was not documented and it was completely unreadable, so it was hard to even know what they were doing. In the end we only used the model building part, and wrote other parts ourselves. Even the building function needed some changes, because they used wrong activation function on the last layer for themodel to be able to learnmulti\u2010type prediction. Even its descrip\u2010 tion in their paper was not very explanatory, but the reason for that was probably that it wasn\u2019t the main part of their research."}], "title": "[Re] Learning Unknown from Correlations: Graph Neural Network for Inter-novel-protein Interaction Prediction", "year": 2022}