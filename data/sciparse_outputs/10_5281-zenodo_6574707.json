{"abstractText": "In the article, the authors of the Transparent Object Tracking Benchmark compare the performance of 25 state\u2010of\u2010the\u2010art tracking algorithms, evaluated on the TOTB dataset, with a new proposed algorithm for tracking transparent objects called TransATOM. Au\u2010 thors claim that it outperforms all other state\u2010of\u2010the\u2010art algorithms. They highlight the effectiveness and advantage of transparency feature for transparent object track\u2010 ing. They also do a qualitative evaluation of each tracking algorithm on various typical challenges such as rotation, scale variation etc.", "authors": [{"affiliations": [], "name": "\u017diga Trojer"}, {"affiliations": [], "name": "Koustuv Sinha"}, {"affiliations": [], "name": "Sharath Chandra Raparthy"}], "id": "SP:da744d193be75ec847407846d643f55da9a6e0bd", "references": [{"authors": ["H. Fan", "H.A. Miththanthaya", "Harshit", "S.R. Rajan", "X. Liu", "Z. Zou", "Y. Lin", "H. Ling"], "title": "Transparent Object Tracking Benchmark.", "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)", "year": 2021}, {"authors": ["E. Xie", "W. Wang", "P. Sun", "H. Xu", "D. Liang", "P. Luo"], "title": "Segmenting Transparent Objects in the Wild with Transformer.", "year": 2021}, {"authors": ["M. Danelljan", "L. Gool", "R. Timofte"], "title": "Probabilistic Regression for Visual Tracking.", "year": 2020}, {"authors": ["G. Bhat", "M. Danelljan", "L. Van Gool", "R. Timofte"], "title": "Learning Discriminative Model Prediction for Tracking.", "venue": "In: Oct", "year": 2019}, {"authors": ["M. Danelljan", "G. Bhat", "F. Khan", "M. Felsberg"], "title": "ATOM: Accurate Tracking by Overlap Maximization.", "year": 2019}, {"authors": ["Q. Wang", "L. Zhang", "L. Bertinetto", "W. Hu", "P. Torr"], "title": "Fast Online Object Tracking and Segmentation: A Unifying Approach.", "year": 2019}, {"authors": ["B. Li", "W. Wu", "Q. Wang", "F. Zhang", "J. Xing", "J. Yan"], "title": "SiamRPN++: Evolution of Siamese Visual Tracking With Very Deep Networks.", "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition", "year": 2019}, {"authors": ["B. Li", "J. Yan", "W. Wu", "Z. Zhu", "X. Hu"], "title": "High Performance Visual Tracking With Siamese Region Proposal Network.", "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "year": 2018}, {"authors": ["L. Bertinetto", "J. Valmadre", "J. Henriques", "A. Vedaldi", "P. Torr"], "title": "Fully-Convolutional Siamese Networks for Object Tracking.", "year": 2016}, {"authors": ["H. Nam", "B. Han"], "title": "Learning Multi-Domain Convolutional Neural Networks for Visual Tracking.", "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "year": 2016}, {"authors": ["B. Yan", "H. Peng", "J. Fu", "D. Wang", "H. Lu"], "title": "Learning Spatio-Temporal Transformer for Visual Tracking.", "venue": "IEEE/CVF International Conference on Computer Vision (ICCV). Los Alamitos, CA, USA: IEEE Computer Society,", "year": 2021}], "sections": [{"text": "R E S C I E N C E C Replication / ML Reproducibility Challenge 2021\n[Re] Transparent Object Tracking Benchmark\n\u017diga Trojer1, ID 1University of Ljubljana, Faculty of Computer and Information Science, Ve\u010dna pot 113, 1000 Ljubljana\nEdited by Koustuv Sinha,\nSharath Chandra Raparthy\nReviewed by Anonymous Reviewers\nReceived 04 February 2022\nPublished 23 May 2022\nDOI 10.5281/zenodo.6574707\n1 Reproducibility Summary"}, {"heading": "Scope of Reproducibility", "text": "In the article, the authors of the Transparent Object Tracking Benchmark compare the performance of 25 state\u2010of\u2010the\u2010art tracking algorithms, evaluated on the TOTB dataset, with a new proposed algorithm for tracking transparent objects called TransATOM. Au\u2010 thors claim that it outperforms all other state\u2010of\u2010the\u2010art algorithms. They highlight the effectiveness and advantage of transparency feature for transparent object track\u2010 ing. They also do a qualitative evaluation of each tracking algorithm on various typical challenges such as rotation, scale variation etc."}, {"heading": "Methodology", "text": "In addition to the TransAtom tracker, we chose ten, best performing on TOTB dataset, state\u2010of\u2010the\u2010art tracking algorithms to evaluate on the TOTB dataset using a set of stan\u2010 dard evaluation tools. On different sequences, we performed a qualitative evaluation of each tracking algorithm and thoroughly compared the ATOM tracker to the TransATOM tracker. We did not implement the trackers from scratch, but instead used GitHub im\u2010 plementations. TOTB dataset had to be integrated into some of the standard evaluation tools. We used an internal server with an Ubuntu 18.04 operating system and a TITAN X graphics card to reproduce the results."}, {"heading": "Results", "text": "The tracking performance was reproduced in terms of success, precision, and normal\u2010 ized precision, and the reported value is in the 95 percent confidence interval, which supports the paper\u2019s conclusion that TransATOM significantly outperforms other state\u2010 of\u2010the\u2010art algorithms on TOTB database. Also, it supports a claim that including a trans\u2010 parency feature in the tracker improves performancewhen tracking transparent objects. However, we refuted the claim that TransATOM well handles all challenges for robust target localization."}, {"heading": "What was easy", "text": "The evaluation of the tracking results and comparison of different trackers with each other was a simple part of the reproduction because the implementation in Matlab is\nCopyright \u00a9 2022 \u017d. Trojer, released under a Creative Commons Attribution 4.0 International license. Correspondence should be addressed to \u017diga Trojer (ziga.trojer20@gmail.com) The authors have declared that no competing interests exist. Code is available at https://github.com/trojerz/TOTB-reproducability \u2013 DOI 10.5281/zenodo.6475970. \u2013 SWH swh:1:dir:b80fa866c01389a46dde8b6f419d893d127f1025. Open peer review is available at https://openreview.net/forum?id=HxZZV3MQ20Y.\nReScience C 8.2 (#41) \u2013 Trojer 2022 1\nvery robust and works for different formats of tracker results."}, {"heading": "What was difficult", "text": "The most difficult aspect of the replication was integrating the TOTB dataset into vari\u2010 ous standard evaluation tools and running all trackers on this dataset. The reason for this is that each tool requires its own dataset format, and it was also difficult to set up so many different tracker environments. It also took a long time to run all of the trackers because some of them are quite slow and the TOTB dataset is quite large. The depreca\u2010 tion of different packages was also a problem for some trackers, necessitating extensive debugging.\nCommunication with original authors We communicated with the author via email. The author provided us with feedback that helped us reproduce the results more accurately.\n2 Introduction\nIn recent years, the tracking community has made amazing progress. Many new track\u2010 ingmethods, particularly neural network trackers, have substantially improved the track\u2010 ing of opaque objects. Existing research in the topic mostly focuses on tracking of opaque objects, with very little attention dedicated to tracking of transparent objects. However, transparency brings additional challenges not well tackled by the state\u2010of\u2010the\u2010 art in opaque object tracking. Tracking of such objects may be very relevant to robotic vision, human\u2010machine inter\u2010 action and security surveillance. A vessel collecting plastic from the sea, for example, could so effectively track plastic in the sea and remove it from the water. Another po\u2010 tential application is the grabbing of produced light bulbs with a robotic arm. That is why it is important to reproduce and verify the results of this article, as it pro\u2010 poses a new state\u2010of\u2010the\u2010art tracker TransAtom, which is currently thought to be the best performing when tracking transparent objects compared to other trackers.\n3 Scope of reproducibility\nIn our work, we focused on reproducing and comparing the results of various trackers on TOTB. The proposed TOTB \u2010 transparent object tracking benchmark, which is the first benchmark dedicated to transparent object tracking, was the original paper\u2019s main contribution. In our opinion, this is a significant contribution because it is the first step toward the development of trackers for transparent objects. We intended to evaluate and compare the proposed TransATOM tracker to other state\u2010 of\u2010the\u2010art trackers on the aforementioned dataset. With this, we hoped to validate the claim that TransATOMsignificantly outperformsother state\u2010of\u2010the\u2010art trackers designed primarily for tracking opaque objects. We hoped to verify the claim that including a transparency feature in the tracker improves performance when tracking transparent objects. Finally, we conducted a qualitative evaluation on various types of recordings to see where different trackers excel and where they fail. Claims that we tested are the following:\n\u2022 TransATOM assessed on TOTB significantly outperforms other evaluated state\u2010of\u2010 the\u2010art algorithms by a large margin.\n\u2022 Including a transparency feature in the tracker improves performancewhen track\u2010 ing transparent objects.\nReScience C 8.2 (#41) \u2013 Trojer 2022 2\n\u2022 TransATOM well handles all challenges for robust target localization owing to the transparency features.\n4 Methodology\nWe used the author\u2019s TransATOM code to reproduce the results, which is available here. Code for evaluation tools and other trackers was obtained from GitHub:\n\u2022 PyTracking tool, which includes code for ATOM, PrDiMP\u201018, PrDiMP\u201050, DiMP\u201018 and DiMP\u201050.\n\u2022 PySOT tool, which includes code for SiamMask, SiamRPN and SiamRPN++.\n\u2022 py\u2010MDNet tool, which includes code for MDNet.\n\u2022 STARK tracker with it\u2019s own evaluation tool.\nWe used an internal server with an Ubuntu 18.04 operating system and a TITAN X graph\u2010 ics card with 12GB VRAM to reproduce the results.\n4.1 Model descriptions Here we list hyperlinks to the models\u2019 descriptions and all of the parameters we used: TransATOM, ATOM, PrDiMP\u201018, PrDiMP\u201050, DiMP\u201018, DiMP\u201050, SiamMask, SiamRPN, SiamRPN++, MDNet and Stark. All models have been pre\u2010trained.\n4.2 Datasets An important part of tracker analysis is to know in which cases the tracker excels and in which cases fails. To this end, the authors have assigned several attributes to each recording. A twelve\u2010dimensional binary vector was provided for each sequence to indi\u2010 cate the presence of an attribute (1 denotes the presence of a certain attribute). From Table 1, which summarizes TOTB dataset, we can observe number of sequences with a certain attribute (bold diagonal) and number of sequences with combination of dif\u2010 ferent attributes. Table 2 shows the distribution of the following attributes on TOTB dataset: illumination variation (IV), partial occlusion (PC), deformation (DEF), motion blur (MB), rotation (ROT), background clutter (BC), scale variation (SV), full occlusion (FOC), fast motion (FM), out-of-view (OV), low resolution (LR) and aspect ratio change (ARC). The most frequent challenges in TOTBdataset are rotation, partial occlusion and scale variation. The TOTB dataset is available for download at the following link.\nresult), with the value varying depending on the threshold (which may be different for each tracker). Normalized precision is used to eliminate the influence of different scales by performing normalization with target areas. Success compares the intersection over union (IoU) of tracking results and groundtruth boxes, and success score is calculated as the percentage of tracking results with IoU greater than 0.5. All the code we needed is available on GitHub.\nTracker TransATOM ATOM PrDiMP\u201018 PrDiMP\u201050 DiMP\u201018 DiMP\u201050 SiamMask SiamRPN SiamRPN++ MDNet Stark\naverage FPS 12 25 7 5 7 6 55 42 40 3 70 maximum FPS 21 32 13 11 9 8 78 64 59 5 98 average OPE 1 h 59 min 57 min 3 h 25 min 4 h 45 min 3 h 25 min 3 h 58 min 26 min 35 min 35 min 7 h 57 min 20 min maximum OPE 2 h 2 min 59 min 3 h 29 min 4 h 50 min 3 h 29 min 4 h 04 min 28 min 36 min 37 min 8 h 13 min 21 min\n5 Results\nThere are two parts to this section. In section 5.1, we examine tracker performance in terms of precision, normalized precision and success and compare it with each other. The results presented there are to support the first two claims in Section 3. The results of the qualitative evaluation are presented in section 5.2, where the results contradicts the claim that TransATOM effectively handles all challenges for robust target localization.\nReScience C 8.2 (#41) \u2013 Trojer 2022 4\nTracker Precision Normalized Precision Success\nTransATOM 65.3\u00b1 0.4 73.5\u00b1 0.5 73.8\u00b1 0.4 ATOM 63.9\u00b1 0.3 71.2\u00b1 0.4 71.8\u00b1 0.4 DiMP18 55.9\u00b1 0.6 64.4\u00b1 0.8 65.1\u00b1 0.7 DiMP50 60.1\u00b1 0.5 67.9\u00b1 0.8 68.5\u00b1 0.7 prDiMP18 58.9\u00b1 0.3 66.3\u00b1 0.4 67.8\u00b1 0.3 prDiMP50 64.3\u00b1 0.3 72.3\u00b1 0.4 73.7\u00b1 0.4 SiamRPN 62.9\u00b1 0.2 70.1\u00b1 0.4 72.2\u00b1 0.4 SiamMASK 64.1\u00b1 0.3 72.4\u00b1 0.4 73.0\u00b1 0.3 SiamRPN++ 64.7\u00b1 0.2 71.9\u00b1 0.5 72.8\u00b1 0.5 Stark 76.2\u00b1 0.4 81.7\u00b1 0.5 83.5\u00b1 0.4 MDNet 59.6\u00b1 1.8 69.3\u00b1 2.9 69.7\u00b1 2.2\nTransATOM and ATOM tracker performance can now be compared. The only distinc\u2010 tion between these two trackers is that TransATOM includes a transparency feature.\nReScience C 8.2 (#41) \u2013 Trojer 2022 5\nWe can confirm the second claim, that including a transparency feature in the tracker improves performance when tracking transparent objects, because TransATOM outper\u2010 forms ATOM by 1.4 percent in precision, 2.3 percent in normalized precision, and 2.0 percent in success, while confidence intervals do not overlap (see Table 4 and Figure 1).\nWe evaluated the current state\u2010of\u2010the\u2010art tracker Stark in addition to the best trackers from the original article. We can see from the above results that it outperforms all other\nReScience C 8.2 (#41) \u2013 Trojer 2022 6\ntracking algorithms and handles all challenges for robust target localization much bet\u2010 ter.\n6 Conclusion\nWewere able to confirm two of the three main claims from the original article, as stated in the results. The claim which states that TransATOM outperforms other evaluated state\u2010of\u2010the\u2010art algorithms by a large margin on TOTB was confirmed, as we demon\u2010 strated that TransATOM outperforms other trackers that the authors evaluated in their original paper. The second claim was also confirmed, because we showed that the dif\u2010 ference in performance of TransATOM and ATOM tracker, which differentiate only on the transparency feature, was significant enough. However, we were unable to confirm the the last claim, which states that TransATOM effectively handles all challenges for robust target localization due to transparency fea\u2010 tures. We evidenced multiple cases where the TransATOM tracker fails to handle trans\u2010 parent object tracking adequately. We believe that this is the most audacious claim, be\u2010 causeweknow that theTOTBdataset containsmanydifficult challenges that no currently\u2010 developed tracker can handle well. The strength of our strategy was that we attempted to follow the steps outlined in the article. We also chose only the top 10 trackers based on their performance on the TOTB dataset, allowing us to focus more on implementation and evaluation quality. In addi\u2010 tion, we compared the current state\u2010of\u2010the\u2010art Stark tracker. We wanted to show that there is still a lot of room for improvement in the field of tracking transparent objects. Because we didn\u2019t know which parameters were used in the original article, we used only the default choice of parameters for all trackers. This was a flaw in our approach. We could also do more in\u2010depth qualitative analysis because we could compare three results for each tracker and pick the best one, but we took the best one in the whole TOTB dataset.\n6.1 Recommendations for reproducibility We recommend using the code from GitHub to reproduce the results of the original ar\u2010 ticle or our work. We recommend to look at which evaluation tool the original code is written in for each tracker and use that evaluation tool. We do not recommend repro\u2010 ducing the results for all trackers, but rather selecting the trackers with the best results, because evaluating the trackers takes a lot of time. We have adapted the TOTB dataset for PySOT, py\u2010MDNet, STARK, and VOT21 evaluation tool, and we recommend to down\u2010 load it from here."}], "title": "[Re] Transparent Object Tracking Benchmark", "year": 2022}