{"abstractText": "Since the first description of spike timing-dependent plasticity (STDP) [1], different STDP models have been published to reproduce various experimental findings. Early implementations such as pair-based STDP learning rules failed to reproduce some experimental observations, such as triplet or quadruplets experiments [2]. Clopath et al. 2010 [3] introduced a STDP model which is able to reproduce the experimental findings of triplet studies. They propose a biologically-motivated model with a voltage-based learning rule where the occurrence of long term depression (LTD) or long term potentiation (LTP) depends on the depolarization of the postsynaptic membrane potential, as observed in voltage-clamp [4] and stationary-depolarization experiments [5]. Further, they could reproduce experimental findings such as spike pair repetition and triplet experiments [6], as well as spike bursting experiments [7]. They were able to show that their learning rule can develop stable weights, as needed for learning the receptive fields of simple cells in the primary visual cortex (V1). They implemented a homeostatic mechanism to control the level of generated LTD, based on the relationship between the average postsynapticmembrane potential and a reference value. Their model led to two different connectivity structures, depending on the spiking behavior of the neurons: if the neurons fire strongly at the same time, they build strong bidirectional connections (as in correlation-based Hebbian learning). If they fire in a specific temporal order, their connectivity structure follows that order (temporal coding). In this work, we present an implementation of the voltage-based triplet STDP rule from Clopath et al. 2010 [3] with the neuro-simulator ANNArchy. Due the need of parameter changing, we report here are partial replication of the STDP rule fromClopath et al. 2010 [3].", "authors": [{"affiliations": [], "name": "Rene Larisch"}, {"affiliations": [], "name": "Georgios Detorakis"}, {"affiliations": [], "name": "Andrew P. Davison"}], "id": "SP:69fe07e9de190799cc22b0699abce47bba4a86c8", "references": [{"authors": ["G.-Q. Bi", "M.-M. Poo"], "title": "Synaptic Modifications in Cultured Hippocampal Neurons: Dependence on Spike Timing, Synaptic Strength, and Postsynaptic Cell Type.", "venue": "Journal of Neuroscience", "year": 1998}, {"authors": ["J.-P. Pfister", "W. Gerstner"], "title": "Beyond Pair-Based STDP: a Phenomenological Rule for Spike Triplet and Frequency Effects.", "year": 2006}, {"authors": ["C. Clopath", "L. B\u00fcsing", "E. Vasilaki", "W. Gerstner"], "title": "Connectivity reflects coding: a model of voltage-based STDP with homeostasis.", "venue": "Nature Neuroscience", "year": 2010}, {"authors": ["A. Ngezahayo", "M. Schachner", "A. Artola"], "title": "Synaptic Activity Modulates the Induction of Bidirectional Synaptic Changes in Adult Mouse Hippocampus.", "venue": "Journal of Neuroscience", "year": 2000}, {"authors": ["A. Artola", "S. Br\u00f6cher", "andW. Singer"], "title": "Different voltage-dependent thresholds for inducing long-term depression and long-term potentiation in slices of rat visual cortex.", "venue": "Nature", "year": 1990}, {"authors": ["P.J. Sj\u00f6str\u00f6m", "G.G. Turrigiano", "S.B. Nelson"], "title": "Rate, Timing, and Cooperativity Jointly Determine Cortical Synaptic Plasticity.", "venue": "Larisch", "year": 2001}, {"authors": ["T. Nevian", "B. Sakmann"], "title": "Spine Ca2+ Signaling in Spike-Timing-Dependent Plasticity.", "venue": "Journal of Neuroscience", "year": 2006}, {"authors": ["J. Vitay", "H. Dinkelbach", "F. Hamker"], "title": "ANNarchy: a code generation approach to neural simulations on parallel hardware.", "venue": "Frontiers in Neuroinformatics", "year": 2015}, {"authors": ["S. Song", "L. Abbott"], "title": "Cortical Development and Remapping through Spike Timing-Dependent Plasticity.", "venue": "Neuron", "year": 2001}, {"authors": ["B.A. Olshausen", "D.J. Field"], "title": "Emergence of simple-cell receptive field properties by learning a sparse code for natural images.", "venue": "Nature", "year": 1996}, {"authors": ["R. Brette"], "title": "Simulation of networks of spiking neurons: A review of tools and strategies.", "venue": "Journal of Computational Neuroscience 23.3 (Dec", "year": 2007}], "sections": [{"text": "R E S C I E N C E C"}, {"heading": "Replication / Computational Neuroscience", "text": "[Re] Connectivity reflects coding a model of voltage-based STDP with homeostasis\nRene Larisch1 1Professorship for Artificial Intelligence, Department of Computer Science, Chemnitz University of Technology, D-09107 Chemnitz, Germany\nEdited by Georgios Detorakis ID\nReviewed by Christian Jarvers ID\nAndrew P. Davison ID\nReceived 01 November 2018\nPublished 12 November 2019\nDOI 10.5281/zenodo.3538217"}, {"heading": "Introduction", "text": "Since the first description of spike timing-dependent plasticity (STDP) [1], different STDP models have been published to reproduce various experimental findings. Early implementations such as pair-based STDP learning rules failed to reproduce some experimental observations, such as triplet or quadruplets experiments [2]. Clopath et al. 2010 [3] introduced a STDP model which is able to reproduce the experimental findings of triplet studies. They propose a biologically-motivated model with a voltage-based learning rule where the occurrence of long term depression (LTD) or long term potentiation (LTP) depends on the depolarization of the postsynaptic membrane potential, as observed in voltage-clamp [4] and stationary-depolarization experiments [5]. Further, they could reproduce experimental findings such as spike pair repetition and triplet experiments [6], as well as spike bursting experiments [7]. They were able to show that their learning rule can develop stable weights, as needed for learning the receptive fields of simple cells in the primary visual cortex (V1). They implemented a homeostatic mechanism to control the level of generated LTD, based on the relationship between the average postsynapticmembrane potential and a reference value. Their model led to two different connectivity structures, depending on the spiking behavior of the neurons: if the neurons fire strongly at the same time, they build strong bidirectional connections (as in correlation-based Hebbian learning). If they fire in a specific temporal order, their connectivity structure follows that order (temporal coding). In this work, we present an implementation of the voltage-based triplet STDP rule from Clopath et al. 2010 [3] with the neuro-simulator ANNArchy. Due the need of parameter changing, we report here are partial replication of the STDP rule fromClopath et al. 2010 [3]."}, {"heading": "Methods", "text": ""}, {"heading": "Overview", "text": "The original model was implemented in Matlab (http://modeldb.yale.edu/144566 ) to demonstrate the stable learning of weights. This model reimplementation is written in Python (2.7 and tested with v3.6) with the help of the neuro-simulator ANNarchy [8]\nCopyright \u00a9 2019 R. Larisch, released under a Creative Commons Attribution 4.0 International license. Correspondence should be addressed to Rene Larisch (rene.larisch@informatik.tu-chemnitz.de) The authors have declared that no competing interests exists. Code is available at https://github.com/rLarisch/ReScience-submission/tree/Larisch-2019/code. Open peer review is available at https://github.com/ReScience/ReScience-submission/pull/57##issuecomment-549728501.\nReScience C 5.3 (#2) \u2013 Larisch 2019 1\n(v4.6.8.1) 1, numpy (v1.11.0) and matplotlib (v1.5.1). The reimplementation is mainly based on the description of neuron model and learning rule in the original publication [3]. Because of the lack of further description of the homeostatic mechanism and the neural behavior after an emitted spike, the Matlab code is used as the second reference for this reimplementation. Besides of the provided code on modelDB, the supplementary material to the original article contains a Matlab code example. However, the supplementary code does not mention the homeostatic mechanism for the learning rule, what is a relevant mechanism."}, {"heading": "Model description", "text": "Neural model \u2014 Clopath et al. 2010 [3] used an adaptive exponential integrate-and-fire (AdEx) neuron in their model. The exact neural model is mainly derived from the description in the Matlab source code (Eq. 1):\nC du\ndt = \u2212gL (u\u2212 EL) + gL \u2206T e\nu\u2212VT \u2206T \u2212 wad + z + I (1)\nwhere u is the membrane potential, C the membrane capacitance, gL the leak conductance andEL the resting potential. The slope factor (\u2206T ) and the spiking threshold (VT ) describe the behavior of the exponential term. If the membrane potential (u) is above VT , the neuron spikes and the membrane potential increases exponentially. To simulate the spike upswing for 2 ms after a spike was emitted, they used the so-called *resolution trick*: they simulate the complete process in themembrane potential through a spike oncewith high precision and integrated over the complete process to calculate the entire change in themembrane potential. For simulations, they used the integrated value and fixed themembrane potential for 2ms. This means that the membrane potential is set to 29.4mV after a spike, one millisecond later to 29.4mV +3.462mV and another millisecond later to EL + 15mV +6.0984mV. The depolarizing spike afterpotential is z and decays over time to zero (Eq. 2).\n\u03c4z dz\ndt = \u2212z (2)\nThe hyperpolarization current is described bywad (Eq. 3). After a spike,wad is increased by b and decreases exponentially to the resting potential EL otherwise.\n\u03c4wad dwad dt = a(u\u2212 EL)\u2212 wad (3)\nThe adaptive spiking threshold (VT ) is set to VTmax after a spike and decays exponentially to VTrest (Eq. 4).\n\u03c4VT dVT dt = \u2212(VT \u2212 VTrest) (4)\nSynaptic model \u2014 The proposed learning rule consists of two terms: long term potentiation (LTP) and long term depression (LTD). The LTP term (Eq. 5) controls the increase in synaptic efficiency:\nLTP = ALTP x\u0304i (u\u2212 \u03b8+)+ (u\u0304+ \u2212 \u03b8\u2212)+ (5)\nALTP is the learning rate for LTP. The parameters \u03b8+ and \u03b8\u2212 are plasticity thresholds for the membrane potential (u) and its temporal average (u\u0304+, Eq. 6), respectively. The original paper does not mention the meaning of these thresholds. The chosen value \u03b8+ = \u221245.3mV is above the spiking threshold. This suggests that this threshold prevents\n1https://bitbucket.org/annarchy/annarchy\nReScience C 5.3 (#2) \u2013 Larisch 2019 2\nthe occurrence of LTP if the postsynaptic neuron has not spiked already. With \u03b8\u2212 = EL, LTP only occurs if the membrane potential is above the resting potential.\n\u03c4+ du\u0304+ dt = \u2212u\u0304+ + u (6)\nAfter each presynaptic spike, the spike trace x\u0304i is increased by 1 and decays exponentially with a time constant \u03c4x (Eq. 7). The spike counter Xi is 1 when the presynaptic neuron spikes at time t, otherwise 0.\n\u03c4x dx\u0304i dt = \u2212x\u0304i +Xi (7)\nWith this term, LTP occurs when the presynaptic spike trace (x\u0304i) is above zero, the postsynaptic membrane potential u is over the threshold \u03b8+ and the membrane potential trace u\u0304+ is above \u03b8\u2212. This happens whenever the postsynaptic neuron spikes shortly after the presynaptic neuron or when the membrane potential is high long enough, i.e. when u\u0304+ exceeds \u03b8\u2212. The LTD term (Eq. 8) governs the decrease of the synaptic efficiency:\nLTD = ALTD ( u\u0304\nu2ref )Xi (u\u0304\u2212 \u2212 \u03b8\u2212)+ (8)\nThe presynaptic spike counter (Xi) is set to one after a spike and zero otherwise. u\u0304\u2212 is a second trace of the postsynapticmembrane potential similar to u\u0304+, but with \u03c4\u2212 > \u03c4+. If it exceeds the threshold \u03b8\u2212 and a presynaptic spike is emitted, LTDoccurs. This happens when the presynaptic neuron spikes after the postsynaptic one. The amplitude of the LTD term, and with that the balance between LTP and LTD, is adjusted with respect to the ratio between u\u0304 and a reference value (u2ref ), hence implementing a homeostatic mechanism (Eq. 9).\n\u03c4u\u0304 du\u0304\ndt = [(u\u2212 EL)+]2 \u2212 u\u0304 (9)\nThe homeostatic variable u\u0304 is computed over the quadratic difference between the postsynaptic membrane potential and the resting potential (EL). When the postsynaptic neuron fires frequently, u\u0304 increases, leading to a higher level of LTD and the weights decreases. In contrast, a lower postsynaptic activity decreases the level of LTD and the weights can increase. Through the ratio of u\u0304with u2ref , this mechanism can enforce the connections to decrease down to the minimum weight bound or increase to the maximum weight bound. This requires hard upper and lower bounds for the weights and leads to a binomial distribution of the weights. The weight change over time depends on both the positive LTP term and the negative LTD term (Eq. 10):\ndw dt = LTP \u2212 LTD (10)\nAll parameters of the neuron model and the basis set of parameters for the learning rule are taken from the original publication [3]. Some parameters of the learning rule differ from experiment to experiment, in particular the reference value of the homeostaticmechanism (u2ref ), the learning rates for the LTP and LTD terms (ALTP andALTD), \u03b8\u2212 and the maximum weight value (wmax). Notice that in the original publication [3] and in the supplementary material only a maximumweight is given for all experiments they analyze the dynamics in a network. To be more specific, it is given for the relation between spiking patterns and connectivity in the ten neuron toy model, in the bigger network with inhibitory neurons, and in the emergence of simple cell receptive fields. Additionally, in the method section of [3], they write that the ten neurons in the toy model are hard bounded between zero and three. For the bigger network, they use hard bounds between [0,0.75]. In the reimplementation, we use [0,0.55] as hard bounds, what\nReScience C 5.3 (#2) \u2013 Larisch 2019 3\nis closer to the upper bound in the bigger network. A table with the different parameters for each task is presented in Table 1 and Table 2. The values for the changed parameters are found experimentally."}, {"heading": "Reproduction of experiments", "text": "Parameter setup \u2014 The experimental protocols are based on the description on the publication of Clopath et al. (2010) [3]. The learning rule was mainly implemented according to the available Matlab source code. The development of the synapses depends on the behavior of the postsynaptic membrane potential, especially for the first two milliseconds after a postsynaptic spike. Although ANNarchy makes it easy to write down the model equations to build up networks, the processing order of the equations is strictly defined by ANNarchy [8]. At each simulation step, neural variables are first updated using spikes emitted at the previous time steps. Spikes are then emitted using the defined conditions. Synaptic variables are then updated (including weight changes if presynaptic or postsynaptic events have been detected). Finally, the value of all desired variables is recorded for that step. Because of this design choice, the execution order for differential equations and non-differential equations of the synapses and neurons is different from the order mentioned in the published Matlab source code. This forced us to change the value of some parameters for some of the simulations. These changed values can be found in the associated Python files and in Table 1 and Table 2. The chosen integration time step can have an influence on the computation result as well. In the original publication, no integration time step ismentioned. In the published Matlab source code, a time step of dt = 1ms is chosen, which we also use.\nExperiment descriptions \u2014 In the original publication, the authors reproduce spike timing triplet experiments in the visual cortex of rats [6]. Furthermore, they investigate the resulting connectivity structure depending on the spiking behavior. To validate the reimplementation, we reproduce the voltage clamp experiment (Fig. 1h in [3]), the classical spike timing-dependent learning window (Fig. 2a in [3]), the frequency repetition task to reproduce a triplet experiment (Fig. 2b in [3]), the burst timingdependent plasticity experiment (Fig. 3 in [3]), the influence of spiking order to connectivity (Fig. 4a, down and Fig. 4b, down in [3]) and the emergence of receptive fields by presenting natural scenes (Fig. 7d in [3]) and the influence of the input firing rate to their size (Fig. 7e in [3]).\nReScience C 5.3 (#2) \u2013 Larisch 2019 4\nFor the sake of readability, a description of the single experiments is given with the corresponding results.\nNon-reproduced experiments \u2014 One experiment that is not reproduced is the experiment with ten excitatory and three inhibitory neurons, using stochastic Poisson input (Fig. 5 in [3]). In the original publication, they presented the emergence of a stable receptive fields and showed that the strength of synapses depends on the input firing rate. Here, we reproduce the emergence of receptive fields by presenting natural scenes and reproduce how the strength of the neuron activity influences the connectivity order. The second experiment not reproduced is the experiment using the same network structure but withmoving input patterns (Fig. 6 in [3]). Clopath and colleagues demonstrated with this experiment that the strength of synapses can depend on the temporal order of emergent spikes and that the receptive field moves over the time, if the input is moving. We reproduce the synapse weight development, depending on the temporal order of spikes. The moving receptive fields are not reproduced here, but by reproducing receptive fields generally, we assume that moving receptive fields would emerge with the here proposed reimplementation."}, {"heading": "Reimplementation", "text": "The reimplementation was done with Python 3.6 (Python 2.7 also works) and the neurosimulator ANNarchy [8] (version 4.6.8.1 or later). With ANNarchy, it is possible to implement neuronal and synaptic behavior by defining the corresponding mathematical equations in a text format, which are solved by ANNarchy using the desired numerical method. ANNarchy supports rate-based and spiking networks and provides a way to combine both kinds of neuronal networks. The network description is done in Python and code generation is used to produce optimized C++ code allowing a good parallel performance. For the voltage-clamp experiment, the pairing repetition task, the STDP learning window and the burst spiking experiments u\u0304 must be u\u0304 = uref as mentioned previously to switch off the homeostatic mechanism. For the connectivity experiments, the emergence of V1 simple-cell-like receptive fields and for the emergence of stable weights, the homeostatic mechanism dynamic as described in the original publication [3] is used. The following explanation of the network is from the implementation in network.py.\nNetwork implementation \u2014 To achieve a correct behavior of the learning rule, a correct implementation of themembrane potential dynamics, especially after a spike, is necessary. The proposed reimplementation relies on the original source code written in Matlab, which uses a counter variable to implement the correct behavior of the membrane potential as shown in the code passage below. The presented code passage is from the aEIF.m file, which is contained in the source code published on modelDB. After the neuron spikes, the counter is set to one. In the next calculation step, the changes in the membrane voltage is set to 32.863 mV. One step later, the membrane potential is set to \u221249.5mV. i f counter ==2\n2 u = E_L+15+6 .0984 ; w = w+b ; 4 w_ta i l = w_jump ; counter = 0 ; 6 V_T = VT_jump+VT_rest ; end\n8\n% Updates of the va r i ab l e s for the aEIF 10 udot = 1 /C*(\u2212g_L * (u\u2212E_L ) + g_L*Delta_T *exp ( ( u\u2212V_T ) / Delta_T )\n\u2212 w +w_ta i l + I ) ; 12 wdot = 1 / tau_w * ( a * (u\u2212E_L ) \u2212 w) ;\nReScience C 5.3 (#2) \u2013 Larisch 2019 5\nu= u + udot ; 14 w = w + wdot ;\nw_ta i l = w_tai l\u2212w_ta i l / t au_wta i l ; 16 V_T = VT_rest / tau_VT+(1\u22121/ tau_VT ) *V_T ;\n18 i f counter == 1 counter = 2 ; 20 u = 29 . 4+3 . 4 62 ; w = w\u2212wdot ; 22 end\n24 i f ( u> th && counter ==0) u = 2 9 . 4 ; 26 counter = 1 ; end\nThe code below shows the definition of the neural equations in ANNarchy. As in the Matlab source code, we use a counter variable to control the behavior of the membrane potential for time steps after a spike, together with the ANNarchy own \u02bcif\u02bc statement. With this variable, we can add the necessary 3.462 mV on the membrane potential one step after the spike, and set it to \u221249.5mV after the second time.\n1 neuron_eqs = \u201d \u201d \u201d dvm/ dt = i f s ta te >=2 :+3 .462 e l se : 3 i f s t a t e ==1: \u2212(vm + 49 . 5 ) + 1 /C* ( Isp \u2212 (wad+b ) ) + g_Exc e l se : 5 1 /C * ( \u2212gL * (vm \u2212 EL ) + gL * DeltaT * exp ( (vm \u2212 VT) / DeltaT ) \u2212 wad + z ) + 7 g_Exc : i n i t = \u221270.6 dvmean / dt = ( pos (vm \u2212 EL ) **2 \u2212 vmean ) / taumean : i n i t = 0 .0 9 dumeanLTD/ dt = (vm \u2212 umeanLTD) / tauLTD : i n i t =\u221270.0 dumeanLTP / dt = (vm \u2212 umeanLTP) / tauLTP : i n i t =\u221270.0\n11 dxtrace / dt = (\u2212 xtrace ) / taux dwad / dt = i f s t a t e ==2:0 e l se : 13 i f s t a t e ==1:+b / tauw e l se : ( a * (vm \u2212 EL ) \u2212 wad) / tauw : i n i t = 0 .0 15 dz / dt = i f s t a t e ==1: \u2212z+Isp\u221210 e l se : \u2212z / tauz : i n i t = 0 .0 17 dVT / dt = i f s t a t e ==1: +(VTMax \u2212 VT)\u22120.4 e l se : ( VTrest \u2212 VT) / tauVT : i n i t =\u221250.4 19 dg_Exc / dt = \u2212g_Exc / tau_gExc s t a t e = i f s t a t e > 0 : s ta te\u22121 e l se : 0 21 Spike = 0 .0 \u201d \u201d \u201d\nthe necessary equations are typed in one multi-string variable (neuron_eqs). The variable vm describes the membrane potential u, vmean the homeostatic variable u\u0304, umeanLTD and umeanLTP represent u\u2212 and u+, respectively. The variable xtrace describes x\u0304, wad is wad, z is z, g_Exc is the input current and Spike is the spike counter (X). To implement the resolution trick, we use a extra discrete variable state. With that, we control the behavior of the different variables after a spike to reproduce the behavior of the variables as in the Matlab source file. The neuron spikes only if the membrane potential exceeds the threshold and if the state variable is equal to zero. spkNeurV1 = Neuron ( parameters = params ,\n2 equations=neuron_eqs , spike= \u201d \u201d \u201d (vm>VT) and ( s t a t e ==0) \u201d \u201d \u201d , 4 r e se t = \u201d \u201d \u201dvm = 29 .0 s t a t e = 2 .0 6 VT = VTMax Spike = 1 .0 8 xtrace += 1 / taux \u201d \u201d \u201d )\nTo define a neuron model, ANNArchy provides the Neuron object, which expects a string object for the parameters (parameters), the equations that describes the neuronal behavior (equations), a string that define the conditions to release a spike (spike) and a string that defines the changes in the variables after a spike (reset).\nReScience C 5.3 (#2) \u2013 Larisch 2019 6\nequatSTDP = \u201d \u201d \u201d 2 l tdTerm_fix = i f w>wMin : ( aLTD* ( vmean_fix / urefsquare ) * pre . Spike * pos ( post .umeanLTD \u2212 thetaLTD ) ) e l s e : 0 .0 4 ltdTerm = i f w>wMin : ( aLTD* ( post . vmean / urefsquare ) * pre . Spike * pos ( post .umeanLTD \u2212 thetaLTD ) ) e l s e : 0 .0 6 ltpTerm = i f w<wMax : ( aLTP * pos ( post .vm \u2212 thetaLTP ) * ( pre . x t race ) * pos ( post . umeanLTP \u2212 thetaLTD ) ) e l s e : 0 . 0 8 deltaW = i f s e t _ f i x ==1: ltpTerm \u2212 l tdTerm_fix e l se :\nltpTerm \u2212 ltdTerm 10 dw/ dt = deltaW :min=0 .0 , max=wMax\u201d \u201d \u201d\nAs for the neuron model, the equations for the STDP learning are defined by strings of the differential equations. ltdTerm describes the LTD term and ltpTerm the LTP term. Variables of the pre- or postsynaptic neuron, defined in the neuron model, can be addressed with the prefixes pre. and post., respectively. With the if w>wMin statement in ltdTerm, the weight only decreases if the weight is above the lower bound. Notice the additional term ltdTerm_fix which contains a fix value for the homeostatic mechanism and the parameter set_fix to decide which term for the weight update is used. In ltpTerm a analogous term is implemented to avoid that weights exceed the upper bound. The parameters are defined in a string, analogous to the parameters of the neuron model. The parameter urefsquare is the homeostatic reference parameter u2ref . The learning rates ALTD and ALTP are defined by aLTD and aLTP, respectively. The threshold \u03b8\u2212 is defined by thetaLTD and \u03b8+ by thetaLTP. The parameter transmit is either zero or one, depending on whether the synaptic current for the experiment should transmit or not. f fSyn = Synapse ( parameters = parameterFF ,\n2 equations= equatSTDP , pre_spike= \u02bc \u02bc \u02bc g_ ta rge t += w* transmit \u02bc \u02bc \u02bc )\nANNarchy provides a Synapse object, that expects a parameters argument, the equations and a description of what happens when the pre-synaptic neuron spikes (pre_spike). After a pre-synaptic spike, the input current of the postsynaptic neurons increases by the value of the synaptic weight. g_target is an alias for the postsynaptic conductance that should be increased (g_Exc in the AdEx neurons).\nImplementation of the Experiments \u2014 The implementation of the different experiments are in the corresponding python files. To perform an experiment, the network with the neuron populations and the weights between them must be initialized.\nTo create a population, ANNarchy provides the Population object. The geometry argument expects a tuple or a integer and defines the spatial geometry, respectively the number of neurons in the population. The neuron arguments expects a Neuron object. It defines the usedmodel for population neurons. There is also a set of predefined Population objects in ANNarchy, for example the PoissonPopulation. This object provides a population of spiking neurons, whose spiking behavior follows a Poisson distribution with a given rate. poisPop = PoissonPopulation ( geometry =10 , r a t e s =100 . 0 )\n2 pop_Ten = Population ( geometry =10 , neuron=spkNeurV1 , name= \u201dpop_Ten \u201d )\nTo connect two neuron populations and define the weightmatrix between them, ANNarchy provides the Projection object. The pre argument defines the pre-synaptic population and the post argument the postsynaptic population. The target argument defines the target variable of the postsynaptic neuron, which is increased by the weight value after a pre-synaptic spike. projInp_Ten = Pro jec t ion (\n2 pre = poisPop , post = pop_Ten , 4 t a r g e t = \u02bc Exc \u02bc ) . connect_one_to_one ( weights = 3 0 . 0 )\n6\nprojTen_Ten = Pro jec t ion ( 8 pre= pop_Ten ,\nReScience C 5.3 (#2) \u2013 Larisch 2019 7\npost= pop_Ten , 10 t a r g e t = \u02bc Exc \u02bc , synapse= ffSyn 12 ) . connec t_a l l _ t o_a l l ( weights = 0 . 1 , a l low_se l f_connect ions =True )"}, {"heading": "Results", "text": "To prove the correctness of the proposed reimplementation, different experiments of the original paper have been reproduced. Although most results are reproduced successfully, some experiments could not be absolutely reproduced or exhibit small differences. All weight changes are shown relatively to the initial weight values, which are not shown in the original publication. Because of that, initial values are mainly found experimentally and are shown in the corresponding tables of parameters.."}, {"heading": "Voltage-Clamp experiment", "text": "To reproduce the voltage clamp experiment, the presynaptic neuron spikes with a constant firing rate of 25 Hz for 50 s. The postsynaptic membrane potential is changed from a fixed value of \u201380mV to 0mV. We recorded for different values of the postsynaptic membrane potential the weight change. Resulting changes in the learning rule are implemented as mentioned in the original publication. The results of the voltage-clamp experiment are shown in Fig. 1-left. The blue line represents the weight change with the standard parameter set for the visual cortex and the red line represents the weight change with the parameter set for the hippocampus, as mentioned in the original publication [3]. The two dotted lines mark the \u03b8\u2212 and \u03b8+ thresholds from the learning rule, with the standard data set. With the visual cortex data set (blue line), the weight decreases slightly if the membrane potential exceeds the \u03b8\u2212 threshold and increases after it exceeds \u03b8+. With the hippocampus data set (red line) (\u03b8\u2212 = \u221241.0 mV, \u03b8+ = \u221238.0 mV, ALTD = 3.8 \u00d7 10\u22124,ALTP = 0.2 \u00d7 10\u22124 ), the weight decreases at a postsynaptic membrane voltage value of \u221241.0mV and increases around \u221220mV. This matches with the results of the Clopath et al. (2010) [3] publication.\nPair-based and triplet STDP experiments To reproduce the STDP learning window, we create a list of discrete time points where the pre- or postsynaptic neurons should emit spikes. The presynaptic neuron spikes every 50ms. The postsynaptic neuron spikes in a range from 1ms to 15msbefore or after the presynaptic neuron. Both neurons are AdEx neurons and connected to one input\nReScience C 5.3 (#2) \u2013 Larisch 2019 8\nneuron each to control the spiking behavior. As mentioned in the original publication the variable of the homeostatic mechanism is set to (u\u0304 = uref ). The classic pair-based spike timing learning window is presented in Fig. 1-middle. If the postsynaptic neuron spikes before the presynaptic one, LTD occurs (red line). If the postsynaptic neuron spikes after the presynaptic one, LTP occurs (blue line). The xaxis represents the time difference between pre- and postsynaptic spikes, relative to the postsynaptic spike. The resulting graph is similar to the one presented in the original publication. A slight difference can be seen in the higher positive and negative changes: in the original publication the normalized weight at the time gap\u221210ms is around 70%, while it is around 80% in our simulation. This could be caused by a different internal processing of ANNarchy, but we coud not isolate the reason.\nFor the repetition frequency experiment and the triplet experiment, the number of preand postsynaptic spike pairs increases from a pair frequency of 0.1 Hz to 50 Hz. The time between the pre- and postsynaptic spikes of a pair is 10 ms. As in the previous experiment, we implement a network with two AdEx neurons connected to each other in order to observe the weight change. To reproduce this experiment, it was necessary to set u\u0304 to a fixed value, as mentioned in the original publication [3]. The parameter changes are shown in Table 2. The analysis of the pairing repetition frequency task is shown in Fig. 1-right. With lower repetition frequencies, post-pre pairs (red line) lead to LTD. At a repetition frequency around 30Hz, the post-pre pairs are under the influence of the next post-pre pair and the post-pre-post triplets lead to LTP. If the repetition frequency of post-pre pairs is around 50 Hz, the same amount of LTP emerges as in pre-post pairs. These results are similar to the original paper."}, {"heading": "Spike bursts", "text": "Clopath and colleagues [3]modeled threeburst timing-dependent plasticity experiments. In the first task, they changed the number of postsynaptic spikes from one up to three, with either +10ms or \u221210ms between the presynaptic and the first postsynaptic spike. The postsynaptic neuron fires with 50 Hz. More precisely, there is 20 ms between each\nReScience C 5.3 (#2) \u2013 Larisch 2019 9\nof the one, two or three spikes. In the second experiment, they recorded the weight change when a presynaptic spike is followed by three postsynaptic spikes with varying postsynaptic firing rates (from 20Hz to 100Hz). As in the first experiment, they observe the weight change for the case where the first postsynaptic spike appears 10 ms after or 10ms before the presynaptic spike. The variation of the time lag between one presynaptic spike and three postsynaptic spikes is the focus of the third experiment. For that, the postsynaptic neuron fires with a constant rate of 50Hz and the time lag varies from \u2212100 to +60ms. To implement these experiments, we define a network with two of the AdEx neurons. Each of these neurons is connected to one input neuron to control the discrete time points of the spiking events. For all three burst spiking experiments, the normal parameter set is used, and u\u0304 is set to a fixed value, as mentioned in the original publication. The result from the reimplementation of the first spiking burst experiment is shown in Fig. 2-upper-left. The upper marks represent the weight change with +10 ms, the lower marks the weight change with \u221210 ms between the presynaptic spikes and the first postsynaptic one. As in Clopath et al. (2010) [3] and the experimental paper of Nevian and Sakmann (2006) [7], one postsynaptic spike, independently of the spiking order, leads only to a small weight change. A second spike leads to a bigger change, especially when the postsynaptic neurons spikes after the presynaptic one. The second task investigates the weight change depending on the frequency between three postsynaptic spikes (Fig. 2-upper-right). As in the previous experiment, the upper line represents the weight changes when the first postsynaptic spike appears 10ms after the presynaptic spike; the lower line when the first postsynaptic spike appears 10 ms before the presynaptic one. As shown in [3], a higher frequency leads to a higher change in the synaptic efficiency. The third task, investigating the weight change as a function of the time between one presynaptic spike and the first of three postsynaptic spikes, is presented in Fig. 2-down. The curve is also very similar to the one presented in [3]. The label on the y-axes is the weight value in percentage, but, as mentioned in the original experimental paper by Nevian and Sakmann (2006) [7], the graphs should actually show the weight changes in percentage, relative to the initial weight value."}, {"heading": "Connectivity analysis", "text": "In addition to the replication of experimental findings of pair-based and triplet STDP experiments, Clopath and colleagues [3] presented how the synaptic connectivity (emerging from the proposed learning rule) is influenced by the spiking patterns of the neurons. Fig. 3 a shows the obtained connectivity structure if neurons fire at different frequencies. The color scheme is similar to the original publication: weak connections (above 34 of themaximum activity) are blue, strongly unidirectional connections are yellowwhile strong bidirectional connections are red. To analyze the dependencybetween the connectivity andfiring rate (or number of spikes) , a small network with ten interconnected AdEx neurons is built. Each neuron receives an input from one additional neuron, with Poisson-distributed spike patterns. The firing rate of each Poisson neuron is increased from 2 Hz to 20 Hz, influencing the firing rate of the 10 corresponding neurons in the network. We repeated the protocol with the pair-based STDP rule by Song and Abbott (2001) [9] to investigate whether the triplet STDP rule by Clopath et al. (2010) led to a different connectivity structure, as mentioned in the original publication [3]. Neurons with similarly high firing rates develop strong bidirectional connections, because they are often active at the same time. This suggests that learning is based on the correlation between the neuronal activities in a Hebbian manner. Weak connections are assigned to neurons with a low firing rate below 5 Hz. If the postsynaptic neuron is firing with a rate above 5 Hz, strong unidirectional weights emerge. This is in line with the connection pattern presented in the original paper (Fig. 4 in [3]). As mentioned in\nReScience C 5.3 (#2) \u2013 Larisch 2019 10\nReScience C 5.3 (#2) \u2013 Larisch 2019 11\nthe original article, we repeated the test with a pair based STDP rule [9]. As shown on Fig. 3 b, strong unidirectional connections are emerge if the pre- and the postsynaptic neurons firing are relatively high. This is interesting, as Clopath and colleagues [3] observed a less specific pattern in the connectivity for the pair based STDP rule. Despite this, we did not observe the emergence of strong bidirectional connections between neurons with a high firing rate, as reported in [3]. To analyze how the temporal order of release spikes can influence the connectivity, we use again a small network with ten interconnected AdEx neurons. Each of these neurons receives an input from one addition neuron. These additional neurons spikes one after another with a time delay of 20 ms. This realize a temporal spiking order of the ten recurrent connected neurons. To establish stable weights, the normal homeostatic mechanism is used. As for the protocol with varying firing rates, we repeated this protocol with the pair-based STDP rule by Song and Abbott (2001) [9]. If the neurons fire in a specific temporal order, this sequence is reflected in the connection pattern (Fig. 3 c). As in the original paper, the connections from neurons which are firing a long time after or before the postsynaptic neuron are weak, while they are strong to neurons which fired a short time after the neurons. Fig. 3 d shows a similar connectivity structure for the pair based STDP rule [9]. A similar result is reported in Clopath et al. (2010) [3]."}, {"heading": "Receptive fields", "text": "Besides the experiments from the original publication, we reimplemented the experiment for the emergence of stable weights out of theMatlab source code. The emergence of stable weights was achieved by presenting a Gaussian input over 500 presynaptic neurons and one postsynaptic AdEx neuron. For each trial (100 ms), ten Gaussian patterns\nReScience C 5.3 (#2) \u2013 Larisch 2019 12\nare created to determine the activity of the 500 input neurons. As in the Matlab source code, the learning rates (ALTP and ALTD) are increased by a factor of ten to speed up the learning. In the original Matlab source code, the emergence of stable weights was demonstrated using a one-dimensional input with 500 values. At each time step, a subset of close neurons are activated. The emergent stable weights are shown in Fig. 4 a. After 500 epochs, a spatially related subset of weights have increased to the maximum and the other weight values have decreased down to zero. This leads to a specific selectivity for the postsynaptic neuron. For the emergence of V1 simple-cell-like receptive fields, a network with one postsynaptic AdExneuron and 16\u00d716\u00d72presynaptic neurons is used. Asmentioned in the original publication [3], the activity of the presynaptic population depends on the pixel values of a 16 \u00d7 16 pixel sized patch, cut out of pre-whitened natural scenes [10]. The maximum input firing rates in the original publication are set to 50.0 Hz. In the here presented reimplementation, the maximum firing rate is set to a higher value of 60 Hz. Further, the learning rates for the LTP term (ALTP ) and the LTD term (ALTD) are reduced (see Table 1). The pixel values of the patch are normalized with the maximum pixel value of the current image and divided into an ON (only positive pixel values) and an OFF (only negative pixel values) image. The presynaptic population spikes are generated by a Poisson process. To establish stable weights, the normal homeostatic mechanism is used. Additionally, Clopath et al. (2010) [3] presented the influence of a different input firing rates. They presented that for 25 Hz, 37.5 Hz and 75 Hz the receptive field sizes shrinks for higher input firing rates. We reproduce the experiment by showing 60Hz and 75Hz. The emergence of a cluster of strong synaptic weights can be interpreted as the formation of a receptive field, which defines the selectivity of the neuron, similar to the receptive fields in the primary visual cortex (V1) as shown in Fig. 4 b. To reproduce it, one postsynaptic neuron receives input from 512 presynaptic neurons, as described in the original publication. Each presynaptic neuron corresponds to one pixel of the 16 \u00d7 16 input, divided into an ON-part for the positive and an OFF-part for the negative values (16\u00d7 16\u00d7 2 = 512). The 300.000 presented patches are randomly cut out of ten natural scenes [10], and each patch is presented for 200 ms. Contrary to the original publication by Clopath et al. (2010) [3], we use a much higher firing rate of 60 Hz to achieve the emergence of similar sized receptive fields. It is to mentioned, that the shaped of the receptive fields depends mainly on the input firing rate, the balance between the learning rates for LTP (ALTP ) and LTD (ALTD), the reference value of the homeostatic mechanism (u2ref ) and the maximum weight. So it is possible to show the emergence of receptive fields with a lower firing rate, but needs more changes on the other parameters. For the reimplementation, we changed different parameters just a bit to be as close as possible on the original parameter set. Fig. 4 c shows that a high firing rate of 75Hz (top) leads to the emergence of a smaller receptive field, and a lower firing rate of 60 Hz to a much bigger one. The same influence of the input firing rate was presented in Clopath et al. (2010) [3]."}, {"heading": "Conclusion", "text": "Our reimplementation of voltage based STDP learning rule from Clopath et al. (2010) [3] is able to reproduce the experimental data and the emergence connectivity structures proposed in the original paper as well as the emergence of orientation selective receptive fields, like those in the primary visual cortex (V1). In comparison to the graphs in the original publication, our reimplementation shows little differences in the curve shapes in the STDP window (Fig. 1-middle), the weight change as a function of the pairrepetition frequency (Fig. 1-left) and for the weight change as a function of the burst frequency of the postsynaptic neuron (Fig. 2-upper-right). However, the curves show the same tendency as in the original publication. To show the emergence of a specific\nReScience C 5.3 (#2) \u2013 Larisch 2019 13\nconnectivity structure depending on the input fire rate and temporal code we had to reduce the hard upper bound of the weights. Interestingly, we observed for the pair based learning rule [9] a different connectivity structure than reported in Clopath et al. (2010) [3]. We did not investigate furtherwhere these differences came from. We assume that differences in the implementation of the learning rule are responsible, but further analysis is necessary. The description of the learning rule in the original publication contained enough details to understand the different components and their interaction. However, twomain components of the learning rule have not been described adequately to allow a direct reimplementation: the resolution trick of the membrane potential after spike emission, and the equation for the homeostatic mechanism (u\u0304). The emergence of stable weights with high and low values depends on a functional homeostatic mechanism, as mentioned in the original publication [3], but the formula to calculate the homeostatic variable u\u0304 is not described in the publication. Because of this, the reimplementation has greatly benefited from the release of the source code on modelDB, where the correct behavior of the neuron and the homeostatic mechanism is written. Furthermore, initial weight values are not given for all experiments in the original publication, complicating the reproduction of the experiments. Initial weights for the experiments can be found in the corresponding Python files. Note that the weights can change from task to task to achieve the proportional values for the figures. As shown in the supplementary material, the reimplementation was created to fit the Matlab code on modelDB as exactly as possible. Nonetheless, changing the parameters suggests that our reimplementation is not fully correct and can lead to differences in the execution and the result of the equations. One reason for that is the execution order of the equations in ANNarchy, what we can not control andwhat differs from the execution order in the provided Matlab code. We have observed that changing some parameters for most of the tasks was necessary to reproduce the results, but this seems to be a common problem in reimplementing models [11]. Together with the post synaptic voltage dependency of the learning rule, this can lead to a different behavior. Small differences in the membrane potential can lead to a change in the weight development, what leads to a different development of the membrane potential and so on. We assume that this is why for long running experiments our reimplementation needs different parameter values, such as the input firing rate, than what is mentioned in Clopath et al. (2010) [3]."}, {"heading": "Acknowledgment", "text": "This work was supported by the European Social Fund (ESF) and the Freistaat Sachsen."}], "title": "[Re] Connectivity reflects coding a model of voltage-based STDP with homeostasis", "year": 2019}