{"abstractText": "The code that was provided by the authors in Pytorch was reimplemented in Tensorflow 1.x for the pretrained StyleGAN and StyleGAN2 architectures. This was done with the help of the APIs provided by the original authors of these models. The experimentswere runona laptopwith an Intel(R) Core(TM) i7\u20108750HCPU@2.20GHz processor, 16GB RAM, NVIDIA GeForce GTX 1060 with Max\u2010Q Design (6GB VRAM) GPU, and Ubuntu 18.04.5 LTS.", "authors": [{"affiliations": [], "name": "Vishnu Asutosh Dasu"}, {"affiliations": [], "name": "Midhush Manohar T.K"}, {"affiliations": [], "name": "Koustuv Sinha"}, {"affiliations": [], "name": "Sharath Chandra Raparthy"}], "id": "SP:5593466ef17e825c89ccb8aa37efcf5e3583a604", "references": [{"authors": ["I.J. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A.C. Courville", "Y. Bengio"], "title": "Generative Adversarial Nets.", "venue": "Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems", "year": 2014}, {"authors": ["Y. Shen", "J. Gu", "X. Tang", "B. Zhou"], "title": "Interpreting the Latent Space of GANs for Semantic Face Editing.", "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition,", "year": 2020}, {"authors": ["E. H\u00e4rk\u00f6nen", "A. Hertzmann", "J. Lehtinen", "S. Paris"], "title": "GANSpace: Discovering Interpretable GAN Controls.", "venue": "Proc. NeurIPS", "year": 2020}, {"authors": ["T. Karras", "S. Laine", "T. Aila"], "title": "A Style-Based Generator Architecture for Generative Adversarial Networks.", "venue": "IEEE Conference on Computer Vision and Pattern Recognition,", "year": 2019}, {"authors": ["T. Karras", "S. Laine", "M. Aittala", "J. Hellsten", "J. Lehtinen", "T. Aila"], "title": "Analyzing and Improving the Image Quality of StyleGAN.", "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition,", "year": 2020}, {"authors": ["A. Brock", "J. Donahue", "K. Simonyan"], "title": "Large Scale GAN Training for High Fidelity Natural Image Synthesis.", "venue": "International Conference on Learning Representations,", "year": 2019}, {"authors": ["H. Hotelling"], "title": "Analysis of a complex of statistical variables into principal components.", "venue": "Journal of educational psychology", "year": 1933}, {"authors": ["I.T. Jolliffe", "J. Cadima"], "title": "Principal component analysis: a review and recent developments.", "venue": "Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences", "year": 2016}], "sections": [{"text": "R E S C I E N C E C Replication / ML Reproducibility Challenge 2021\n[Re] GANSpace: Discovering Interpretable GAN Controls\nVishnu Asutosh Dasu1, ID and Midhush Manohar T.K.2, ID 1TCS Research and Innovation, Bangalore, India \u2013 2Akamai Technologies, Inc., Bangalore, India\nEdited by Koustuv Sinha,\nSharath Chandra Raparthy\nReviewed by Anonymous Reviewers\nReceived 04 February 2022\nPublished 23 May 2022\nDOI 10.5281/zenodo.6574645"}, {"heading": "Reproducibility Summary", "text": ""}, {"heading": "Scope of Reproducibility", "text": "The authors introduce a novel approach to analyze Generative Adversarial Networks (GANs) and create interpretable controls for image manipulation and synthesis. This is done by identifying important latent directions based on Principal Component Anal\u2010 ysis (PCA) applied either in the latent space or the feature space. We aim to validate the claims and reproduce the results of the original paper."}, {"heading": "Methodology", "text": "The code that was provided by the authors in Pytorch was reimplemented in Tensorflow 1.x for the pretrained StyleGAN and StyleGAN2 architectures. This was done with the help of the APIs provided by the original authors of these models. The experimentswere runona laptopwith an Intel(R) Core(TM) i7\u20108750HCPU@2.20GHz processor, 16GB RAM, NVIDIA GeForce GTX 1060 with Max\u2010Q Design (6GB VRAM) GPU, and Ubuntu 18.04.5 LTS."}, {"heading": "Results", "text": "Wewere able to reproduce the results and verify the claims made by the authors for the StyleGAN and StyleGAN2 models by recreating the modified images, given the seed and other configuration parameters. Additionally, we also perform our own experiments to identify new edits and extend the truncation trick to images generated using StyleGAN."}, {"heading": "What was easy", "text": "The paper provides detailed explanations for the different mathematical concepts that were involved in the proposedmethod. This, augmentedwith a well\u2010structured and doc\u2010 umented code repository, allowed us to understand the major ideas in a relatively short period of time. Running the experiments using the original codebase was straightfor\u2010 ward and highly efficient as well, as the authors have taken additional steps to employ batch processing wherever possible.\nCopyright \u00a9 2022 V.A. Dasu and M.M. T.K., released under a Creative Commons Attribution 4.0 International license. Correspondence should be addressed to Vishnu Asutosh Dasu (vishnu98dasu@gmail.com) The authors have declared that no competing interests exist. Code is available at https://github.com/midsterx/ReGANSpace \u2013 DOI 10.5281/zenodo.6511501. \u2013 SWH swh:1:dir:4dc4de7856350a4671d97840c5f9ae013c275112. Open peer review is available at https://openreview.net/forum?id=BtZVD2f7n0F.\nReScience C 8.2 (#10) \u2013 Dasu and T.K. 2022 1"}, {"heading": "What was difficult", "text": "Originallywewere attempting to recreate identical imageswith zero delta in the RGB val\u2010 ues. However, due to differences in the random number generators between PyTorch\u2010 CPU, PyTorch\u2010GPU and Numpy, the random values were not the same even with the same seed. This resulted in minute differences in the background artifacts of the gen\u2010 erated images. Additionally, there is a lack of open source Tensorflow 1.x APIs to access the intermediate layers of the BigGAN model. Due to time constraints, we were unable to implement these accessors and verify the images that the authors of GANSpace cre\u2010 ated using BigGAN.\nCommunication with original authors While conducting our experiments, we did not contact the original authors. The paper and codebasewere organizedwell and aided us in effectively reproducing and validating the authors\u2019 claims.\nReScience C 8.2 (#10) \u2013 Dasu and T.K. 2022 2\n1 Introduction\nA Generative Adversarial Network (GAN)[1] is a machine learning framework where two neural networks, the discriminator and the generator, competewith each other in a zero\u2010 sum game. The generator tries to trick the discriminator into believing that artificially generated samples belong to real data. GANs have proven to be powerful image synthesis tools and are capable of producing high quality images. However, they provide little control over the features of the gen\u2010 erated image. Existing solutions[2] that add user control over the generated images re\u2010 quire expensive supervised training on latent vectors. GANSpace[3] proposes a simple technique to discover interpretable GAN controls in an unsupervised manner. This is done by identifying important latent directions based on Principal Component Analysis (PCA) applied either on the latent space or the fea\u2010 ture space. The author\u2019s experiments on StyleGAN[4], StyleGAN2[5] and BigGAN512\u2010 deep[6] demonstrate that layer\u2010wise decomposition of PCA directions leads to many in\u2010 terpretable controls, which affect both low and high level attributes of the output image.\n2 Scope of reproducibility\nFor our reproduction study, we aim to validate the effectiveness of the proposed tech\u2010 nique in offering powerful interpretable controls on the output images in an unsuper\u2010 vised manner. The following claims of the paper have been verified and tested successfully:\n\u2022 PCA can be used to highlight important directions in the GAN\u2019s latent space. \u2022 The GAN\u2019s output can be controlled easily in an unsupervised fashion. \u2022 The earlier components control the higher\u2010level aspects of an image, while the later directions primarily affect the minute details. \u2022 Random directions do not yield meaningful decompositions as compared to the principal components identified using PCA.\n3 Methodology\nThe principal components[7] of a collection of points in real coordinate space are a se\u2010 quence of p unit vectors, where the ith vector is a direction of the line that best fits the data while being orthogonal to the remaining i\u22121 vectors. Principal Component Analy\u2010 sis (PCA) is an unsupervised algorithm used to compute the principal components and perform a change of basis of the data using one or more of the computed components, increasing the interpretability of the data while minimizing its information loss[8]. It is commonly used in exploratory data analysis and for dimensionality reduction when dealing with high\u2010dimensional noisy data. The authors of GANSpace propose a tech\u2010 nique for identifying interpretable controls in an unsupervised fashion on pretrained GANs using PCA. Specifically, they show that layer\u2010wise perturbations along the princi\u2010 pal components generated using PCA on the latent space of StyleGAN based networks can be used to generate human\u2010interpretable transformations on the synthesized im\u2010 ages. Mathematically, a GAN can be expressed as a neural network G(z) that generates an image I : z \u223c p(z), I = G(z). Here, p(z) is a probability distribution from which the latent vector z is sampled. The network G(z) can be further decomposed into L inter\u2010 mediate layersG1 . . . GL. In the StyleGAN/StyleGAN2models, the input to the first layer is a constant y0. The output and input to the remaining layers is computed as:\nyi = Gi(yi\u22121,w), where w =M(z) (1)\nReScience C 8.2 (#10) \u2013 Dasu and T.K. 2022 3\nM is a an 8\u2010layer multilayer perceptron which is a non\u2010linear function of z. The num\u2010 ber of layers L depends on the resolution of the generated image. At each layer, the generated image is upsampled by a factor of 2.\nThe images generated by StyleGAN and StyleGAN2 can be controlled by identifying the principal axes of p(w), which is the probability distribution of the output of themapping networkM . First, we sampleN latent vectors z1:N and compute the correspondingwi = M(zi). The PCA of these w1:N values gives us the basis V for W. The output attributes of a new image given by w can then be controlled by varying the PCA coordinates of x before feeding them into the synthesis network:\nw\u2032 = w+ Vx (2)\nEach entry xk of x is a separate control parameter which can be modified to update the desired attributes of the output image. We follow the same notation used by the authors to denote edit directions in this report. E(vi, j \u2212 k) means moving along component vi from layers j to k. Identifying specific edits, for example \u201dchanging the color of a car\u201d, is done via exploratory analysis using\nReScience C 8.2 (#10) \u2013 Dasu and T.K. 2022 4\na trial\u2010and\u2010error method. The authors have created a GUI\u2010based application for this purpose.\n3.1 Model descriptions\nWe use NVIDIA\u2019s official implementation of StyleGAN1 and StyleGAN22 models. The original code uses a PyTorch/NumPy implementation of StyleGAN and StyleGAN2which creates a PyTorch model and copies the weights from NVLabs\u2019 implementations which are in Tensorflow. However, we directly use the NVLabs\u2019 APIs with NumPy and make changes to the official GANSpace codebase to support the same.\n3.2 Datasets The experiments in the paper were performed using the FFHQ, LSUN Car, CelebA\u2010HQ, Wikiart, Horse and Cat datasets. The official Tensorflow implementation of StyleGAN contains links to download pretrained models on FFHQ, LSUN Car, Wikiart, Horse and Cat. Themodels trainedonWikiartwere downloaded fromawesome\u2010pretrained\u2010stylegan3. In addition to the datasets used by the authors, we also perform our own experiments on the Beetles dataset which was downloaded from awesome\u2010pretrained\u2010stylegan24.\n3.3 Experimental setup All the experiments were conducted on a laptopwith an Intel(R) Core(TM) i7\u20108750HCPU @ 2.20GHz processor, 16GB RAM, NVIDIA GeForce GTX 1060 with Max\u2010Q Design (6GB VRAM) GPU, and Ubuntu 18.04.5 LTS. The generated images from our experiments were evaluated visually to determine whether the edits were working as expected.\n4 Results\nWe were able to reproduce the results and verify the claims (mentioned in Section 2) made by the authors for the StyleGAN and StyleGAN2models by recreating themodified images, given the configuration parameters. Additionally, we also perform our own ex\u2010 periments to provide additional results that validate the effectiveness of the technique employed by GANSpace.\n4.1 Effectiveness of PCA\n1https://github.com/NVlabs/stylegan 2https://github.com/NVlabs/stylegan2 3https://github.com/justinpinkney/awesome-pretrained-stylegan 4https://github.com/justinpinkney/awesome-pretrained-stylegan2\nReScience C 8.2 (#10) \u2013 Dasu and T.K. 2022 5\nFigure 3 highlights the effectiveness of PCAon changing the lowandhigh level attributes of the image. We are able to control object shape, colour and pose as well as nuanced landscape attributes. The edit directions corresponding to eachof the edits are: E(v22, 9\u221210) (\u201dChangeColor\u201d), E(v11, 9\u2212 10) (\u201dAdd Grass\u201d), E(v0, 0\u2212 4) (\u201dRotate\u201d) and E(v16, 3\u2212 5) (\u201dChange type\u201d).\n4.2 Unsupervised vs. Supervised methods\nPrevious methods for finding interpretable directions in GAN latent spaces require ex\u2010 ternal supervision, such as labeled training images or pretrained classifiers. GANSpace, on the other hand, automatically identifies variations intrinsic to the model without su\u2010 pervision. This has been validated using the CelebA\u2010HQ Faces dataset by comparing the edit directions found through PCA to those found in previous works using supervised methods. Figure 4 shows that comparable edits can be obtained in a completely unsupervised fashion. Additionally, GANSpace can be used to identify new edits which have not been previously demonstrated. Supervisedmethods are not viable for this task as supervising each new edit would be costly. It is also difficult to know in advancewhich edits are even possible in supervised approaches.\nReScience C 8.2 (#10) \u2013 Dasu and T.K. 2022 6\n4.3 PCA components vs. Random directions\nThe original authors claim that the earlier PCA components primarily control the geom\u2010 etry and other high\u2010level aspects (pose and style), while the lower components capture minute details. Additionally, they claim that fixing and randomizing randomly\u2010chosen directions do not yield PCA\u2010like meaningful decompositions, thus showing the impor\u2010 tance of identifying good directions using PCA. This has been illustrated in Figure 5, where different subsets of principal coordinates and random coordinates are random\u2010 ized while keeping the latent vector constant. In Figure 5a, the first eight principal co\u2010 ordinates x0:7 are fixed and the remaining 504 coordinates x8:512 are randomized. This changes the background and appearance of the cat while keeping the cat\u2019s pose and cam\u2010 era angle constant. Conversely, Figure 5b shows that fixing the last 504 coordinates and randomizing the first eight yields images where the camera and orientation vary, but the color and appearance are held roughly constant. Figure 5c and Figure 5d shows the results of the same process applied to random directions. The images illustrate that any given 8 directions have no distinctive effect on the output.\n4.4 Additional results not present in the original paper\nNew edits \u2014We identify new edits on the Stylegan2 Beetles dataset. Edit E(v2, 0 \u2212 17), referred to as \u201dPatterns\u201d, adds a pattern on the shell of the beetle as well as increasing the overall size of the beetle. The generated pattern varies depending on the seed used to sample w.\nReScience C 8.2 (#10) \u2013 Dasu and T.K. 2022 7\nTruncation Trick on StyleGAN \u2014 The \u201dTruncation Trick\u201d is a procedure applied to the latent vectors to improve the quality of the generated images at the expense of variety in the images. It does this by sampling the latent vectors from a truncated distribution that is closer to the average of the latent vectors sampled during training, thereby reducing the variance of the latent vectors used during inference. The authors of [6] show that using the truncation trick improves the Fr\u00e9chet Inception Distance (FID) and Inception Score (IS). In the StyleGAN/StyleGAN2models, the truncation trick is applied on the latent spacew, which is the output of the mapping networkM . During the training process, a running average wavg of the latents is computed. Later, the latents sampled during inference are truncated to lie close to wavg. Equation 3 shows the truncation process on Style\u2010 GAN/StyleGAN2 models:\nw\u2032 = wavg + \u03c8(w\u2212wavg) (3)\nDuring our experiments, we noticed that the original authors use the truncation trick on images generated using StyleGAN2 to reduce the number of artifacts. However, this is not enabled for StyleGAN images. We found that enabling truncation while applying edits on StyleGAN images improved their quality as well. We demonstrate this using the Wikiart dataset through the \u201dHeadRotation\u201d (E(v7, 0\u22121)) and \u201dSimple Strokes\u201d (E(v9, 8\u2212 14) edits. In Figure 7, we can see that the generated faces contain less noise and artifacts when the truncation trick is used. For example, the lower half of the person\u2019s face in the \u201dHead Rotation\u201d image does not contain as much noise as their counterpart which does not employ the truncation trick. Here, we can also observe the change in the generated images as truncation psi is decreased a lower value of 0.25. This truncates the sampled latents to lie very close to the average and results in images that look very similar to each other. If truncation psi is set to 0, then according to Equation 3, we can see that the truncated latent w\u2032 is always equal to wavg.\n5 Discussion\nAfter performing our experiments, we feel that the results justify the claims of the pa\u2010 per. This is further bolstered by the fact that the proposed method worked on different datasets which were not covered by the original authors.\nReScience C 8.2 (#10) \u2013 Dasu and T.K. 2022 8\nReScience C 8.2 (#10) \u2013 Dasu and T.K. 2022 9\nWe were not able to replicate the author\u2019s experiments on BigGAN512\u2010deep due to time constraints.\n5.3 Communication with original authors While conducting our experiments, we did not contact the original authors. The paper and codebasewere organizedwell and aided us in effectively reproducing and validating the authors\u2019 claims."}], "title": "[Re] GANSpace: Discovering Interpretable GAN Controls", "year": 2022}