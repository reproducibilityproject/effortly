{"abstractText": "In this paper, we attempt to verify the claims that the paper [1] makes about their pro\u2010 posed CGN framework that decomposes the image generation process into independent causal mechanisms. Further, the author claims that these counterfactual images im\u2010 proves the out\u2010of\u2010distribution robustness of the classifier. We use the code provided by the authors to replicate several experiments in the original paper and draw conclusions based on these results.", "authors": [{"affiliations": [], "name": "Ankit Ankit"}, {"affiliations": [], "name": "Sameer Ambekar"}, {"affiliations": [], "name": "Baradwaj Varadharajan"}, {"affiliations": [], "name": "Mark Alence"}, {"affiliations": [], "name": "Koustuv Sinha"}, {"affiliations": [], "name": "Sharath Chandra Raparthy"}], "id": "SP:0bda648476dee677879adb7a2f307eeb6439df18", "references": [{"authors": ["A. Sauer", "A. Geiger"], "title": "Counterfactual generative networks.", "venue": "arXiv preprint arXiv:2101.06046", "year": 2021}, {"authors": ["A. Brock", "J. Donahue"], "title": "and K", "venue": "Simonyan. Large Scale GAN Training for High Fidelity Natural Image Synthesis.", "year": 2019}, {"authors": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li"], "title": "K", "venue": "Li, and F.-F. Li. \u201cImageNet: a Large-Scale Hierarchical Image Database.\u201d In: June", "year": 2009}, {"authors": ["X. Qin", "Z. Zhang", "C. Huang", "M. Dehghan", "O.R. Zaiane", "M. Jagersand"], "title": "U2-Net: Going deeper with nested U-structure for salient object detection.", "venue": "Pattern Recognition", "year": 2020}, {"authors": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li"], "title": "and L", "venue": "Fei-Fei. \u201cImagenet: A large-scale hierarchical image database.\u201d In: 2009 IEEE conference on computer vision and pattern recognition. Ieee.", "year": 2009}, {"authors": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "title": "Gradient-based learning applied to document recognition.", "venue": "Proceedings of the IEEE", "year": 1998}, {"authors": ["O. Russakovsky"], "title": "ImageNet Large Scale Visual Recognition Challenge.", "venue": "In: International Journal of Computer Vision (IJCV)", "year": 2015}, {"authors": ["Z. Wang", "A.C. Bovik", "H.R. Sheikh", "E.P. Simoncelli"], "title": "Image quality assessment: from error visibility to structural similarity.", "venue": "IEEE transactions on image processing", "year": 2004}, {"authors": ["H. Zhao", "O. Gallo", "I. Frosio", "J. Kautz"], "title": "Loss functions for image restoration with neural networks.", "venue": "IEEE Transactions on computational imaging", "year": 2016}, {"authors": ["P. Pandey", "A.K. Tyagi", "S. Ambekar"], "title": "and A", "venue": "Prathosh. \u201cUnsupervised domain adaptation for semantic segmentation of NIR images through generative latent search.\u201d In: European Conference on Computer Vision. Springer.", "year": 2020}, {"authors": ["M.T. Ribeiro", "S. Singh"], "title": "and C", "venue": "Guestrin. \u201c\u201dWhy Should I Trust You?\u201d: Explaining the Predictions of Any Classifier.\u201d In: Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, San Francisco, CA, USA, August 13-17, 2016.", "year": 2016}, {"authors": ["T. Chen", "S. Kornblith", "M. Norouzi"], "title": "and G", "venue": "Hinton. \u201cA simple framework for contrastive learning of visual representations.\u201d In: International conference on machine learning. PMLR.", "year": 2020}, {"authors": ["K. He", "H. Fan", "Y. Wu", "S. Xie"], "title": "and R", "venue": "Girshick. \u201cMomentum contrast for unsupervised visual representation learning.\u201d In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.", "year": 2020}], "sections": [{"text": "R E S C I E N C E C"}, {"heading": "Replication / ML Reproducibility Challenge 2021", "text": "[Re] Counterfactual Generative Networks\nAnkit Ankit1, ID , Sameer Ambekar1, ID , Baradwaj Varadharajan1, ID , and Mark Alence1, ID 1University of Amsterdam, Amsterdam, The Netherlands \u2013 1Equal contribution\nEdited by Koustuv Sinha,\nSharath Chandra Raparthy\nReviewed by Anonymous Reviewers\nReceived 04 February 2022\nPublished 23 May 2022\nDOI 10.5281/zenodo.6574625"}, {"heading": "Reproducibility Summary", "text": ""}, {"heading": "Scope of Reproducibility", "text": "In this paper, we attempt to verify the claims that the paper [1] makes about their pro\u2010 posed CGN framework that decomposes the image generation process into independent causal mechanisms. Further, the author claims that these counterfactual images im\u2010 proves the out\u2010of\u2010distribution robustness of the classifier. We use the code provided by the authors to replicate several experiments in the original paper and draw conclusions based on these results."}, {"heading": "Methodology", "text": "Weuse the same hyperparameters and architecture asmentioned in CGN [1]. We use the PyTorch code publicly available by the author. Wemake several changes to their code for theMNIST datasets since it gives spurious results/errors. Since we use ImageNet 1000 as a replacement for the ImageNet dataset, wemodify the code accordingly. We reproduce tables 1\u20106 from CGN [1] paper, excluding results for models from other papers."}, {"heading": "Results", "text": "We validated each of the author\u2019s claim through the experiments given in the original paper and few additional experiments of our own. Overall, we foundmany experiments yielding identical results while some deviations were observed with both the Counter\u2010 factual Generative Network and the subsequent classification task. We were able to ex\u2010 plain most of these deviations through our additional experiments while some couldn\u2019t be validated due to computational limitations."}, {"heading": "What was easy", "text": "Overall, clear environment setup instructions, well working code and availability of pre\u2010 trained CGN models for both datasets proved valuable to validate the authors\u2019 claim."}, {"heading": "What was difficult", "text": "Some experimental details were not reported in the original paper which made vali\u2010 dations time consuming. ImageNet based experiments were replaced with ImageNet\u2010\nCopyright \u00a9 2022 A. Ankit et al., released under a Creative Commons Attribution 4.0 International license. Correspondence should be addressed to Ankit Ankit (ankitnitt1721@gmail.com) The authors have declared that no competing interests exist. Code is available at https://github.com/ambekarsameer96/FACT_AI/. \u2013 SWH swh:1:dir:88d89da0661c0f855f7b6f27c77acac1c2572a93. Open peer review is available at https://openreview.net/forum?id=BSHg22G7n0F.\nReScience C 8.2 (#2) \u2013 Ankit et al. 2022 1\n1k(mini) due to the computational limitation which made it difficult to validate the au\u2010 thor\u2019s original claims. Pre\u2010trained classification models could have proven helpful in this case, but were unavailable, which meant we had to train the classifier from scratch. Code changes were required to obtain baseline results which was tedious considering different code architecture was implemented for MNIST & ImageNet.\nCommunication with original authors We emailed the authors regarding inception score, MNIST dataset hyperparameters and ImageNet hyperparameters.\nReScience C 8.2 (#2) \u2013 Ankit et al. 2022 2\n1 Introduction\nNeural Networks (NNs) have become ubiquitous in machine learning due to their pre\u2010 dictive power. However, a shortcoming of NNs is their tendency to learn simple correla\u2010 tions that lead to good performance on test data rather than more complex correlations that generalise better. This shortcoming is apparent in the task of image classification, where NNs tend to overfit to factors like background or texture. To address this short\u2010 coming, [1] proposes a method of generating counterfactual images that prevent classi\u2010 fiers from learning spurious relationships. The authors take a causal approach to image generation by splitting the generation task into independent causal mechanisms. The authors considered three separately learned Independent Mechanisms (IMs) to generate shapes, textures and backgrounds for an image. For the MNIST setting, all IM specific losses are optimized end\u2010to\u2010end from scratch, while in the ImageNet setting, each IM is initialized with weights from pre\u2010 trained BigGAN\u2010deep\u2010256 [2]. The counterfactual image is then generated by passing the result of each IM to a deterministic composer function. In this report, we use the publicly available code provided by the authors to reproduce the results of the paper and validate the authors\u2019 claims. In this endeavour, we made modifications to the code to determine the efficacy of their generative model and vali\u2010 date its impact on improving the out of distribution robustness of a classifier.\n2 Scope of reproducibility\nIn this report, we investigate the following claims from the original paper:\n1. Generating high\u2010quality counterfactual images that decompose into independent causal inductive biases, these mechanisms disentangle object shape, object tex\u2010 ture and background\n2. Using counterfactual images improves the shape vs texture bias which is an inher\u2010 ent problem of deep classifiers\n3. Using counterfactual images improve the out\u2010of\u2010distribution robustness for the classifier during the classification task\n4. The Generative model can be trained efficiently on a single GPU with the help of powerful pre\u2010trained models\nWe attempt to reproduce the experiments from the paper [1] and perform exploratory analysis on the abovementioned claims. We propose using an extra loss function tomit\u2010 igate some of the shortcomings during counterfactual generation process and generate heatmap plots to study the classifier behaviour.\n3 Methodology\nAlex et al. [1] propose a Counterfactual Generative Network (CGN) framework to gener\u2010 ate high\u2010quality counterfactual images, which can be used to train invariant classifiers. The architecture of a CGN is composed of three IMs that are trained to generate back\u2010 grounds, shapes, and textures. Each IM is provided with a label. The task of the invari\u2010 ant classifier is to predict the label of a specific IM, regardless of the labels of the others. In conjunction with the composer function, the use of counterfactual images generated by the three IMs prevents the classifier from learning spurious relationships that arise from training on a natural dataset only. The architecture of the CGN consists of a GAN as the backbone of each IM. Each IM sam\u2010 ples random noise \u00b5 \u223c N(0,1), along with an independently sampled label to generate\nReScience C 8.2 (#2) \u2013 Ankit et al. 2022 3\nsamples. The output xgen is generated using an analytical function from the Composer \u2019C\u2019,\nxgen = C(m, f, b) = m\u2297 f + (1\u2212m)\u2297 b\nwhere \u2019m\u2019 is the mask (alpha map), f is foreground and b is background. \u2297 denotes the element wise multiplication. The losses Lrec (xgt, xgen), L1 reconstruction loss, Lperceptual as shown in Fig. 1 are used to improve the quality of generated images. Once the CGN is trained, u and y are randomized per mechanism such that new counterfactual xgen are generated. Further\u2010 more, hyperparameters such as CF ratio (the ratio indicates how many counterfactuals are generated per sampled noise) can be used to control the number of samples that are being generated. These samples are then used to train the classifier and evaluated on the corresponding test set.\ncGAN\nCGN\nBigGAN\nBigGAN\nBigGAN\nBigGAN U2-Net\nU2-Net\n3.4 Computational requirements All models are run on Nvdia GTX1080Ti GPUs (11Gb VRAM). For the MNIST datasets, training a CGN and a classifier each took approximately one hour.\n4 Results\nA lack of compute power prevented us from replicating the experiments on ImageNet. As aworkaround, we limit ourselves to verifying the results using the ImageNet\u20101k(mini) dataset. This is beneficial because it extends the results of the paper and evaluates the method on a new dataset, and ensures that results can be reproduced with limited re\u2010 sources by referring to our report/code and the CGN paper.\n4.1 Results reproducing original paper\nCan Image generation process be decomposed into independent causal inductive biases effectively? \u2014 We begin the experiment by training a CGN on the three variants of the MNIST dataset. We observe in Fig. 2 that the digits in case of colored MNIST dataset lose their shape when reconstructed, whereas for double colored and wildlife MNIST, the digits look much better. Since we do not clearly understand why the shape in Colored MNIST is poor, we generated a mask timeline to verify any patterns. Fig. 3a details the same. Fur\u2010 ther analysis on this was conducted and recorded in 4.2. We also propose an additional loss function to help mitigate this problem.\nReScience C 8.2 (#2) \u2013 Ankit et al. 2022 5\nQuality of Counterfactual Images on ImageNet-1k \u2014 To quantify the quality of the composite images produced by the CGN, the authors calculate the inception score (IS). The details of the IS calculations (inception model used, number of images used) were not men\u2010 tioned in the paper. In an attempt to recreate the results regarding IS, we use the Ope\u2010 nAI implementation 1. We plot the results of IS vs the number images using 10 splits in Fig. 9. We observe the IS converges to an IS of 198. Wemade use of the pre\u2010trained CGN trained on ImageNet\u20101k that was present as part of the codebase to generate counterfactual images. Since there is no quantitative way to measure the quality of counterfactual images, we reproduced the images given in the original paper. We achieved a similar quality of counterfactual images but also noted deviations. Fig. 7 shows all the images that were given in the original paper. A deviation in the mask is observed for the class \u2019Agaric\u2019 and \u2019Cauliflower\u2019. The difference in the im\u2010 ages to the original paper prompted us to collect the classes with poorer counterfactual images to observe any patterns. Fig. 8 is generated from the pre\u2010trained CGN that have a low quality of images picked from random classes. Since the analysis is qualitative, we relied on the realism of the counterfactual compared to original images from that class. Images under the classes \u2019Cliff dwelling\u2019 \u2019American Chameleon\u2019 suffer from Texture\u2010background entanglement re\u2010 sulting in the counterfactual with no subject. On the other hand, the images under the class \u2019Goldfinch\u2019, \u2019Junco\u2019 suffer from reduced realism due to linear constraints applied on the composer.\nImpact of counterfactual images towards shape-bias of the classifier \u2014\nExperiments conducted with ImageNet-1k(mini) dataset \u2014 In order to identify the impact of shape bias on the classifier, we made use of the proposed architecture for the classifier ensemble that included 3 different heads. The ensemble includes a pre\u2010trained classi\u2010 fier(we made use of Resnet\u201050) as the backbone, while attaching 3 different heads to it. Each head controls the variance with respect to one of the 3 independent mecha\u2010 nism(Shape, Texture, Background) which are individually trained from scratch. The result from these heads are averaged to get the prediction accuracy of the classifier en\u2010 semble. The results in Table 2(a) for ImageNet\u20101k(Mini) showed a considerable deviation. The shape bias is marginally lower compared to the baseline result while the texture bias is high. The reduction in the shape bias could be due to the smaller dataset that we are using. Since this is ambiguous to validate the original claim we conducted additional experiments which are detailed in section 4.2."}, {"heading": "Do Counterfactual images improve the OOD robustness of the classifier? \u2014", "text": "Classification Accuracy (MNIST Dataset) \u2014 Firstly, we trained a classifier on counterfactuals generated by the pre\u2010trained CGN provided by the authors. It was not clear how many counterfactual images the classifier should be trained on, but the accuracies in Table 3 were similar to the results in the ablation study in Fig. 7 using 106 counterfactuals, so this is the number we chose. There was also ambiguity between the statements in the paper and the code about the classifier being trained on any real images, so we trained two classifiers. One classifier was shown real images, and the other was not. The classifier trainedwith counterfactuals generatedby thepre\u2010trainedmodels achieved comparable results to those in the paper. From table 3, it can be seen that the pre\u2010trained models achieved train accuracies that differed by less than 3%, and test less than 1.5% compared to the results in the paper. However, the classifier trained on counterfactu\u2010 als generated by CGNs that we trained (using the provided configurations) performed\n1https://github.com/nnUyi/Inception\u2010Score\nReScience C 8.2 (#2) \u2013 Ankit et al. 2022 6\nsignificantly worse on colored MNIST and wildlife MNIST in terms of test accuracy. We anticipate that the provided configurationswere not the same as the configurations used to acquire the results in the paper. The presence of real images in the dataset for the pretrained models appeared not to have a significant effect on train or test accuracy. The largest gain obtained by includ\u2010 ing real images was approximately 4%. This demonstrates that the ambiguity regarding whether or not real images were used in the training of the classifier was inconsequen\u2010 tial. For the CGNs that we trained, however, the presence of real images improved the performance of the classifier significantly.\nClassification Accuracy(ImageNet Dataset) \u2014 The classifier was trained on counterfactual im\u2010 ages from pre\u2010trained CGN and ImageNet\u20101k(mini). The results in table 2(c) indicate the trend that was observed. The training accuracy showed a similar trend to the original paper\u2019s classifier (trained on ImageNet). There is a similar drop in the training accuracy compared to the baseline(ImageNet\u20101k). Even though the original paper does not include the test accuracy for the classifier for the same distribution, we found that the classifier does not perform well with respect to the test data. The drop in top\u20101(the predicted class is the correct class that the image corresponds to) & top\u20105(5 out of 1000 classes with the highest probability as predicted by the classifiermatches the actual label) accuracy compared to the baselinewas attributed to the ability of the counterfactual models to reduce the shape bias of classifier which would improve the classifier\u2019s robustness to unseen data. However, this is invalidated by the low percentage of the test accuracy. To further understand why the classifier ensem\u2010 ble is not performing well with unseen test data, we conducted additional experiments to explain the same behaviour.\nOut of distribution accuracy \u2014 : A similar study as given in the paper was conducted to un\u2010 derstand how the trainedmodel performswith an out\u2010of\u2010distribution dataset. Table 2(b) contains the information with respect to the ImageNet\u20101k(mini) + CGN. There is a sig\u2010\nReScience C 8.2 (#2) \u2013 Ankit et al. 2022 7\nnificant reduction in the accuracy of the out\u2010of\u2010distribution dataset. The baseline also showed a similar trend, andwe could not achieve the higher percentage reported as part of the paper. We concluded that the baseline result is on the lower side primarily be\u2010 cause of the size of the ImageNet\u20101k(mini) dataset that was used for training. Since the results show that the ensemble classifier improves the out\u2010of\u2010distribution robustness compared to the baseline, the percentage was still very low to make any conclusion. Both the trend with the test accuracy and out\u2010of\u2010distribution accuracy falls on the lower side, which prompted us to investigate further. We generated explainability plots us\u2010 ing the same distribution and out\u2010of\u2010distribution data to determine how the model is behaving with and without the heads that disentangle shape, texture, background. We recorded All of the experiments as part of section 4.2.\n4.2 Results beyond original paper\nFor Additional Result 1 we make use of CGN [1] architecture that has been designed for MNIST datasets due to computational limitations. \u2014\nAdditional Result 1 - Does fbg , ftexture, fshape and Lperceptual (Perceptual loss) proposed in [1] cover all aspects of background, shape, texture? \u2014 CGN [1] makes use of texture loss Ltext (xgt, xgen), = sampling 36 patches of size 15 x 15 grid from regions wherever mask has values near 1. Further, from these 36 patches, a patch grid of 6 x 6 is used. It is then upscaled to 256 x 256 resolution, which is in turn used an input to the Perceptual loss Lperceptual between foreground f and patch grid Ltext(f, pg). However, we observe that important image properties such as luminance, contrast, structure are not taken into consideration with the Ltext loss proposed in CGN [1] for the generated image and the ground truth image and also because Hence, we propose the usage of an additional Loss function Lssim (SSIM) [8]. In ad\u2010 dition, motivated by results as shown in [9], [10] L2 loss unlike SSIM [8] over different distortions of the image remains constant instead of recognising them . It complements the structural lossLrec. Default Gaussian Kernel of 11 was used as a hyperparameter for SSIM [8]. We observe from Table 4 that using SSIM [8] loss improves classification accuracy on the Wildlife MNIST dataset. Qualitative improvements in the generated images can be seen in Fig 3b. Images trained with SSIM [8] loss show better structure and crisper outlines. Improvements can be seen using SSIM [8] loss on the Double Colored MNIST dataset to a lesser extent. However, accuracy on the colored MNIST dataset decreases. This may be due to the dataset\u2019s shape/structure/bias.\nReScience C 8.2 (#2) \u2013 Ankit et al. 2022 8\nExploring classifier robustness with ImageNet \u2014 From 2(c), we find a considerable drop in the training and test accuracies(top\u20101) compared to the baseline. To explain the perfor\u2010 mance of the model, we integrated lime[11] package to generate explainability heatmap plots.(code reference lime_plots.py) Same distribution Test set Fig 4 shows the outcome of the plots using the same im\u2010 age(fromanunseen set) run through 2 different classifiers. Firstly, we used a pre\u2010trained Resnet\u201050 to find out the robustness of the same towards unseen dataset. Secondly, we made use of a fully trained classifier ensemble with a pre\u2010trained Resnet\u201050 as the back\u2010 bone and 3 different heads as specified in the original paper[1]. The results are recorded by obtaining the top\u20105 classes with highest probability. The image on the left of Fig 4 was classified as \u2019iPod\u2019 with regions including the object and the background contributing towards it. The plot shows how the classifier is ex\u2010 tracting information from not only the object but also the background to determine the correct class. On the other hand, the image on the right shows the explainability plot when the suggested classifier ensemble is used. It performs poorly categorising the im\u2010 age as \u2019American_chameleon\u2019 with a higher probability when compared to the actual classification \u2019iPod\u2019. The heatmap sheds the light into this behavior showing that the classifier does not include the background(as evident from the red zone) and focuses primarily on the object shape to make a decision. From the above experiments through visual plots, we are able to determine that the counterfactual images to skew the shape\u2010bias of the classifier does not contribute to the robustness towards unseen data within the same distribution. This can be attributed to the inclusion of counterfactual images that are of reduced realism which affects the classifier from learning meaningful information from the dataset at hand.\n5 Discussion\n5.1 What was easy It was easy to set up the environment as listed/indicated in the README file of the Github repository. Although not all commands were explicitly listed, it helped us navi\u2010\nReScience C 8.2 (#2) \u2013 Ankit et al. 2022 9\ngate through and run the code. The presence of .yaml files for each dataset in the case ofMNIST [6] helped us to train CGNs and classifiers with well\u2010working hyperparameters quickly. ImageNet experiments were structured clearly inmultiple sections within the codebase. It made it easier to understand the difference in the architecture that was followed to handle Mnist, ImageNet. Since, reliance on pre\u2010trained network for ImageNet was im\u2010 portant, the presence of scripts to download all the data, weights made the setup easier.\n5.2 What was difficult In the case of the architecture for ImageNet, replacing it with ImageNet\u20101k or Mini\u2010 ImageNet required code changes. The python parameters to load the dataset(\u2013data) had no effect that prompted changes in the dataloader.py. The classifier(train_classifier.py) did not have provision to generate the values without mandatorily providing the coun\u2010 terfactual information. This proved to be a challenge as we needed the baseline results to compare the performance of the proposed model. Code modification was done to accommodate the same and the experiment was conducted. The results from the original paper included the inception score for the proposed CGN, but we could not find a code block to calculate the same. Considerable amount was spent on trying to find out the hyperparameters that was needed to generate the coun\u2010 terfactual images. Since the inception score was dependent on the number of counter\u2010 factuals generated, we worked towards identifying the correct hyperparameters before continuing with classifier training. Can the generativemodel be trained on a single GPU? From table 5, wewere able to train the generative model from scratch for all variations of MNIST. However, for Imagenet architecture, with the default parameters, it was going to take upwards of 200 hours. Therefore, we were unable to verify this claim.\n5.3 Suggestions for reproducibility In general, the resources provided by the authors on GitHub in conjunction with the explanations in the paperwere sufficient to generate similar results to those found in the paperwith relative ease. However, in the future, itmaybehelpful if the authors provided the weights of the exact models used in the paper, along with the hyperparameters used to train them. In addition, the size of the ImageNet dataset makes running several experiments infea\u2010 sible without significant compute power. Therefore, we suggest that additional exper\u2010 iments using a subset of ImageNet (i.e. Mini\u2010ImageNet) be added to the report for the sake of reproducibility."}, {"heading": "Appendix", "text": "A Ablation Study\nWe conducted experiments to recreate the MNIST Ablation Study. For this study, the pre\u2010trainedmodel provided by the authors was used. We observed a similar trend to the authors. An increase in the number of counterfactual images used in training resulted in higher training accuracies. However, our values differed significantly from those in the report, as seen in Fig. 5 and Fig. 6 . In particular, we observed higher accuracies for each dataset, especially when only 104 counterfactuals were used in training. This differencemay be explained by differences between the pre\u2010trainedmodels provided by the authors and the models that were used to generate the plots.\nB Training time for Generative Model\nThe following table shows the training time for each generative network against the dataset that was used. Note: The ImageNet based CGN depends only on the BigGAN\u2010256 backbone and U2\u2010net to train. The MNIST based CGN architecture however, trains using the dataset without any pre\u2010trained weight as backbone. ImageNet counterfactual generation was going to run for 1.2 million iterations(0.5s/iteration), which was not computationally feasible with our resources.\nC Counterfactual Images\nThe following images using the pre\u2010trained CGNmodel that was provided with the code\u2010 base. Minor deviations were observed with the image given in the paper to the result we obtained.\nReScience C 8.2 (#2) \u2013 Ankit et al. 2022 13\nD SSIM Loss function\nSSIM [8] is defined using the three aspects of similarities, luminance (l (x, xgen)), con\u2010 trast (c (x, xgen)) and structure (s (x, xgen)) that aremeasured for a pair of images {xg, xgen} as follows: Given two images ground truth x and generated image xgen, the SSIM [8] loss is defined as follows:\nLssim(\u03b1) = 1\u2212 Ex[l(\u03b1) \u00b7 cs(\u03b1)]\nl (x, xgen) = 2\u00b5x\u00b5xgen + C1\n\u00b52x + \u00b5 2 xgen + C1\nc (x, xgen) = 2\u03c3x\u03c3xgen + C2\n\u03c32x + \u03c3xgen 2 + C2\ns (x, xgen) = \u03c3xxgcn + C3\n\u03c3x\u03c3xgen + C3\nwhere \u00b5\u2032 \u2019s denote sample means and \u03c3\u2032 \u2019s denote variances. C1, C2 and C3 are con\u2010 stants. With these, SSIM and the corresponding loss functionLssim, for a pair of images {x, xgen} are defined as:\nSSIM (x, xgen) = l (x, xgen)\u03b1 \u00b7 c (x, xgen)\u03b2 \u00b7 s (x, xgen)\u03b3\nwhere \u03b1 > 0, \u03b2 > 0 and \u03b3 > 0 are parameters used to adjust the relative importance of the three components.\nLssim (x, xgen) = 1\u2212 SSIM (x, xgen)\nAdditional Result 2 - Exploring the biased behaviour of CGN[1] with the datasets \u2014 To investigate the robustness of the CGN architecture [1] to varied color augmentations, we applied color jitter to augment the training data. We found that applying a color jitter decreased classification accuracy by 10% on double\u2010colored MNIST and 50% on wildlife MNIST. Amongst all widely known augmentations we make use of color jitter since from [12], [13] it is evident that color jitter, sobel flter augmentations are imperative to learn useful representations from the given dataset. We observe that from Table 6 that when we used it on Double Colored dataset the clas\u2010 sifier\u2019s accuracy decreases by almost 10 %. Similarly, there is decrease in accuracy of Wildlife MNIST dataset by almost around 50% as indicated in Table 4.\nTo determine why the color jitter augmentation decreases training accuracy, we ob\u2010 served the results visually through the samples generated across 40K iterations by the CGN. It can be seen that digit 6 loses its shape over iterations. Digits 0 and 1 have the same background and similar digit font. These artefacts produced by the CGN[1] are a likely cause of the classifier\u2019s decreased performance. Which might indicate that the CGN is overfitting itself to the image backgrounds while learning the generative model cGAN using the loss functions.\nReScience C 8.2 (#2) \u2013 Ankit et al. 2022 14\nReScience C 8.2 (#2) \u2013 Ankit et al. 2022 15"}], "title": "[Re] Counterfactual Generative Networks", "year": 2022}