{"abstractText": "We are reproducing Comparing Rewinding and Fine-tuning in Neural Networks, by [1]. In this work the authors compare three different approaches to retraining neural networks after pruning: 1) fine\u2010tuning, 2) rewindingweights as in [2] and 3) a new, originalmethod involving learning rate rewinding, building upon [2]. We reproduce the results of all three approaches, but we focus on verifying Renda\u2019s original approach: learning rate rewinding, since it is newly proposed and is described as a universal alternative to other methods. As the authors of [1], we used CIFAR10 for most of the experiments, but we added exper\u2010 iments on a larger version of this dataset: CIFAR100. We have also extended the list of tested network architectures to include Wide ResNets [3]. The new experiments led us to discover the limitations of learning rate rewinding which in some cases can worsen pruning results on large neural network architectures.", "authors": [{"affiliations": [], "name": "Szymon Mikler"}, {"affiliations": [], "name": "Koustuv Sinha"}, {"affiliations": [], "name": "Sharath Chandra Raparthy"}], "id": "SP:d06f2844345588db4c7b0108401275400587baba", "references": [{"authors": ["A. Renda", "J. Frankle", "M. Carbin. \u201cComparing Rewinding", "Fine-tuning in Neural Network Pruning.\u201d In"], "title": "2020)", "venue": "arXiv: 2003.02389. URL: http://arxiv.org/abs/2003.02389", "year": 2020}, {"authors": ["J. Frankle", "M. Carbin"], "title": "The lottery ticket hypothesis: Finding sparse, trainable neural networks.", "venue": "International Conference on Learning Representations,", "year": 2019}, {"authors": ["S. Zagoruyko", "N. Komodakis"], "title": "Wide Residual Networks.", "venue": "Proceedings of the British Machine Vision Conference (BMVC)", "year": 2016}, {"authors": ["Z. Liu", "M. Sun", "T. Zhou", "G. Huang", "T. Darrell"], "title": "Rethinking the value of network pruning.", "venue": "International Conference on Learning Representations,", "year": 2019}, {"authors": ["N. Lee", "T. Ajanthan", "P.H.S. Torr"], "title": "SNIP: Single-shot Network Pruning based on Connection Sensitivity.", "year": 2018}, {"authors": ["Y. LeCun", "J. Denker", "S. Solla"], "title": "Optimal Brain Damage.", "venue": "Advances in Neural Information Processing Systems. Ed. by D. Touretzky. Vol. 2. Morgan-Kaufmann,", "year": 1990}, {"authors": ["E.J. Crowley", "J. Turner", "A. Storkey", "M. O\u2019Boyle"], "title": "A Closer Look at Structured Pruning for Neural Network Compression.", "venue": "Citation Key:", "year": 2018}, {"authors": ["A. Botev", "G. Lever", "D. Barber"], "title": "Nesterov\u2019s Accelerated Gradient and Momentum as approximations to Regularised Update Descent", "venue": "DOI: 10.48550/ARXIV.1607.01981. URL: https://arxiv.org/abs/1607.01981", "year": 2016}, {"authors": ["Y. LeCun. \u201cHandwritten Digit Recognition with a Back-Propagation Network.\u201d In"], "title": "Neural Information Processing Systems", "venue": "American Institute of Physics, 1988. URL: https : / /proceedings .neurips .cc/paper/1987/file / a684eceee76fc522773286a895bc8436-Paper.pdf", "year": 2021}, {"authors": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "title": "Deep residual learning for image recognition.", "venue": "Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition", "year": 2016}, {"authors": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "title": "Identitymappings in deep residual networks.\u201d In: Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics", "year": 2016}, {"authors": ["Mart\u0131\u0301n Abadi"], "title": "TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems. Software available from tensorflow.org", "year": 2015}, {"authors": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification.", "venue": "Proceedings of the IEEE International Conference on Computer Vision (ICCV)", "year": 2015}, {"authors": ["L.N. Smith"], "title": "Cyclical Learning Rates for Training Neural Networks. 2017", "year": 2022}], "sections": [{"text": "R E S C I E N C E C Replication / ML Reproducibility Challenge 2021\n[Re] Reproducibility Study: Comparing Rewinding and"}, {"heading": "Fine-tuning in Neural Network Pruning", "text": "Szymon Mikler1, ID 1University of Wroclaw, Poland\nEdited by Koustuv Sinha,\nSharath Chandra Raparthy\nReceived 04 February 2022\nPublished 23 May 2022\nDOI 10.5281/zenodo.6574679\n1 Reproducibility Summary"}, {"heading": "Scope of Reproducibility", "text": "We are reproducing Comparing Rewinding and Fine-tuning in Neural Networks, by [1]. In this work the authors compare three different approaches to retraining neural networks after pruning: 1) fine\u2010tuning, 2) rewindingweights as in [2] and 3) a new, originalmethod involving learning rate rewinding, building upon [2]. We reproduce the results of all three approaches, but we focus on verifying Renda\u2019s original approach: learning rate rewinding, since it is newly proposed and is described as a universal alternative to other methods. As the authors of [1], we used CIFAR10 for most of the experiments, but we added exper\u2010 iments on a larger version of this dataset: CIFAR100. We have also extended the list of tested network architectures to include Wide ResNets [3]. The new experiments led us to discover the limitations of learning rate rewinding which in some cases can worsen pruning results on large neural network architectures."}, {"heading": "Methodology", "text": "We implemented the code ourselves in Python with TensorFlow 2, basing our imple\u2010 mentation of the paper alone and without consulting the source code provided by the authors. We ran two sets of experiments. In the reproduction set, we have striven to ex\u2010 actly reproduce the experimental conditions of [1]. We have also conducted additional experiments, which use other network architectures, effectively showing results previ\u2010 ously unreported by the authors. We did not cover all originally reported experiments \u2013 we covered as many as needed to state the validity of claims. We used Google Cloud resources and a local machine with 2x RTX 3080 GPUs."}, {"heading": "Results", "text": "We were able to reproduce the exact results reported by the authors in all originally re\u2010 ported scenarios. However, extended results on larger Wide Residual Networks have demonstrated the limitations of the newly proposed learning rate rewinding \u2013 we ob\u2010 served a previously unreported accuracy degradation in low sparsity ranges. Neverthe\u2010 less, the general conclusion of the paper still holds and was indeed reproduced.\nCopyright \u00a9 2022 S. Mikler, released under a Creative Commons Attribution 4.0 International license. Correspondence should be addressed to Szymon Mikler (sjmikler@gmail.com) The authors have declared that no competing interests exist. Code is available at https://github.com/gahaalt/reproducing-comparing-rewinding-and-finetuning \u2013 DOI 10.5281/zenodo.6519109. \u2013 SWH swh:1:dir:886a4c9a0bdecdbf65f2cab3ae7404a6796bc451. Open peer review is available at https://openreview.net/forum?id=HxWEL2zQ3AK.\nReScience C 8.2 (#27) \u2013 Mikler 2022 1"}, {"heading": "What was easy", "text": "Re\u2010implementation of pruning and retraining methods was technically easy, as it is based on a popular and simple pruning criterion \u2013 magnitude pruning. Original work was descriptive enough to reproduce the results with satisfying results without consult\u2010 ing the code."}, {"heading": "What was difficult", "text": "Not every design choice was mentioned in the paper, thus reproducing the exact re\u2010 sults was rather difficult and required a meticulous choice of hyper\u2010parameters. Exper\u2010 iments on ImageNet andWMT16 datasets were time consuming and required extensive resources, thus we did not verify them.\nCommunication with original authors We did not consult the original authors, as there was no need to.\nReScience C 8.2 (#27) \u2013 Mikler 2022 2\n2 Introduction\nNeural network pruning is an algorithm that intends to decrease the size of a network, usually by removing someof its connections or setting theirweights to 0. This procedure generally allows obtaining smaller and more efficient models. It often turns out that these smaller networks are as accurate as their bigger counterparts or the accuracy loss is negligible. A common way to obtain such high quality sparse network is to prune it after the training has finished [2], [4]. Networks that have already converged are easier to prune than randomly initialized networks [5], [4]. After pruning, more training is usually required to restore the lost accuracy. Although there are a few ways to retrain the network, finetuning might be the easiest and most often chosen by researchers and practitioners [1], [4]. Lottery Ticket Hypothesis from [2] formulates a hypothesis that for every non\u2010pruned neural network, there exists a smaller subnetwork that matches or exceeds results of the original. The algorithm originally used to obtain examples of such networks is iter\u2010 ative magnitude pruning with weight rewinding, and it is one of the three methods of retraining after pruning compared in this work.\n2.1 Structured and Unstructured Pruning One of the first papers about neural network pruning [6] focused solely on unstructured pruning. However, current hardware limitations do not allow to take full advantage of this form of pruning. Structured pruning is a workaround to this problem. In struc\u2010 tured pruning, we remove the basic building blocks of a network instead of single con\u2010 nections. In the case of dense linear neural networks, these structures are neurons and their connections \u2013 neuron\u2019s inputs and outputs. Depending on the network\u2019s type, this can be something else. In every case, it should be a minimal unit such that the remain\u2010 ing neural network can be represented as a smaller, but still dense (non\u2010pruned) neural network. In the case of structured pruning of convolutional neural networks, whole channels and their corresponding parameters in convolutional filters are removed.\n3 Scope of reproducibility\nOur reproducibility study tries to confirm claims from [1]. Following claims were for\u2010 mulated:\nClaim 1: Widely usedmethod of training after pruning: finetuning yieldsworse results than rewinding based methods (supported by figures 2, 3, 1, 4 and Table 5)\nClaim 2: Newly introduced learning rate rewindingworks as goodor better asweight rewind\u2010 ing in all scenarios (supported by figures 2, 3, 1, 4 and Table 5, but not supported by Figure 5)\nClaim 3: Iterative pruning with learning rate rewinding matches state\u2010of\u2010the\u2010art pruning methods (supported by figures 2, 3, 1, 4 and Table 5, but not supported by Figure 5)\n4 Methodology\nWe aimed to compare three retraining approaches: 1) finetuning, 2) weight rewinding and 3) learning rate rewinding. Our general strategy that repeated across all experi\u2010 ments was as follows:\n1. train a neural network to convergence,\nReScience C 8.2 (#27) \u2013 Mikler 2022 3\n2. prune the network using magnitude criterion: remove parameters with the small\u2010 est absolute value,\n3. retrain the network using one of the three retraining approaches.\nIn the case of structured pruning: in step 2, we removed structures (either neurons or convolutional channels) with the smallest L1 norm [7], rather than removing separate connections. In the case of iterative pruning: the network in step 1 was not randomly initialized, but instead: weights from a model from a previous iterative pruning step were loaded as the starting point. Then the three steps were repeated. On the other hand, one\u2010shot pruning is a procedure where pruning is done only once, so there was only one cycle. In some methods, this might be done on a randomly initialized neural network, like in [5]. Here, however, one\u2010shot pruning is done after the network reaches convergence. So the three steps are not repeated in case of one\u2010shot pruning. We trained all our networks using Stochastic Gradient Descent with Nesterov Momen\u2010 tum [8]. The learning rate was decreased in a piecewisemanner during the training, but momentum coefficient was constant and equal to 0.9.\n5 Model descriptions\nIn this report, wewere focusing on an image recognition task using convolutional neural networks [9]. For most of our experiments, we chose to use identical architectures as [1] to better validate their claims and double\u2010check their results, rather than only provide additional ones. Therefore, most of the used networks are residual networks, which were originally proposed in [10]. Additionally, to verify the general usefulness of pruning and retrainingmethods proposed in [1]we extend the list of tested network architectures to much larger wide residual networks from [3].\n5.1 Residual networks (ResNet) Just as [1], we chose to use the original version of ResNet as described in [10] rather than the more widely used, improved version (with preactivated blocks) from [11]. We created the models ourselves, using TensorFlow [12] and Keras. We strove to replicate the exact architectures used by [1] and [10] and train them from scratch.\nResNet hyperparameters \u2014 Learning rate started with 0.1 and was multiplied by 0.1 twice, after 36 000 and 54 000 iterations. One training cycle had 72 000 iterations in total. For all batch normalization layers, we set the batch norm decay to 0.997, following [1], which is also the default used in the original TensorFlow implementation1. We initialize net\u2010 work\u2019s weights with what is known as He uniform initialization from [13]. We regularize\n1https://github.com/tensorflow/models/blob/r1.13.0/official/resnet/resnet_model.py\nReScience C 8.2 (#27) \u2013 Mikler 2022 4\nResNets, during both training and finetuning, usingL2 penalty with 10\u22124 coefficient. In other words, the loss function (from which we calculate the gradients) looks as follows:\nL = CC(y, p) + 10\u22124 \u00d7 \u2211 i\u2208W w2i (1)\nwhere:\nL = value of the final loss function\nCC = categorical crossentropy loss function\ny = ground truth label of a sample or batch\np = model\u2019s prediction\nW = parameters of the model\n5.2 Wide Residual Networks (Wide ResNet, WRN) WRN networks were introduced in [3]. They are residual networks created by simply increasing the number of filters in preactivated ResNet networks [11].\nWRN hyperparameters \u2014 AsWide ResNets are newer andmuch larger than ResNets, hyper\u2010 parameters are slightly different. To choose them, we follow [3]. Learning rate starts with 0.1 and multiplied by 0.2 thrice: after 32 000, 48 000 and 64 000 iterations. Training lasts for 80 000 iterations. For all batch normalization layers, we use hyper\u2010parameters from the newer TensorFlow implementation2 with batch norm decay set to 0.9. Follow\u2010 ing [3], we use larger L2 penalty for this network: 2\u00d7 10\u22124. Finally, the loss function is as follows:\nL = CC(y, p) + 2\u00d7 10\u22124 \u00d7 \u2211 i\u2208W w2i (2)\nwhere:\nL = value of the final loss function\nCC = categorical crossentropy loss function\ny = ground truth label of a sample or batch\np = model\u2019s prediction\nW = parameters of the model\n2https://github.com/tensorflow/models/blob/r2.5.0/official/vision/image_classification/resnet/resnet_model.py\nReScience C 8.2 (#27) \u2013 Mikler 2022 5\n5.3 Datasets CIFAR\u201010 and CIFAR\u2010100 are image classification datasets introduced in [14]. Following [1], we use all (50 000) training examples to train the model.\n5.4 Preprocessing and data augmentation We used a standard data processing for both CIFAR\u201010 and CIFAR\u2010100 datasets [1], [2], [3]. During training and just before passing data to the model, we:\n1. standardized the input by subtracting the mean and dividing by the std of RGB channels (calculated on training dataset),\n2. randomly flipped in horizontal axis,\n3. added a four pixel reflection padding,\n4. randomly cropped the image to its original resolution.\nDuring the validation, we did only the first step of the above.\n5.5 Experimental setup and code Our ready\u2010to\u2010use code, which includes experiment definitions, can be found at https: //github.com/gahaalt/reproducing-comparing-rewinding-and-finetuning. It\u2019s written using Tensor\u2010 Flow [12] version 2.4.2 in Python. More details are included in the repository.\n5.6 Computational requirements Recreating the experiments required amodern GPU, training all models on CPUwas vir\u2010 tually impossible. Training time varies depending on a lot of factors: network variation and size, exact version of the deep learning library, and even the operating system. In our case, using TensorFlow 2.4.2 on Ubuntu and a single RTX 3080 GPU, the smallest of the used models, ResNet\u201020, takes about 20 minutes to train on CIFAR\u201010 dataset. To replicate our experiments, training at least a single baseline network and then, once more, a single pruned network, is required. To reduce computational requirements, we reused one non\u2010pruned baseline for multiple compression ratios. Approximated train\u2010 ing time requirements can be seen in the table below.\nReScience C 8.2 (#27) \u2013 Mikler 2022 6\nFor all our experiments together, we estimate the total number of GPU hours spent to be around 540.\n6 Method description\nWe compare three methods of retraining after pruning. For all of them, the starting point is a network that was already trained to convergence, then pruned to a desired sparsity. The difference between the three retraining methods is what follows after it.\n6.1 Fine-tuning Fine\u2010tuning is retraining with a small, constant learning rate \u2013 in our case, whenever fine\u2010tuning was used, the learning rate was set to 0.001 as in [1]. We finetune the net\u2010 work for the same number of iterations as the baseline \u2013 72 000 iterations in the case of the original ResNet architecture. In this method, such long retraining would not be necessary in practical applications, since the network converges much faster.\n6.2 Weight rewinding Weight rewinding restores the network\u2019s weights from a previous point (possibly begin\u2010 ning) in the training history and then continues training from this point using the origi\u2010 nal training schedule \u2013 in our case a piecewise constant decaying learning rate schedule. When rewinding a network to iteration K that originally trained for N iterations: first prune the non\u2010pruned network that was trained forN iterations. Then, for connections that survived, restore their values toK\u2010th iteration from the training history. Then train to the convergence for the remaining N \u2212K iterations.\n6.3 Learning rate rewinding Learning rate rewinding continues training with weights that have already converged, but restores the learning rate schedule to the beginning, just as if we were training from scratch, and then trains to the convergence once again. This reminds the cyclical learn\u2010 ing rates from [15]. Learning rate rewinding really is weight rewinding for K = N , but the final retraining is always for N iterations.\n7 Results reproducing original paper\nIn most of our experiment, just as [1], we investigate how does the trade\u2010off between prediction accuracy and compression ratio look like. In one of the experiments (Table 5)\nReScience C 8.2 (#27) \u2013 Mikler 2022 7\nwe verify only one compression ratio, but for the rest, we verify multiple. We report a median result out of 2 up to 12 trials for each compression ratio. To better utilize our compute capabilities, wedecided to spendmore training cycles in situationswhere there is no clear winner between the compared methods. On each plot, we include error bars showing 80% confidence intervals. In this section, we include experiments that we successfully reproduced. Most of them match the original ones within 1% error margin. We noticed some of our results were slightly better than authors of [1] originally reported. Across all scenarios where finetuning was tested, it was by far the worst of the three methods, which directly supports claim 1 (Section 3). Weight rewinding and learning rate rewinding most often are equally matched, but in some cases learning rate rewind\u2010 ing works a little better.\n7.1 ResNets on CIFAR-10 dataset Results we observe here are consistent with what we see in [1], [2]. Iterative pruning is better than one\u2010shot pruning, but more time consuming. In extreme cases, iterative pruning requires 20 times as many iterations than one\u2010shot pruning to complete. But it is not as bad for moderate sparsity pruning. Larger networks work better than smaller ones. Even when the number of parameters left after pruning is the same \u2013 originally larger network will outperform the smaller one. Out of the retraining methods, weight rewinding and learning rate rewinding seem to be similar, but finetuning is visibly worse. In some cases, learning rate rewinding out\u2010 performs weight rewinding. Similar conclusions can be drawn from both structured and unstructured pruning results.\nReScience C 8.2 (#27) \u2013 Mikler 2022 8"}, {"heading": "Network Dataset Retraining Sparsity Test Accuracy", "text": "ReScience C 8.2 (#27) \u2013 Mikler 2022 9\n8 Results beyond original paper\n8.1 ResNets on CIFAR-100 dataset\n8.2 WRN-16-8 on CIFAR-10 dataset WRN\u201016\u20108 shows consistent behaviour \u2013 accuracy in the low sparsity regime is reduced in comparison to the baseline. In the case of iterative pruning, where each step is an\u2010 other pruning in the low sparsity regime, it leads to a large difference between the two retraining methods. Since for WRN\u201016\u20108 one\u2010shot, low sparsity pruning shows a small accuracy regression in comparison to the baseline, this regression accumulates when pruning multiple times, which we do in iterative pruning. We think that the degrada\u2010 tion we observe in the case of iterative pruning is an effect of stacking multiple smaller defects that we observe in the case of one\u2010shot pruning. This can be seen in Figure 5.\nReScience C 8.2 (#27) \u2013 Mikler 2022 10\nFor iterative pruning (figures 2, 3, 5) we used a nonstandard step size of 30% per itera\u2010 tive pruning iteration, which was a way to reduce the computational requirements. We believe this choice does not change the validity of the claims, and to support it, we pro\u2010 vide a comparison of our step size to themore commonly used 20%. We show that there is virtually no difference between both versions and the aforementioned catastrophic degradation occurs in both cases, as long as the step size is in the low sparsity regime.\n9 Discussion\nWe were able to confirm the general conclusion of [1]. Fine\u2010tuning can be mostly re\u2010 placed by other retraining techniques, e.g., by weight rewinding as it was done by [2]. We confirmed that learning rate rewinding worked even better in some scenarios. How\u2010 ever, we have also shown in Figure 5 that the newly proposed learning rate rewinding is a poor choice when we are pruning large networks \u2013 in our case that is WRN\u201016\u20108. We believe this should be examined further as theremight exist a simpleworkaround to this problem \u2013 a retraining procedure between weight rewinding and learning rate rewind\u2010 ing, which works even for larger networks. Furthermore, it would be interesting to see what happens in the network when this catastrophic accuracy degradation occurs. Per\u2010 haps, the reason for it not occurring with the original ResNet, but occurring with larger architectures, is the degree to which the larger networks overtrain \u2013 larger networks tend to overfit more. And such an overfitted network might be not a good starting point for the retraining."}, {"heading": "Acknowledgements", "text": "The authors thank Polish National Science Center for funding under the OPUS\u201018 2019/35/B/ST6/04379 grant and the PlGrid consortium for computational resources."}], "title": "[Re] Reproducibility Study: Comparing Rewinding and Fine-tuning in Neural Network Pruning", "year": 2022}