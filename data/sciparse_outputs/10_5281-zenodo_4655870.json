{"abstractText": "Theophile Boraud1,4, ID and Anthony Strock1,2,3, ID 1INRIA Bordeaux Sud-Ouest, Bordeaux, France \u2013 2LaBRI, Universit\u00e9 de Bordeaux, Institut Polytechnique de Bordeaux, Centre National de la Recherche Scientifique, UMR 5800, Talence, France \u2013 3Institut des Maladies Neurod\u00e9g\u00e9n\u00e9ratives, Universit\u00e9 de Bordeaux, Centre National de la Recherche Scientifique, UMR 5293, Bordeaux, France \u2013 4Department of Computer Science, University of Warwick, Coventry, United Kingdoms", "authors": [{"affiliations": [], "name": "Theophile Boraud"}, {"affiliations": [], "name": "Anthony Strock"}, {"affiliations": [], "name": "Beno\u00eet Girard"}, {"affiliations": [], "name": "Vahid Rostami"}, {"affiliations": [], "name": "Christian Jarvers"}], "id": "SP:b28e29f8f64828928ded8fc1b8b9dee00af37538", "references": [{"authors": ["R. Pascanu", "H. Jaeger"], "title": "A Neurodynamical Model for Working Memory.", "venue": "Neural Networks", "year": 2010}, {"authors": ["T. Boraud", "A. Strock"], "title": "theoboraud/ESN: Release 1.1 (ReScience submission)", "venue": "Version 1.1. Oct. 2019. DOI: 10.5281/zenodo.3518002. URL: https://doi.org/10.5281/zenodo.3518002. ReScience C", "year": 2021}], "sections": [{"text": "R E S C I E N C E C"}, {"heading": "Replication / Computational Neuroscience", "text": "[Re] A Neurodynamical Model for Working Memory\nTheophile Boraud1,4, ID and Anthony Strock1,2,3, ID 1INRIA Bordeaux Sud-Ouest, Bordeaux, France \u2013 2LaBRI, Universit\u00e9 de Bordeaux, Institut Polytechnique de Bordeaux, Centre National de la Recherche Scientifique, UMR 5800, Talence, France \u2013 3Institut des Maladies Neurod\u00e9g\u00e9n\u00e9ratives, Universit\u00e9 de Bordeaux, Centre National de la Recherche Scientifique, UMR 5293, Bordeaux, France \u2013 4Department of Computer Science, University of Warwick, Coventry, United Kingdoms\nEdited by Beno\u00eet Girard ID\nReviewed by Vahid Rostami ID\nChristian Jarvers ID\nReceived 24 October 2019\nPublished 01 April 2021\nDOI 10.5281/zenodo.4655870\n1 Introduction\nIn this work, we successfully replicated the results of the articleANeurodynamicalModel for Working Memory1 written by Razvan Pascanu and Herbert Jaeger. This replication has been done using Python, and the code is available on GitHub2. In their article, they propose (1) a way to implement working memory and (2) a way to characterise working memory states.\n(1) First, they implemented a working memory model in the form of a Recurrent Neural Network (RNN), more precisely an Echo State Network (ESN). In their model, working memory corresponds to special output units trained to maintain information through feedback connections. Their model had been trained to predict the next character in a sequence randomly generated depending on a context, and to maintain this context in workingmemory in order to help the prediction. We replicated the samemodel, trained it to solve mostly the same task and obtained comparable results.\n(2) They characterised the working memory states of their model by defining a notion of attractors for input driven dynamical systems. In autonomous dynamical systems the memory states could have been characterised for instance by the stable fixed points of the dynamic. However with inputs, these points are transformed into blobs stable against input that does not aim to change the memory state. In our replication we also obtained qualitatively the same result. However in the paper they go further and pro\u2010 pose a method to automatically find the attractors of an input driven dynamical system. They applied this method only to a toy model and not to the working memory model so we did not implement that part.\nIn Section 2 we give all the implementation details of the model and the tasks it aims to solve. We also highlight the few details that were missing in the paper and that we had to make a choice on in order to reproduce the results. Then, in Section 3, we compare the results we obtain to their results in the former article, to see whether or not we had been able to successfully replicate the experiments.\n2 Methods\nCopyright \u00a9 2021 T. Boraud and A. Strock, released under a Creative Commons Attribution 4.0 International license. Correspondence should be addressed to Theophile Boraud (theoboraud@outlook.com) The authors have declared that no competing interests exist. Code is available at https://github.com/theoboraud/JaegerESN.. Open peer review is available at https://github.com/ReScience/submissions/issues/8.\nReScience C 7.1 (#1) \u2013 Boraud and Strock 2021 1\n2.1 Model Theirmodel is a slightlymodifiedEcho StateNetwork, a special kind ofRecurrentNeural Network where only the readout weights are trained. Figure 1 illustrates and sums up the overall architecture of the model. It is composed of a recurrent internal layer of size N, a.k.a. reservoir, K input, L normal output units that are not fed back to the reservoir, andWM additional special output units. They call the latter the Working Memory units (or WM\u2010units). These WM\u2010units have a recurrent trainable connection to other WM\u2010 units (including themselves), and a feedback connection to the reservoir units. In the following we use the following notations to describe the dynamic of their model:\n\u2022 Input units activations u, with weights matrixWin for input connections;\n\u2022 Internal units activations x, with weights matrixW for internal connections;\n\u2022 Output units activations y, with weights matrixWout for output connections;\n\u2022 WM\u2010units activations m, with weights matrix Wmem for connections from the in\u2010 put units, the reservoir and the WM\u2010units to the WM\u2010units;\n\u2022 Weights matrix Wb for the feedback connections from the WM\u2010units to the reser\u2010 voir.\nGiven these notations, the dynamic of their model is as follow:\n\u2022 Eq. (1) represents the dynamic of the internal units. This dynamic involves at the same time the state of the internal units given their previous state, the previous state of the WM\u2010units and the actual state of input units. We use f to subsume its activation functions, which are the hyperbolic tangent in this model.\nx(n+ 1) = f(W inu(n+ 1) +Wx(n) +W bm(n)) (1)\n\u2022 Eq. (2) represents the dynamic of theWM\u2010units. This dynamic involves the state of theWM\u2010units given the actual state of input and internal units, and the actual state of theWM\u2010units. The sharp threshold function fm is the activation function for the WM\u2010units, and is given by Eq. (3), while Wmem is computed by linear regression during the step 1 of the model Training phase as shown in Eq. (5).\nm(n+ 1) = fm(Wmem(u(n+ 1), x(n+ 1))) (2)\n\u2022 Eq. (3) is the sharp threshold function used as the activation function of the WM\u2010 units.\nfm = { \u22120.5 x \u2264 0. +0.5 x > 0.\n(3)\n\u2022 Eq. (4) represents the dynamic of the output units. This dynamic involves the state of the output units given the actual state of input and internal units. fout is the activation function of the output units, and is the Identity function, while Wout will be computed by linear regression during the step 2 of the model Training phase as shown in Eq. (6).\ny(n+ 1) = fout(W out(u(n+ 1), x(n+ 1))) (4)\n\u2022 Eq. (5) is the equation to compute the weights matrix for WM\u2010units using linear regression. The activations of the reservoir, the input and the target of the WM\u2010 units are all stored in matrix H,Mtarget is the target of the WM\u2010units, and \u2020 stands for the pseudo\u2010inverse.\nWmem = (H\u2020 \u00b7Mtarget)T (5)\nReScience C 7.1 (#1) \u2013 Boraud and Strock 2021 2\n\u2022 Eq. (6) is the equation to compute the weights matrix for output units using linear regression. The activations of the reservoir and the input units are all stored in matrix G, Ytarget is the target of the output units, and \u2020 stands as before for the pseudo\u2010inverse.\nW out = (G\u2020 \u00b7 fout \u22121 (Ytarget)) T (6)\nWhen the model starts, the weights are initialized as stated:\n\u2022 We have K = 13, N = 1200, L = 65 andWM = 6.\n\u2022 Win is of size N\u00d7 K, with 10% of which are +0.5, 10% are \u20100.5, and the rest is 0.\n\u2022 W is of size N\u00d7 N, with only 12000 random non\u2010zero connections, which will ran\u2010 domly either take the value +0.1540 or \u20100.1540.\n\u2022 Wb is of size N\u00d7WM, where every weight is either +0.4 or \u20100.4.\n2.2 Input generation Pascanu and Jaeger also describe the method they used to generate the input of the model. The input consists of a constant bias (\u20100.5) and a column of 12 pixels of an im\u2010 age. Each column of 12 pixels of the image is given as input to the network one after the other.\nThis image represents a sequence of characters, i.e. curly brackets (opened or closed) or 65 other ASCII symbols (e.g. letters in lower cases, numbers...). To each of these charac\u2010 ters we associate a number that is its position in an alphabet sequence (see Table 1). For the sake of simplicity, in the following we say character i for the i\u2010th character in that\nReScience C 7.1 (#1) \u2013 Boraud and Strock 2021 3\nsequence. The sequence of characters is generated randomly using different 7 Markov Chains, and the Markov Chain used depends on the current curly bracket level. More precisely, the next character is either:\n\u2022 an open (resp. closed) curly bracket thus increasing (resp. decreasing) the current bracket level by 1 to a maximum (resp. minimum) of 6 (resp. 0)\n\u2022 or generated with a Markov chain given the last character that was not a curly bracket\nGiven that the next character is not a curly bracket, if the current bracket level is j, the next character after i will either be:\n\u2022 i+ j+ 1modulo 65 with probability 0.8;\n\u2022 any of the 64 other characters from the alphabet with probability 0.264 = 0.003125.\nDuring the training phase, the probability for the next character to be an open (resp. closed) curly bracket is 0.15. Whereas in the testing phase it is 0.03, which forces the WM\u2010unit to maintain the current bracket level for a longer time.\nThe sequence is transformed into an image by displaying and concatenating each of its characters. The characters are displayed using a randomly selected font and then uni\u2010 formly randomly stretched between 6, 7 or 8 pixels in width. Finally a salt\u2010and\u2010pepper noise of amplitude 0.1 and probability 1 is added to the whole image.\nSee Table 2 to observe an example of input sequence generated from the alphabet in Table 1 in Section 2.6.2.\n2.3 Task The aim of theWM\u2010units is to keep track of the current bracket level. When the current bracket level is k, the k first WM\u2010units should be at +0.5, whereas the remaining ones should be at \u20100.5.\nThe aim of the output units is to predict the next character in the sequence when the current character is not a curly bracket. The prediction for the next character should be made in the middle of receiving the current character. If character i should be pre\u2010 dicted, then all the output units but the i\u2010th one should be set to 0 whereas the i\u2010th one should be 1.\nReScience C 7.1 (#1) \u2013 Boraud and Strock 2021 4\n2.4 Training and testing\nAs they did, we train Wmem and W out separately with different input sequences. First we train Wmem using teacher forcing and then we train W out. We use a sequence of 10000 characters to train Wmem, and of 49000 characters to train W out. The teacher signal for training Wmem is defined at all times whereas for training W out it is defined only in the middle of characters that are not curly brackets. Finally we test the model with a 35000 characters long sequence. As an error on the bracket level would propagate and create more errors, each time themodel makes amistake on the bracket level in the WM\u2010unitswe count it and correct it. These errors are classifiedbetween false positives (i.e. the network detects a bracket when there is none) and false negatives (i.e. the network fails to detect a bracket). The errors made by the output units are also counted but not corrected. The error rate of prediction is then computed given the number of characters the network failed to predict correctly and the total length of the sequence.\n2.5 Attractors Following the instructions in the original article, we run the WM model 7 times, each time with a different sequence with no bracket, and forcing the WM\u2010unit to a fixed bracket level (k = 0 for the first, k = 1 for the second, etc...). Each sequence has a length of 6500 characters, for about 45,000 network updates in total. We collect in each case both the reservoir states and the input units. Then, every sets of reservoir states (resp. input vectors) are concatenated, and their first principal components (or PCs) are computed. Finally, for each of the 7 original sets, the first PC of the inputs is plotted against the first two PCs of the reservoir states.\n2.6 Implementation choices In the original article very few parts were unclear. Pascanu and Jaeger briefly explain how they choose to generate the input, but did not specify the exact method and tools they used to implement it. This subsection aims at listing out the choices we made and the tools we used to obtain similar results than the original paper.\nLanguage and libraries \u2014 Since the language used for the former model was not specified, we decide to use Python (3.7.4) to implement our model. The article did not specify which tools they used to transform their symbolic sequence into an image. Thus, we choose to use the libraries PIL, FreeType and SciPy for this purpose.\nInput sequence alphabet \u2014 In the original paper Pascanu and Jageger did not specify which exact characters they used for their 65 symbols, and therefore not the exact order of this alphabet. For this, we decide to create our own, as shown on Table 1. We can observe in Table 2 an input sequence example in order to further understand the behaviour of the input generation algorithm with our alphabet.\nFont \u2014We have not been able to find the fonts used in the original model (FreeMono, FreeMono Bold, FreeMono Oblique and FreeMono Bold Oblique of Gimp 2.3.6). Therefore, we try in the first place to use these FreeMono (classic, oblique, bold and bold oblique) fonts. However, as we encounter more errors than in the original paper, we use the Incosolata (regular and bold) fonts distributed by Google, which give far better results.\nReScience C 7.1 (#1) \u2013 Boraud and Strock 2021 5\nNoise \u2014 Pascanu and Jaeger did not specify in the original the probability of the salt\u2010and\u2010 pepper noise. Thus, we tried a salt\u2010and\u2010pepper noise of amplitude 0.1, and probability 1.\nWarm up in Principal Component Analysis \u2014We can observe that for each memory state the first few points do not necessarily gather with their cluster. Thus, the first 100 time steps are removed from display only for clarity purposes.\n3 Results\n3.1 Network According to the former article, the experiment has been run over 30 runs for the net\u2010 work. We assume that each time, it uses different randomly generated training and testing sequences, and reservoir, input and feedback weights. Those 30 runs are then used to compute mean and standard deviation of different performance measures. In the following Tables we use the color light green for the FreeMono1 fonts (first row on re\u2010 sult tables), dark green for Inconsolata2 (second row), and black for their former results (third row).\nFirst, as they did in the original paper we inspect the WM\u2010units performance. We dis\u2010 sociate the errors made by the WM\u2010units into false negative errors (when the network did not recognize a curly bracket as such) and false positive errors (when the network detects the character as a curly bracket while it is not). These errors are counted, and then transformed into three different percentages, depending on the number of curly brackets in the sequence, the number of characters, or the number of time steps (i.e. length of the input image in pixels). The Table 3 shows the results obtained.\nThen we look at the characters falsely identified as curly brackets. The Table 4 shows how the characters highlighted in the original experiment are falsely identified as curly brackets. As in the former article, we measure the average absolute value of Wmem weights over the 30 runs, which can be observed in Table 5.\n1Seed used for results using FreeMono: 1639617780 2Seed used for results using Inconsolata: 3939310522\nReScience C 7.1 (#1) \u2013 Boraud and Strock 2021 6\nAs we can see in Tables 3 and 4, by using the FreeMono font we obtain way more errors in the WM\u2010unit than in the original paper. However by using the Inconsolata font we obtain similar results.\nFinally, we compute the average error rate for the prediction made in the output units. We count only the errors in the middle of the presentation of a character that is not a curly bracket, as the target is defined only during these time steps. Over the 30 runs, with the Freemono font we find an error rate of 26.02 \u00b1 0.32%, for 24.83 \u00b1 0.27% in the former article. With the Inconsolata font we obtain an error rate of 22.22\u00b1 0.25%. These three results are very similar.\nIt is important to note that, during our various testing, and as in the original model,\nReScience C 7.1 (#1) \u2013 Boraud and Strock 2021 7\nthe network never changes the WM\u2010units to an invalid state or by increasing or decreas\u2010 ing the counter bymore than one, attesting their assumption that any errorwould be the result of misclassification of a character (as expected), and not any other subprocesses of the WM\u2010units.\n3.2 Attractors As in the original paper, for both Freemono and Inconsolata font we can see clusters of points in the first two principal components(see dark projection in Figure 4). Each of this cluster seems to be an attractor (as they defined in the original paper) associated to a memory state. It is stable against the characters that are not curly brackets, and the curly brackets makes it move from one to the other.\n4 Conclusion\nConsidering the original toolswerenot sourced,weneeded touse our own tools, whether it be for the language, the fonts but also for the input generation, which could easily jus\u2010 tify the differences between the original results and the ones found in this replication. However, these differences also highlight the fact that this model is really modular and consistent, giving similar results despite the differences in fonts and error rates.\nReScience C 7.1 (#1) \u2013 Boraud and Strock 2021 8\nIn the end, we consider that we have been able to successfully replicate the result ob\u2010 tained by Pascanu and Jaeger in A Neurodynamical Model for Working Memory. Very few information were missing in the original paper, such as the input generation methods from string to image they used, a more precise definition of the noise they added or the ASCII characters alphabet they used."}], "title": "[Re] A Neurodynamical Model for Working Memory", "year": 2021}