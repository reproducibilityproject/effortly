{"abstractText": "In computer vision, convolutional architectures [1, 2, 3, 4] have dominated across vari\u2010 ous image recognition tasks like classification, segmentation, etc. However, they have some limitations like lack of rotation invariance, inability to aggregate informationbased on the image content, etc. This has inspired researchers to explore a different design space and introduce models with interesting new capabilities. Self\u2010attention based net\u2010 works, in particular Transformers [5] have become the model of choice for various Nat\u2010 ural Language Processing (NLP) tasks. The major difference between transformers and previous methods, such as recurrent neural networks and Convolutional Neural Net\u2010 works (CNN), is that the former can simultaneously learn to attend to various parts of the input sequence. To utilize this capacity to learn meaningful inter\u2010dependencies, many recent works have tried to incorporate self\u2010attention [6, 7], some even replacing convolutions entirely [8, 9], in networks for vision tasks. The main highlights of the paper \u201dOn the Relationship between Self\u2010Attention and Convolutional Layers\u201d [10] are:", "authors": [{"affiliations": [], "name": "Mukund Varma"}, {"affiliations": [], "name": "Nishant Prabhu"}, {"affiliations": [], "name": "Olivia Guest"}, {"affiliations": [], "name": "Nicholas Sexton"}], "id": "SP:6f4aae034408b4653ed1eee92755b1f9b305473b", "references": [{"authors": ["K. Simonya"], "title": "and A", "venue": "Zisserman. \u201cVery Deep Convolutional Networks for Large-Scale Image Recognition.\u201d In: 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings. Ed. by Y. Bengio and Y. LeCun.", "year": 2015}, {"authors": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S.E. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "title": "Going Deeper with Convolutions.", "year": 2014}, {"authors": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "title": "Deep Residual Learning for Image Recognition.", "year": 2015}, {"authors": ["J. Hu", "L. Shen", "S. Albanie", "G. Sun"], "title": "and E", "venue": "Wu. Squeeze-and-Excitation Networks.", "year": 2019}, {"authors": ["A. Vaswani", "N. Shazeer", "N. Parmar", "J. Uszkoreit", "L. Jones", "A.N. Gomez", "L. Kaiser"], "title": "and I", "venue": "Polosukhin. Attention Is All You Need.", "year": 2017}, {"authors": ["N. Carion", "F. Massa", "G. Synnaeve", "N. Usunier", "A. Kirillov"], "title": "and S", "venue": "Zagoruyko. End-to-End Object Detection with Transformers.", "year": 2020}, {"authors": ["X. Wang", "R.B. Girshick", "A. Gupta", "K. He"], "title": "Non-local Neural Networks.", "year": 2017}, {"authors": ["P. Ramachandran", "N. Parmar", "A. Vaswani", "I. Bello", "A. Levskaya", "J. Shlens"], "title": "Stand-Alone Self-Attention in Vision Models.", "venue": "CoRR abs/1906.05909", "year": 2019}, {"authors": ["H. Wang", "Y. Zhu", "B. Green", "H. Adam"], "title": "A", "venue": "Yuille, and L.-C. Chen. Axial-DeepLab: Stand-Alone Axial-Attention for Panoptic Segmentation.", "year": 2020}, {"authors": ["J.-B. Cordonnier", "A. Loukas", "M. Jaggi"], "title": "On the Relationship between Self-Attention and Convolutional Layers.", "venue": "CoRR abs/1911.03584", "year": 2019}, {"authors": ["A. Dosovitski"], "title": "et al", "venue": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.", "year": 2020}, {"authors": ["H. Zhao", "J. Jia", "V. Koltun"], "title": "Exploring Self-attention for Image Recognition", "venue": "2020. arXiv: 2004 .", "year": 1362}, {"authors": ["Z. Dai", "Z. Yang", "Y. Yang", "J.G. Carbonell", "Q.V. Le", "R. Salakhutdinov"], "title": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context.", "year": 2019}, {"authors": ["A. Krizhevsky"], "title": "Learning multiple layers of features from tiny images", "venue": "Tech. rep.", "year": 2009}, {"authors": ["T.M. Pham", "T. Bui", "L. Mai"], "title": "and A", "venue": "Nguyen. Out of Order: How important is the sequential order of words in a sentence in Natural Language Understanding tasks?", "year": 2020}], "sections": [{"text": "R E S C I E N C E C"}, {"heading": "Editorial", "text": "[Re] On the Relationship between Self-Attention and"}, {"heading": "Convolutional Layers", "text": "Mukund Varma1, ID and Nishant Prabhu1, ID 1Indian Institute of Technology Madras, Chennai, India\nEdited by Olivia Guest ID\nReviewed by Nicholas Sexton ID\nXiaoliang (Ken) Luo ID\nReceived 03 April 2021\nPublished 27 August 2021\nDOI 10.5281/zenodo.5217602\n1 Introduction\nIn computer vision, convolutional architectures [1, 2, 3, 4] have dominated across vari\u2010 ous image recognition tasks like classification, segmentation, etc. However, they have some limitations like lack of rotation invariance, inability to aggregate informationbased on the image content, etc. This has inspired researchers to explore a different design space and introduce models with interesting new capabilities. Self\u2010attention based net\u2010 works, in particular Transformers [5] have become the model of choice for various Nat\u2010 ural Language Processing (NLP) tasks. The major difference between transformers and previous methods, such as recurrent neural networks and Convolutional Neural Net\u2010 works (CNN), is that the former can simultaneously learn to attend to various parts of the input sequence. To utilize this capacity to learn meaningful inter\u2010dependencies, many recent works have tried to incorporate self\u2010attention [6, 7], some even replacing convolutions entirely [8, 9], in networks for vision tasks. The main highlights of the paper \u201dOn the Relationship between Self\u2010Attention and Convolutional Layers\u201d [10] are:\n1. Theoretical proof that self\u2010attention layers can behave similar to convolution lay\u2010 ers.\n2. Empirical validation of the performance of the self\u2010attention model on the image classification task using the CIFAR10 dataset. Visual evidence that self\u2010attention applied to images learns convolutional filters around the query pixels.\nIn this study, we perform a detailed analysis on the various experiments outlined in the paper. We observe certain differences from the original paper which lead to interesting observations. We go beyond verifying the claims by trying to solve the observed prob\u2010 lems and propose a novel attention operation, which we refer to asHierarchical Attention (HA). We incorporate HA in various existing architectures [10, 11, 12] and our detailed experiments suggest significantly improved performance (\u2248 5%) with roughly 1/5th the number of parameters.\nCopyright \u00a9 2021 M. Varma and N. Prabhu, released under a Creative Commons Attribution 4.0 International license. Correspondence should be addressed to Nishant Prabhu (me17b084@smail.iitm.ac.in) The authors have declared that no competing interests exists. Code is available at https://github.com/NishantPrabhu/Self-Attention-and-Convolutions. Open peer review is available at https://github.com/ReScience/submissions/issues/53.\nReScience C 7.1 \u2013 Varma and Prabhu 2021 1\n1.1 Outline of this report We structure the report as follows:\n1. Section 2 formally introduces the attention operation, followed by the fundamen\u2010 tal principle of the transformer.\n2. We validate the claims made by the paper in Section 3. We visualize the attention patterns of the suggested models and compare with those mentioned in the paper, commenting on any similarities or differences.\n3. We introduce a novel Hierarchical Attention operation in Section 4. We compare the modified operation on various methods and empirically show significant per\u2010 formance gains.\n4. Section 5 concludes and suggests possible improvements for future research.\n2 Fundamentals\nIn this section, we introduce the attention operation, first in terms of its origin in NLP and how it can be extended for images. We also provide a short introduction to trans\u2010 formers since most methods described in this report heavily rely on it.\n2.1 Attention Attention was first proposed for NLP, where the goal is to focus on a subset of important words. Consequently, relations between inputs are highlighted that can be used to cap\u2010 ture context and higher\u2010order dependencies. The attentionmatrixA(.) indicates a score betweenN queriesQ andNk keys, which indicates which part of the input sequence to focus on. \u03c3(.) is an activation function (generally softmax(.)).\nA(Q,K) = \u03c3(QKT ) (1)\nTo capture the relations among the input sequence, the values V are weighted by the scores from Equation 1. Therefore, we have\nSelfAttention(Q,K, V ) = A(Q,K) \u00b7 V (2)\nWhile in NLP each element in the sequence corresponds to a word, the same idea is applicable for a sequence of N discrete objects, like pixels of an image. A key property of the self\u2010attention model described above is that it is equivariant to the input order, i.e. it gives the same output independent of how the N input tokens are shuffled. This is quite problematic in cases where the order actually matters like in images. Hence, a positional encoding is learnt for each token in the sequence and added before the self\u2010 attention operation. Hence, the Q and K vectors are derived from the summation of inputX and positional encoding P .\n2.2 Transformer Attention The Transformer network is an extension of the attentionmechanism fromEqn. 2 based on the Multi\u2010Head Attention (MHA) operation. Rather than computing the attention once, theMHAoperation computes itmultiple times (heads). This helps the transformer jointly attend to different information derived from each head. The output from each of these heads are concatenated before projecting onto a final output dimension. A transformer layer also contains a residual connection followed by a layer normalization. The overall operation can be summarized as:\nMHA(Q,K, V, heads) = concatheads[A(Q,K, V )] Transformer = LayerNorm(MHA+MLP(MHA))\n(3)\nReScience C 7.1 \u2013 Varma and Prabhu 2021 2\n2.3 Positional Encoding for Images There are two types of positional encodings used in transformer\u2010based architectures: absolute and relative encoding. Absolute encodings assign a (fixed or learned) vector Pp to to every pixel p whereas the relative positional encoding [13] considers only the position difference between the query pixel (pixel we compute the representation of) and the key pixel (pixel we attend to). The authors from the paper have elegantly proved how the attention operation mimics a convolution. The main result is the following: A multihead selfattention layer with Nh heads of dimensionDh, output dimensionDout and a relative positional encoding of dimension Dp >= 3 can express any convolutional layer of kernel size \u221a Nh \u2217 \u221a Nh and min(Dh, Dout)\noutput channels. In this proposed construction, the attention scores of each head must attend to different relative pixel shifts within a kernel (Lemma 1 from the paper). The above condition is satisfied for the relative positional encoding referred to as Quadratic Encoding. However, experiments suggest that a relative position encoding learned by a neural network (Learned Relative Position Encoding) can also satisfy the conditions of the lemma. We strongly urge the readers to refer to the original paper to get a complete understanding.\n3 Reproducibility\nThe aim of this section is to validate the results claimed by the paper \u2010 to examine whether self\u2010attention layers in practice do actually learn to operate like convolutional layers when trained on the standard image classification task. For all our experiments mentioned in this report, we use the CIFAR10 dataset to benchmark the performance of the model. We implement the original paper from scratch in PyTorch and refer to the author\u2019s source code for verification1.\n3.1 Dataset - CIFAR10 The CIFAR\u201010 dataset [14] consists of 60,000 colour images of size 32 \u00d7 32 split among 10 classes. There are 6,000 images per class split into 5,000 training and 1,000 validation samples.\n3.2 Computational Requirements Experiments involving smaller models, namely ResNet18 and Quadratic embedding, were trained on an 8GB NVIDIA RTX 2060 GPU. Learned embedding\u2010based models were trained on 16GB NVIDIA V100 virtual GPUs rented from Amazon Web Services (AWS). Each training run for the smaller models required around 20 hours while the larger ex\u2010 periments took over 2 days for convergence.\n3.3 Experiments and Results The results mentioned in the paper uses a fully\u2010attentional model consisting of 6 multi\u2010 head self\u2010attention layers each with 9 heads. In all the experiments, the input image undergoes a 2 \u00d7 2 down\u2010sampling operation to reduce its size. The final image vector is derived by average\u2010pooling the representations derived from the last layer and then passed to a linear layer for classification. Please refer to Table. 4 (Appendix) for a de\u2010 tailed list of hyperparameters used in each experiment. We closely refer to the official implementation and were able to reproduce all the results within 1% of the reported value. Table. 1 compares the results mentioned in the paper and the ones obtained\n1https://github.com/epfml/attention-cnn\nReScience C 7.1 \u2013 Varma and Prabhu 2021 3\nusing our implementation. Fig. 1a visualizes the test accuracy on CIFAR10 at every 10 epochs for each model and it is quite evident that fully convolutional networks like ResNet18 tend to converge faster. The following subsections describe these results in detail.\nQuadratic Encoding \u2014 The authors show that the attention probabilities in the quadratic positional encoding is similar to an isotropic bivariateGaussiandistributionwithbounded support. To validate their claims, all the attention matrices in the model are replaced with theseGaussianpriors, with learnable parameters to determine the center andwidth of each attention head. Further, this is extended to a non\u2010isotropic distribution over pixel positions as it might be interesting to see if the model would learn to attend to such groups of pixels \u2010 thus forming representations unseen in CNNs. Fig. 2 visualizes the attention centers for each head for all the layers and at different epochs. After op\u2010 timization, we can see that the heads attend to a specific pixel of the image forming a grid around the query pixel. This confirms the intuition that self\u2010attention applied to images learns convolution\u2010like filters around the query pixel. Also, it can be seen that the initial layers (1\u20102) focus at local patterns while the deeper layers (3\u20106) attend to larger patterns by positioning the center of attention further from the queried pixel position. Fig. 2b shows that the network did learn non\u2010isotropic attention patterns especially in the last layers. However, there is no performance improvement suggesting that it is not particularly helpful in practice.\nLearned Relative Position Encoding \u2014 In this experiment, the authors try to study the posi\u2010 tional encoding generally used in fully\u2010attentional models [8]. The positional encoding vector for each row and column pixel shift is learnt. The final relative position encod\u2010 ing of a key pixel with a query pixel is derived as the concatenation of row and column\nReScience C 7.1 \u2013 Varma and Prabhu 2021 4\n.\nshift embeddings. First, the authors completely discard the input data and compute the attention weights solely with the derived encoding (Learned embedding w/o content). Fig. 3a visualizes the attention probabilities for a given query pixel, confirming the hy\u2010 pothesis that: even when left to learn positional encoding from randomly initialized vectors, certain self\u2010attention heads learn to attend to individual pixels while the others learn non\u2010localized patterns and long range dependencies. In another setting (Learned embedding w/ content), both the positional and content based attention information is used which corresponds to a full\u2010blown stand alone self\u2010attentionmodel. Fig. 3b visual\u2010 izes the attention probabilities for a given query pixel in this setting and it is interesting to note that even when left to learn the encoding from the data, some attention heads exploit positional information like CNNs while the others focus on the content. In Fig. 4, we visualize the attention probabilities averaged across an entire batch of images to understand the focus of each head and remove dependency on the input image for both the experiments.\nAverage AttentionVisualization: The authors from the original paper visualize the atten\u2010 tion probabilities for a single image or across a batch images for a specific query pixel. A single query pixel does not convey information regarding where the model focuses on in the entire image and it is not practical to plot individual figures for every query pixel. Hence, we also visualize the attention probabilities using what we refer to as Average Attention, to identify which portions of the entire image the model attends to. Given a softmax normalized attention matrix of size N \u00d7 N , every row represents the relation\u2010 ships between a query pixel and the others. First, every element (\u03b1i,j) is divided by the row\u2010wise sum to ensure that all the values are in scale. Then the row\u2010wise mean is com\u2010 puted to determine the importance value for each pixel. If a pixel is strongly correlated to multiple pixels, the importance value will be higher determining that the model has a stronger focus on that given pixel. We mathematically describe the operation in Eqn. 4. Fig. 5 visualizes the average attention for the learned embedding with and without content. In Fig. 5a, since the content data is discarded, the model is clearly focusing on positional patterns while in Fig. 5b, the model attends to both positional and content\nReScience C 7.1 \u2013 Varma and Prabhu 2021 5\ninformation. We visualize additional figures in Sections. B.1\u2010B.3 (Appendix).\n\u03b1i,j = softmax(\u03b1i,j) = exp\u03b1i,j\u2211\nk(\u03b1i,k)\n\u03b1i,j = \u03b1i,j\u2211 k(\u03b1k,i)\nAvg Attni = \u2211 k(\u03b1i,k)\nN\n(4)\n3.4 Increasing the number of heads As per the analogy derived between self\u2010attention and convolutions, the number of heads is directly related to the kernel size in a convolution operation. Hence, we in\u2010 crease the number of heads from 9 to 16. It is important to note here that unlike the general procedure of settingDh = Dout/Nh in transformer\u2010based architectures, the pa\u2010 per suggests to concatenate heads of dimension Dh = Dout since the effective number of learned filters is min(Dh, Dout). Given the limited compute, we reduced Dout from 400 to 256 while increasing the number of heads to 16. As seen in Table. 2 there seems to\nReScience C 7.1 \u2013 Varma and Prabhu 2021 6\nbe no significant impact to the model\u2019s performance. However, the model takes longer to converge due to the increased number of parameters (Fig. 1b). We visualize the at\u2010 tention probabilities in Sections. B.4\u2010B.6 (Appendix).\n3.5 Additional Observations\nInductive biases in Transformers \u2014 As seen from the results mentioned in Table. 1, the fully attentional model utilizing learnable embeddings with image content performs poorly when compared to the othermethods. Asmentioned in the paper [11], transformers lack biases inherent to CNNs like translation equivariance and retention of 2D neighborhood structure, etc. Only the Multi\u2010Layer Perceptron (MLP) layers used in these methods are local and translation equivariant, while the self attention layers are global. Even in the case of NLP, almost 75\u201090% of the predictions are correct even when the input words are randomly shuffled [15]. This implies that transformer\u2010basedmethods do not sufficiently capture spatial information even with positional encodings and require a large amount of training data to do so. This could be the reason for improved performance in the case of Learned embedding w/o content and Quadratic embedding as the attention matrices are directly replaced with positional information.\nOver-expressive power of attention matrices \u2014 The most important step of the self\u2010attention operation is the generation of the attention matrix of size N \u00d7 N . In NLP, the value of N tends to be small (<100) in most cases as we are dealing with words in a sentence. On the contrary, images when broken down result in very long sequences of pixels, hence creating large attentionmatrices. Therefore, these attentionmatrices can be sparse and themodel has a very high tendency of focusing on very high level information. This can lead to over\u2010fitting and is talked about in the case of pointclouds2 where the number of\n2https://github.com/juho-lee/set_transformer/issues/3#issuecomment-586711062\nReScience C 7.1 \u2013 Varma and Prabhu 2021 7\npoints are very large (>1000). This has also been observed in our experiments. As seen in Figs. 3b, 4b, 5b, the attention heads in the last few 2 layers are very sparse and do not capture any information. This can also be seen in the case of Quadratic encoding (Fig. 2), where certain attention heads focus on \u201dnon\u2010intuitive\u201d portions of the image (i.e) a thin strip of pixels or attend uniformly across a large patch of pixels. A simple and naive way to overcome this problem is to reduce the number of heads or layers but this is not an effective method as the model loses its capacity to learn strong features.\n4 Hierarchical Attention\nGiven the problems described above, we need to propose a method which can avoid the over\u2010expressive nature of independent attention headswhile still being able to learn and derive strong features from the input image across layers. We now introduce a novel attention operation which we refer to as Hierarchical Attention (HA) operation. In the following sections, we explain the core idea behind the operation and perform detailed experiments to showcase its effectiveness.\n4.1 Methodology Inmost of the transformer\u2010basedmethods, independent self\u2010attention layers are stacked sequentially and the output from one is passed onto the next to derive the Q, K and V vectors. This allows each attention head to freely attend to specific features and derive a better representation. Deviating from these methods, the HA operation updates only the Q, V vectors while the K remains the same after each attention block. Further, the weights are shared across these attention layers inducing the transformer model to iteratively refine its representation across layers. This can be considered analogous to an unrolled recurrent neural network (RNN) aswe are trying to sequentially improve the representation across layers based on the previous hidden state. This helps the model hierarchically learn complex features by focusing on corresponding portions of the K vector, and aggregating required information from the V vector. Figs. 6, 7 visualize the normal attention and the hierarchical attention operations respectively. For the sake of simplicity, we do not visualize the entire inner workings of the transformer like residual connections, layer normalization, etc. This is a very simple yet effective method which can be easily adapted to any existing attention\u2010based network as described in the next section.\nReScience C 7.1 \u2013 Varma and Prabhu 2021 8\nAsmentioned earlier, there has been a lot of recent hype in replacing convolutions with attention layers. Hence, we choose two other popular and similar papers \u2010 \u201dExploring Self\u2010attention for Image Recognition\u201d [12], \u201dAn Image is worth 16x16 words: Transform\u2010 ers for Image Recognition at scale\u201d [11] and apply the proposed HA operation to justify its effectiveness. For better understanding, we briefly introduce these papers in the fol\u2010 lowing subsections.\n4.2 Pairwise and Patchwise self attention (SAN) Introduced by [12], pairwise self\u2010attention is essentially a general representation of Eqn. 2. It is fundamentally a set operation, does not attach stationary weights to specific locations and is invariant to permutation and cardinality. The paper presents a number of variants of the pairwise attention that have greater expressive power than dot\u2010product attention. Specifically, theweight computation does not collapse the channel dimension and allows the feature aggregation to adapt to each channel. It can be mathematically formulated as follows:\nyi = \u2211 j\u03f5Ri \u03b1(xi, xj)\u2299 \u03b2 (xj)\n\u03b1(xi, xj) = \u03b3(\u03b4(xi, xj))\n(5)\nHere, i is the spatial index of feature vector xi, \u03b4(.) is the relation function mapped onto another vector by \u03b3(.). The adaptive weight vectors \u03b1(xi, xj) aggregate feature vectors obtained from \u03b2(.). An important point to note here is that \u03b4 can produce vectors of different dimensions when compared to \u03b2, allowing for a more expressive weight con\u2010 struction. The patch\u2010wise self attention is a variant of the pairwise operation, where xi is replaced by a patch of feature vectors xR(i) which allows the weight vector to incorporate infor\u2010 mation from all the feature vectors in the patch. The equations are hence rewritten as:\nyi = \u2211 j\u03f5Ri \u03b1(xR(i))j \u2299 \u03b2 (xj)\n\u03b1(xR(i)) = \u03b3(\u03b4(xR(i)))\n(6)\n4.3 Vision Transformer (VIT) The Vision Transformer [11], has successfully shown that reliance on convolutions is no longer necessary and a pure transformer outperforms all convolution based techniques significantly when pre\u2010trained on large amounts of data. In this method, an image is split into patches which is then projected onto another representation using a trainable layer and then passed through a standard set of transformer operations as described in Eqn. 3.\nReScience C 7.1 \u2013 Varma and Prabhu 2021 9\nIn Fig. 8a, we visualize the attention probabilities for a given query pixel. The rela\u2010 tionship between self\u2010attention and convolutions is striking as the model is attending to distinct pixels at a fixed shift from the query pixel reproducing the receptive field of the convolution operation. The initial layers attend to local patterns while the deeper layers focus on larger patterns positioned further away from the query pixel. Similarly, in Fig. 8b, the attention heads from the last two layers are no longer sparse and help capture more information. Hence the visual correctness verifies the operation and its increased performance. We also visualize the attention probabilities for SAN, VIT in Sections. B.7\u2010B.8, B.9\u2010B.10 (Appendix) respectively. We summarize the hierarchical operation as follows:\n1. Enable weight sharing between the layers of the model by reusing the K vectors and updating theQ and V vectors only. This helps themodel progressively extract higher level features.\n2. The method of progressive refinement ensures that the attention matrices do not get over expressive. This leads to a significant improvement over the correspond\u2010 ing non\u2010hierarchical cases.\n3. The total number of parameters in the model is independent of the number of layers and this property helps significantly reduce thenumber of parameterswhen compared to the non\u2010hierarchical versions. Also, this helps make deeper models without worrying about memory constraints.\n4. Even if the model is provided more layers than are necessary, every layer learns to attend to a different pattern. The new features learnt at every layer add on to those learnt by the previous ones, which provides its characteristic hierarchical nature.\n3https://github.com/hszhao/SAN 4https://github.com/google-research/vision_transformer\nReScience C 7.1 \u2013 Varma and Prabhu 2021 10\nBy visualizing the attention scores on a test image, we obtain convincing evidence to support this hypothesis.\n5 Conclusion\nIn this report, we study the application of self\u2010attention for image recognition, specifi\u2010 cally image classification. We validate the original paper\u2019s claimsbyperforming detailed experiments on the CIFAR10 dataset. We were able to reproduce all the results from the paper within 1% of the reported value, hence validating the claims of the original pa\u2010 per. However, there seems to be some differences in the attention figures which lead to interesting insights and the proposed Hierarchical Attention. To validate our hypothe\u2010 sis, we perform detailed experiments by incorporating HA with various methods which helps significantly improve the performance while reducing the number of parameters. These preliminary results raise various questions: Do we actually need multiple inde\u2010 pendent layers in large transformers? Does this improved performance also translate to large datasets and across various other image recognition tasks like object detection and image segmentation? We would like to answer all these questions and provide a more rigorous understanding of the proposed method in the future.\nWhat was easy The original paper is well written, quite complete and mentioned the required details to carry out various experiments mentioned in the paper.\nWhat was difficult The authors have possibly ported code from the transformers repos\u2010 itory by HuggingFace5 and left behind lots of unnecessary code making it difficult to follow. We observed major differences in the attention plots from our implementation and those reported in the paper. We attempted to contact the authors to discuss these findings, but we received no response from them. However, we provide reasoning that supports our results, which also led us to conceptualizeHierarchical Attention. For com\u2010 putational reasons, our scope was limited to smaller datasets like CIFAR10 with limited hyper\u2010parameter study.\n5https://github.com/huggingface/transformers\nReScience C 7.1 \u2013 Varma and Prabhu 2021 11"}, {"heading": "Appendix", "text": "ReScience C 7.1 \u2013 Varma and Prabhu 2021 13\nB Attention Visualization\nWe present more examples for visualising the attention probabilities in various models.\nB.1 Learned embedding w/ content; 9 heads\nReScience C 7.1 \u2013 Varma and Prabhu 2021 14\nB.2 Learned embedding w/o content; 9 heads\nReScience C 7.1 \u2013 Varma and Prabhu 2021 15\nB.3 Hierarchical Learned embedding w/ content; 9 heads\nReScience C 7.1 \u2013 Varma and Prabhu 2021 16\nReScience C 7.1 \u2013 Varma and Prabhu 2021 17\nB.5 Learned embedding w/o content; 16 heads\nReScience C 7.1 \u2013 Varma and Prabhu 2021 18\nB.6 Hierarchical Learned embedding w/ content; 16 heads\nReScience C 7.1 \u2013 Varma and Prabhu 2021 19\nB.7 Hierarchical SAN Pairwise\nB.8 Hierarchical SAN Patchwise\nReScience C 7.1 \u2013 Varma and Prabhu 2021 20\nB.9 Vision Transformer (VIT)\nReScience C 7.1 \u2013 Varma and Prabhu 2021 21\nB.10 Hierarchical Vision Transformer (VIT)\nReScience C 7.1 \u2013 Varma and Prabhu 2021 22"}], "title": "[Re] On the Relationship between Self-Attention and Convolutional Layers", "year": 2021}