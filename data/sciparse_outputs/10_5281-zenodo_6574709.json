{"abstractText": "StylEx is an approach for classifier\u2010conditioned training of a StyleGAN2 model [1], in\u2010 tending to capture classifier\u2010specific attributes in its disentangled StyleSpace [2]. At\u2010 tributes can be adjusted to generate counterfactual explanations of the classifier deci\u2010 sions. StylEx is domain and classifier\u2010agnostic, while its explanations are claimed to be human\u2010interpretable, distinct, coherent and sufficient to produce flipped classifier decisions. We verify these claims by reproducing a selection of the experiments in the paper.", "authors": [{"affiliations": [], "name": "Noah van der Vleuten"}, {"affiliations": [], "name": "Tadija Radusinovi\u0107"}, {"affiliations": [], "name": "Rick Akkerman"}, {"affiliations": [], "name": "Meilina Reksoprodjo"}, {"affiliations": [], "name": "Koustuv Sinha"}, {"affiliations": [], "name": "Sharath Chandra Raparthy"}], "id": "SP:dc5c973cae7957f6ef9681bc9989211ff34a313f", "references": [{"authors": ["T. Karras", "S. Laine", "M. Aittala", "J. Hellsten", "J. Lehtinen", "T. Aila"], "title": "Analyzing and Improving the Image Quality of StyleGAN", "year": 2020}, {"authors": ["Z. Wu", "D. Lischinski", "E. Shechtman"], "title": "StyleSpace Analysis: Disentangled Controls for StyleGAN Image Generation.", "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "year": 2021}, {"authors": ["R.R. Selvaraju", "M. Cogswell", "A. Das", "R. Vedantam", "D. Parikh", "D. Batra"], "title": "Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization.", "venue": "IEEE International Conference on Computer Vision (ICCV)", "year": 2017}, {"authors": ["O. Lang"], "title": "Explaining in Style: Training a GAN to explain a classifier in StyleSpace.", "year": 2021}, {"authors": ["T. Karras", "S. Laine", "T. Aila"], "title": "A style-based generator architecture for generative adversarial networks.", "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition", "year": 2019}, {"authors": ["Z. Wu", "D. Lischinski", "E. Shechtman"], "title": "StyleSpace Analysis: Disentangled Controls for StyleGAN Image Generation", "year": 2020}, {"authors": ["I.J. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"], "title": "arXiv:1406.2661 [stat.ML", "venue": "Generative Adversarial Networks", "year": 2014}, {"authors": ["R. Zhang", "P. Isola", "A.A. Efros", "E. Shechtman", "O. Wang"], "title": "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric.", "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Los Alamitos, CA, USA: IEEE Computer Society,", "year": 2018}, {"authors": ["Z. Liu", "P. Luo", "X. Wang", "X. Tang"], "title": "Deep Learning Face Attributes in the Wild.", "venue": "Proceedings of International Conference on Computer Vision (ICCV)", "year": 2015}, {"authors": ["R. Or-El", "S. Sengupta", "O. Fried", "E. Shechtman", "I. Kemelmacher-Shlizerman"], "title": "Lifespan Age Transformation Synthesis", "year": 2020}, {"authors": ["A.G. Howard", "M. Zhu", "B. Chen", "D. Kalenichenko", "W. Wang", "T. Weyand", "M. Andreetto", "H. Adam"], "title": "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications. 2017", "year": 2017}, {"authors": ["M. Sandler", "A. Howard", "M. Zhu", "A. Zhmoginov", "L.-C. Chen"], "title": "MobileNetV2: Inverted Residuals and Linear Bottlenecks", "year": 2019}, {"authors": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "title": "Deep Residual Learning for Image Recognition.", "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "year": 2016}, {"authors": ["D.P. Kingma", "J. Ba"], "title": "Adam: A Method for Stochastic Optimization. 2017", "year": 2017}, {"authors": ["C.-K. Yeh", "B. Kim", "S.O. Arik", "C.-L. Li", "T. Pfister", "P. Ravikumar"], "title": "On Completeness-aware Concept-Based Explanations in Deep Neural Networks", "year": 2020}, {"authors": ["M. Heusel", "H. Ramsauer", "T. Unterthiner", "B. Nessler", "S. Hochreiter"], "title": "Gans trained by a two time-scale update rule converge to a local nash equilibrium.", "venue": "Advances in neural information processing systems", "year": 2017}, {"authors": ["M. Seitzer"], "title": "pytorch-fid: FID Score for PyTorch", "venue": "https://github.com/mseitzer/pytorch- fid. Version", "year": 2020}, {"authors": ["M. Mirza", "S. Osindero"], "title": "Conditional Generative Adversarial Nets", "venue": "Vleuten et al", "year": 2014}], "sections": [{"text": "R E S C I E N C E C Replication / ML Reproducibility Challenge 2021\n[Re] Explaining in Style: Training a GAN to explain a classifier in StyleSpace\nNoah van der Vleuten1, ID , Tadija Radusinovi\u01071, ID , Rick Akkerman1, ID , and Meilina Reksoprodjo2, ID 1University of Amsterdam, Amsterdam, The Netherlands \u2013 2Eindhoven University of Technology, Eindhoven, The Netherlands\nEdited by Koustuv Sinha,\nSharath Chandra Raparthy\nReceived 04 February 2022\nPublished 23 May 2022\nDOI 10.5281/zenodo.6574709\n1 Reproducibility Summary"}, {"heading": "Scope of Reproducibility", "text": "StylEx is an approach for classifier\u2010conditioned training of a StyleGAN2 model [1], in\u2010 tending to capture classifier\u2010specific attributes in its disentangled StyleSpace [2]. At\u2010 tributes can be adjusted to generate counterfactual explanations of the classifier deci\u2010 sions. StylEx is domain and classifier\u2010agnostic, while its explanations are claimed to be human\u2010interpretable, distinct, coherent and sufficient to produce flipped classifier decisions. We verify these claims by reproducing a selection of the experiments in the paper."}, {"heading": "Methodology", "text": "We verified a selection of the experimental results on the code available by the authors. However, the training procedure, network architecture and hyperparameter configura\u2010 tions were missing. As such, we re\u2010implemented the model and available TensorFlow code in PyTorch, to enable a more comprehensive reproducibility of the proposed case studies. All experiments were run in approximately 20\u201050 GPU hours per dataset, de\u2010 pending on the batch size, gradient accumulation and GPU used."}, {"heading": "Results", "text": "We verified that the publicly available pre\u2010trained model has a \u2019sufficiency\u2019 measure within 1% of the value reported in the paper. Additionally, we evaluate the Fr\u00e9chet inception distance (FID) scores of images generated by the released model. We show that the FID score increases with the number of attributes used to generate a counterfac\u2010 tual explanation. Custom models were trained on three datasets, with a reduced image dimensionality (642px). Additionally, a user study was conducted to evaluate the distinc\u2010 tiveness and coherence of the images. We report a significantly lower accuracy in the identification of the extracted attributes and \u2019sufficiency\u2019 scores on our model.\nCopyright \u00a9 2022 N.V.D. Vleuten et al., released under a Creative Commons Attribution 4.0 International license. Correspondence should be addressed to Noah van der Vleuten (noahvdvleuten@gmail.com) The authors have declared that no competing interests exist. Code is available at https://github.com/NoahVl/Explaining-In-Style-Reproducibility-Study \u2013 DOI 10.5281/zenodo.6512392. \u2013 SWH swh:1:dir:04e11a55f476b115b40fd6af9d06ed70eb248535. Open peer review is available at https://openreview.net/forum?id=SYUxyazQh0Y.\nNone 8.2 (#42) \u2013 Vleuten et al. 2022 1"}, {"heading": "What was easy", "text": "Itwas easy to run the provided JupyterNotebook, and verify the results of the pre\u2010trained models on the FFHQ dataset. Extending an existing StyleGAN2 model implementation to fit this study was relatively easy."}, {"heading": "What was difficult", "text": "Reproducing the experiments on the same scale as the authors, as well as the develop\u2010 ment of the full training procedure, model architecture and hyperparameters, partic\u2010 ularly due to underspecification in the original paper. Additionally, the conversion of code from TensorFlow to PyTorch.\nCommunication with original authors We correspondedwith the first author of the paper through several emails. Through our mail contact, additional details were released on the network architecture, the training procedure and the hyperparameter configurations.\nNone 8.2 (#42) \u2013 Vleuten et al. 2022 2\n2 Introduction\nExisting post hoc visual explainability measures, such as heatmaps[3], can highlight regions that influence model decisions. However, they do not visualize non\u2010spatially localized attributes, nor do they indicate how these areas may be changed to influence the classification. Counterfactual explanations, which are statements of the form \u201dHad the input x been x\u2032, the classifier output would have been y\u2032 instead of y\u201d, has been pro\u2010 posed as an alternative which both allows for the visualization of salient features and directly explains how they can be altered to achieve an alternative classification. As such, these explanations are promising as they can provide a suggestive recourse to non\u2010domain experts in a machine learning\u2010based decision system. The effectiveness of visual methods strongly depends on the intuitive difference that humans observe; there\u2010 fore one of the primary objectives is to find interpretable, salient attributes. Secondary objectives involve the visualization and control of the impact of these attributes on the classifier output. In thiswork, we reproduce thepaper \u2018Explaining in Style: Explaining aGAN in StyleSpace\u2019 [4]. The paper proposes a novelmethod for explaining the classification of a given image, by altering discovered human\u2010interpretable features discovered to affect the classifica\u2010 tion output. We re\u2010implemented the model in PyTorch together with the unreleased training procedure, as the original TensorFlow implementation lacked the training pro\u2010 cedure code. We performed training on the FFHQ and PlantVillage dataset using a lower resolution. Using our implementation, we check whether the results are consistent with the descriptions provided in the paper. We substantiate this with the addition of a human\u2010grounded evaluation of the generated images. Additionally, we used the FID measure to evaluate the image quality of the counterfactual generated images.\n3 Scope of Reproducibility\nThe StylEx model, in addition to the AttFind algorithm defined in the paper, is pre\u2010 sented as a viable option for generating counterfactual explanations of black\u2010box classi\u2010 fiers. The StylEx procedure aims tomake individual style coordinates classifier\u2010relevant, through a novel training procedure which is outlined in 4. As no benchmark metrics exist to evaluate and assess attribute\u2010based counterfactual explanations, the authors propose three evaluation criteria themselves: 1) visual coherence, 2) distinctness and 3)\nNone 8.2 (#42) \u2013 Vleuten et al. 2022 3\n\u2018effect of attributes on classification\u2019 (sufficiency). We reformulate these criteria as the main claims of the paper in the following manner:\n1. Visual Coherence: Attributes detected by StylEx should be clearly identifiable by humans.\n2. Distinctness: The attributes extracted by StylEx should be distinct.\n3. Sufficiency: Changing attributes should result in a change of classifier output, where changing multiple attributes has a cumulative effect.\n4 Methodology\nTo evaluate claim 1 and 2, the authors conduct a user study in two parts. To evaluate claim 3, they study the percentage of flipped classifications when modifying top\u2010k (in their case k = 10) attributes. To reproduce these claims, we conduct the same experi\u2010 ments, albeit at a lower dimensionality of 642px. The complex network architecture of StyleGAN, as well as the encoder, requires a significant number of training epochs un\u2010 til its convergence and thus, training these at the full resolution of 2562px is extremely computationally expensive. We verify the sufficiency scores of the released model, by making use of the supplied Jupyter Notebook. However, several elements crucial for reproduction were missing, including the training procedure, the omission of hyperparameter configurations and the details on the optimization procedure. As such, we ported the available TensorFlow code to PyTorch, and implemented the missing parts, to enable a more comprehensive reproducibility of StylEx. We reimplemented the StylEx procedure in PyTorch, using an open\u2010source StyleGAN2 model implementation as a starting point1. For running our code, we havemade use of an NVIDIA GTX 1080 Ti, RTX 2070 Super and a laptop RTX 3060 graphics card, running on different machines. In the conduction of the user study, we have made use of the online survey tool Qualtrics [5].\n4.1 Model descriptions In addition to a pre\u2010trained classifierC, StylEx is comprised of three trainable elements, which are a 1) generatorG, 2) a discriminatorD and 3) an encoderE. TheD andG follow the StyleGAN2 model architecture, with minor alterations toD which will be explained below. Figure 2 provides an overview of the network architecture. Some design details were unspecified or omitted in the original paper. We contacted the authors to provide clarification on these details, which are stated as follows:\n1. StylEx is trained using both encoder input and noise input transformed through StyleGAN2\u2019s mapping network, using alternating steps;\n2. The output of D is a weighted sum of the 2\u2010dimensional output of its last layer with the classifier probabilities of the 1) original image if using the encoder, 2) randomly sampled image if using noise input;\n3. Lrec and Lcls are only calculated during the generator training steps.\nThe GAN is trained jointly with the encoder, which embeds an image into the W latent space of StyleGAN2, forming a latent vector w. A recent observation by [7] highlighted the disentanglement of this space (called StyleSpace) that is used in StylEx to extract classifier\u2010specific attributes. Logits of the original image C(x) are then appended to\n1https://github.com/lucidrains/stylegan2\u2010pytorch\nNone 8.2 (#42) \u2013 Vleuten et al. 2022 4\nw, to condition the training on classifier inputs. The current architecture includes a StyleVectorizer that obtains the latent vector w from z, which is sampled from a normal distribution. In alternating steps, the generator was fed input from the encoder and in\u2010 put from the StyleVectorizer mapping network [6]. The original authors noticed a slight improvement in image quality using alternating training, compared to only using the encoder input. Note that we used two slightly different implementation choices for training our mod\u2010 els. The first implementation does not include the discriminator change mentioned in 2, while the second implementation does and uses probabilities instead of logits for con\u2010 catenation to w. We call these two choices \u2018Model 1\u2019 and \u2018Model 2\u2019 in results on datasets where we have trained both. We additionally noted that the MobileNet classifier \u2018Model 1\u2019 was trained with did not performwell on the faces. This is why, for both faces models, a ResNet classifier was used to perform the AttFind algorithm. Additionally, we skipped discriminator filtering for \u2018Model 2\u2019. Discriminator filtering skips encoded images the discriminator deems unrealistic. We did this because the discriminator was too unsta\u2010 ble to give reliable estimates. This might explain the poor performance of this model. We are unsure if this was caused by the changes to the architecture, training time or just bad luck. This expanded latent vector w, either obtained by the encoder or StyleVectorizer, is passed on to the StyleGAN2 model, where it is transformed into the StyleSpace by a set of concurrent affine transformations to style vectors s0, ..., sn. These style vectors are used to generate novel images, that aim to reconstruct the original image as closely as possible. Several losses are used to aid the training procedure. The cumulative training loss for the algorithm is a sum of losses, denoted as follows:\nStylExLoss = Ladv + Lreg + Lrec + Lcls. (1)\nA logistic adversarial loss [8] Ladv is used as in standard GAN training, followed by the regularization lossLreg, as described in the original StyleGAN [1] paper. The reconstruc\u2010 tion loss Lrec is given by the sum of Lxrec + Lwrec + LLPIPS, where the first two terms are the L1 distance between original and reconstructed input image, and the original and reconstructed w latent vector, respectively. The LLPIPS term is the LPIPS distance\nNone 8.2 (#42) \u2013 Vleuten et al. 2022 5\nbetween original and reconstructed input, as described in [9]. This loss ensures that re\u2010 constructed images resemble the original input as close as possible, to serve as an input for generating counterfactual examples. The classifier loss is defined as the Kullback\u2010 Leibler divergence between the original input imageX and the newly generated image G(E(X), C(X)) , defined as follows: Lcls = DKL[|C(x\u2032)||C(x)]. This loss ensures that the generator does not disregard image attributes that are important for the classifica\u2010 tion. To extract classifier\u2010specific attributes, the AttFind algorithm is proposed in the paper. As input, it takes the trainedmodelD and a set ofN images of which the predicted labels do not match the target label y. For each class label, AttFind encodes the images and iteratively tries to find a set Sy ofM style coordinates that represent the largest possible shift to the opposing class. Next to this, it finds the set of directions Dy \u2208 {\u00b11}M in which the attribute needs to be adjusted to flip the classifier decision. In each iteration, it considers all style coordinatesK and determines the coordinate with the largest effect. All images in which changing this coordinate results in a large effect on their probability are removed from the iteration. The process is repeated until no images are left, or until M attributes are found.\n4.2 Datasets We reproduce a selection of the findings of the authors on two of the given datasets in our PyTorch implementation:\n1. CelebA [10] The original Large\u2010scale CelebFaces Attributes (CelebA) dataset2 con\u2010 tains 200000 image entries, each containing 40 attribute annotations. We have trained classifiers on the \u2018perceived gender\u2019 attribute.\n2. FFHQ [11]The original Flickr\u2010Faces\u2010HQdataset containing 70000 images of human faces. This datasetwas used for StylEx training, while the pre\u2010trained classifierwas trained on the CelebA dataset, following the procedure of the original paper.3\n3. Plant\u2010Village: This dataset contains 54303 entries of plant images, with 38 cate\u2010 gories. This dataset was used to train the classifier to differentiate between sick and healthy leaves.\nFor the classification tasks, the FFHQ dataset was split into train/validation/test sets of 70/15/15, while the Plant\u2010Village retained a proportion of 70/20/10.\n4.3 Hyperparameters Original research: For the partial reproduction of Table 3 of the original paper, we lim\u2010 ited ourselves to a sample of n = 250 images, rather than the n = 1000 randomly sam\u2010 pled images, as denoted in the Jupyter Notebook. Reimplementation: The computational costs of training StylEx precluded an in\u2010depth hyperparameter search. For all modules except the encoder, we found a learning rate of 2e \u2212 4 for the Adam optimizer, with \u03b21 = 0.5 and \u03b22 = 0.9. We found the training to diverge unless the encoder learning rate was lowered significantly to 1e\u2212 5. We ascribe this difference to the significantly smaller input size in our models or subtle implemen\u2010 tation differences from the original paper that are unknown to us. The classifier used in the paper was MobileNetV1 [12], but we opted for a MobileNetV2 [13] or ResNet\u201018 models[14]. The authors asserted that the use of advanced networks identified more subtle cues from the datasets on the classification problems at hand, and for this purpose, we opted for ResNet\u201018. Additionally, we observed that the Mo\u2010 bileNet model did not perform well on the CelebA dataset for gender classification on\n2https://www.kaggle.com/jessicali9530/celeba\u2010dataset 3This is a detail that was revealed through contact with the authors.\nNone 8.2 (#42) \u2013 Vleuten et al. 2022 6\nthis image resolution. The components of the Lrec loss were scaled according to au\u2010 thors\u2019 suggestion in our correspondence: 0.1 for Lxrec and LLPIPS, 1 for Lwrec. Other loss components were not scaled. On the local GPUs, weused a batch size of 4with 8 gradient accumulation steps, whilewe use a batch size of 16 with 4 gradient accumulation steps on the computer cluster. For the training of the MobileNet V2 classifier and the ResNet\u201018 classifier, we finetuned the pretrained models by slowly unfreezing the top layers, we have set the learning rate to lr = 1e \u2212 4, used a batch size of 128 and used the Adam [15] with default PyTorch parameters.\n4.4 Experimental setup and code We aimed to follow the experimental setup as close as possible for our experiments. Our PyTorch implementation is available on GitHub4 to further support and advance reproducibility in machine learning research. The repository provides explanations to run the described experiments.\n4.5 Computational requirements Locally, ourmodels were trained on two differentmachines, which contained a 1) laptop NVIDIA RTX 3060, 2) an NVIDIA RTX 2070 Super. A computer cluster containing GTX 1080 Ti GPUs was also used to train some of our models. The first machine makes use of the Windows operating system, while the latter two are Linux\u2010based. For both the FFHQ dataset as well as the Plant\u2010Village dataset, training was done until convergence, which was reached in 150K training steps for the FFHQ dataset and 260K training steps on the Plant\u2010Village dataset. On the local GPUs, a batch size of 4 (RTX 3060) and 8 (RTX 2070 Super) was used along\u2010 side gradient accumulation for 8 (RTX 3060) and 2 (RTX 2070 Super) steps. On the com\u2010 puter cluster, a batch size of 16 was used, with a gradient accumulation parameter of 4. Depending on the hyperparameters of the batch size and gradient accumulation, the computational time to run the experiments ranged between 20\u201050 GPU hours. Training for 150000 steps took 20 hours on an RTX 2070 Super.\n5 Results\n5.1 Results reproducing original paper\nSufficiency \u2014Wecalculate the percentage of flipped classifications after changing the top\u2010 10 most influential attributes found by the AttFind procedure. The results can be seen in table 1. Our results using the author\u2019s model are within 1% of the accuracy reported in the paper. Our models show significantly worse performance on both perceived gen\u2010 der (51% vs 83.2%) and plant healthiness (30% vs 91.2%), showing that the attributes discovered are not very relevant for classification.\nCoherency and Distinctness \u2014 Similar to the original paper, we have conducted a user study (n = 54) to evaluate the distinctiveness of the found attributes and the coherence of the generated images. The user study was divided into two parts \u2010 1) a classification study and 2) a verbal description study, following a similar setup as presented in [16]. For the classification study, users are shown one animation of four images in a grid format. The two images on the left switch between their original image and have the same transformation applied. The two images on the right swap between their original image and one of two transformations. The user then has to find the transformation on\n4https://github.com/NoahVl/Explaining\u2010In\u2010Style\u2010Reproducibility\u2010Study\nNone 8.2 (#42) \u2013 Vleuten et al. 2022 7\nthe right that matches with those on the left. In the verbal description study, the users were asked to look at an animation of four images, and consequently describe in 1\u20104 words the changing attribute. We have done this for the plant dataset as well as the FFHQ datasets. The order of the datasets was randomized to avoid biases and learning effects. All participants are under\u2010 graduate and graduate students who have some affinity with and knowledge ofmachine learning. None of them reported having color blindness. In Appendix 7, a few examples can be found on the posed questions (without animations) and the type of provided an\u2010 swers. The full user evaluation data and questions from the questionnaire can be found on our GitHub repository, under the folder all_user_studies.\nAlthough our results seem to slightly outperform the results by Wu et al. (2021) on the perceived gender classifier, it does not seem to outperform the method posed by Lang et al. (2021).\n5.2 Results beyond original paper\nFID scores \u2014 To investigate the impact of attribute per\u2010 turbation on the quality of the generated images, we compute the Fr\u00e9chet Inception Distance (FID) [17] between the original images and the generated im\u2010 ages, as described by Seitzer18. We perturbed the im\u2010 ages with increasingly more attributes in a cumula\u2010 tive fashion, starting from zero perturbed attributes, which corresponds to only encoding and decoding the image. For the pre\u2010trained model from the orig\u2010 inal authors, we used the provided subset of 250 la\u2010 tent vectors and their corresponding original images that were found in FFHQ. For our models, we used subsets of 100 images (500 images for model 2) due to computational constraints with regard to running the AttFind algorithm. Our results, seen in 3, show that the FID increases with the number of stacked perturbed attributes.\nNone 8.2 (#42) \u2013 Vleuten et al. 2022 8\nThis result is not surprising for three reasons. Firstly, making an image to be classi\u2010 fied as another class might introduce perturbations for a certain class that are out\u2010of\u2010 distribution for that particular class. For example, a man with lipstick does not appear often in the male class. Therefore, counterfactuals are more likely to be out of distribu\u2010 tion to a particular degree. Secondly, a combination of perturbations seems to be more likely to produce an image that ismore out of distribution thanwhen one perturbation is applied individually. For example, a womanwith thicker eyebrows in combination with more facial hair might be more out of distribution than one of those perturbations indi\u2010 vidually. Moreover, we noticed that perturbing several attributes leads to an increasing number of image artefacts, which could be an additional cause for the increasing FID score. This holds both for the original authors\u2019 models and our implementation.\n6 Discussion\nOur experimental results support the claims made in the original paper \u2010 the attributes detected by StylEx are identifiable by humans to a certain degree, distinct and sufficient. However, due to the significantly lower resolution and poorer image quality of the mod\u2010 els, these results are not comparable to the ones displayed in the original paper. Reflection on our reproducibility study An important insight obtained during the con\u2010 duction of the study is that the provided code did not cover the entire scope of the paper. Through a thorough study of both the code aswell as the paper, we quickly noted discrep\u2010 ancies and missing elements that were fundamental \u2010 such as the network architecture, scaling of the losses and the hyperparameter configurations \u2010 to the original research. We believe that researchers could enhance transparency and reproducibility inmachine learning research by the addition of a reproducibility statement within their research, including the used hardware, releasing written software and adding details relevant to the paper (e.g. such as clarifications on the exact network architecture). Moreover, it is important to detail hyperparameter search spaces and final parameter settings for all the used architectures and baselines. We believe that transparency is fundamental to stimulating the large\u2010scale deployment of machine learning algorithms.\n6.1 What was easy It was relatively easy to run the code as the provided Jupyter Notebook by the authors. The provided notebook was thoroughly documented and written in a consistent coding style, making the interpretation of the notebook easier. However, the provided note\u2010 book lacked the elements to fully reproduce the research; the training procedure of the network was missing, only one pre\u2010trained model was provided and four datasets were missing that we were required to add. As such, we had to implement the framework in PyTorch, while porting the limited released code from TensorFlow. Adding a dataset not used in the original notebook to accommodate the experiments was a relatively easy task.\n6.2 What was difficult Given the limited computational resources that were available to us, reproducing the ex\u2010 periments at the same computational scale as the authors were deemed to be the largest challenge. For the training of the model, the original authors made use of 8 NVIDIA V100s, which took the original authors a week to train at the full resolution of 2562px, whereas we were restricted to the use of the computer cluster, Colab/Kaggle, and our lo\u2010 cal GPUs. Due to this limitation, we had to scale down the resolution of the new images across the different datasets significantly. We scaled down the resolution of the gen\u2010 erated images across the different datasets to a resolution of 642px, which reduced the\nNone 8.2 (#42) \u2013 Vleuten et al. 2022 9\nfidelity of the reconstructed images. Additionally, we experienced the following issues with the original paper:\n1. Little to no hyperparameters were given in the paper, e.g. on the scaling of the losses, the learning rates etc;\n2. Ambiguities about the trainingprocedure: the classifier in thenotebookwas trained on CelebA, instead of the FFHQ dataset, which we did not expect. This appeared to be a design choice by the authors, as the CelebA dataset contained labels, which the network could leverage information from. Additionally, softmax logits ap\u2010 peared to be added to the discriminator \u2013 which was not mentioned explicitly in the paper \u2013 but appeared to follow the cGAN [19] training procedure;\n3. Ambiguities on the network architecture: It was not entirely clear what the di\u2010 mensionality and the function were of the z vector, as the paper did not explicitly mention this;\n4. Ambiguities about the preprocessing pipeline of the images before it enters the encoder/classifier \u2010 in contact with the authors, they appeared to scale the RGB values from [\u22121, 1].\nThe original authors did provide the hyperparameter configurations early on, which slightly reduced the time to explore the different possibilities, but the provided learning rate for example was too high for us. Additionally, the conversion of the AttFind algo\u2010 rithm from TensorFlow to PyTorch also proved to be a somewhat difficult exercise. The challenge predominantly concerned the integration of this algorithm within the new PyTorch codebase, which required a thorough understanding of the internal workings of the algorithm.\n6.3 Communication with original authors Three emails were sent to the first author of the paper. In these emails, we have asked for additional details on the proposed network architecture, hyperparameter configura\u2010 tions and the training procedure of the networks. These details were not noted in the paper, nor in the provided code. Answers to these questions were provided promptly. Unfortunately, they were not able to share their code for the training procedure, as it contained too many internal dependencies from their perspective.\nNone 8.2 (#42) \u2013 Vleuten et al. 2022 10"}, {"heading": "Appendix", "text": "7 User Study\n7.1 Classification Study The participants were provided with the following instructions for the classification study:\n\u2022 Look at the animations on the left. Both are examples of the same transformation (change in the image).\n\u2022 Then look at the two candidates on the right, A (top\u2010right) and B (bottom\u2010right).\n\u2022 Choose which one does a similar transformation to those on the left."}, {"heading": "Correct answer: B", "text": "Accuracy: 20/54 participants were correct.\n7.2 Verbal Description Study Theparticipantswere providedwith the following instructions for the verbal description study:\n\u2022 Look at the animation.\n\u2022 Describe in 1\u20104 words the single most prominent attribute that changes for all im\u2010 ages.\nUsers description: lighting, colour/color, brightness, changes Most common word: lighting\nNone 8.2 (#42) \u2013 Vleuten et al. 2022 12\n8 Top attributes\n8.1 FFHQ - Model 1\nNone 8.2 (#42) \u2013 Vleuten et al. 2022 13\n8.2 Plant Village\nNone 8.2 (#42) \u2013 Vleuten et al. 2022 14\n8.3 FFHQ - Model 2\nNone 8.2 (#42) \u2013 Vleuten et al. 2022 15"}], "title": "[Re] Explaining in Style: Training a GAN to explain a classifier in StyleSpace", "year": 2022}