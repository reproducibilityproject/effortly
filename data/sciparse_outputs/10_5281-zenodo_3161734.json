{"abstractText": "The paper1 proposes an improvement over the Variational Auto-Encoder (VAE) architecture2,3 by explicitly modelling sparsity in the latent space with a Spike and Slab prior distribution and drawing ideas from sparse coding theory. The main motivation behind their work lies in the ability to infer truly sparse representations from generally intractable non-linear probabilistic models, simultaneously addressing the problem of lack of interpretability of latent features. Moreover, the proposed model improves the classification accuracy using the low-dimensional representations obtained, and significantly adds robustness while varying the dimensionality of latent space.", "authors": [{"affiliations": [], "name": "Alfredo De la Fuente"}, {"affiliations": [], "name": "Robert Aduviri"}, {"affiliations": [], "name": "Koustuv Sinha"}], "id": "SP:47448c57d91983b4c287161b0c88dff6ad3a62db", "references": [{"authors": ["F. Tonolini", "B.S. Jensen", "R. Murray-Smith"], "title": "Variational Sparse Coding", "year": 2019}, {"authors": ["D.J. Rezende", "S. Mohamed", "D. Wierstra"], "title": "Stochastic Backpropagation and Approximate Inference in Deep Generative Models.", "venue": "(Jan", "year": 2014}, {"authors": ["D.P. Kingma", "M. Welling"], "title": "Auto-Encoding Variational Bayes.", "venue": "(Dec", "year": 2013}, {"authors": ["C. Doersch"], "title": "Tutorial on Variational Autoencoders.", "year": 2016}, {"authors": ["E. Nalisnick", "P. Smyth"], "title": "Stick-Breaking Variational Autoencoders.", "year": 2016}, {"authors": ["J.T. Rolfe"], "title": "Discrete variational autoencoders.", "venue": "arXiv preprint arXiv:1609.02200", "year": 2016}, {"authors": ["F.P. Casale", "A. Dalca", "L. Saglietti", "J. Listgarten", "N. Fusi"], "title": "Gaussian Process Prior Variational Autoencoders.", "venue": "Advances in Neural Information Processing Systems", "year": 2018}, {"authors": ["X. Chen", "D.P. Kingma", "T. Salimans", "Y. Duan", "P. Dhariwal", "J. Schulman", "I. Sutskever", "P. Abbeel"], "title": "Variational lossy autoencoder.", "year": 2016}, {"authors": ["J. Walker", "C. Doersch", "A. Gupta", "M. Hebert"], "title": "An uncertain future: Forecasting from static images using variational autoencoders.", "venue": "European Conference on Computer Vision. Springer", "year": 2016}, {"authors": ["M.J. Kusner", "B. Paige", "J.M. Hern\u00e1ndez-Lobato"], "title": "Grammar variational autoencoder.", "venue": "arXiv preprint arXiv:1703.01925", "year": 2017}, {"authors": ["W. Jin", "R. Barzilay", "T. Jaakkola"], "title": "Junction Tree Variational Autoencoder for Molecular Graph Generation.", "year": 2018}, {"authors": ["C. Louizos", "M. Welling", "D.P. Kingma"], "title": "Learning Sparse Neural Networks through L_0 Regularization.", "year": 2017}, {"authors": ["T. Salimans"], "title": "A Structured Variational Auto-encoder for Learning Deep Hierarchies of Sparse Features.", "year": 2016}, {"authors": ["A. van den Oord", "O. Vinyals"], "title": "Neural discrete representation learning.", "venue": "Advances in Neural Information Processing Systems", "year": 2017}, {"authors": ["M. Chalk", "O. Marre", "G. Tkacik"], "title": "Relevant sparse codes with variational information bottleneck.", "year": 2016}, {"authors": ["I.J. Goodfellow", "A. Courville", "Y. Bengio"], "title": "Large-Scale Feature LearningWith Spike-and-Slab Sparse Coding.", "venue": "Arxiv 1 (June", "year": 2012}, {"authors": ["H. Ishwaran", "J.S. Rao"], "title": "Spike and slab variable selection: frequentist and Bayesian strategies.", "venue": "The Annals of Statistics", "year": 2005}, {"authors": ["M.K. Titsias", "M. L\u00e1zaro-Gredilla"], "title": "Spike and slab variational inference for multi-task and multiple kernel learning.", "venue": "Advances in neural information processing systems", "year": 2011}, {"authors": ["Y. Bengio", "A. Courville", "P. Vincent"], "title": "Representation learning: A review and new perspectives.", "venue": "IEEE transactions on pattern analysis and machine intelligence", "year": 2013}, {"authors": ["S. Yeung", "A. Kannan", "Y. Dauphin", "L. Fei-Fei"], "title": "Epitomic Variational Autoencoders.", "year": 2016}, {"authors": ["I. Higgins", "L. Matthey", "A. Pal", "C. Burgess", "X. Glorot", "M. Botvinick", "S. Mohamed", "A. Lerchner"], "title": "beta-vae: Learning basic visual concepts with a constrained variational framework.", "year": 2016}, {"authors": ["C.P. Burgess", "I. Higgins", "A. Pal", "L. Matthey", "N. Watters", "G. Desjardins", "A. Lerchner"], "title": "Understanding disentangling in \u03b2-VAE.", "venue": "(Apr", "year": 2018}, {"authors": ["H. Kim", "A. Mnih"], "title": "Disentangling by Factorising.", "venue": "(Feb", "year": 2018}, {"authors": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "title": "Gradient-based learning applied to document recognition.", "venue": "Proceedings of the IEEE", "year": 1998}, {"authors": ["H. Xiao", "K. Rasul", "R. Vollgraf"], "title": "Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms.", "year": 2017}, {"authors": ["Z. Liu", "P. Luo", "X. Wang", "X. Tang"], "title": "Deep learning face attributes in the wild.", "venue": "Proceedings of the IEEE International Conference on Computer Vision", "year": 2015}, {"authors": ["A. Paszke", "S. Gross", "S. Chintala", "G. Chanan", "E. Yang", "Z. DeVito", "Z. Lin", "A. Desmaison", "L. Antiga", "A. Lerer"], "title": "Automatic differentiation in PyTorch.", "venue": "NIPS-W", "year": 2017}, {"authors": ["D.P. Kingma", "T. Salimans", "M. Welling"], "title": "Variational Dropout and the Local Reparameterization Trick.", "venue": "Advances in Neural Information Processing Systems 28", "year": 2015}, {"authors": ["D.P. Kingma", "J. Ba"], "title": "Adam: A method for stochastic optimization.", "year": 2014}, {"authors": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification.", "venue": "CoRR abs/1502.01852", "year": 2015}], "sections": [{"text": "R E S C I E N C E C Replication / Machine Learning\n[Re] Variational Sparse Coding\nAlfredo De la Fuente1,2, ID and Robert Aduviri3, ID 1Skolkovo Institute of Science and Technology, Moscow, Russia \u2013 2National Research University Higher School of Economics, Moscow, Russia \u2013 3Pontifical Catholic University of Peru, Lima, Peru\nEdited by Koustuv Sinha ID\nReviewed by Anonymous reviewers\nReceived 04 May 2019\nPublished 22 May 2019\nDOI 10.5281/zenodo.3161734\n1 Introduction\nThe paper1 proposes an improvement over the Variational Auto-Encoder (VAE) architecture2,3 by explicitly modelling sparsity in the latent space with a Spike and Slab prior distribution and drawing ideas from sparse coding theory. The main motivation behind their work lies in the ability to infer truly sparse representations from generally intractable non-linear probabilistic models, simultaneously addressing the problem of lack of interpretability of latent features. Moreover, the proposed model improves the classification accuracy using the low-dimensional representations obtained, and significantly adds robustness while varying the dimensionality of latent space.\nThe authors of the paper derive an analytic expression for the evidence lower bound (ELBO) of the VAE model by choosing the sparsity-inducing Spike and Slab prior distribution for the latent variables, which is later optimizedusing standard gradientmethods to recover the encoding and decoding mappings. After training on well-known datasets, the Variational Sparse Coding (VSC) model is able to recover sparse, informative and interpretable representations given a fixed number of latent dimensions, which authors claim to be advantageous over standard VAE representations for classification tasks.\nIn this reproducibility report we study in detail the VSC model to implement the architectures described in the paper, run the experiments (detailed in Section 4), provide insights and suggestions for replicating results, and analyze the results obtained in comparison with the ones reported by the authors of the paper (Section 6). Furthermore, we addmodifications to improve themodel performance and propose some possible future work directions (Section 7).\n2 Related Work\nVariational Auto-Encoders have been extensively studied4 andwidelymodified in the recent years in order to encourage certain behavior of the latent space variables5,6,7 or to be further applied for particular tasks8,9,10,11. Regarding the sparsity of the latent space for VAEs, previous work in the literature has focused on either explicitly incorporating a regularization term to benefit sparsity12, or fixing a prior distribution, such as Rectified Gaussians by13, discrete distributions by14, student-t distribution for Variational Information Bottleneck by15 and Stick Breaking Processes by5.\nCopyright \u00a9 2019 A.D.L. Fuente and R. Aduviri, released under a Creative Commons Attribution 4.0 International license. Correspondence should be addressed to Robert Aduviri (robert.aduviri@pucp.edu.pe) The authors have declared that no competing interests exists. Code is available at https://github.com/Alfo5123/Variational-Sparse-Coding \u2013 DOI 10.5281/zenodo.2657330. Open peer review is available at https://github.com/reproducibility-challenge/iclr_2019/pull/146.\nReScience C 5.2 (#2) \u2013 Fuente and Aduviri 2019 1\nNonetheless, previous works have not allowed to explicitly model sparsity by incorporating linear sparse coding to non-linear probabilistic generative models. The paper we aim to reproduce offers a connection between both areas through the Spike-and-Slab distribution, chosen as a prior distribution for the latent variables. Although this distribution has been commonly used for modeling sparsity16, it has rarely been applied to generative models. Moreover, since sparse coding imposes efficient data representations17,18,19, the authors demonstrate qualitatively how the sparse learned representations can capture subjectively understandable sources of variation.\nFollowing the line of latent features interpretability, we can observe that the authors\u02bc idea is closely related to the Epitomic VAE by20, which learns the latent dimensions the recognition function should exploit. Many recent approaches, mostly related to disentangled representations, such as \u03b2-VAE21,22 or Factor-VAE by23, focus on learning interpretable factorized representations of the independent data generative factors via generative models. However, these approaches although explicitly induce interpretation of the latent features, do not directly produce sparse representations in contrast with the VSC model. Hence, the authors\u02bc aim is to develop a model that directly induces sparsity in a continuous latent space, which in addition, results into a higher expectation of interpretability in large latent spaces.\nOurwork, as part of the reproducibility challenge, will contribute in clarifying the implementation details of the VSC model, corroborate the results, and assess a few concerns of the reviewers by adding small modifications to the model and experiments based on the insights obtained during the reproducibility challenge.\n3 Target Questions\nIn order to assess the reproducibility of the paper and validate its conclusions, the main questions we will focus our efforts on answering are:\n\u2022 Can we actually validate the reported results?\n\u2022 Is it possible to interpret the latent features learned by the VSC model?\n\u2022 How the proposed model can be further improved?\n4 Experimental Methodology\nWithin the experiments described in the paper, we focus on replicating the precise settings for:\n\u2022 ELBO evaluation: to observe ELBO drop while optimizing the model loss.\n\u2022 Classification Performance: to use the learned presentations for classification tasks.\n\u2022 Interpretation of sparse codes: to visually inspect the role of learned latent features.\n\u2022 Visualization / Traversal of Latent Space: to qualitatively evaluate the reconstructed images.\nWe found that the paper was well written and moderately amenable for reproduction. Although the authors did notmake the code available, writing the code from scratchwas not as challenging as we anticipated. Thus, in this section, we describe in detail how our implementation of the model was carried out, clarify the adjustments considered and display the results obtained.\nReScience C 5.2 (#2) \u2013 Fuente and Aduviri 2019 2\n4.1 Datasets\nWe test the VSC model in two commonly used image datasets: MNIST24 and FashionMNIST25, both composed of 28\u00d728 grey scale images of handwritten digits and pieces of clothing respectively. Following the paper description, we run most of the experiments with these datasets. In addition to this, CelebA faces26 dataset was used to showcase qualitative results. We include in our repository routines to download and preprocess the datasets."}, {"heading": "Observations \u2014", "text": "\u2022 For the CelebA dataset, we used a subset of 100K samples for training and 20K samples for testing, which were center cropped and downsampled to a size of 32\u00d7 32 using all 3 RGB channels, as described in the paper.\n\u2022 We applied a 0-1 Min-Max scaling to all the datasets. We observed that normalizing vectors to unitary norm, as the paper suggests, produced lower quality image reconstruction.\n4.2 Implementation Details\nWe decided to replicate the architecture described in the paper using PyTorch27. Our repository includes a few instructions on how to install and set up all the required libraries needed for running our implementation. We organize the code on scripts for each model architecture, as well as Jupyter Notebooks for preprocessing, running experiments and visualization.\nIn order to establish a valid benchmark, we implemented the VAE architecture from3, which, in the sameway as the Variational Sparse Coding (VSC)model, was implemented by an encoder and a decoder function, both parametrized as fully connected neural networks. The architecture, implemented from scratch, allows to explicitly define the hyperparameters of the model (e.g., hidden layer dimension, latent space dimension, learning rate, epochs, batch size, etc.) and is highly modular, to encourage future modifications.\nFor the loss function, the authors used a continuous relaxation for the discrete binary component in the reconstruction term of the ELBO and applied the reparametrization trick28 for the Spike and Slab distribution to obtain a differentiable expression which can be optimized. Implementing this code was straightforward, our implementation forces the parameter c to increase by 0.001 per iteration to benefit convergence stability.\nWe stored all the checkpoints for the trained models, for reproducibility purposes, together with the training logs which can be visualized using TensorBoard."}, {"heading": "Observations \u2014", "text": "\u2022 One of the missing details in the paper was the batch size. We assumed it to be 32 samples per batch, due to our memory restrictions.\n\u2022 The original paper suggests using 20, 000 iterations for model training using the ADAM optimizer29 with learning rate ranging between 0.001 and 0.01. In particular, we implemented the VSC model in a way that the number of epochs is one hyperparameter. Thus, we fixed the number of epochs to be equivalent the number of iterations given by the paper; i.e., forMNIST and Fashion-MNISTwe trained the VSC model for 11 epochs with a batch size of 32. We fixed a learning rate of 0.001.\nReScience C 5.2 (#2) \u2013 Fuente and Aduviri 2019 3\n\u2022 A minor downside of the paper is that the weights initialization method was not specified. We initialized the weights with uniform random variables using the Kaiming initializationmethod30 for all the layers, which is also the default method for linear layers in PyTorch.\n\u2022 For the recognition function, in order to avoid numerical instability, we suggest to either use clamp or Sigmoid activation function to avoid spike values of zero (thus ensuring \u03b3i < 1).\n4.3 Reproducibility cost Given that there was no mention in the paper about computational resources required, we describe the computational cost required for running the experiments. SinceMNIST and Fashion-MNIST are relatively small datasets, the training procedure run on CPU took around 1 minute per epoch and 8 seconds per epoch on a Titan Xp GPU, using a latent size of 200 units and a single hidden layer of 400 units for both the encoder and decoder, as described in the paper. On the other hand, the VSC model training time for the CelebA dataset was around 30 seconds per epoch on a Titan Xp GPU, using a latent size of 800 units and two hidden layers of 2000 units also for the encoder and decoder. Regarding memory requirements on GPU, the network trained on MNIST and FashionMNIST consumed around 529MB, which scaled up to 850MB during training, while the network trained onCelebA consumed around 637MB,which scaled up to 1322MBduring training. In conclusion, the computational cost for running the experiments is not high, which facilitated the reproduction of the results.\n5 Results\n5.1 ELBO Evaluation We evaluated how the Variational Lower Bound (VLB) varies while changing the latent space dimension of the models: VSC - \u03b1 = 0.2, VSC - \u03b1 = 0.5 and VAE (Figure 1). We observed that in general the ELBO decreases until reaching an optimal latent dimension and then they slowly increase as we add more latent dimensions.\n5.2 Classification Performance We studied the classification accuracy obtained by using the latent representations as input (Figure 2), in order to validate the authors\u02bc conclusion that the Variational Sparse\nReScience C 5.2 (#2) \u2013 Fuente and Aduviri 2019 4\nCoding learned representations improve the classification results and significantly increase robustness with respect to the latent number of dimensions. We can observe that for both MNIST and Fashion-MNIST, after surpassing the optimal latent dimension of the VAE, the representations learned by the VSC are significantly more informative as the latent dimension increases.\n5.3 Intepretation of sparse codes Authors claim the interpretability of the sparse learned representations. We qualitatively examined the interpretation of the non-zero elements in the sparse codes recovered with the VSC model by running interpolations in the sparse space varying the dimension with the highest absolute value (Figure 3). In addition, we observe the effect of the latent dimensionality at capturing the image content (Figure 4).\n5.4 Visualization / Traversing Latent Space We explored how sampling from the latent space distribution can allow us to obtain interpretable variations in the generated images (Figure 5), and also how conditional sampling produces arguably realistic new samples from the same conceptual entity (Figure 6). The traversal of the latent space is performed varying the latent codes with a high absolute value for a given image, one at a time. We can observe that these latent codes indeed represent interpretable features of the datasets, such as the digits shape in MNIST,\nReScience C 5.2 (#2) \u2013 Fuente and Aduviri 2019 5\nthe color and shape of the clothes in Fashion-MNIST and the orientation, background color, skin color and hair color in the CelebA results.\n6 Analysis and discussion of findings\nWe should note that the paper we reproduced was clearly written and overall, despite a few implementation details unspecified (batch size, number of epochs for the CelebA training and weights initialization procedure), it is possible to replicate the results reported. Moreover, we are able to validate the authors hypothesis that the VSC models generate sparse, informative and interpretable representations.\nMinor discrepancies in the results can be explained by the restricted number of training epochs. In particular, in Figure 1, we observe a similar trend that the one shown in the paper; however, the gap between the curves can be decreased by training the model for larger number of iterations. Furthermore, in the log optimization files, we can easily notice that by using only 20K iterations the model has not reached a local optimum yet. This situation is also present during training on the CelebA dataset: we noticed that at least 50 epochs are needed to be close to a local optimum, as it is shown in Figure 7. We believe that authors must provide more specifications on the optimization hyperparameters to facilitate the reproducibility task. Similarly, for the classification task (Figure 2) although we were able to obtain more informative representations using the VSC model with respect to VAE, the classification accuracy can still be improved by using a higher capacity classification model, instead of the simple classifier used in the original paper.\nMany of the reviewers addressed the issue of how we can interpret the learned latent features by the VSC-model. Although authors affirm that the model does not explicitly induce intepretation, it certainly results into a higher expectation of interpretability in large latent spaces, provided that the sources of variations in the observed data can be considered sparse.\nTo account for the blurriness of generated images (Figures 3 and 5), we must under-\nReScience C 5.2 (#2) \u2013 Fuente and Aduviri 2019 6\nstand that the model capacity is limited and perhaps convolutional architecture could drastically improve the quality of results.\n7 Conclusions\nOverall the paper describes in enough detail the VSC model implementation and we must applaud the authors in their ability to convey such a complex topic in an approach-\nReScience C 5.2 (#2) \u2013 Fuente and Aduviri 2019 7\nable and replicable manner. The hypothesis of using Variational Sparse Coding to obtain sparse, informative and interpretable representations was confirmed by reproducing the experiments.\nFurther development on testing themodel and comparing it against other sparsemodels on well known benchmarks is critical. In order to assess how interpretable the learned latent features are, we could draw ideas from disentangled representations, to measure the effect of sparsity in the disentanglement metric, against benchmark models such as \u03b2-VAE or Factor-VAE.\nTo conclude our reproducibility report we propose the following directions for future research and improvement of current results:\n\u2022 Increase the number of iterations suggested for training themodels and re-run the experiments using the notebooks in our repository.\n\u2022 In order to improve the quality of generated images, we suggest to expand the model capacity by either stacking more layers or trying convolutional architectures for the encoder and decoder.\n\u2022 Apply the model to the Disentanglement testing Sprites dataset, to measure the effect of sparsity on a disentanglement metric.\nAcknowledgments \u2014We would like to thank to Emilien Dupont for clarifying distinct aspects on variational autoencoders\u02bc implementation, and Lukas Mosser for his suggestions on training generative models."}], "title": "[Re] Variational Sparse Coding", "year": 2019}