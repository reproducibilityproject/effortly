{"abstractText": "The main goal of the paper \u201dValue Alignment Verification\u201d [1] is to test the alignment of a robot\u2019s behavior efficiently with human expectations by constructing a minimal set of questions. To accomplish this, the authors propose algorithms and heuristics to create the above questionnaire. They choose a wide range of gridworld environments and a continuous autonomous driving domain to validate their put forth claims. We explore value alignment verification for gridworlds incorporating a non\u2010linear feature reward mapping as well as an extended action space.", "authors": [{"affiliations": [], "name": "Siba Smarak Panigrahi"}, {"affiliations": [], "name": "Sohan Patnaik"}, {"affiliations": [], "name": "Koustuv Sinha"}, {"affiliations": [], "name": "Sharath Chandra Raparthy"}], "id": "SP:21b64e13de4c1fae815ee5e076dde9e281c1b880", "references": [{"authors": ["D.S. Brown", "J. Schneider", "A. Dragan", "S. Niekum"], "title": "Value Alignment Verification.", "venue": "In: International Conference on Machine Learning. PMLR", "year": 2021}, {"authors": ["S.H. Huang", "K. Bhatia", "P. Abbeel", "A.D. Dragan"], "title": "Establishing appropriate trust via critical states.", "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)", "year": 2018}, {"authors": ["D. Hadfield-Menell", "S.J. Russell", "P. Abbeel", "A. Dragan"], "title": "Cooperative inverse reinforcement learning.", "venue": "Advances in neural information processing systems", "year": 2016}, {"authors": ["P. Christiano", "J. Leike", "T.B. Brown", "M.Martic", "S. Legg", "D. Amodei"], "title": "Deep reinforcement learning from human preferences.", "year": 2017}, {"authors": ["D. Sadigh", "A.D. Dragan", "S. Sastry", "S.A. Seshia"], "title": "Active preference-based learning of reward functions.", "year": 2017}, {"authors": ["K. Amin", "N. Jiang", "S. Singh"], "title": "Repeated inverse reinforcement learning.", "year": 2017}, {"authors": ["B.D. Ziebart", "A.L. Maas", "J.A. Bagnell", "A.K. Dey"], "title": "Maximum entropy inverse reinforcement learning.", "year": 2008}, {"authors": ["A. Barreto", "W. Dabney", "R. Munos", "J.J. Hunt", "T. Schaul", "H. Van Hasselt", "D. Silver"], "title": "Successor features for transfer in reinforcement learning.", "year": 2016}, {"authors": ["D. Brown", "R. Coleman", "R. Srinivasan", "S. Niekum"], "title": "Safe imitation learning via fast bayesian reward inference from preferences.", "venue": "In: International Conference on Machine Learning. PMLR", "year": 2020}, {"authors": ["P. Abbeel", "A.Y. Ng"], "title": "Apprenticeship learning via inverse reinforcement learning.", "venue": "Proceedings of the twenty-first international conference on Machine learning", "year": 2004}, {"authors": ["S. Russell", "P. Norvig"], "title": "Artificial intelligence: a modern approach.", "year": 2002}, {"authors": ["A.Y. Ng", "S.J. Russell"], "title": "Algorithms for inverse reinforcement learning.", "venue": "In: Icml. Vol", "year": 2000}, {"authors": ["D.S. Brown", "S. Niekum"], "title": "Machine teaching for inverse reinforcement learning: Algorithms and applications.", "venue": "Proceedings of the AAAI Conference on Artificial Intelligence", "year": 2019}], "sections": [{"text": "R E S C I E N C E C"}, {"heading": "Replication / ML Reproducibility Challenge 2021", "text": "[Re] Value Alignment Verification\nSiba Smarak Panigrahi1,2, ID and Sohan Patnaik1,2, ID 1Equal Contributions \u2013 2Indian Institute of Technology Kharagpur, India\nEdited by Koustuv Sinha,\nSharath Chandra Raparthy\nReviewed by Anonymous Reviewers\nReceived 04 February 2022\nPublished 23 May 2022\nDOI 10.5281/zenodo.6574687"}, {"heading": "Reproducibility Summary", "text": ""}, {"heading": "Scope of Reproducibility", "text": "The main goal of the paper \u201dValue Alignment Verification\u201d [1] is to test the alignment of a robot\u2019s behavior efficiently with human expectations by constructing a minimal set of questions. To accomplish this, the authors propose algorithms and heuristics to create the above questionnaire. They choose a wide range of gridworld environments and a continuous autonomous driving domain to validate their put forth claims. We explore value alignment verification for gridworlds incorporating a non\u2010linear feature reward mapping as well as an extended action space."}, {"heading": "Methodology", "text": "We re\u2010implemented the pipeline with Python using mathematical libraries such as numpy and scipy. We spent approximately twomonths reproducing the targeted claims in the paper with the first month aimed at reproducing the results for algorithms and heuristics for exact value alignment verification. The second month focused on extend\u2010 ing the action space, additional experiments, and refining the structure of our code. Since our experiments were not computationally expensive, we carried out the experi\u2010 ments on CPU. The code is available at https://github.com/AIExL/vav_rc2021."}, {"heading": "Results", "text": "The techniques proposed by authors in [1] can successfully address the value alignment verification problem in different settings. We empirically demonstrate the effectiveness of their proposals by performing exhaustive experiments with several variations to their original claims. We showhigh accuracy and low false positive and false negative rates in the value alignment verification task with aminimumnumber of questions for different algorithms and heuristics."}, {"heading": "What was easy", "text": "The problem statement, as well as the implementation of algorithms and heuristics, were straightforward. We also took aid from the original repository published with the paper. However, we implemented the entire pipeline from scratch and incorporated several variations to our code to perform additional designed experiments.\nCopyright \u00a9 2022 S.S. Panigrahi and S. Patnaik, released under a Creative Commons Attribution 4.0 International license. Correspondence should be addressed to Siba Smarak Panigrahi (sibasmarak.p@gmail.com) The authors have declared that no competing interests exist. Code is available at https://github.com/AIExL/vav_rc2021. \u2013 SWH swh:1:dir:4d43ea96458cc573dd2b57208fae0b12f8da896f. Open peer review is available at https://openreview.net/forum?id=BFLM3nMmhCt.\nReScience C 8.2 (#31) \u2013 Panigrahi and Patnaik 2022 1"}, {"heading": "What was difficult", "text": "Comprehending different algorithms and heuristics proposed in prior works along with their mathematical formulation and reasoning for their success in the given task was considerably difficult. Additionally, the original code base had several redundant files, which created initial confusion. We iterated and discussed the arguments in the paper and prior work several times to thoroughly understand the pipeline. Nevertheless, once the basics were clear, the implementation was comparatively simple.\nCommunication with original authors We reached out to the authors numerous times via email to seek clarifications and addi\u2010 tional implementation details. The authors were incredibly receptive to our inquiries, and we appreciate their thorough and prompt responses.\nReScience C 8.2 (#31) \u2013 Panigrahi and Patnaik 2022 2\n1 Introduction\nAutonomous agents are used for complex, challenging, riskier, and dangerous tasks which brings up the need of verifyingwhether the agents act in away that is both optimal and safe w.r.t another agent that has already been performing the said task (example, a human agent). This problem of verifying the alignment of one agent\u2019s behavior w.r.t an\u2010 other agent is known as Value Alignment Verification. The original paper [1] proposes a framework for efficient value alignment verification. They discuss three different set\u2010 tings of increasing difficulty in terms of verification:\n1. explicit human, explicit robot: where both the agents are completely aware of their reward functions.\n2. explicit human, implicit robot: where the human agent is aware of its reward func\u2010 tion but the robot agent can only be queried about its action preferences on differ\u2010 ent states.\n3. implicit human, implicit robot: where the only basis of value alignment is through preferences over trajectories.\nDepending on the setting, value alignment can be either exact or approximate. We try to reproduce and validate the results for the proposed framework on the first and second setting, i.e., (explicit human, explicit robot) and (explicit human, implicit robot). The exper\u2010 iments involve gridworld environments with a deterministic action space. The aim of value alignment verification is to create a questionnaire using the human agent\u2019s knowl\u2010 edge (reward function or trajectory preferences) that can be given to any agent in order to verify alignment. Efficient verification aims to minimize the number of queries in the questionnaire. While few works on value alignment discuss qualitative evaluation of trust [2] or asymptotic alignment of an agent\u2019s performance via interactions and ac\u2010 tive learning [3] [4] [5], [1] solely focuses on verifying value alignment for two or more agents with a learned policy. The objective is to efficiently test compatibility of different robots with human agents. In the following sections, we reiterate the formal definition of value alignment as stated by the original authors (Value Alignment Verification in Sec\u2010 tion 3 and Exact Value Alignment Verification in Section 4), followed by our experiment settings in Section 7 and subsequent observations in Section 8.\n2 Notation\nWe use the notation proposed in [6], where a Markov Decision Process (MDP) M is de\u2010 fined by an environmentE and a reward functionR. An environmentE = (S,A, P, S0, \u03b3) where S is a set of states,A is a set of actions, P is a transition function, P : S\u00d7A\u00d7S \u2192 [0, 1], \u03b3 \u2208 [0, 1) is discount factor and a distribution over initial states S0. The reward function R : S \u2192 R. A policy \u03c0 : S \u00d7 A \u2192 [0, 1] from states to a distribution over ac\u2010 tions. The state and state\u2010action values of a policy \u03c0 are V \u03c0R (s) = E\u03c0[ \u2211\u221e t=0 \u03b3\ntR(st)|s0 = s] and Q\u03c0R(s, a) = E\u03c0[ \u2211\u221e t=0 \u03b3\ntR(st)|s0 = s, a0 = a] for s \u2208 S and a \u2208 A. The op\u2010 timal value functions are, V \u2217R(s) = max\u03c0 V \u03c0R (s) and Q\u2217R(s, a) = max\u03c0 Q\u03c0R(s, a). Let AR(s) = argmaxa\u2032\u2208AQ \u2217 R(s, a\n\u2032) denote the set of optimal actions at a state s under the reward function R. Then AR(s) = {a \u2208 A|\u03c0\u2217R(a|s) > 0}. It is assumed that reward func\u2010 tion is linear under state features ([7], [8], [9]) \u03d5 : S \u2192 Rk, such that R(s) = wT\u03d5(s), where w \u2208 Rk. Note that there is no restriction on the features \u03d5, therefore these features could be complex non\u2010linear functions of the state as well. The state\u2010action value function can be written in terms of features ([10]) as Q\u03c0R(s, a) = wT\u03a6 (s,a) \u03c0 where \u03a6 (s,a) \u03c0 = E\u03c0[ \u2211\u221e t=0 \u03b3 t\u03d5(st)|s0 = s, a0 = a].\nReScience C 8.2 (#31) \u2013 Panigrahi and Patnaik 2022 3\n3 Value Alignment Verification\nConsider two agents (for instance, a human and a robot) where the first agent\u2019s (human) reward function provides the ground truth for the value alignment verification of the second agent (robot). The definition is as follows:\nDefinition 1 Given reward function R, a policy \u03c0\u2032 is \u03f5-value aligned in environment E if and only if\nV \u2217R(s)\u2212 V \u03c0 \u2032 R (s) \u2264 \u03f5, \u2200s \u2208 S (1)\nThe aim of the study [1] is efficient value alignment verification which, formally, is a solution for the following:\nmin T\u2286T\n|T |, s.t. \u2200\u03c0 \u2032 \u2208 \u03a0, \u2200s \u2208 S\nV \u2217R(s)\u2212 V \u03c0 \u2032 R (s) > \u03f5 \u21d2 Pr[\u03c0 \u2032 passes test T ] \u2264 \u03b4fpr\nV \u2217R(s)\u2212 V \u03c0 \u2032 R (s) \u2264 \u03f5 \u21d2 Pr[\u03c0\u2032 fails test T ] \u2264 \u03b4fnr\n(2)\nwhere T is the set of all possible queries, \u03a0 is set of all policies for which the test is designed, \u03b4fnr, \u03b4fpr \u2208 [0, 1] are the false negative and false positive rates, and |T | is the size of test T . When \u03f5 = \u03b4fpr = 0, the authors call this setting exact value alignment verification.\n4 Exact Value Alignment Verification\nExact value alignment verification is not possible, even for finite MDPs, when we can only query the robot agent for its action preferences. Therefore, it is possible only in the most idealized setting, i.e., explicit human, explicit robot.\nDefinition 2 Define an agent \u03c0\u2032 to be rational ([11]) if:\n\u2200a \u2208 A, \u03c0\u2032(a|s) > 0 \u21d2 a \u2208 argmax a Q\u2217R\u2032(s, a) (3)\nwhere argmaxaQ \u2217 R\u2032(s, a) is the optimal state-action value function for the reward functionR \u2032.\nAs there exist infinitely many reward functions which can return the same optimal pol\u2010 icy ([12]), determining that \u2203s \u2208 S,R(s) \u0338= R\u2032(s) does not necessarily imply that agents with the reward functions R,R\u2032 are not aligned. We provide an example of this in Fig\u2010 ure 1, where the optimal policy for human and robot is the same; thus, they are aligned. However, the rewards are different, as mentioned in Table 1.\nDefinition 3 Define the set of all the optimal policies under the reward function R as OPT(R).\nOPT (R) = {\u03c0|\u03c0(a|s) > 0 \u21d2 a \u2208 argmax a Q\u2217R(s, a)}\nReScience C 8.2 (#31) \u2013 Panigrahi and Patnaik 2022 4\nLooking at Definition 1 and Equation 3 simultaneously makes it evident that for a ra\u2010 tional robot, if all of its optimal policies are also optimal under ground truth reward function R; the robot is exactly aligned with the human.\nCorollary 1 We have exact value alignment in environment E between a rational robot with reward functionR \u2032 and a human with reward functionR ifOPT (R \u2032 ) \u2286 OPT (R).\nRevisiting the inspiration ([12]) of the original author\u2019s proposed approach for efficient exact value alignment \u2010\nDefinition 4 Given an environmentE, the consistent reward set (CRS) of a policy \u03c0 in environment E is defined as the set of reward functions under which \u03c0 is optimal\nCRS(\u03c0) = {R|\u03c0 \u2208 OPT (R)} (4)\nWhen R(s) = wT\u03d5, the CRS is of the form ([12], [13]):\nCorollary 2 Given an environment E, the CRS(\u03c0) is given by the following intersection of half-spaces:\n{w \u2208 Rk|wT (\u03a6(s,a)\u03c0 \u2212 \u03a6(s,b)\u03c0 ) \u2265 0, \u2200a \u2208 argmax a\u2032\u2208A Q\u03c0R(s, a \u2032), b \u2208 A, s \u2208 S}\nSince the boundaries of the CRS polytope is consistent with a policy that may not be aligned with optimal policy (e.g. zero reward), we remove all such boundary cases to obtain a modified set called aligned reward polytope (ARP).\n5 Reproducing Exact Value Alignment\nIn this section, we explain the procedure in order to verify the claims made in the pa\u2010 per regarding sufficient conditions for provable verification of exact value alignment (explained in Section 4). We verify exact value alignment in disparate settings proposed by the authors for explicit human - explicit robot setting. If we have access to the value or reward function of a human, we term it as explicit human. A similar notion is applicable for the robot as well.\nTheorem 1 Under the assumption of a rational robot (defined in Section 4) that shares linear reward features with the human, efficient exact value alignment verification is possible in the following query settings: (1) Query access to reward function weights w \u2032 , (2) Query access to samples of the reward functionR \u2032 (s), (3) Query access to V \u2217\nR\u2032 (s) andQ\u2217 R\u2032 (s, a), and (4) Query\naccess to preferences over trajectories.\nCase 1 Reward Weight Queries\nA brute\u2010force paradigm can be implemented to evaluate an explicit robot optimal policy under the human reward function. However, there exists another succinct verification test. We need to query the weight vectorw \u2032 of the robot (here,R \u2032 (s) = (w \u2032 )T\u03d5(s), \u03d5(s) is the feature vector of state s). The paper asserts that it is possible to form a test (defined later as \u2206) that uses the obtained w \u2032 to verify alignment. Additionally, this query to the weight vector w \u2032 is done in constant time, and the test is linear in the number of questions.\nReScience C 8.2 (#31) \u2013 Panigrahi and Patnaik 2022 5\nDefinition 5 Given anMDPMcomposed of environmentE and reward functionR, the aligned reward set (ARS) is defined as the following set of reward functions:\nARS(R) = {R \u2032 |OPT (R \u2032 ) \u2286 OPT (R)}\nWe state the lemmawhich proves the sufficient condition for exact value alignment and direct the interested readers for the proof of the lemma to refer the paper.\nLemma 1 Given an MDP M = (E, R), the human\u2019s and robot\u2019s reward function R and R\u2032 respectively can be represented as linear combinations of features \u03d5(s) \u2208 Rk, i.e., R(s) = wT\u03d5(s), R \u2032 (s) = (w\n\u2032 )T\u03d5(s), and given an optimal policy \u03c0\u2217R under R, we have\nw \u2032 \u2208 \u2229(s,a,b)\u2208OHRs,a,b \u21d2 R \u2032 \u2208 ARS(R)\nwhere\nHRs,a,b = {w|wT (\u03a6(s,a)\u03c0 )\u2212 \u03a6(s,b)\u03c0 ) > 0} andO = {(s, a, b)|s \u2208 S, a \u2208 AR(s), b \u0338= AR(s)}\nDefinition 6 The intersection of half-spaces ( \u2229(s,a,b)\u2208O HRs,a,b ) is defined as the Aligned Reward Polytope (ARP). The design of ARP in the form of\u2206matrix is defined as follows:\n\u2206 =\n[ \u03a6 (s,a) \u03c0 )\u2212 \u03a6(s,b)\u03c0\n...\n]\nIn the above equation, a is an optimal action at state s, and b is a non\u2010optimal action. The actions in the trajectory following a and b are optimal. Each row of\u2206 represents the nor\u2010 mal vector for a strict half\u2010space constraint based on feature count differences between an optimal and sub\u2010optimal action. Therefore, for a robot weight vector w \u2032 , if\u2206w \u2032 > 0, the robot is aligned. We follow the stepsmentioned in the original paper to include only non\u2010redundant half\u2010space normal vectors in \u2206. We enumerate all possible half\u2010space normal vectors corresponding to each state s, optimal action a, and non\u2010optimal action b. We accumulate only non\u2010redundant half\u2010space normal vectors:\n1. Removal of Duplicate Vectors: To remove duplicate vectors, we compute the cosine distance between the half\u2010space normal vectors. One vector in each of the pairs of vectors with cosine distance within a small precision value (we select 0.0001) is retained in\u2206, others being discarded. All zero vectors are also removed.\n2. Removal of Redundant Vectors: According to the paper, the set of redundant vectors can be found efficiently using the Linear Programming approach. To check if a constraint aTx \u2264 b is necessary, wefirst remove that constraint and solve the linear programming problem. If the optimal solution is still constrained to be less than or equal to b, that constraint can be safely discarded. After removing all such redundant vectors, we get only a set of non\u2010redundant half\u2010space normal vectors.\nCase 2 Reward Queries\nIn this case, the tester seeks for the rewards of the robot. Here, a tester is same as a user (human) who wishes to verify the alignment of a robot. Since it is assumed that both human and robot have access to their state feature vectors, and from the equation R(s) = wT\u03d5(s), we obtain theweight vector for the robot, and this case reduces to Case 1. Let\u03a6M be defined as thematrix where each row corresponds to the feature vector \u03d5(s)T for a distinct state s \u2208 S. In order to solve the system of linear equation for obtaining the weight vector, the number of queries needed is rank(\u03a6M ).\nCase 3 Value Function Queries\nReScience C 8.2 (#31) \u2013 Panigrahi and Patnaik 2022 6\nThe tester seeks the action value function and the value function for each state in this case setting. Subsequently, the reward weights for the robot are obtained with the aid of the following equations:\nR \u2032 (s) = (w \u2032 )Tx and R \u2032 (s) = Q\u2217\nR\u2032 (s, a)\u2212 \u03b3Es\u2032 [V \u2217 R\u2032 (s \u2032 )]\nThis case also boils down to Case 1 as we obtain the weight vector for the robot. Ac\u2010 cording to the paper, if we define the maximum degree of the MDP transition function as\ndmax = max s\u2208S,a\u2208A\n|{s \u2032 \u2208 S|P (s, a, s \u2032 ) > 0}|,\nthen at most dmax possible next state queries are needed to evaluate the expectation. Therefore, atmost rank(\u03a6M )(dmax+1) queries are required to recover the robot\u2019s weight vector.\nCase 4 Preference Queries\nWe obtain preference over trajectories \u03be as judged by the human. Each preference \u03beA > \u03beB, induces a constraint (w \u2032 )T (\u03a6(\u03beA)\u2212 \u03a6(\u03beB)) > 0, where \u03a6(\u03be) = \u2211n i=1 \u03b3\ni\u03d5(si) is the cumulative discounted reward features (linear combination of state features) along a trajectory. Therefore, we construct \u2206 where each row corresponds to a half\u2010space normal resulting from preference over individual trajectories. In this case, only a log\u2010 arithmic number of trajectories are needed from all possible trajectory space to obtain \u2206matrix and proceed to verify alignment of robot. We obtain all valid trajectories, per\u2010 form preprocessing (remove duplicate & redundant vectors), and observe that the total number of queries is bounded by logarithmic number of trajectories we started with ([14]).\n6 Value Alignment Verification Heuristics\nWhen the robot acts as a black box and can provide state action preferences instead of a policy, the authors propose three heuristics; Critical States, Machine Teaching and ARP Heuristic. Each heuristic consists of a method for selecting the states at which the robot is tested and queries for an action, subsequently checking if the action is optimal under human\u2019s reward function. It is important to note that for these heuristics, \u03b4fpr > 0, as there is no guarantee for the robot to always take the same action at a given state.\n1. Critical States Heuristic: Inspired by the notion of critical states (CS) [2], the heuris\u2010 tic test consists of states for which Q\u2217R(s, \u03c0\u2217R(s)) \u2212 1|A| \u2211 a\u2208A Q \u2217 R(s, a) > t, where\nt is a threshold value. This intuitively states the importance of a particular state and tends to make the verification efficient.\n2. Machine Teaching Heuristic: This heuristic is based on Set Cover Optimal Teach\u2010 ing (SCOT) [13], which approximates the minimal set of state\u2010action trajectories necessary to teach a specific reward function to an IRL agent. [13] show that in the intersection of half\u2010spaces that define the CRS (Corollary 2), the learner recov\u2010 ers a reward function. The authors use SCOT to create informative trajectories and create alignment tests by seeking a robot action at each state along the trajec\u2010 tory. Producing a test with SCOT takes longer than CS heuristic, but unlike CS, SCOT prevents repetitive inquiries by reasoning about reward features over a set of trajectories.\n3. ARP Heuristic: This heuristic is a black\u2010box alignment heuristic (ARP\u2010bb) based on the ARP definition. ARP\u2010bb first computes\u2206, then uses linear programming to remove duplicate half\u2010space constraints, subsequently asks for robot actions from\nReScience C 8.2 (#31) \u2013 Panigrahi and Patnaik 2022 7\nthe states corresponding to the non\u2010redundant constraints (rows) in\u2206. Intuitively, the states probed by ARP\u2010bb are significant because different actions disclose vital information about the reward function. ARP\u2010bb approximates testing each half\u2010 space constraint by using single\u2010state action queries. As a result, ARP\u2010bb trades off increased approximation error in exchange for a lower query and computational complexity.\n7 Experiments\nIn this section, we describe several experiments carried out in order to investigate the following:\n1. Algorithms and Heuristics: Comparison of different algorithms and heuristics in different gridworlds. We tabulate the performance of testers (accuracy, false posi\u2010 tive rate, false negative rate, and the number of queries presented to the robot for verification) w.r.t different gridworld widths ranging from 4 to 8 and feature size from 3 to 8. The dimension of feature for a state is termed as number of features or feature size. Our experiments confine these state features \u03d5 to be one\u2010hot vectors only.\n2. Diagonal Actions: Comparison of algorithms and heuristics in gridworlds with an extended action space. We allow diagonal movement between standard move\u2010 ments. This increases the standard 4 actions (left, up, right, and down) to 8 ac\u2010 tions (left\u2010up\u2010diag, up\u2010right\u2010diag, right\u2010down\u2010diag, and down\u2010left\u2010diag). Here, diag refers to diagonal movement. Again, we tabulate the performance of testers w.r.t different gridworld widths.\n3. Non\u2010linear reward and state\u2010feature relationships: Comparison of different algo\u2010 rithms and heuristics with non\u2010linear (cubic and exponential) reward R and state\u2010 feature \u03d5(s) relationships. In cubic, we approximate the linear behavior when wT\u03d5(s) \u2248 0, else not. The exact relationship we consider is R = x3 + 10x where x = wT\u03d5(s). In exponential, we completely remove the linear relationship between R and \u03d5(s) and considerR = ew\nT\u03d5(s). We tabulate the performance of testers w.r.t different gridworld widths in both cases.\n4. Critical States Tester for different thresholds: Comparison of Critical States Tester performance with different threshold values (0.0001, 0.2 and, 0.8) for a state to be critical.\nSection 8 provides the results for one algorithm (Reward Weight Tester) and one heuris\u2010 tic (Critical State Tester) and plots relevant to their accuracy and number of test queries. We redirect readers to Section 2 of Supplementary Material for the detailed tabulated performance of all algorithms, heuristics, and the plots related to false positive and false negative rates. Also, note that the default gridworld rows are 4, gridworld width is 8, number of actions is 4, feature size is 5, reward and state\u2010feature relationship is linear (R = wT\u03d5(s)), and threshold value of Critical States Tester is 0.2. We created 100 different human agents for each experiment, and for each human agent, we created 100 different robots to check their alignment. Each human agent corre\u2010 sponds to a different human weight vector whose each element is sampled from a nor\u2010 mal distribution with mean 0 and variance 1. Different robot agents correspond to dif\u2010 ferent robot weights that are obtained by adding a random normal noise vector to the corresponding humanweight vector. The elements of the noise vector are sampled from the same normal distribution. Further, we normalize the robot and human weight vec\u2010 tor to have a unit norm. In total, we run 1.32 million experiments to address the points mentioned above.\nReScience C 8.2 (#31) \u2013 Panigrahi and Patnaik 2022 8\n8 Results\nIn the plots and following discussion, rwt indicates Reward Weight Queries Tester, rt indicates Reward Queries Tester, vft indicates Value Function Queries Tester, ptt indi\u2010 cates Preference Trajectory Queries Tester, cst indicates Critical States Tester, scott indicates SCOT Tester, and arpbbt indicates ARP Black Box Tester."}, {"heading": "Tester Width Accuracy False False Number of", "text": "positive rate negative rate queries\n4 0.995\u00b10.013 0.005\u00b10.011 0.001\u00b10.005 1 rwt 6 0.997\u00b10.007 0.002\u00b10.005 0.001\u00b10.005 1\n8 0.999\u00b10.004 0.001\u00b10.004 0.000\u00b10.002 1\n4 0.973\u00b10.043 0.000\u00b10.000 0.027\u00b10.043 13 cst 6 0.987\u00b10.018 0.000\u00b10.000 0.013\u00b10.018 24\n8 0.996\u00b10.007 0.000\u00b10.001 0.004\u00b10.007 29\nAs per Definition 1, we require \u03b4fpr = \u03f5 = 0 for Exact Value Alignment Verification; hence false negatives can be present in the corresponding algorithms. Further, we dis\u2010 cussed with the authors the possibility of false positives in these algorithms, and we concluded that since we do not consider all possible trajectories in a gridworld (which is exponential in the number of actions), false positives can be present. However, we observe that both false positive and false negative rates are negligibly small. These re\u2010\nReScience C 8.2 (#31) \u2013 Panigrahi and Patnaik 2022 9\nsults empirically show that indeed the proposed algorithms and heuristics successfully identify the alignment between human agents and robots.\nIn Figure 2b, the number of queries indicates the size of the questionnaire, i.e., |T |. The total number of queries required to verify the value alignment with cst is higher than other heuristics owing to its simplermechanism for obtaining state queries. We observe that arpbbt is also bounded by the logarithm of the total number of queries, i.e., trajec\u2010 tories of a certain maximum length (this value is set at 10), possible in a gridworld. The number of states to be queried in scott is fixed at the maximum length of a trajectory possible (this value is set at 5 for scott). Also, with the increase in the size of the grid\u2010 world, the number of queries with cst increases. Further, we have not presented the number of queries for rt and vft in plots because they havewell\u2010definedmathematical formulae to calculate |T |.\nReScience C 8.2 (#31) \u2013 Panigrahi and Patnaik 2022 10"}, {"heading": "Tester Width Accuracy False False Number of", "text": "8.4 Critical States Tester with different thresholds Theperformance of cstwith different thresholds (0.0001 and 0.8, 0.2 iscst row inTable 2) is presented in Table 6. The corresponding figures are presented in Section 2 of the Supplementary Material. We observe that the accuracy for low threshold values is high whereas the accuracy drops considerably with higher threshold value. This is due to a decrease in the number of test queries with higher thresholds leading to a decrease in alignment verification ability. The comparison between the number of test queries for different thresholds displays an expected trend, i.e., the number of states to be queried with lower thresholds is higher than those with a higher threshold.\nIn this work, we implemented the algorithms and heuristics for Exact Value Alignment Verification. We observe that all the methods proposed in [1] can identify the alignment between a robot and a human agent with high confidence in two distinct scenarios, implicit and explicit robot with an explicit human agent. In this work, we have not inves\u2010 tigated implicit robot, implicit human (approximate value alignment verification) setting due to lack of time. Additionally, we have carried out ablation studies to study the per\u2010 formance of these proposed methods in different settings, including an extended de\u2010 terministic action space and non\u2010linear reward state\u2010feature relationship. Ultimately, a human agent could use any of the algorithms or heuristic (depending on the ability of the robot to access its rewards) to create a driver\u2019s test to test the robot\u2019s alignment."}, {"heading": "Tester Width Accuracy False False Number of", "text": "exponential completely ignores the linear relationship. We observed that since the accu\u2010 racy is high for cubic and low for exponential relationships, the false positive rates are low and high respectively.\n11.4 Critical States Tester with different thresholds Table 11 details the performance metrics for cst in different gridworld widths and dif\u2010 ferent threshold values (0.0001, 0.2, and 0.8). As noted earlier, we observed that the number of queries decreases with strict threshold values such as 0.8, which results in reduced verification abilities. The corresponding plots for performance metrics are\nReScience C 8.2 (#31) \u2013 Panigrahi and Patnaik 2022 16\npresent in Figure 13.\nReScience C 8.2 (#31) \u2013 Panigrahi and Patnaik 2022 17"}, {"heading": "Tester Width Accuracy False False Number of", "text": "ReScience C 8.2 (#31) \u2013 Panigrahi and Patnaik 2022 19\ntics, the time taken increases 2X when width of a gridworld increases from 4 to 8 while for scott, the time taken increases by at least 3X."}], "title": "[Re] Value Alignment Verification", "year": 2022}