{"abstractText": "The paper\u2019s central claim revolves around the newly introduced Background Aware Pool\u2010 ing (BAP)method to generate high\u2010quality pseudo labels using bounding boxes as super\u2010 vision and Noise Aware Loss (NAL) to train a segmentation network using those noisy labels. The authors assert that these two techniques combined set the new state\u2010of\u2010the\u2010 art for weakly supervised semantic segmentation on PASCAL VOC 2012 [1].", "authors": [{"affiliations": [], "name": "Aryan Mehta"}, {"affiliations": [], "name": "Karan Uppal"}, {"affiliations": [], "name": "Kaushal Jadhav"}, {"affiliations": [], "name": "Monish Natarajan"}, {"affiliations": [], "name": "Mradul Agrawal"}, {"affiliations": [], "name": "Debashish Chakravarty"}, {"affiliations": [], "name": "Koustuv Sinha"}, {"affiliations": [], "name": "Sharath Chandra Raparthy"}], "id": "SP:c8e6e44ccc2f0092fef8d65c03cea0a848d5b03e", "references": [{"authors": ["M. Everingham", "L. Van Gool", "C.K. Williams", "J. Winn", "A. Zisserman"], "title": "The pascal visual object classes (voc) challenge.", "venue": "In: International journal of computer vision", "year": 2010}, {"authors": ["M. Lin", "Q. Chen", "S. Yan"], "title": "Network in network.", "venue": "arXiv preprint arXiv:1312.4400", "year": 2013}, {"authors": ["B. Zhou", "A. Khosla", "A. Lapedriza", "A. Oliva", "A. Torralba"], "title": "Learning deep features for discriminative localization.", "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition", "year": 2016}, {"authors": ["Z. Huang", "X. Wang", "J. Wang", "W. Liu"], "title": "Weakly-supervised semantic segmentation network with deep seeded region growing.", "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition", "year": 2018}, {"authors": ["A. Kolesnikov", "C.H. Lampert"], "title": "Seed, expand and constrain: Three principles for weakly-supervised image segmentation.", "venue": "European conference on computer vision. Springer", "year": 2016}, {"authors": ["F. Saleh", "M.S. Aliakbarian", "M. Salzmann", "L. Petersson", "S. Gould", "J.M. Alvarez"], "title": "Built-in foreground/background prior for weakly-supervised semantic segmentation.", "venue": "European conference on computer vision. Springer", "year": 2016}, {"authors": ["B. Zhang", "J. Xiao", "Y. Wei", "M. Sun", "K. Huang"], "title": "Reliability does matter: An end-to-end weakly supervised semantic segmentation approach.", "venue": "Proceedings of the AAAI Conference on Artificial Intelligence", "year": 2020}, {"authors": ["L.-C. Chen", "G. Papandreou", "I. Kokkinos", "K. Murphy", "A.L. Yuille"], "title": "Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs.\u201d In: IEEE transactions on pattern analysis and machine intelligence", "year": 2017}, {"authors": ["B. Hariharan", "P. Arbel\u00e1ez", "L. Bourdev", "S. Maji", "J. Malik"], "title": "Semantic contours from inverse detectors.", "venue": "In: 2011 international conference on computer vision. IEEE", "year": 2011}, {"authors": ["T.-Y. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C.L. Zitnick"], "title": "Microsoft coco: Common objects in context.", "venue": "European conference on computer vision. Springer", "year": 2014}, {"authors": ["K. Simonyan", "A. Zisserman"], "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition.", "venue": "(Sept", "year": 2014}, {"authors": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "title": "Deep residual learning for image recognition.", "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition", "year": 2016}, {"authors": ["A. Paszke", "S. Gross", "F. Massa", "A. Lerer", "J. Bradbury", "G. Chanan", "T. Killeen", "Z. Lin", "N. Gimelshein", "L. Antiga"], "title": "Pytorch: An imperative style, high-performance deep learning library.", "venue": "Advances in neural information processing systems", "year": 2019}, {"authors": ["L. Biewald"], "title": "Experiment TrackingwithWeights and Biases", "venue": "Software available fromwandb.com", "year": 2020}, {"authors": ["Y. Grandvalet", "Y. Bengio"], "title": "Semi-supervised learning by entropy minimization.", "venue": "Advances in neural information processing systems", "year": 2004}, {"authors": ["S. Reed", "H. Lee", "D. Anguelov", "C. Szegedy", "D. Erhan", "A. Rabinovich"], "title": "Training deep neural networks on noisy labels with bootstrapping.", "venue": "arXiv preprint arXiv:1412.6596", "year": 2014}], "sections": [{"text": "R E S C I E N C E C Replication / ML Reproducibility Challenge 2021\n[Re] Background-Aware Pooling and Noise-Aware Loss for Weakly-Supervised Semantic Segmentation\nAryan Mehta1,2, ID , Karan Uppal1,2, ID , Kaushal Jadhav1,2, ID , Monish Natarajan1,2, ID , Mradul Agrawal1,2, ID , and Debashish Chakravarty1, ID 1IIT Kharagpur, West Bengal, India \u2013 2Equal Contributions\nEdited by Koustuv Sinha,\nSharath Chandra Raparthy\nReviewed by Anonymous Reviewers\nReceived 04 February 2022\nPublished 23 May 2022\nDOI 10.5281/zenodo.6574677"}, {"heading": "Reproducibility Summary", "text": ""}, {"heading": "Scope of Reproducibility", "text": "The paper\u2019s central claim revolves around the newly introduced Background Aware Pool\u2010 ing (BAP)method to generate high\u2010quality pseudo labels using bounding boxes as super\u2010 vision and Noise Aware Loss (NAL) to train a segmentation network using those noisy labels. The authors assert that these two techniques combined set the new state\u2010of\u2010the\u2010 art for weakly supervised semantic segmentation on PASCAL VOC 2012 [1]."}, {"heading": "Methodology", "text": "We started with the publicly available code\u2010base provided by the authors and repro\u2010 duced the results associated with Stages 1 and 2 involving pseudo label generation. Fur\u2010 ther, we implemented NAL for Stage 3 training and used it to train a semantic segmen\u2010 tation network, reproducing its claims. We performed many refactoring and upgrades on the author\u2019s code to include various procedures mentioned in the paper."}, {"heading": "Results", "text": "We reproduced and verified all the central claims made by the authors in the paper, confirming the intuition behind the novel methodologies introduced in the paper. Our results differ using the parameters given in the paper for the segmentation experiments but still support the claim of NAL being superior to its counterpart losses."}, {"heading": "What was easy", "text": "The completed code for training the classification network and pseudo label generation using BAP was available in the authors\u2019 code\u2010base, and the results associated with them were straightforward to reproduce."}, {"heading": "What was difficult", "text": "Implementing someparts of Stage 1 and Stage 2 and the complete Stage 3 code, including NAL and further experimenting with them to resolve the minute issues, was the most\nCopyright \u00a9 2022 A. Mehta et al., released under a Creative Commons Attribution 4.0 International license. Correspondence should be addressed to Karan Uppal (karan.uppal3@iitkgp.ac.in) The authors have declared that no competing interests exist. Code is available at https://github.com/karan-uppal3/BANA. \u2013 SWH swh:1:dir:24495d2fbb5d4af66607261c2171ed42173a72cf. Open peer review is available at https://openreview.net/forum?id=rUQllTGQhAY.\nReScience C 8.2 (#26) \u2013 Mehta et al. 2022 1\nchallenging part of the reproduction. Even though authors gave detailed feedback, VOC\u2010 to\u2010COCO conversion for unseen classes also posed many challenges.\nCommunication with original authors Contact with authors was made via Email regarding specifications in methodologies in\u2010 volving pseudo label generation and VOC\u2010to\u2010COCO experiments. Apart from the code, comprehensive and helpful replies were given by them.\nReScience C 8.2 (#26) \u2013 Mehta et al. 2022 2\n1 Introduction\nSemantic segmentation, which is the pixel\u2010wise classification of objects in images, finds crucial applications in areas such as autonomous driving, medical imaging, and aug\u2010 mented reality, to name a few. Training deep neural networks to perform this task accurately requires extensive and quality training data and annotating it, which is la\u2010 borious and intensive. Weakly\u2010supervised semantic segmentation (WSSS) techniques aim to ease the task of annotation by using image\u2010level labels or object bounding boxes as a weak form of supervisory signal to generate possibly noisy \u201dpseudo\u2010ground\u2010truth labels.\u201d While existing methods come at the expense of additional overheads, WSSS us\u2010 ing background\u2010aware pooling (BAP), introduces a technique to discriminate foreground and background regionswithin bounding boxes to generate quality pseudo labels at neg\u2010 ligible overhead. On the other hand, Noise\u2010Aware Loss (NAL) improves the performance of models by lessening the effect of incorrect pseudo labels during training.\n2 Scope of reproducibility\nThe paper introduces a new weakly supervised semantic segmentation technique using bounding box annotations to generate pseudo labels and train a segmentation network using those labels as supervisors. Here are the major claims, summarized as follows:\n1. High\u2010quality pseudo segmentation labels are generated with the proposed Back\u2010 ground Aware Pooling method using bounding box annotations in comparison to the conventional Global Average Pooling method [2, 3].\n2. The novel Noise Aware Loss can use the unreliable regions present in the noisy pseudo labels.\n3. Fully trained classification and Segmentation networks achieved the current state\u2010 of\u2010the\u2010art performance for weakly\u2010supervised semantic segmentation on PASCAL VOC data\u2010set using the above\u2010presented methods.\n3 Methodology\nThe main experiments of the paper are divided into three stages, as shown below:\n1. Training a classifier network using Background\u2010Aware Pooling (BAP) on the VOC dataset.\n2. Generation and Evaluation of Pseudo labels generated on VOC for a model trained using BAP.\n3. Training and evaluation of a model using Noise\u2010Aware Loss (NAL) on the pseudo labels generated in Stage 2.\nReScience C 8.2 (#26) \u2013 Mehta et al. 2022 3\n3.1 Method Descriptions\nBAP in the training of Classification Network \u2014 The task of discriminating the foreground and background regions within a bounding box is approached as a retrieval task. Firstly, the feature map f obtained from the model is divided into N x N regular grids denoted by G(j). For each G(j), features are aggregated as per Eq. (1) and are used as queries qj for the retrieval of background features within each bounding box. For this purpose, a binary maskM is defined, where for a position p within a bounding boxM(p) = 0, and one otherwise.\nqj =\n\u2211 p\u2208G(j) M(p)f(p)\u2211\np\u2208G(j) M(p) (1)\nFor a given grid cellG(j), the termA(j) is computed as shownbyEq. (2). Upon averaging overallAj(p), attentionmap,A is obtained, corresponding to the likelihood that a given pixel belongs to the background. This is represented by Eq. (2), where J denotes the total number of valid grid cells.\nA(p) = 1\nJ \u2211 j Aj(p), where Aj(p) =\n{ ReLU ( f(p)\n\u2225f(p)\u2225 \u00b7 qj \u2225qj\u2225\n) ,p \u2208 B\n1 ,p /\u2208 B (2)\nFor a given bounding box Bi, foreground features ri are aggregated using the attention map A(p) by means of a weighted average pooling, as per Eq (3). The authors refer to this process as Background\u2010Aware Pooling (BAP). Finally, the (L + 1) \u2010 way softmax classifier w is applied to ri and qj corresponding to the foreground and background features, respectively, to train the model using standard cross\u2010entropy loss.\nri =\n\u2211 p\u2208Bi(1\u2212A(p))f(p)\u2211\np\u2208Bi(1\u2212A(p)) (3)\nGeneration of Pseudo Labels \u2014 Two pseudo ground\u2010truth labels namely Ycrf and Yret are generated from two complementary approaches. The first method involves using the background attention map and class activation maps (CAMs) [3] obtained from the clas\u2010 sification network, and using them as the unary term for DenseCRF [4, 5, 6, 7]. The unary term for the background u0 and unary term for object class c denoted by uc, is computed as shown in Eq. (4) and Eq. (5). The terms u0 and uc for each class c are then concatenated and provided as the unary term for DenseCRF to obtain Ycrf . Here Bc de\u2010 notes the regions within bounding box(es) for class c and wc is the classifier weight for object class c.\nu0(p) = A(p) (4)\nuc(p) =\n{ CAMc(p)\nmaxp(CAMc(p)) ,p \u2208 Bc 0 ,p /\u2208 Bc , where CAMc(p) = ReLU (f(p) \u00b7 wc) . (5)\nGeneration of Yret, on the other hand, involves capturing the high\u2010level features ob\u2010 tained from the classifier. Queries qc corresponding to prototypical features for each class c is computed as per Eq. (6), whereQc is the set of regions in Ycrf labelled as class c. Following this, the correlation map Cc for each class c is shown below.\nqc = 1 |Qc| \u2211 p\u2208Qc f(p), and Cc(p) = f(p) \u2225f(p)\u2225 \u00b7 qc \u2225qc\u2225 . (6)\nReScience C 8.2 (#26) \u2013 Mehta et al. 2022 4\nHowever, the authors have applied the ReLU function over the mentioned cosine simi\u2010 larity in their official implementation. Finally, the argmax function is applied over the correlation map Cc to obtain pseudo labels Yret.\nPseudo\u2010labels for Unseen Classes: \u201dVOC\u2010to\u2010COCO\u201d The authors mention in the paper that their pseudo label generator is generic in that for classes unseen during training, 1\u2212u0 can be used as a class agnostic foreground attentionmap in place of the attention map obtained using the corresponding CAM. We illustrate this in Eq (7).\nuc(p) =  CAMc(p) maxp(CAMc(p)) , c \u2208 C and p \u2208 Bc 1\u2212 u0(p) , c /\u2208 C and p \u2208 Bc\n0 ,p /\u2208 Bc (7)\nWhere C represents the set of classes whose classifier weights are available with the generator, and u0 corresponds to the background attention map attained in Eq. (5).\nNoise-Aware Loss for Semantic Segmentation with Noisy Labels \u2014 The authors use Noise\u2010Aware Loss to train DeepLab [8] models using Ycrf and Yret . Feature map \u03d5 is extracted from the backbone network and probability map Ypred is obtained by passing feature map \u03d5 through the forward classifier. Probability mapH is obtained by passing Ypred through Softmax classifierW . The authors denote the regions where both Ycrf and Yret give the same label as S and where both give different labels as \u223c S. For the confident regions S, ce loss is calculated using Eq. (11).\nLce = \u2212 1\u2211 c |Sc| \u2211 c \u2211 p\u2208Sc logHc(p), (8)\nHereHc is a probability for the class c and Sc is the set of locations labeled as the class c in S. The unreliable regions \u223c S cannot be ignored, and for determining the accuracy of the label prediction,wce loss is proposed. For the loss computation, the authors build upon the assumption that the weights of the classifier network Wc can be treated as a feature representing the corresponding class c. A correlation map Dc is calculated per class using cosine similarity as a metric as described in Eq. (9).\nDc(p) = 1 + (\n\u03d5(p) \u2225\u03d5(p)\u2225 \u00b7 Wc \u2225Wc\u2225\n) , (9)\n\u03c3(p) = (\nDc\u2217(p) maxc (Dc(p))\n)\u03b3 (10)\nA confidence map is then calculated using Eq.(10). Here c\u2217 is obtained as Ycrf labels corresponding to the respective class. \u03b3 is a damping parameter that is always set greater than 1. The confidence map can predict the probability of each label being correct. Thus, wce loss is calculated according to Eq.(11).\nLwce = \u2212 1\u2211\nc \u2211 p\u2208\u223cSc \u03c3(p) \u2211 c \u2211 p\u2208\u223cSc \u03c3(p) logHc(p) (11)\nThe final loss is calculated using Eq. (12), where \u03bb is a weighing parameter which bal\u2010 ances Lce and Lwce .\nL = Lce + \u03bbLwce (12)\nReScience C 8.2 (#26) \u2013 Mehta et al. 2022 5\n3.4 Code details The complete code containing the proposed NAL and all ablation studies both using Py\u2010 Torch [13] and PyTorch Lightning along withWandB [14] integration is available at these links: (PyTorch, PyTorch Lightning). Links to all obtained pseudo labels and pre\u2010trained models are also provided in README. Detailed discussion about the implementation is provided in the following sections.\nPseudo label generation fromVOC toCOCO \u2014Weperformacross\u2010dataset evaluation of pseudo generator on the MS COCO dataset for a model trained of PASCAL VOC. While the au\u2010 thors do not provide an implementation for the same, we implement the experiment from details provided in the paper and communication with the authors. We appro\u2010 priately map the VOC classes to the corresponding classes in COCO using information available about both datasets to facilitate Eq (6). We follow standard protocols for eval\u2010 uating the pseudo labels using the official COCO API.\nSemantic segmentation with NAL \u2014 The original authors\u2019 code implementation contained Stage 1 and Stage 2, but the Stage 3 codewas incomplete. We thus implemented the com\u2010 plete Stage 3 training from scratch, including the proposed NAL and the other loss func\u2010 tions discussed in section 4.2.2 based on the details from the paper. We train the model using cross\u2010entropy loss and Noise Aware Loss and utilize the Polynomial LR Scheduler. Dense\u2010CRF is also applied as post\u2010processing as per the code provided in the authors\u2019 repository.\nReScience C 8.2 (#26) \u2013 Mehta et al. 2022 6\n4 Results\nWeexperimented and verified all the central claimsmadeby thepaper about BAPmethod\u2010 ology and NAL on PASCAL VOC 2012 dataset. Following are the detailed description of the results obtained.\n4.1 Results reproducing original paper\nExperimentswith BackgroundAwarePooling \u2014Wesuccessfully replicated the results reported in Table 3 from the original paper, and it supports claim 1 of BAP being a superior method to GAP presented in Section 2 .\nAs discussed in section 3.1.2, we verified the authors\u2019 claims that the classifier model is generic and can be used for the detection of classes unseen during training. We trained the classifier model over the Pascal VOC dataset and generated pseudo labels over the MS\u2010COCOdataset. Weuse the COCO\u2010API evaluator of pycocotools to evaluate our results on the COCO benchmark. The comparison of our results with the authors\u2019 results is given in Table. 4.\nReScience C 8.2 (#26) \u2013 Mehta et al. 2022 7\nExperiments with Noise Aware Loss \u2014 Comparison between our and the authors\u2019 results re\u2010 gardingNAL is provided inTable 5, which shows thatNALoutperforms the cross\u2010entropy loss computed on Ycrf and Yret, thus supporting the claim 2 presented in section 2.\n4.2 Results beyond original paper\nExperiments with grid size \u2014We performed a hyperparameter search for the grid size (N) and observed that lower values of N for generating pseudo labels provide the best results. In contrast, the opposite was true for training the classification network.\nReScience C 8.2 (#26) \u2013 Mehta et al. 2022 8\nExperiments with NAL and it\u2019s counterpart losses \u2014 Besides NAL, various other losses have been defined in the paper to deal with unreliable regions such as entropy regularisation andbootstrapping. The comparison between our results and the authors\u2019 results is given in Table 7, with both before and after applying Dense\u2010CRF.\nExperimentswith different values of lambda anddampparameters. \u2014 To justify the selection of the values of lambda anddamppa\u2010 rameters, comparison studies were performed by choosing different values of lambda and damp parameters. We train the DeepLabV1 (LargeFOV) model for a range of lambda and damp parameters and report the results as a heat\u2010map representation in Fig. 3.\n5 Discussion\nThrough our experiments, we reproduce and verify the cen\u2010 tral claims of the original paper about the two newly intro\u2010 duced techniques \u2010 BAP and NAL. We additionally perform ablation studies on differentmodel hyper\u2010parameters and various losses to gain insights into the original author\u2019s choice of the same. We obtained very similar results in the reproducibility of BAP. The above claim that BAP is a superior method to GAP is well verified by the increased results obtained using BAP compared to GAP on PASCAL VOC, as reported in Table 3. We further analyze that usingu0 (corresponding to background attentionmap) yields better results thanusingub (corresponding to background class activation map) for generation of the pseudo labels, suggesting superior discrimination of background regions in this method. In implementing the authors\u2019 cross\u2010dataset evaluation results on the COCO dataset, we obtain considerably lower results despite following the protocols mentioned in the pa\u2010\nReScience C 8.2 (#26) \u2013 Mehta et al. 2022 9\nper. However, our results support the claim that BAP serves as a promising technique in implementing a class\u2010agnostic pseudo label generator. We implemented NAL from scratch and performed all the weakly\u2010supervised training experiments with the obtained pseudo labels Ycrf and Yret. We report slightly lower results compared to authors, which we attribute to the minor implementational differ\u2010 ences and a possible tuning of the parameters in DenseCRF. This can be shown by Ta\u2010 ble 7 in which all the results before DenseCRF match the author\u2019s results, but there are some differences after using DenseCRF. However, a relative gain in performance for both DeepLab v1 and v2 is clearly observed from Table 5 when unreliable regions are exploited with the help of NAL. Furthermore, our experiments using different losses for regions with different predicted labels in Ycrf and Yret, as listed in Table 7, provide supporting evidence that NAL outperforms the contemporary losses and suggests it is a robust technique for weakly\u2010supervised training when there are regions with less confi\u2010 dence. For Stage 1 and Stage 2, we perform experiments with different choices of grid size in BAP, and for Stage 3, we analyze model performance for different values of damping parameter \u03b3 and weighting parameter \u03bb. From Table 6, we infer that the best result is obtained for grid size 4 for training and 1 for label generation, which is in coherencewith the values used in the original paper. For Stage 3, Fig. 3 supports the authors\u2019 choice of values assigned to \u03b3 and \u03bb. Using a higher damping coefficient value (\u03b3) makes the model biased towards most confident labels. On the other hand, using a higher value of \u03bb gives more weight to wce loss, increasing the reliance on regions with low confidence. All the ablation experiments with the selected hyper\u2010parameters yielded validation IoU lower than that obtained in Table 5. In our qualitative analysis of the generated pseudo labels (refer Fig. (2)) Ycrf and Yret we infer that Ycrf particularly performs well in capturing low level image features. In Fig. 2, it is seen to discriminate the background region between the wheel\u2019s spokes cor\u2010 rectly. Yret, on the other hand, captures high\u2010level features in the same image although mildly exaggerated. Thus, the two labels complement each other, and together is a good indication of unreliable regions identified and suppressed by NAL. After porting the code base into PyTorch Lightning, we also concluded the implemen\u2010 tations and experiments that ensured the correctness of various bits of training and evaluation process such as data loading, loss calculation, model weights optimization, and checkpoint re\u2010loading for further reproducibility experiments in the future.\n6 Conclusion\nIn this paper, we reproduce all the original results provided by the authors. Reproduc\u2010 ing the first claim involving Background Aware Pooling, we were able to achieve similar results to the author. Hence, we support the claim that BAP is a superior method for WSSS than GAP. Cross dataset evaluation was performed on the COCO dataset. Our experiments verify the claim that the model works as a class agnostic pseudo label gen\u2010 erator and achieves satisfactory results in performing VOC\u2010to\u2010COCO evaluation. For Stage 3, we implemented Noise Aware Loss from scratch and trained the DeepLab mod\u2010 els for WSSS. Our results are slightly lower than the actual results. Nonetheless, our experiments still support the claim that NAL outperforms the contemporary losses and suggests it is a robust technique for weakly supervised learning. Our additional experi\u2010 ments also provide further insights on the performance of NAL for different values of hy\u2010 perparameters. We thus believe it would be of interest to perform further experiments focused on modifying NAL, which might lead to better results.\nReScience C 8.2 (#26) \u2013 Mehta et al. 2022 10"}, {"heading": "Appendix", "text": "7 Method Code-flow\nWe present the code\u2010flows for each of the three training stages in the following sections.\n7.1 Stage 1: Training a classification network using BAP\nReScience C 8.2 (#26) \u2013 Mehta et al. 2022 12\n7.2 Stage 2: Obtaining pseudo labels from the trained classification model\nReScience C 8.2 (#26) \u2013 Mehta et al. 2022 13\n7.3 Stage 3: Training segmentation network using the pseudo labels obtained and NAL loss\nReScience C 8.2 (#26) \u2013 Mehta et al. 2022 14\n8 Class Agnostic Pseudo Label Generation using u0 In our VOC to COCO experiment, mapping was done between the classes of VOC to the corresponding COCO classes to facilitate usage of CAMs. Herewe further investigate the usage 1\u2212u0 as a class agnostic foreground attention map for all classes instead of using CAMs. We perform this experiment on the VOC train set, wherein no CAMs have been used in label generation. The results in comparison with usage of CAMs for uc strongly exhibit the generic nature of pseudo label generator using the background attention map. Visual comparison of the images is shown in Fig. (7)."}, {"heading": "Method CAMS for uc 1\u2212 u0 in place of uc", "text": "ReScience C 8.2 (#26) \u2013 Mehta et al. 2022 15\n9 Wandb Training Logs\n9.1 Experiments with NAL loss Following are the training logs obtained during the Stage 3 training with cross\u2010entropy loss on Ycrf and Yret individually, and with NAL using both.\nReScience C 8.2 (#26) \u2013 Mehta et al. 2022 16\n9.2 Experiments with NAL and it\u2019s counterpart loss Shown below are the training logs of Stage 3 experiments using NAL and other contem\u2010 porary losses. Mean IoU score and mean accuracy shown are obtained on training set.\nReScience C 8.2 (#26) \u2013 Mehta et al. 2022 17\n9.3 Experiments with GAP and BAP Here we present training logs from Stage 1 experiments using BAP and GAP on aug\u2010 mented dataset and non\u2010augmented dataset.\n10 Detailed study: BAP vs GAP\nThe complete results in our comparison of BAP and GAP are shown below. In both the methods, we notice a significant improvement in mean IoU upon using the augmented dataset. As seen from the results, BAP is superior than GAP for the different experimen\u2010 tal configurations.\nReScience C 8.2 (#26) \u2013 Mehta et al. 2022 18"}], "title": "[Re] Background-Aware Pooling and Noise-Aware Loss for Weakly-Supervised Semantic Segmentation", "year": 2022}