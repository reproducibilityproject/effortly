{"abstractText": "The reproduction of the original paper as well as the extended implementationwere suc\u2010 cessful. We were able to reproduce the original results and examine the performance of the proposed model in an environment where strategic and non\u2010strategic users both present. Linear models seem to struggle with different proportions of strategic users, while the non\u2010linear model (RNN) achieves good performance regardless of the propor\u2010 tion of strategic users.", "authors": [{"affiliations": [], "name": "Guilly Kolkman"}, {"affiliations": [], "name": "Jan Athmer"}, {"affiliations": [], "name": "Alex Labro"}, {"affiliations": [], "name": "Maksymilian Kulicki"}, {"affiliations": [], "name": "Koustuv Sinha"}, {"affiliations": [], "name": "Sharath Chandra Raparthy"}], "id": "SP:3ce752b7de917b8c292e7cefa1247641071fb3a7", "references": [{"authors": ["S. Levanon", "N. Rosenfeld"], "title": "Strategic Classification Made Practical.", "venue": "arXiv preprint arXiv:2103.01826", "year": 2021}, {"authors": ["M. Hardt", "N. Megiddo", "C. Papadimitriou", "M. Wootters"], "title": "Strategic classification.", "venue": "Proceedings of the 2016 ACM conference on innovations in theoretical computer science", "year": 2016}, {"authors": ["H. Costa", "L.H. Merschmann", "F. Barth", "F. Benevenuto"], "title": "Pollution, bad-mouthing, and local marketing: The underground of location-based social networks.", "venue": "Information Sciences", "year": 2014}, {"authors": ["M.L. G"], "title": "ULB. Financial Distress Prediction: Bankruptcy Prediction. data retrieved from Kaggle, https://www. kaggle.com/shebrahimi/financial-distress", "year": 2018}, {"authors": ["M.L. G"], "title": "ULB. Credit Card Fraud Detection:Anonymized credit card transactions labeled as fraudulent or genuine. data retrieved from Kaggle, https://www.kaggle.com/mlg-ulb/creditcardfraud. 2018", "venue": "ReScience C", "year": 2022}, {"authors": ["B. Ustun", "A. Spangher", "Y. Liu"], "title": "Actionable recourse in linear classification.", "venue": "Proceedings of the Conference on Fairness, Accountability, and Transparency", "year": 2019}], "sections": [{"text": "R E S C I E N C E C"}, {"heading": "Replication / ML Reproducibility Challenge 2021", "text": "[Re] Strategic classification made practical: reproduction\nGuilly Kolkman1, ID , Jan Athmer1, ID , Alex Labro1, ID , and Maksymilian Kulicki1, ID 1University of Amsterdam, Amsterdam, Netherlands\nEdited by Koustuv Sinha,\nSharath Chandra Raparthy\nReviewed by Anonymous Reviewers\nReceived 04 February 2022\nPublished 23 May 2022\nDOI 10.5281/zenodo.6574665\n1 Reproducibility Summary\n1.1 Scope of Reproducibility In this work, the paper Strategic Classification Made Practical[1] is evaluated through a reproduction study. The results from the reproduction examines if the claims made in the paper are valid. We could find two main claims that were made by the authors that we will attempt to reproduce. Those are as follows:\n1. \u201dWe propose a novel learning framework for strategic classification that is practi\u2010 cal, effective, and flexible. This allows for differentiation through strategic user responses, which supports end\u2010to\u2010end training.\u201d\n2. \u201dWe propose several forms of regularization that encourage learnedmodels to pro\u2010 mote favorable social outcomes.\u201d\nWe interpret practical, effective and flexible as such that the model should work better on a variety of real life problems than their non\u2010strategic counterpart.\n1.2 Methodology In this paper, the same code, datasets and hyperparameters were used as the original paper to reproduce the results. To further validate the claims from the original paper, we extended the original implementation to include an experiment that tests performance on a dataset containing both strategic (also referred to as gaming) and non\u2010strategic users.\n1.3 Results The reproduction of the original paper as well as the extended implementationwere suc\u2010 cessful. We were able to reproduce the original results and examine the performance of the proposed model in an environment where strategic and non\u2010strategic users both present. Linear models seem to struggle with different proportions of strategic users, while the non\u2010linear model (RNN) achieves good performance regardless of the propor\u2010 tion of strategic users.\nCopyright \u00a9 2022 G. Kolkman et al., released under a Creative Commons Attribution 4.0 International license. Correspondence should be addressed to Guilly Kolkman (gkolkm@gmail.com) The authors have declared that no competing interests exist. Code is available at https://github.com/GuillyK/FACT-ai. \u2013 SWH swh:1:dir:31ff8f4d7da6e70c88e4d28ba0c18ee5f04ac424;. Open peer review is available at https://openreview.net/forum?id=rNgg03fXnRY.\nReScience C 8.2 (#20) \u2013 Kolkman et al. 2022 1\n1.4 What was easy The codebase for the paper was available on GitHub which meant that we didn\u2019t have to start from scratch. They also provided us with the original data. The codebase also camewith the original results from the authors whichmeant that comparing the results was easy.\n1.5 What was difficult Although the codewas available, documentation of the codewas quite sparse. Therefore, it was hard to figure out what each part of the code did and made it difficult to interpret what the results actually meant at certain stages.\n1.6 Communication with original authors The University of Amsterdam communicated before the course with the authors about the datasets. While working on the reproduction we sent one email about clarification of their method and to request a missing dataset.\nReScience C 8.2 (#20) \u2013 Kolkman et al. 2022 2\n2 Introduction\nAs consequential decisions such as loan approval and fraud detection are increasingly made by predictive machine learning systems, it is important to consider the weak\u2010 nesses and vulnerabilities of these systems. Users may gain knowledge of the model and use this knowledge to modify their features to improve their outcomes. Therefore, amodelmust be resilient against strategicmodification of features to classify these users properly.\nThis problem of classification while users strategically modify their features is referred to as strategic classification, and it is the main subject of the paper Strategic Classifi\u2010 cation Made Practical[1]. In this paper, a novel framework is proposed that claims to be more practical and flexible than previous methods, along with novel methods to improve social outcomes in automated decision\u2010making. This work will examine the results demonstrated in this paper through reproduction of their experiments.\n3 Scope of reproducibility\nThis paper describes our efforts to reproduce the work from the paper Strategic Classi\u2010 fication Made Practical[1], which addresses the problem of strategic classification in a manner that is more practical than previous approaches, more flexible than previous approaches and takes social good into account. The original paper describes strategic classification as a classification task on a set of points x \u2208 RD. A classifier h(x) is tasked to classify x to into classes y = {\u22121, 1}, which is determined by a score function f via the decision rule h(x) = sign(f(x)). In strategic classification, the assumption ismade that points canmove according to a cost function c(x, x\u2032). Therefore, users can modify their original features x to their response x\u2032 to im\u2010 prove their outcome using the best move for x;\u2206h(x). Where this best move\u2206h(x) can be described as follows:\n\u2206h(x) = argmax x\u2032\u2208\u03c7\nh(x\u2032)\u2212 c(x, x\u2032) (1)\nTo accurately classify points that are able to to modify their features, the error function has to take this into account. This results in an empirical loss function of:\nmin f\u2208F m\u2211 i=1 I{y \u0338= h(\u2206f (x))}+ \u03bbR(f) (2)\nWhich translates to choosing a score function f such that misclassification of manipu\u2010 lated datapoints is minimized according to regularization method R and \u03bb that deter\u2010 mines the regularization strength. Optimizing equation 2 is referred to as strategic em\u2010 pirical riskminimization (SERM) in the original paper, which functions as themain loss function of the framework.\nThe claims that the paper made are as follows:\n1. Flexible, practical and effective modeling: By using SERM, the claim is made that the framework can extend beyond the original formulation of strategic classifica\u2010 tion, i.e. outperform the original paper by Hardt et al.[2] and demonstrate good performance in new and realistic environments.\n2. Socially\u2010aware learning: By regularizing based on social objectives such as ex\u2010 pected utility, social burden and recourse, the claim is made that the model can promote socially favourable outcomes, i.e. increase positive user outcomes as reg\u2010 ularization increases.\nReScience C 8.2 (#20) \u2013 Kolkman et al. 2022 3\nIn addition to reproducing the results presented in the paper, we perform novel experi\u2010 ments that test the claimed practicality, effectiveness and flexibility of the approach. In the experiments, we lift the assumption that all the users of the system are modifying their features to game the classifier. We make the case that this assumption cannot be applied to many real life settings. The experimental results show that in some settings the proposed method leads to decreased performance.\n4 Methodology\n4.1 Model descriptions The focus of the original paper is on proposing a new framework, rather than a new model. Therefore, the models that were used in the original paper were relatively sim\u2010 ple, consisting of linear classifiers and a basic RNN. All models were optimized using Adam. The claims made in the paper were proven independently of each other in different ex\u2010 periments using different datasets, spam, credit, fraud and financial distress (datasets will be elaborated upon in section 4.2). This was done to demonstrate the flexibility of the framework as well as to demonstrate the generalizability of the framework on different datasets.\nTo verify said generalizability and flexibility, we repeated these experiments using the code provided by the authors. Similar to the original paper, results were compared to two \u201dsimpler\u201d, non\u2010strategic models to verify performance. By non\u2010strategic, we mean a standard classifier which assumes that the datapoints cannot move in any way. These models are referred to as benchmark, which is a non\u2010strategic classifier trained and evaluated on non\u2010strategic data, and blind, which is a non\u2010strategic classifier evaluated on strategic data. To clarify: performing strategic classification is the act of training a model on a dataset and consequently performing a classification task where strategic movement of points according to a cost function is taken into account. In the original paper, the model calculates the cost depending on the scale. Therefore, we will also be using scale to calculate the cost. Similar to the original paper, the in\u2010 fluence of scale on model performance will be examined, where scale will take on the following values: [0.5, 1, 2]. These values were chosen such that the results can be com\u2010 pared to the original paper. This means that for each run of the model the cost is in\u2010 creased or decreased depending on the scale. The cost function used is:\nCost = scale \u2217 squared_error (3)\nExperimental setup claim 1 \u2014 The claim of model flexibility is tested by performing strate\u2010 gic classification on the spam dataset, which is the same dataset used by Hardt et al[2] in their original definition of strategic classification. The model used for this experi\u2010 ment is a linear classifier with a SERM loss function. Average accuracy was monitored and compared to a blind model and a benchmark model. To further validate the perfor\u2010 mance of the framework, strategic classification was also performed on the remaining datasets.\nExperimental setup claim 2 \u2014 The claim of socially\u2010aware learning was verified by perform\u2010 ing strategic classification on the credit dataset. To account for social good, several regu\u2010 larization techniques are used. Specifically expected utility, social burden and recourse are considered in the loss function:\nReScience C 8.2 (#20) \u2013 Kolkman et al. 2022 4\nExpected utility: summed utility that the users gain from classification results, minus the total cost of gaming\nRutility = \u2212 m\u2211 i=1 h(\u2206(x))\u2212 c(x,\u2206(x)) (4)\nSocial burden: minimal cost value from among users classified as positive.\nRburden = \u2211 min c(x,\u2206(x)) (5)\nRecourse: the capacity of a user who classified negative to restore approval through reasonable action\nRrecourse = m\u2211 i=1 \u03c3(\u2212f(x)) \u2217 \u03c3(\u2212f(\u2206f (x))) (6)\nWhere \u2206(x) is the best response for x, \u2206f (x) is the best response for x with regard to f , x\u2032 is the strategically modified datapoint (as described in section 3), and \u03c3(x) is the sigmoid function of x. Similar to the first experiment, this experiment uses a linear classifier with a SERM loss function. For every regularisation method, differing ranges of regularisation were analysed and compared to a benchmark model. The experiment on utility used a log range for \u03bb \u2208 [\u22120.4,\u22120.2] of 10 steps, the experiment on social burden used a log range for \u03bb \u2208 [\u22122, 1] of 30 steps, and the experiment on recourse used a log range for \u03bb \u2208 [0, 0.3] of 15 steps.\nAdditional experiments \u2014 The authors make an assumption that all the users of the system will game according to their cost function. However, in many real life situations this assumption may not hold. For instance, in an email spam classification setting, people who write regular non\u2010spam emails will most likely not think about gaming the spam classifier system. Assuming that every user is gaming might lead to a situation where a non\u2010gaming email is falsely classified as spam due to the classifier being too strict. The case in which the strategic model is evaluated on a dataset consisting of non\u2010gaming users is not taken into account in the paper.\nWe performed an additional experiment to investigate that case, with the hypothesis that the strategic model would perform worse on non\u2010strategic data and would result in more false negative errors. In addition, we investigated a mixed data situation, in which gaming users make up some proportion of the data, defined by an additional pa\u2010 rameter. After being trained on the original dataset, the classifiers were evaluated on n datapoints, out of which \u03b2 \u2217 n points chosen at random were strategically modified. \u03b2 is the share of gaming users in the data, ranging from [0, 1] in steps of 0.1. In this exper\u2010 iment, we tracked the classification accuracy and error types, false negatives and false positives, of the strategic and non\u2010strategic model. The accuracy benchmark used with this model is the performance of the non\u2010strategic model on non\u2010strategic data (\u03b2 = 0), which is the same as in the original paper. The experiment was performed on the four datasets from the paper in a new Jupyter notebook. We used the cost scale of 0.5 in the experiments.\n4.2 Datasets Thedatasets thatwere used in the original paperwere also used in thiswork. All datasets and their details are shown in figure 1. Spam, as used by Hardt et al[2] contains features of users and spammers from a Brazilian social network. The features consists of nu\u2010 merical values about the user and their activity, such as amount of followers or number of words in a post. The dataset can be obtained from Costa et al.[3]. Financial Distress,\nReScience C 8.2 (#20) \u2013 Kolkman et al. 2022 5\ncreated for a Kaggle challenge [4] contains time\u2010series data describing the measure of financial distress for 422 companies. Each company has a maxium of 14 time steps af\u2010 ter which the company has or has not gone bankrupt. Fraud, which was also created for a Kaggle challenge[5], contains 284000 credit card transactions that are either real or fraudulent. Features include numerical features related to the transaction, such as time of transaction and amount of money in the transaction. Credit, created by Ustun et al[6], contains credit card spending patterns as well as labels that define if the pattern is regular or not. There are 30000 data points, each with 11 features that include features such as age, payment history and education level.\nDataset size Features Format Description\nSpam 7,076 15 .csv Collection ofsocial network users and posts Credit 30,000 11 .csv Collection ofcredit card spending patterns Fraud 284k 29 .csv Collection ofcredit card transaction Financial distress 422 83 time series Collection offinancial situations of companies\n4.5 Computational requirements Experiments were able to be reproduced on laptops without a dedicated graphics card. Times per experiment differed, but individual experiments generally took 60 minutes. The exception to this rule was calculating the performance of the model for social bur\u2010 den, which took around 9 hours to run on a laptop with a GTX 1050.\n5 Results\nThe reproduction study reveals minimal differences between the reported and repro\u2010 duced accuracies. The first claim for a novel learning framework for strategic classifica\u2010 tion is supported by our reproduced results. The framework performs better than the Hardt et al.[2] strategic classification baseline. Thus, it also supports the second claim of the original paper.\n5.1 Results reproducing original paper\nResult 1 \u2014 As mentioned in section 3.1, the claim of flexible modeling was reproduced and evaluated by running the code that was provided by the original authors and com\u2010 paring the outcome to the results produced by Hardt et al.[2], as visible in figure 3 and appendix A. Similar to the results proposed in the original paper, our reproduced results outperform the algorithm proposed by Hardt et al. [2]. To further evaluate the claim of flexibility, the original paper examines performance in environments other than the one proposed in Hardt et al.[2]. The new environments are the datasets credit, financial distress and spam. These datasets give an accuracy ofmore than 0.7 for all except for the blind testing. This is true in both the original results and our reproduction. The lower accuracy in the blind testing can be explained by making the assumption that nobody is gaming when everyone is gaming in the dataset. This causes the agents who are gaming to cross the decision boundary by gaming the system. However, as visible in figure 3, the results of our reproduction of strategic classification on the fraud dataset using the SERMmethod has a significantly higher accuracy than the original paper. The other noticeable result in the accuracy is again in the fraud dataset.\nReScience C 8.2 (#20) \u2013 Kolkman et al. 2022 7\nHowever, it is between the benchmark and the SERM method which in all other cases had a difference of less than 0.1.\nResult 2 \u2014 To prove the second claim about social impact, the authors made plots similar to figure 4. In figure 4 we see that there is an initial range where the accuracy doesn\u2019t drop significantly with increase in regularisation, similarly to the plots of the original authors. This means that the model can be fitted to accomplish lower social burden without having a detrimental effect to the accuracy. To test the true social impact it would be good to also test these regularized models on mixed data, since we saw a large amount of false negatives in the previous section. However we did not have time for that in this paper and these results show that the model can be regularized succesfully which was the main focus of the claim made by the original authors.\n5.2 Results beyond original paper The results in terms of accuracy, presented in Figure 5, show that for 3 out of 4 datasets, there is a clear linear relationship between the amount of gaming users and both mod\u2010 els\u2019 performance, with roughly the same slope and opposite direction. For fraud and spam data, the point of equal performance is around 0.6, which suggests that the non\u2010 strategic model is better, assuming uniform probability distribution of \u03b2. The plots of false positive and negative errors (Fig 6) show, in line with our initial hypothesis, that the drop in performance of the strategic model is caused by more false negative errors, and by false positive in case of the non\u2010strategic model. The distress data is an outlier, in which the strategic model is at the benchmark level for all levels of \u03b2 and the non\u2010strategic model\u2019s accuracy significantly decreases with \u03b2. This might be due to the fact that the distress classifier is a RNN network, while the other ones are linearmodels. The RNN can learn amore complex representation, which enables the strategicmodel to adjust in amore refinedway, not just bymoving the linear boundary. The experimental results show a practical limitation in the presented method, which comes from the assumption that all users game in the same way and according to the same cost function. However, the distress model shows that decreased performance for non\u2010strategic users is not always the case. Looking into this relationship is a potential direction of new research, which might result in improvements to the proposed frame\u2010 work.\n6 Discussion\nAfter comparing the results, we conclude that the experimental results support the first claim to a large extent. The model is flexible since it performs well on multiple prob\u2010 lems and it is effective since it performs significantly better on strategic data than the\nReScience C 8.2 (#20) \u2013 Kolkman et al. 2022 8\nnon\u2010strategic model. On the practical part there are still some unanswered questions. Asmentionedwe thought that the original test environmentwith the assumption that all users are gaming might not be very representative for certain problems, like the spam dataset. We think most people don\u2019t adjust their emails to account for spam filters. We see this in the amount of false negatives from the strategic model on mixed data. For other problems like the distress dataset the strategic model performs really well on our mixed data, showing great potential for real\u2010life problems. Follow\u2010up research might involve investigating how different types of non\u2010linear classifiers (e.g. decision trees, different neural network architectures) deal with the problem of mixed data.\nThe second claim is also supported by the results, they show that themodel can be regu\u2010 larizedwithout completely compromising the accuracy. This is an interesting result and it shows that their technique does work in practice. However since the strategic models show more false negatives for a mixed data set than non\u2010strategic ones, it might have a more severe social burden than initially thought. An interesting followup experiment could be to check a regularized model on mixed data to see what the impact actually is.\n6.1 What was easy Since we had not worked on the problem of strategic classification before, it would have been very difficult to implement it in the limited timeframe that was available. Luck\u2010 ily, the authors of the paper had created a codebase containing all experiments and results. Along with the code, the dataset sources were also mentioned in the original paper. Therefore, we were able to verify that the results from the original paper were valid as soon as we got the code working.\n6.2 What was difficult Although the code was available, documentation of the code was quite sparse and un\u2010 clear, and getting every part of the code to run took some trial and error due to the lack of comments. There were some short comments in the vanilla notebook, which we could consequently use to try to understand what was happening in the code. As well as a lack of documentation in the code, the counterpart of equations 1 and 2 written in the original paper lacked sufficient explanation about what the variables/functions meant\nReScience C 8.2 (#20) \u2013 Kolkman et al. 2022 9\nand what their purpose was in the equation. When taking all these factors into account, understanding the code was quite a challenge. Another problem was the fact that the results from the original paper were not sum\u2010 marised in a table, which meant we had to manually keep track of the accuracies for every experiment. Another problem was the fact that hyperparameter selection also was not elaborated upon. Although these are minor issues, they still took time to look into and were part of what was difficult about this project.\n6.3 Communication with original authors The communicationwith the authors wasminimal. We asked about a clarification point about their method that had to do with them not using a combined dataset where users where partially gaming the system."}], "title": "[Re] Strategic classification made practical: reproduction", "year": 2022}