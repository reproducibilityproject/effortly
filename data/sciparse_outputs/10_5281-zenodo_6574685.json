{"abstractText": "We re\u2010implement the method proposed by Pan et al. [1] with regards to 3D shape re\u2010 construction, and extend their work. Our extensions include novel prior shapes and two new training techniques. While the code\u2010base relating to GAN2Shape was largely rewritten, many external dependencies, which the original authors relied on, had to be imported. The project used 189 GPU hours in total, mostly on a single Nvidia K80, T4 or P100 GPU, and a negligible number of runs on a Nvidia V100 GPU.", "authors": [{"affiliations": [], "name": "Alessio Galatolo"}, {"affiliations": [], "name": "Alfred Nilsson"}, {"affiliations": [], "name": "Koustuv Sinha"}, {"affiliations": [], "name": "Sharath Chandra Raparthy"}], "id": "SP:c0c8167026d13dd1acb2b7637bbfdf0a7bd5e3a6", "references": [{"authors": ["X. Pan", "B. Dai", "Z. Liu", "C.C. Loy", "P. Luo"], "title": "Do 2DGANsKnow3DShape? Unsupervised 3D shape reconstruction from 2D Image GANs.", "year": 2021}, {"authors": ["F. Yu", "A. Seff", "Y. Zhang", "S. Song", "T. Funkhouser", "J. Xiao"], "title": "Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop.", "year": 2015}, {"authors": ["Z. Liu", "P. Luo", "X. Wang", "X. Tang"], "title": "Deep Learning Face Attributes in the Wild.", "venue": "Proceedings of International Conference on Computer Vision (ICCV)", "year": 2015}, {"authors": ["T. Karras", "S. Laine", "T. Aila"], "title": "A style-based generator architecture for generative adversarial networks.", "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition", "year": 2019}, {"authors": ["T. Karras", "S. Laine", "M. Aittala", "J. Hellsten", "J. Lehtinen", "T. Aila"], "title": "Analyzing and improving the image quality of stylegan.", "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition", "year": 2020}, {"authors": ["T. Karras", "M. Aittala", "J. Hellsten", "S. Laine", "J. Lehtinen", "T. Aila"], "title": "Training generative adversarial networks with limited data.", "year": 2006}, {"authors": ["T. Karras", "M. Aittala", "S. Laine", "E. H\u00e4rk\u00f6nen", "J. Hellsten", "J. Lehtinen", "T. Aila"], "title": "Alias-Free Generative Adversarial Networks.", "venue": "Proc. NeurIPS", "year": 2021}, {"authors": ["S. Lunz", "Y. Li", "A. Fitzgibbon", "N. Kushman"], "title": "Inverse graphics gan: Learning to generate 3d shapes from unstructured 2d data.", "year": 2002}, {"authors": ["P. Henzler", "N.J. Mitra", "T. Ritschel"], "title": "Escaping plato\u2019s cave: 3d shape from adversarial rendering.", "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision", "year": 2019}, {"authors": ["Z. Wu", "S. Song", "A. Khosla", "F. Yu", "L. Zhang", "X. Tang", "J. Xiao"], "title": "3d shapenets: A deep representation for volumetric shapes.", "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition", "year": 2015}, {"authors": ["H. Wang", "J. Yang", "W. Liang", "X. Tong"], "title": "Deep single-view 3D object reconstruction with visual hull embedding.", "venue": "Proceedings of the AAAI Conference on Artificial Intelligence", "year": 2019}, {"authors": ["S.Wu", "C. Rupprecht", "A. Vedaldi"], "title": "Unsupervised learning of probably symmetric deformable 3d objects from images in the wild.", "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition", "year": 2020}, {"authors": ["C. Yu", "J. Wang", "C. Peng", "C. Gao", "G. Yu", "N. Sang"], "title": "Bisenet: Bilateral segmentation network for real-time semantic segmentation.", "venue": "Proceedings of the European conference on computer vision (ECCV)", "year": 2018}, {"authors": ["C. Yu", "C. Gao", "J. Wang", "G. Yu", "C. Shen", "N. Sang"], "title": "Bisenet v2: Bilateral network with guided aggregation for real-time semantic segmentation.", "venue": "In: International Journal of Computer Vision", "year": 2021}, {"authors": ["H. Kato", "Y. Ushiku", "T. Harada"], "title": "Neural 3D Mesh Renderer.", "year": 2017}, {"authors": ["J. Johnson", "A. Alahi", "L. Fei-Fei"], "title": "Perceptual Losses for Real-Time Style Transfer and Super-Resolution.", "year": 2016}, {"authors": ["T. Zhou", "M. Brown", "N. Snavely", "D.G. Lowe"], "title": "Unsupervised Learning of Depth and Ego-Motion from Video.", "year": 2017}, {"authors": ["H. Zhao", "J. Shi", "X. Qi", "X. Wang", "J. Jia"], "title": "Pyramid scene parsing network.", "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition", "year": 2017}, {"authors": ["P. Paysan", "R. Knothe", "B. Amberg", "S. Romdhani", "T. Vetter"], "title": "A 3D face model for pose and illumination invariant face recognition.", "venue": "Galatolo and Nilsson 2022", "year": 2009}], "sections": [{"text": "R E S C I E N C E C Replication / ML Reproducibility Challenge 2021\n[Re] Replicating and Improving GAN2Shape Through"}, {"heading": "Novel Shape Priors and Training Steps", "text": "Alessio Galatolo1,2, ID and Alfred Nilsson1,2, ID 1Equal Contribution \u2013 2KTH Royal Institute of Technology, Stockholm, Sweden\nEdited by Koustuv Sinha,\nSharath Chandra Raparthy\nReviewed by Anonymous Reviewers\nReceived 04 February 2022\nPublished 23 May 2022\nDOI 10.5281/zenodo.6574685"}, {"heading": "Reproducibility Summary", "text": ""}, {"heading": "Scope of Reproducibility", "text": "Pan et al. [1] propose an unsupervised method named GAN2Shape that purportedly is able to recover 3D information stored in the weights of a pre\u2010trained StyleGAN2 model, to produce 3D shapes from 2D images. We aim to reproduce the 3D shape recovery and identify its strengths and weaknesses."}, {"heading": "Methodology", "text": "We re\u2010implement the method proposed by Pan et al. [1] with regards to 3D shape re\u2010 construction, and extend their work. Our extensions include novel prior shapes and two new training techniques. While the code\u2010base relating to GAN2Shape was largely rewritten, many external dependencies, which the original authors relied on, had to be imported. The project used 189 GPU hours in total, mostly on a single Nvidia K80, T4 or P100 GPU, and a negligible number of runs on a Nvidia V100 GPU."}, {"heading": "Results", "text": "We replicate the results of Pan et al. [1] on a subset of the LSUN Cat, LSUN Car [2] and CelebA [3] datasets and observe varying degrees of success. We perform several exper\u2010 iments and illustrate the successes and shortcomings of the method. Our novel shape priors improve the 3D shape recovery in certain cases where the original shape prior was unsuitable. Our generalized training approach shows initial promise, but has to be confirmed with increased computational resources."}, {"heading": "What was easy", "text": "The original code is easily runnable on the correct machine type (Linux operating sys\u2010 tem and CUDA 9.2 compatible GPU) for the specific datasets used by the authors."}, {"heading": "What was difficult", "text": "Porting the model to a new dataset, problem setting or a different machine type is far from trivial. The poor cohesion of the original code makes interpretation very difficult,\nCopyright \u00a9 2022 A. Galatolo and A. Nilsson, released under a Creative Commons Attribution 4.0 International license. Correspondence should be addressed to Alessio Galatolo (galatolo@kth.se) The authors have declared that no competing interests exist. Code is available at https://github.com/alessioGalatolo/GAN-2D-to-3D. \u2013 SWH swh:1:dir:531d1456baa3bb553ce549785158be7005c682c7. Open peer review is available at https://openreview.net/forum?id=B8mxkTzX2RY.\nReScience C 8.2 (#30) \u2013 Galatolo and Nilsson 2022 1\nand that is why we took care to re\u2010implement many parts of the code using the decou\u2010 pling principle. The code depends on many external implementations which had to be made runnable, which caused a significant development bottleneck as we developed on Windows machines (contrary to the authors). The exact loss functions and the number of training steps were not properly reported in the original paper, whichmeant it had to be deduced from their code. Certain calculations required advanced knowledge of light\u2010 transport theory, which had no familiarity to us, and had to be mimicked and could not be verified.\nCommunication with original authors We did not communicate with the original authors.\nReScience C 8.2 (#30) \u2013 Galatolo and Nilsson 2022 2\n1 Introduction\nImage generation has been a hot topic within generative models as they represent an intuitive problem whose results are easily accessible by the public. One of the models that has received a lot of public attention is StyleGAN (Karras, Laine, and Aila [4]). The network\u2019s architecture has been refined through multiple iterations in StyleGAN2 [5], StyleGAN2\u2010ADA [6] and StyleGAN3 [7]. StyleGAN2 improves on thefirst versionby, among other things, adding a projection method onto the latent space, which allows the inver\u2010 sion of an image into its latent representation.\nMethods like GAN2Shape [1] aim at exploiting the information that is already stored in the generator of a pre\u2010trained StyleGAN2 model to go beyond generating synthetic 2D images. In particular, thismethod aims to extract the 3D shape of the preeminent object in any image. This is intuitively possible due to the size of the training dataset of the StyleGAN2model, and its ability to generate images of an object frommultiple views and lighting directions by varying w. The authors of GAN2Shape use StyleGAN2 networks pre\u2010trained on different dataset categories andfive different feature extractionmodels to derive the shape information for images belonging to the same dataset categories. This method, compared to many others [8, 9, 10, 11], has the advantage of being completely unsupervised, and not requiring a change in the training process of the classical 2D GAN.\nIn this article, we describe our replication of GAN2Shape [1] and report mixed results. We perform several experiments and we illustrate the successes and shortcomings of the method. Further, we extend the method improving the original results in several cases.\n2 Scope of reproducibility\nThe authors of GAN2Shape make the following claims:\n1. Their framework does not require any kind of annotation, keypoints or assump\u2010 tion about the images\n2. Their framework recovers 3D shapewith high precision onhuman faces, cats, cars, buildings, etc.\n3. GAN2Shape utilizes the intrinsic knowledge of 2D GANs\n4. The 3D shape generated immediately allows for re\u2010lighting and rotation of the im\u2010 age.\n3 Methodology\nOur initial intent of re\u2010implementing the source code from the description of the paper had to be abandoned due to the lack of detailed information of some key points in the method. We, therefore, decided to follow a different approach integrating both the de\u2010 tails from the authors\u2019 code and the paper\u2019s description. While trying to always base our implementation on the paper\u2019s description we found some parts (particularly the loss functions) that differed from the actual code and decided to follow the latter instead.\nThe resources we used were mainly the authors\u2019 code, the code and documentation of all the out\u2010sourced methods the authors borrowed: StyleGAN2 [5] (code), Unsup3D [12] (code), Semseg [13] (code) and BiSeNet [14, 15] (code). The GPUs used weremultiple and varied depending on availability: Nvidia Tesla K80, T4, V100 and P100.\nReScience C 8.2 (#30) \u2013 Galatolo and Nilsson 2022 3\n3.1 Model descriptions To extract the implicit 3D knowledge of pre\u2010trained StyleGAN network, Pan et al. [1] pro\u2010 pose an elaborate scheme involving five different neural networks. Each network mod\u2010 els a particular quantity corresponding to the view and lighting directions, the depth of the image, and the albedo. The View and Light (V and L, resp.) networks operate in a encoder type manner, trying to obtain a low\u2010dimensional vector representation of the camera view direction v and the direction of light l illuminating the object in the picture. The Depth and Albedo (D and A, resp.) networks utilize auto-encoder architectures1 to obtain image\u2010resolution depth maps d and diffuse reflections (albedo) a off the object\u2019s presumed surface.\nThe real GAN knowledge extraction happens in the final network, the Offset encoder E, combinedwith the pre\u2010trained StyleGAN2 generator,G. The offset encoder aims to learn a latent representation w of images with randomly sampled view and light directions, pseudo-samples. Paired with G, this allows the creation of new realistic samples I\u0303i = G(w\u2032i) with new view and lighting directions, denoted projected samples. The projected samples then serve as extended training data, providing multiple view\u2010light direction variations of the original image.\nTo use the components v, l, d and a to obtain a reconstructed image, the authors utilize a pre\u2010trained neural renderer developed by Kato, Ushiku, and Harada [16], which we denote by \u03a6.\nTraining Procedure \u2014 The training process of this method can be divided into 3 different steps, where the different networks involved are trained separately. In the original paper, these steps are done sequentially and for one image at a time, as shown in Figure 1, and each step is repeated multiple times before moving into the following one. The result is a model that can predict the depth map for only one image. All of the networks are trained using the Adam optimization algorithm.\nPrior pre\u2010training. Before attempting to learn the true shape of an object, the depth network is initialized by pre\u2010training it on a fixed prior shape. For this purpose Pan et al. [1] propose to use an ellipsoid shape as the shape prior. We utilized this ellipsoid prior to reproduce the results of Pan et al. [1], and we extended their work by also evaluating two new different priors.\nStep 1 optimizes only the A network according to Equation 1. Given an input I, the first four networks predict their components v, l, d, a, and we obtain a reconstructed image I\u0302 = \u03a6(v, l,d, a)2.\nLstep1(I, I\u0302) = \u2225I\u2212 I\u0302\u22251 + \u03bbsLs(D(I)) + \u03bbpLp(I, I\u0302) (1)\nStep 2 optimizes the E network according to Equation 2. Using the d and a compo\u2010 nents given in the last step 1 iteration, and random directions v\u2032i, l \u2032\ni, we generate Np new pseudo\u2010images I \u2032 i. For each I \u2032 i we predict\u2206wi = E(I \u2032\ni), which serves as input to the StyleGAN generator network G and obtain the projected images I\u0303i.\nLstep2(I) = 1\nNp Np\u2211 i=1 \u2225I \u2032 i \u2212G(w+ E(I \u2032 i))\u22251 + \u03bb1\u2225E(I \u2032 i)\u22252 (2)\nStep 3 optimizes the L, V ,D andA networks according to Equation 3. It consists in part of Lstep1. The second part utilizes the projected samples from the last iteration of step\n1We refer to tables 5\u20107 of the original paper ([1]) for the exact architectures. 2Lp is a neural network trained to predict similarities between images [17] andLs is a term that encourages\nsmoothness of the resulting depth maps (as described in [18]). We refer to our code for the weights \u03bbi.\nReScience C 8.2 (#30) \u2013 Galatolo and Nilsson 2022 4\n2. For each projected sample v\u0303i = V (\u0303Ii), l\u0303i = L(\u0303Ii) is calculated. Combined with d and a from the original image, they can be used to reconstruct each projected sample from the components I\u0304 = \u03a6(v\u0303i, l\u0303i,d, a)).\nLstep3(I, I\u0304) = 1\nNp Np\u2211 i=1 [Lp(I, I\u0304i) + ||I\u2212 I\u0304i||1] + Lstep1(I, I\u0302) + \u03bb2Ls(D(I)) (3)\nStages. The steps are repeated for a number of stages. In each, the steps are trained for a different number of iterations (see Table 1 in subsection 5.5 in the appendix for details).\nNovel Shape Priors \u2014 The first novel prior we consider is a masked box. Using the mask returned by the parsing model developed by Zhao et al. [19] we extrude the relevant object from the background, in a step\u2010like manner. Improving on this idea, we also smooth the transition from the object to the background. This is done by using three 2D convolutions, where we convolve the masked box shape with a 11\u00d7 11 filter of ones. Renormalizing the convolved shape, we obtain Figure 2c denoted as \u2018smoothed box\u2019.\nThe last prior we tested is obtained by normalizing the score (or \u201cconfidence\u201d) that the parsing model gives to each pixel. We use this confidence to project the object, i.e. a pixel that is within the category with more confidence will be farther projected. This prior is similarly smoothed by convolutions and is denoted as \u2018confidence based\u2019.\nFigure 2 shows a visual representation of the prior shapes used for an example image taken from the CelebA dataset.\n3.2 Generalized Training Procedure Given the single\u2010use nature of the model obtainable with the original training proce\u2010 dure, we decided to develop an alternative training procedure to favor a general model M\u2217 usable for all images belonging to the same distribution as the training dataset D. We propose to pre\u2010train the depth netD on all images first, instead of repeating the pro\u2010 cess for each image. We also modify Step 1, 2 and 3 by greatly lessening the number of iterations given to a single image and breaking up the sequential training of the original method into a few iterations per example, and instead introducingNe epochs and batch training to compensate, increasing resource utilization and training speed. To facilitate\nReScience C 8.2 (#30) \u2013 Galatolo and Nilsson 2022 5\nunderstanding of our modifications to the training procedure, we provide a schematic in Figure 3. It can be compared to the original shown in Figure 1. Let us note that the\noriginal authors also briefly mention a \u2018joint training\u2019 that should improve the general\u2010 ization ability of the model, however, its performance is not properly reported and it only represents a mini\u2010batch extension of the pre\u2010training step.\n3.3 Datasets Weaimed to reproduce the authors\u2019 results on the LSUNCar, LSUNCat [2] and CelebA [3]. From these datasets, the authors selected a subset consisting of 10 images of cars, 216 images of cat faces, and 399 celebrity faces. Like the authors, we used RGB images of three color channels, resized to 128 \u00d7 128 pixel resolution. No further preprocessing was applied.\n3.4 Hyperparameters For replication purposes, the original hyperparameters by Pan et al. [1] were used, but we also tried tuning some parameters that we believe are key to the method: the num\u2010 ber of projected samples,Np, for each image and the number of epochs for pre\u2010training the depth network. Np was varied within {2, 4, 8, 16, 32}. In our tests we found the val\u2010 ues 4, 8 and 8, respectively for the LSUN Car, LSUN Cat and CelebA dataset, to be the threshold after which the improvements in image quality start greatly decreasing (see subsection 5.11 in Appendix for more details).\nThe number of epochs for the depth network pre\u2010training was varied within {100, 500, 1000, 2000}. This pre\u2010training affects how irregular the depth map predictions are. We believe that using a threshold for the loss to check the convergence would be preferable as the number of epochs selected by the authors (1000) is enough in most cases but not in all. We attribute irregularity in some of our results to this issue.\n3.5 Experimental setup and code For each dataset we run our implementation of the framework from Pan et al. [1] on the images that were selected by the authors, the procedure saves a checkpoint for each network. These checkpoints are later fed the original image to get the generated result. The evaluation of the results was only qualitative as all the datasets we explored do not have a ground truth for comparison. We instead relied on a manual evaluation.\nReScience C 8.2 (#30) \u2013 Galatolo and Nilsson 2022 6\nOur code is available at https://github.com/alessioGalatolo/GAN-2D-to-3D. Our results are avail\u2010 able interactively under the docs folder and at alessiogalatolo.github.io/GAN\u20102D\u2010to\u20103D/.\n3.6 Computational requirements Most of the experiments we ran were on a Intel(R) Xeon(R) CPU @ 2.20GHz with 2 cores available and a Nvidia Tesla P100\u2010PCIE\u201016GB. Since the framework described by Pan et al. [1] is instance\u2010specific, we report the average time for completing the projection of a single image: 96m and 28s for an image in the CelebA dataset, 95m and 43s for a LSUN Cat image and 74m and 32s for a LSUN Car image.\n4 Results\nThe model correctly learned the shape and the texture of many images, although some examples were less successful than others. For example, the model converged to be\u2010 lievable shapes for two of the cars in Figure 4, but the shape of the right\u2010most car is debatable.\nIn the following sections we show the reconstructed depth map and 3D projection of some images chosen as representative of the dataset. All of the images that follow have the background cut from the actual object, this was only done for ease of illustration and was not done for the actual training process since the original authors do not mask the background in all cases. It is also difficult to illustrate the results fairly in 2D images, so we invite the reader to visit our website with interactive 3D plots3.\n4.1 Results reproducing the original paper\nLSUN Car \u2014We present the results on LSUN Car dataset in Figure 4. Most features are projected in the correct direction and show details that are correctly outward projected from themain object. This result supports all the claimsmade in section 2 as we did not use any annotation or assumption for the images, many details were retrieved with high precision using the StyleGAN knowledge and we were able to easily make a rotation of the image (see interactive web\u2010page).\nLSUN Cat \u2014 The second experiment was conducted on the LSUN Cat dataset. The results were slightly poorer compared to the LSUNCar dataset. The face of the cats gets properly recognized, but some details like the nose are not protruded from the rest of the face and are generally on the same plane, see Figure 4. Some images present some irregularities in the form of spikes and hills (d). The rotation (f) does not result in a completely natural\n3alessiogalatolo.github.io/GAN\u20102D\u2010to\u20103D/\nReScience C 8.2 (#30) \u2013 Galatolo and Nilsson 2022 7\nimage as part of the face of the cat appears on the same plane. This experiment does not support claims 2 and 4 in some cases (e.g. figures 4 (d) and (f) negate claims 2 and 4 respectively) while it does for claims 1 and 3 (section 2).\nCelebA \u2014 The third experiment conducted on the CelebA dataset shows that most of the face are correctly portrayed with the only exception of the border of the face e.g. chin and forehead that sometimes is not included in the projection, see Figure 5 (b). Also we found out that themethod does not behave well with faces that are viewed from the side, see Figure 5 (c), where the face still gets a projection as it was viewed from the front. As a consequence of this, the rotation of side faces does not result in a good image. This experiment supports claims 1\u20104 (section 2) only for some faces and claims 1 and 3 for those viewed from the side.\n4.2 Results beyond the original paper\nThe effects of shape priors \u2014 The original paper did not specify the exact reasons for choos\u2010 ing an ellipsoid prior for the pre\u2010training of the depth net, therefore we decided to ex\u2010 periment with multiple prior shapes as well as no prior shape.\nNo prior. With the goal of assessing the results of this method when no prior shape is given, we ran a test on one image from the LSUN Car dataset without any prior pre\u2010 training, and with random initialization. The reconstruction objective is still satisfied very well, but it has converged to an extremely noisy depth map (see Figure 9 in subsec\u2010 tion 5.6 in the appendix). This briefly shows that this method would not work without a strong shape prior to guide it towards a reasonable shape.\nSmoothed Box Prior. The first extention experiment was done by testing the first of the prior shapes we proposed, the smoothed box prior. Figure 6 shows the smoothed box prior tested on the LSUN Cat and CelebA dataset where it can be seen how it is better at understanding the structure of the nose and face in general.\nConfidence\u2010BasedPrior. Another experimentweperformed focusedon theperformance of the second prior we presented, the confidence based prior. Figure 7 shows some re\u2010 sults on the datasets considered in this paper. The results are most promising in the CelebA dataset where the image of a face is correctly projected even if viewed from the side.\nReScience C 8.2 (#30) \u2013 Galatolo and Nilsson 2022 8\nGeneralized Training Procedure \u2014We demonstrate the results of our new training loop on LSUN Cat. We note again that the difference to the previous demonstration on LSUN Cat, is that a single network D\u2217 was used to predict all of the images, as opposed to a different network Di for each image Ii. The general model was trained on a limited subset of 30 images from LSUN Cat. It was trained for a modest 60 epochs which results in approximately 60% of the weight updates per image of the original method. Figure 8 shows the projection of some images from the LSUN Cat dataset. One can observe that the method recognizes the general structure of the cat\u2019s face but also presents some artefacts in some specific parts of the face e.g. the second cat\u2019s cheek is further projected than where it should and similarly for the third cat\u2019s chin.\nImproved initialization \u2014 Our final experiment is inspired by the observation of the depen\u2010 dency of the method to the number of pseudo\u2010samples Np, and the variability that fol\u2010 lows in the results depending on their quality, as discussed in subsubsection 5.3.1. We experimentwith drastically increasing this number from16 to 128 for 10 short epochs, in which each training step is performed only once. We observemarginal improvements in the predicted shape (Figure 8) and larger improvements in the smaller details/features. See the subsection 5.10 in the appendix for further detail.\nTraining step 1 was not changed and it is allowed to converge in the first stage, as it does not involve the projected samples. See Table 2 in the appendix for an exact description of the number of iterations. All other parameters were left as in subsubsection 4.2.1, with the smoothed box prior. We experimented with two of the worst performers from the LSUN Cat dataset to evaluate whether this method could improve the results, see Figure 16. We applied the same idea to the general model described in sections 3.2, 4.2.2 and saw improvements, see Figure 8. The results can be compared to Figure 14.\nReScience C 8.2 (#30) \u2013 Galatolo and Nilsson 2022 9\n5 Discussion\n5.1 What was easy The authors provide a clear specification of the Python package dependencies, as well as other dependencies. Additionally, they provide scripts for easy downloading of a select few datasets and pre\u2010trained model weights. They precisely state how to execute the training script and how to run themodel for evaluation. Note that this refers to running the original code and that modifying and extending the code brought many difficulties, as explained in the next section.\n5.2 What was difficult The paper by Pan et al. [1] did not contain enough information for a successful reimple\u2010 mentation. Many details had to be discerned or guessed from their code. Furthermore, the quality of said code does not allow for a quick interpretation. For example, deduc\u2010 ing the training loop and the number of iterations for each stepwas further complicated by the poor cohesion of the original code: the trainer script was heavily mingled with model class, using class members of the model object to increment training steps and nested function calls back and forth between the trainer and model classes.\nThe components v, l, d and a were not enough to pass in to the neural renderer to re\u2010 construct an image. In reality, several calculations of quantities such as diffuse shading and texture were needed to be fed into the neural renderer, using concepts from light transport theory that were not mentioned in the paper.\nAnother difficulty was the heavy reliance on external pre\u2010trained neural networks. The neural renderer [16], in particular, posed several problems. The major one was incom\u2010 patibility withWindows machines. To be able to develop on our personal machines, we had to make manual edits of the neural renderer script and different CUDA files.\nAnother challenge with this method is the lack of objective quantitative metrics to eval\u2010 uate the success of the models. One instead has to rely almost entirely on qualitatively gauging the shape reconstructions by eye.\nReScience C 8.2 (#30) \u2013 Galatolo and Nilsson 2022 10\n5.3 Conclusions\nVariability of the results \u2014We observed that themethod is very sensitive to various random factors and identical runs may yield different results, see Figure 12. One factor may be the random initialization of the networks, but we do not believe it is the dominating factor, since the depth network is pre\u2010trained on a fixed prior shape each run. Rather, as mentioned by the authors [1], the quality of the projected samples varies. Additionally, we only sample 8 \u2212 16 different view\u2010light directions in each step 2 iteration, which may be too few projected samples for a robust model. Since this sampling is random, increasing the number of samples should assure the inclusion of meaningful view and light projections (experimental backing in subsection 5.7 in the appendix).\nCatastrophic forgetting \u2014We have observed that the instance\u2010specific model forgets the previous training images (see subsection 5.8 in the appendix, Figure 13), and thus has no generalization capability. This is not necessarily a problem if one has time and com\u2010 putational resources. It can also be argued that this is exactly what is intended with this model, and that generalization is up to the training dataset of the StyleGAN model. It does, however, limit the usefulness of the model. As an example, the training time for one 128\u00d7 128 pixel RGB image using a Tesla K80 GPU was about 2.5 hours, which seems exceedingly costly for just one low\u2010resolution depth map. We argue that a generalmodel would have more use. The ideal scenario would be a modelD\u2217 trained onD that is able to accurately predict di = D\u2217(Ii) \u2200 Ii \u2208 D, and even extend to unseen testing data be\u2010 longing to the same distribution as D. This discussion is what urged us to explore the altered training procedure of sections 3.2 and 4.2.2.\nFinal conclusions \u2014We were able to replicate some of the results of Pan et al. [1] on the datasets LSUN Car, LSUN Cat and CelebA. We identified several failure modes and lim\u2010 itations of the model, and back it up with experimental evidence. Examples are the variability and sensitivity to the projected samples, the heavy dependence on shape pri\u2010 ors and the computational costliness of the single\u2010use model \u2010 all of which were not adequately accounted for in the original paper.\nWe propose a new prior shape, the smoothed box prior, that has shown very promising results especially for fine details and complex object structures. We propose a second prior shape, confidence\u2010based, that has shown best results in the face dataset. We fi\u2010 nally suggest two new training procedures that produce better results and are better at generalizing than the original model by Pan et al. [1].\nWe recognize the limitations of this work as we were only able (due to the restricted computational power) to test the method on part of the dataset. For example, the Cat\u2019s dataset used by the authors contains more than 200 images but we were able to only test few of them. We speculate that some images in the dataset could yield better results than those reported here. However, we believe that few bad projected images should be enough to claim the uneffectiveness of the method at least in some particular cases.\nAnother limitation of our work is the lack of quantitative evaluation methods. The orig\u2010 inal authors propose their results also on the BFM benchmark [20] where it is possible to use some metrics to accurately evaluate the results.\n5.4 Future work We speculate that it would be interesting to adapt the same method to StyleGAN3 [7] where the network has been modified to support training with fewer samples, leaving the question if thenetwork still retains enough information that is needed forGAN2Shape to work. Future work could also explore the use of our priors on datasets where the orig\u2010 inal method failed (e.g. the LSUN Horse dataset). We speculate that, since our prior\nReScience C 8.2 (#30) \u2013 Galatolo and Nilsson 2022 11\ncaptures the boundaries of the object very well (compared to the ellipsoid where the boundaries are only used to position the origin), it could achieve better results in com\u2010 plex 3D objects where the shape cannot be simplified into an ellipse. A limitation of this method is that it does not use voxels, but learns a height map. This disallows realistic shape reconstructions and more complex geometries with multiple x and y values for each z value etc. Future work should investigate whether this model could be extended to predict voxels instead of height maps. Given our promising results with the general\u2010 izing trainer, which was obtained through only a few epochs of training, we believe that it should be further explored with increased epochs and training set size."}], "title": "[Re] Replicating and Improving GAN2Shape Through Novel Shape Priors and Training Steps", "year": 2022}