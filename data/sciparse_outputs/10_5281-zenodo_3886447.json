{"abstractText": "The article that is the subject of this reproduction attempt was my first publication for which I provided code as supplementarymaterial [1]. In computational biophysics, that was uncommon back in 2008, and unfortunately it remains the exception until today. The code consisted of two Python scripts plus instructions and was meant to help read\u2010 ers with applying the methods discussed in the article to different proteins, rather than ensure reproducibility which at that time I had never heard of. This explains why the supplied code does not reproduce the figures shown in the paper, but merely produces data files in plain text format, from which the plots were originally assembled by hand and by additional scripts that I did not consider worthy of publication.", "authors": [{"affiliations": [], "name": "Konrad Hinsen"}, {"affiliations": [], "name": "Marc Andr\u00e9 Delsuc"}], "id": "SP:c7adfe75745617b10e5ddf1492fc79dd2870c724", "references": [{"authors": ["K. Hinsen"], "title": "Structural Flexibility in Proteins: Impact of the Crystal Environment.", "venue": "In:Bioinformatics", "year": 2008}, {"authors": ["K. Hinsen"], "title": "The Molecular Modeling Toolkit: A New Approach to Molecular Simulations.", "venue": "J Comput Chem", "year": 2000}, {"authors": ["T.E. Oliphant"], "title": "A Guide to NumPy", "venue": "Trelgol Publishing,", "year": 2006}, {"authors": ["G. Ewing"], "title": "Pyrex", "venue": "https://web.archive.org/web/20200216002742/https://www.cosc.canterbury.ac.nz/greg.ewing/python/Pyrex/.", "year": 2010}, {"authors": ["S. Behnel", "R. Bradshaw", "C. Citro", "L. Dalcin", "D.S. Seljebotn", "K. Smith"], "title": "Cython: The Best of Both Worlds.", "venue": "Computing in Science Engineering", "year": 2011}, {"authors": ["L. Court\u00e8"], "title": "and R", "venue": "Wurmus. \u201cReproducible and User-Controlled Software Environments in HPC with Guix.\u201d In: European Conference on Parallel Processing. Springer,", "year": 2015}, {"authors": ["R. Wurmus", "B. Uyar", "B. Osberg", "V. Franke", "A. Gosdschan", "K. Wreczycka", "J. Ronen", "A. Akalin"], "title": "PiGx: Reproducible Genomics Analysis Pipelines with GNU Guix.", "venue": "GigaScience", "year": 2018}, {"authors": ["wwPDB consortium"], "title": "Protein Data Bank: The Single Global Archive for 3D Macromolecular Structure Data.", "venue": "Nucleic Acids Research 47.D1 (Jan. 2019),", "year": 2019}, {"authors": ["T. Williams", "C. Kelley"], "title": "Gnuplot", "venue": "http://www.gnuplot.info/.", "year": 2020}, {"authors": ["W.B. Arthur"], "title": "The Nature of Technology: What It Is and How It Evolves", "venue": "OCLC: 695688782. New York, NY: Free Press,", "year": 2009}, {"authors": ["K. Hinsen"], "title": "Dealing With Software Collapse.", "venue": "In: Comput. Sci. Eng", "year": 2019}, {"authors": ["N.P. Rougier"], "title": "Rp] Loupe.", "venue": "ReScience C (under review) (Dec. 2019)", "year": 2019}, {"authors": ["P. Greenfield"], "title": "J", "venue": "T. Miller, J.-c. Hsu, and R. L. White. \u201cNumarray: A New Scientific Array Package for Python.\u201d In: PyCon DC.", "year": 2003}, {"authors": ["R. Gommers"], "title": "NEP 23 \u2014 Backwards Compatibility and Deprecation Policy", "venue": "https://web.archive.org/web/20191007231112/https://numpy.org/neps/nep-0023-backwards-compatibility.html.", "year": 2018}], "sections": [{"text": "R E S C I E N C E C Reproduction / Biophysics\n[Rp] Structural flexibility in proteins - impact of the crystal environment\nKonrad Hinsen1,2, ID 1Centre de Biophysique Mol\u00e9culaire, CNRS UPR4301, Orl\u00e9ans, France \u2013 2Synchrotron SOLEIL, Division Exp\u00e9riences, Gif sur Yvette, France\nEdited by Marc Andr\u00e9 Delsuc ID\nReviewed by Fran\u00e7ois-Xavier Coudert ID\nReceived 30 January 2020\nPublished 09 June 2020\nDOI 10.5281/zenodo.3886447"}, {"heading": "Introduction", "text": "The article that is the subject of this reproduction attempt was my first publication for which I provided code as supplementarymaterial [1]. In computational biophysics, that was uncommon back in 2008, and unfortunately it remains the exception until today. The code consisted of two Python scripts plus instructions and was meant to help read\u2010 ers with applying the methods discussed in the article to different proteins, rather than ensure reproducibility which at that time I had never heard of. This explains why the supplied code does not reproduce the figures shown in the paper, but merely produces data files in plain text format, from which the plots were originally assembled by hand and by additional scripts that I did not consider worthy of publication."}, {"heading": "Historical context", "text": "The work described in the original article was performed in summer 2007. The scripts make heavy use of two libraries of which I am the principal author: the Molecular Mod\u2010 eling Toolkit (MMTK) [2], then at version 2.5, and ScientificPython, then at version 2.7. These two libraries, first published in 1997, are among the oldest scientific computing packages in the SciPy ecosystem. They were initially written on the basis of Numerical Python [3], the original array package for Python that was published in 1995. With the transition of the ecosystem to its successor NumPy [4], initially released in 2006, I ported MMTK and ScientificPython using NumPy\u2019s compatibility module called oldnumeric. This combinationwas used for the original work, but unfortunately I did not write down the exact version of NumPy. My instructions recommend \u201cNumerical Python 23.8.2 or NumPy 1.x\u201d, which turned out to be overly optimistic: NumPy is still in the 1.x version range, but frequent breaking changes make it impossible to run my code with recent NumPy releases. In fact, with version 1.9 NumPy removed the oldnumeric interface, breaking all of my molecular simulation code. I had envisaged to port the code to the official NumPy API, but decided not to do so because of the high risk of introducing errors. There are in fact several subtle changes in the API, such as using the transpose of a matrix compared to the original Numeric API, which are easy to overlook because incorrect use yields an incorrect result but no error message. With the end of support\nCopyright \u00a9 2020 K. Hinsen, released under a Creative Commons Attribution 4.0 International license. Correspondence should be addressed to Konrad Hinsen (konrad.hinsen@cnrs.fr) The authors have declared that no competing interests exists. Code is available at https://github.com/khinsen/rescience-ten-year-challenge-paper-3. \u2013 SWH swh:1:dir:8a73b8438b411cc8a58a3db7789ff45690808afa. Open peer review is available at https://github.com/ReScience/submissions/issues/14.\nReScience C 6.1 (#5) \u2013 Hinsen 2020 1\nfor Python 2, there is no longer any point in such a change, as it makes little difference if my code depends on one or two unmaintained software packages. The remaining dependencies turned out to be less critical. netCDF, which my scripts use for data storage, was at version 3.4, but any more recent version can be substituted. Python itself, then at version 2.5, caused no problems either up to release 2.7. Python 3 is frommy point of view a different language, the two major changes for scientific com\u2010 puting being a very different C API layer and a different definition of division. Porting would involve a significant amount of work because of the large number of extension modules in MMTK, most of which are C code hand\u2010written for the C API of Python 1.4, long before Pyrex [5] and then its fork Cython [6] introduced more convenient ways to write extension modules."}, {"heading": "Reproduction", "text": "The GitHub repository for this reproduction contains all the code, input data, and in\u2010 structions for running the reproduction on a modern GNU/Linux system with the Guix package manager [7, 8]. It also lists the exact version numbers of all the software in\u2010 volved in the reproduction. For the numerical calculations alone, i.e. excluding the less critical tasks of downloading files and generating plots, a total of 254 Guix packagemust be rebuilt identically to guarantee bit\u2010for\u2010bit reproduction of the results.\nFinding the code and the input data The project\u2010specific scripts are still available for download from the journal\u2019sWeb site. A copy can be found in the directory scripts-from-suplementary-material-to-original-paper of the code repository for this article. The directory updated-scripts-from-obsolete-web-server contains updated versions of the scripts, added for completeness. They were available from my laboratory\u2019s Web site for a few years, and contain minor improvements that are not relevant for this reproduction attempt, see the file notes.org for details. In the following, I use the original scripts. All the dependencies have been available in public version\u2010controlled repositories for many years. Obtaining the published code is therefore not a problem. The additional scripts that generated the original plots were not published as explained in the intro\u2010 duction. They have not survived two changes of computers. I still have backups that should contain them, but I have not been able to find a working reader for the DAT/DDS tapes on which these backups are stored. Another backup on a CD\u2010ROM turned out to be unreadable. The input data for reproducing the figures in the original paper consists of three protein structures from the Protein Data Bank (PDB) [9]. The PDB updates its files from time to time. The file for a specifc entry is intended to represent the original data deposited by its authors, but may be modified to conform to newer versions of the file format, or to fix technical mistakes. There is thus no guarantee that a file downloaded today is the same as in 2008, but the scientific information it contains is supposed to stay the same. In practice, PDB updates have occasionally broken analyses of the annotations they contain, but not analyses of the original experimental data.\nRunning the scripts using the Guix package manager Mypreferred softwaremanagement tool for reproducible computations is the Guix pack\u2010 age manager [7], which permits the bit\u2010for\u2010bit reconstruction of software environments at any later time. All the required dependencies have already been packaged in Guix, but since Guix only started in 2012, and the Python ecosystem was added even later, the original software versions of 2008 are not available.\nReScience C 6.1 (#5) \u2013 Hinsen 2020 2\nFigure 1. Blank figure included for aligning the figure numbers in this reproduction with the figure numbers in the original paper.\nThe two scripts are meant to be run in sequence, once for each protein structure and crystal size. The first script, calculate_crystal_fluctuations.py, computes the normal modes of the crystal and stores them in a netCDF file. The second script, analyze_crystal_fluctuations.py extracts the relevant data for the plots from the netCDF file and writes them to text files. Using the dependencies as defined in Guix in January 2020 (more precisely: commit 7357b3d7a52eb5db1674012c50d308d792741c48), the first script runswithout any apparent problem, but the second one crashes with an error message. This is the con\u2010 sequence of a breaking change in NumPy which modified the rules for the conversion of sequence\u2010like Python objects into arrays. This problem can be fixed by changing a single line of code; however, neither the diagnosis nor the correction are likely to be obvious to someone who is not intimately familiar with NumPy. I explored the possibility of using an earlier NumPy release in order to run the scripts unmodified. From the release history that is available on NumPy\u2019s GitHub repository, it appears that release 1.0.4 was current at the time of submission of my paper in late 2007. Unfortunately, this NumPy release cannot be installed with Python 2.7 because NumPy is distributed with a modified version of the distutils package. distutils reads a configuration file produced during the installation of Python, whose format has changed between Python 2.5 and 2.7. I briefly envisaged installing Python 2.5, but that would have required backporting the modifications made for Guix, which seemed an unreasonable effort for performing a simple test. In addition to the fix described above, I modified my scripts for convenience, making them read their originally hard\u2010coded input parameters from command line arguments instead. In fact, the scripts I had used for the original work also accepted command line arguments, but I had hard\u2010coded them in the published versions in order to simplify the usage instructions."}, {"heading": "Reproducing the figures", "text": "As explained in the introduction, my goal with publishing the code of my computations was to enable reuse, not reproducibility. The code therefore does not reproduce the full figures, which need to be re\u2010generated by hand from the numerical output. I have limited myself to producing figures that are similar enough to the originals to convince the reader that the results have been reproduced correctly. I also used different plotting software, Gnuplot [10], in replacement of the originally used Grace [11] which has not yet been packaged for Guix. Figure 1 of the original publication does not contain any computed data and has there\u2010 fore not been reproduced. The data it shows comes directly from the Protein Data Bank. Figures 2 and 3 of the original publication compare \u201csingle molecule\u201d to \u201ccrystal\u201d results for two proteins. Only the latter are computed by the scripts in the supplementary ma\u2010 terial, and are shown in Figs. 2 and 3. The \u201csingle molecule\u201d curves were obtained by a script that was not published, andwhich I have lost. It would not be difficult to replicate, but that is not the goal of this reproduction attempt. Figure 4 of the original publication shows the dispersion relations for four different di\u2010 rections of wave propagation in the crystal. Unfortunately, the supplied scripts only produce a single file with points on the dispersion plots from all directions combined. The additional script required to separate these points by direction was not published and has been lost. The dashed lines labelled \u201celastic medium\u201d were also computed by unpublished and lost scripts. Fig. 4 shows unconnected points that each lie on one of the drawn\u2010out lines of the original Figure 4.\nReScience C 6.1 (#5) \u2013 Hinsen 2020 3\nReScience C 6.1 (#5) \u2013 Hinsen 2020 4\nFigure 5 of the original publication is reproduced almost entirely in Fig. 5. One curve from the original plots, labelled \u201cextrapolation\u201d, is missing because the script used for doing the extrapolation was not published and has been lost. Finally, Figure 6 of the original publication has not been reproduced. It contains one curve each from Figs. 2 and 3, combined with experimental data from the Protein Data Bank and a scaled curve using a scaling factor computed by yet another script that was not published and has been lost."}, {"heading": "Conclusion", "text": "The goal of this reproduction attempt was to answer three questions:\n1. Can the code published in 2008 still be run today?\n2. Does it produce results equivalent to those shown in the original figures?\n3. Does the code fully reproduce the original results?\nMy answers are\n1. Yes, but only using obsolete versions of dependencies and after modification due to a breaking change in one of them.\n2. Yes.\n3. No, because not all the required code was published, and the unpublished code has been lost in the meantime.\nThe obvious lesson for the future to draw from this exercise is the importance of pub\u2010 lishing all the code, up to the automatic generation of figures and tables. Another re\u2010 grettable omission I made in 2008 is not writing down the precise version numbers of all the code involved. It might have been useful in this case to know the precise version\nReScience C 6.1 (#5) \u2013 Hinsen 2020 5\nReScience C 6.1 (#5) \u2013 Hinsen 2020 6\nof NumPy used in the original work. In fact, my claim that the modification to a script was required as a consequence of a breaking change in NumPy is based merely on my memories and notes from other projects in which I had to make similar changes. The final lesson would have to be drawn by a wider community of scientific software developers: breaking changes in widely used infrastructure code such as NumPy can cause a lot of damage in terms of lost reproducibility that may be difficult to diagnose and fix for someone else than the original authors. The open question that the scientific community has to figure out is where to place reproducibility on our scale of values, and for which time spans we consider it desirable. Structural biology is a methodologically mature domain of research, in which the main source of progress is not disruptive new methods, but incremental improvements of existing methods in the course of ongoing applications to biologically relevant systems. This is in fact the norm in science and technology [12]. In this context, my methodological work published in 2008 is by no means outdated. The last time I have referred to it myself (in so\u2010far unpublished work) was in 2018. Reproducibility lifetimes of just a few years, corresponding to the current habits in the scientific Python ecosystem, are therefore problematic.\nThe two pieces of software whose evolution has most impacted this work, Python and NumPy, deserve a more detailed discussion because of their importance in today\u2019s sci\u2010 entific computing. In the 12 years since the publication of my original paper, both have changed in a way that breaks compatibility with earlier version, causing software col\u2010 lapse [13] that has affected not just my two scripts, but much scientific software. How\u2010 ever, the stories of the two pieces of software are also very different. The Python language, first published in 1991, has evolved with no major compatibil\u2010 ity issues up to its version 2.7, released in 2009 and maintained for another ten years [14]. This was largely the result of an unwritten policy decided by Python\u2019s \u201cbenevolent dictator for life\u201d, Guido van Rossum, who had the last word on all strategic decisions. However, new requirements due to changes in computing technology, in particular full Unicode support, turned out to be impossible to implementwithout breaking changes to the language. Van Rossum decided to make a one\u2010time major revision of his language, both to address such fundamental issues and a large number of minor ones that had accumulated over time. The new language, Python 3, was first published in 2008 and was supposed to co\u2010exist with Python 2 for a transition time of one decade. In retro\u2010 spect, van Rossum and his core developer team seriously underestimated the impact of this decision, which was unprecedented and turned out to become themost destructive event in the history of Open Source software. The Python development team has officially ended support for Python 2.7 at the end of 2019, and its future is unclear at this time. Without maintenance, Python 2.7 is likely to remain usable for a few more years, before changes in operating systems, compilers, and Python\u2019s own dependencies will require adaptation. As noted by Rougier [15], \u201cthis end of life might be a good thing for science because we now have at our disposal an advanced programming language that is guaranteed not to evolve anymore (i.e. Python 2.7)\u201d. An important open question is whether someone else will take over maintenance of Python 2, which is always a possibility with Open Source software. The importance of Python 2 in the commercial sector (e.g. banks) makes this an economically viable pos\u2010 sibility. Another important open question is how software distributions (Linux distri\u2010 butions but also platform\u2010neutral distributions such as Anaconda [16]) will react. They have an interest in removing Python 2 in order to reduce their own workload, and they may also do so for fear of security issues that won\u2019t be fixed any more. On the other hand, if user demand for software dependent on Python 2 is large enough, they may work out a compromise (such as offering Python 2 without networking modules), or take up minimal maintenance of Python 2 themselves. NumPy was first published in 2006 as the merger of its direct predecessor, Numeric [3], and an alternative array implementation called numarray that was optimized for large\nReScience C 6.1 (#5) \u2013 Hinsen 2020 7\ndatasets [17]. Unlike Python, it has never had a clearly defined policy on backwards com\u2010 patibility, and many breaking changes have happened in between the initial version 1.0 and today\u2019s version 1.18. It also differs from Python in having a much narrower appli\u2010 cation domain and thus a much smaller developer community. In 2018, an explicit pol\u2010 icy about backwards compatibility was proposed as NumPy Enhancement Proposal #23 [18], which has so far been neither accepted nor rejected. If accepted, it would increase code stability because it is stricter in many ways than the ad hoc decisions made in the past. However, the proposed policy states that \u201cBackwards incompatible changes can be made, provided the benefits outweigh the costs\u201d, where the major cost is breaking user code. The proposal adds that \u201cBenefits include improved functionality, usability and performance (in order of importance), as well as lower maintenance cost and improved future extensibility.\u201d The decision whether benefits outweigh the costs, like all major decisions concerning NumPy, is made by \u201cconsensus of all interested contributors\u201d [19]. While contributors are in a good position to judge the benefits, there is little reason to believe that they can fully appreciate the costs, which are borne by the users. Given the small size of the development team and the large user base, there is not much reason either to believe that the former group is representative for the latter one. This reproducibility challenge highlights another issue that Open Source communities seem to be largely unaware of. What they consider their users are people interacting directly with their software. For NumPy, that\u2019s people who write Python code starting with import numpy. Developers silently expect such users to maintain their software, and in particular adapt it to breaking changes in dependencies. Whether or not this is a reasonable expectation is debatable. Making it silently certainly isn\u2019t. But more im\u2010 portantly, for infrastructure software such as NumPy, there is a vastly larger group of second or higher degree users, which includes scientists who want to build on the code that I published in 2008 withmy original paper. Such users are unlikely to have the tech\u2010 nical competence and the willingness to do software archaeology, i.e. read the release notes of all the dependencies of the software they use. A first step to help such users would be clearer communication about breaking changes. In 2017, I made a request to the NumPy community to adopt semantic versioning in order to permit packagers, ap\u2010 plication developers, and end users to identify backwards\u2010incompatible changes more easily. After a heated discussion [20], the request was rejected, the main arguments be\u2010 ing that (1) semantic versioning is not common in the Python ecosystem, (2) it would cause version numbers to increase too rapidly.\nUltimately, the increasingly common phenomenon of software collapse in scientific computing is due to a lack of coordination between the needs of developers and users, which in turn is a consequence of the failure of the scientific community to take care of its software infrastructure. Young scientists and engineers join Open Source com\u2010 munities, mostly as unpaid volunteers, to improve software for the use cases they have encountered in their own work. Maintaining infrastructure over longer time spans for a wide user base requires dedicated personnel under the supervision of experienced research software engineers. It requires institutions that have infrastructure mainte\u2010 nance as their main objective. As long as such institutions do not exist, computational scientists will have to work around software collapse if the software they use evolves on a faster time scale than their scientific methods, as I have shown in this reproduction work using Guix."}], "title": "[Rp] Structural flexibility in proteins - impact of the crystal environment", "year": 2020}