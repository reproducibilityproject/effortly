{"abstractText": "Understanding the visual cortex and how patterns in images are interpreted in the brain has long been a goal of computational neuroscience. This article re-examines an early paper in the development of this field by Hancock et al. 1991 [1]. The aim of their paper is to use neural networks to perform Principal Component Analysis (PCA) on images. This technique finds a small set of linear descriptors that convey the most information about a set of images. The extremely high dimensional data from the images is projected onto a much smaller number of orthogonal axes which produce the most variation amongst the images. To do this exactly requires finding the eigenvectors of the correlation matrix of the input data. However this matrix was considered too large at the time (1991) to compute. The paper applies neural network methods to solve this problem in a viable number of computations. Sanger s\u0313 algorithm [2] is used to produce receptive fields that converge to the principal components (PCs). Hancock et al. showed that the receptive fields can detect features such as orientation in natural images and gaps between words in text images. The simplest of these receptive fields have also been found experimentally in the V1 cells of cats [3].", "authors": [{"affiliations": [], "name": "Iain Davies"}, {"affiliations": [], "name": "Stephen Eglen"}, {"affiliations": [], "name": "Tiziano Zito"}, {"affiliations": [], "name": "Pietro Berkes"}, {"affiliations": [], "name": "Christian Jarvers"}], "id": "SP:ba154637868315139ba6e4fae7be8248fac4536e", "references": [{"authors": ["P.J.B. Hancock", "R.J. Baddeley", "L.S. Smith"], "title": "The principal components of natural images.", "venue": "In: Network: Computation in Neural Systems", "year": 1991}, {"authors": ["T.D. Sanger"], "title": "Optimal unsupervised learning in a single-layer linear feedforward neural network.", "venue": "In:Neural Netw. 2.6 (Jan", "year": 1989}, {"authors": ["D.H. Hubel", "T.N. Wiesel"], "title": "Receptive fields, binocular interaction and functional architecture in the cat\u2019s visual cortex.", "venue": "In: J. Physiol", "year": 1962}], "sections": [{"text": "R E S C I E N C E C"}, {"heading": "Replication / Computational Neuroscience", "text": "[Re] The principal components of natural images\nIain Davies1, ID and Stephen Eglen1, ID 1Department of Applied Mathematics and Theoretical Physics, Cambridge University, Cambridge, UK\nEdited by Tiziano Zito ID\nReviewed by Pietro Berkes\nChristian Jarvers ID\nReceived 17 July 2020\nPublished 28 October 2020\nDOI 10.5281/zenodo.4139359\n1 Introduction\nUnderstanding the visual cortex and how patterns in images are interpreted in the brain has long been a goal of computational neuroscience. This article re-examines an early paper in the development of this field by Hancock et al. 1991 [1]. The aim of their paper is to use neural networks to perform Principal Component Analysis (PCA) on images. This technique finds a small set of linear descriptors that convey the most information about a set of images. The extremely high dimensional data from the images is projected onto a much smaller number of orthogonal axes which produce the most variation amongst the images. To do this exactly requires finding the eigenvectors of the correlation matrix of the input data. However this matrix was considered too large at the time (1991) to compute. The paper applies neural network methods to solve this problem in a viable number of computations. Sanger s\u0313 algorithm [2] is used to produce receptive fields that converge to the principal components (PCs). Hancock et al. showed that the receptive fields can detect features such as orientation in natural images and gaps between words in text images. The simplest of these receptive fields have also been found experimentally in the V1 cells of cats [3].\n2 Methods\n2.1 Sanger\u2019s Algorithm Sanger s\u0313 algorithm [2] trains a neural network so that the principal components of input data are encoded into the strength of the network connections. Suppose we want to find the first M PCs of N dimensional input data. Then we create a network with N input units and M output units and define wij to be the strength of the connection between the ith input unit and the jth output unit. For a sample of input data x = (x1, ..., xN ) the output is defined to be:\nyj = N\u2211 i=1 xiwij\nThe connection strengths are then updated according to Sanger s\u0313 rule:\n\u25b3wij = \u03b7yj(xi \u2212 j\u2211\nk=1\nykwik)\nCopyright \u00a9 2020 I. Davies and S. Eglen, released under a Creative Commons Attribution 4.0 International license. Correspondence should be addressed to Iain Davies (id318@cam.c.uk) The authors have declared that no competing interests exist. Code is available at https://github.com/IainDaviesMaths/Reproduction-Hancock.git \u2013 DOI 10.5281/zenodo.4139727.. Open peer review is available at https://github.com/ReScience/submissions/issues/45.\nReScience C 6.3 (#6) \u2013 Davies and Eglen 2020 1\nHere \u03b7 is the learning rate, chosen by the implementer. The algorithm is repeated with a new sample of input data x = (x1, ..., xN ). For fixed j the weight vector wij can be shown to converge to the eigenvector of the correlation matrix of all the input data with jth largest eigenvalue [2]. This is the jth principal component of the data.\ngreyscale images and the text images are 750x1125 pixel greyscale images. Each pixel has a value from 0 to 255 corresponding to its grey level 64x64 pixel samples were taken by choosing a random image and a random area from it. Themean grey level was calculated from 20000 such samples. Sample input data was then prepared by taking a 64x64 pixel random sample, subtracting the mean grey level and then multiplying component wise by a Gaussian window of standard deviation 10 pixels centred at the centre of the image. This last step was done to reduce edge effects. The resulting 64x64 matrix was reshaped into a 1x4096 vector and normalised to unit length. This vector was then used as x in Sanger s\u0313 algorithm above. Following the method of [1], the network strengths wij were initialised to random values between \u00b10.03. \u03b7 was initially set at 1 and reduced by a factor of a half every 20000 input samples for a total of 120000 samples.\n2.3 Description of Experiments Hancock et al. produce several sets of results studying different parts of their model which are reproduced below.\nNatural Image Experiments \u2014 Firstly in their Figure 2 they extract the first 15 PCs from a run with 15 natural images. They display these as receptive fields - 64x64 pixel squares with grey level representing the strength of the connection to the input unit in that position. They repeat this using 40 natural images in their Figure 3 and note the similarity of the principal components. Their next experiment is on orientation (Figure 4 in [1]). They aim to show that the orientation of the PCs is determined by the orientation of the images and not due to edge effects from the square samples. They complete a run with the original 15 images rotated by 45 degrees and extract the first 6 PCs, showing that they are indeed rotated as expected. The variance experiment is given in Figure 5 in [1]. This aims to verify that the PCs are ordered by how much information they convey. They plot a log-log plot of the output variances of thefirst 15 PCs of the 15 image set (denotedby circles) andof the 40 image set (crosses). For each data point 10000 samples were used to calculate the output variance. Next Hancock et al. investigate the effect of scale on the PCs of natural images. They repeat the run on the set of 15 natural images first at double the scale using 128x128 pixel sample sizes and a Gaussian window of size 20, and then at half the scale using 32x32 pixel sample sizes and a Gaussian window of size 5. The first 6 PCs are displayed in their Figure 6, and they note the lack of scale dependence.\nReScience C 6.3 (#6) \u2013 Davies and Eglen 2020 2\nText Image Experiments \u2014 In Figure 7 in [1], Hancock et al. repeat the scale experiment with text images, this time producing significant scale dependence. Four text images are used at three different magnification scales, with the first 8 PCs of each displayed. Scales which are proportional to the size of letters and line spacings are found in the PCs. Finally, Hancock et al. show how the 7th PC of the finest scale images is tuned to gaps between words. They use the PC as a filter which they convolve with an original text image. Their Figure 8 shows that when this is thresholded and superimposed on the text images the word gaps are marked.\n2.4 Re-Implementation of Experiments Neither the original source code nor the original text images could be obtained, however the original natural images were obtained from the original authors, and are available at: http://pics.stir.ac.uk/ in the collection entitled \u201cPCA Images\u201d. We first re-implemented the model in Matlab. We then converted the code to run in Julia; this process was relatively easy given that only minor edits were required to the code. The resulting Julia code takes about five minutes to run all computations for this paper (on a modern i5 laptop). We have also provided a mybinder image at: https://mybinder.org/v2/gh/sje30/ Reproduction-Hancock/master. The model re-implementation used Sanger s\u0313 algorithm and the presentation of images as described in the original paper except in the rotation experiment. For the rotation experiment, Hancock et al. rotate the natural images in a scanner and use the resulting images. Instead, in the reimplementation samples of the non-rotated images were chosen and then rotated using the MATLAB/Julia function \u201dimrotate\u201d. For the reproduction of the text image experiments a new text image (textimage.png) was created by one of us (ID). This was done by copy-and-pasting text into a Word Document and editing it so it had similar features to the original text images. This included adding large whitespaces and finding a similar font - Century was used. This was then converted into a PNG file and random sections of it at different scales were used as input to Sanger s\u0313 algorithm to produce Figures 7 and 8 below. A second text image (textimage2.png) was also created using the same method except with Calibri font to produce Figure 9.\n2.5 Results All of the main features of Figures 2-8 in [1] were reproduced successfully. Successful results are given in Figures 2-8 below. We have also replicated Figure 1, so that the figure labels in the original and resubmitted manuscript match. Some images of PCs in the figures below are the negative images of those in the originals, but by the nature of Sanger s\u0313 algorithm these represent the same PCs. Small differences were found in the order of the PCs, especially in the text image experiments, and the text image needed to be modified to produce a PC that detected word spacings (Century font was used). Figure 9 displays the result of the text scale image using a font (Calibri) that failed to produce the PC that detects word spacing.\nNatural Image Experiments \u2014 The first 15 PCs of the set of 15 natural images are shown in Figure 2. Thesematch those of [1]. The first component is approximately Gaussian. The second is the horizontal derivative of a Gaussianwhilst the third is the vertical derivative. The relative ordering of the derivatives reflects the fact that horizontal correlation scales are longer than vertical scales in natural images. Figure 3 repeats the first experiment with 40 natural images instead of 15. Again, the PCs match those found by Hancock et al. They are similar in features to those of the 15 natural images except with the ordering of some PCs swapped, namely 5 and 6, 10 and\nReScience C 6.3 (#6) \u2013 Davies and Eglen 2020 3\nReScience C 6.3 (#6) \u2013 Davies and Eglen 2020 4\n11, and 13 and 14. The rotation experiment is reproduced in Figure 4. PC numbers 5 and 6 are swapped in comparison with the original. The fact that the horizontal and vertical derivative operators are rotated by 45 degrees shows that the PCA is detecting the orientation of the images and not the edge effects of using a square sample. This matches the result found in [1]. Figure 5 displays the reproduction of the variance experiment. The output variances of the PCs in Figure 2 and Figure 3 were measured over 10000 samples of their respective input image sets. Figure 5 is a log-log plot of PC number against output variance. As found by [1], the plot is approximately a straight line with variance decreasing with PC number. Hancock et al. found some minor deviations to this rule amongst PCs with very similar variance; the reproduction did not find this, instead finding that the PCs were ordered exactly by variance. This may have been down to the randomness of the inputs. The scale experiment on natural images is reproduced in Figure 6. The first 6 PCs at half the scale and at double the scale are displayed. These match those of [1] exactly apart from the ordering of the final two PCs at half the scale. They are also very similar to the PCs at the original scale, confirming the lack of scale dependence of correlations in natural images.\nReScience C 6.3 (#6) \u2013 Davies and Eglen 2020 5\nText Image Experiments \u2014 The original text images could not be obtained, hence a new text image was prepared. It was in these experiments that the most difference was found with [1]. The successfully reproduced scale experiment on text images is found in Figure 7. The early PCs match those of Hancock et al. very well. The main finding that the PCs of text images depend on scale was reproduced: the first two PCs at each scale represent line size and the next two represent letter size. However the latter PCs were significantly different from [1]. In the finest scale the horizontal filters (PCs 3 and 4 in [1]) are missing from the reproduction, and the PCwhichmarks word gaps is PC 5 rather than PC 7. This may have been due to the font used, as the shape and ordering of the PCs was found to be sensitive to font. The text image used to produce Figure 7 had Century font which was the most similar font the author could find to that used in [1]. To provide comparison, Figure 9 displays the result of the scale experiment using the same text except in Calibri font. The same scale dependence was found but there is no PC corresponding to word gaps. Figure 8 displays the convolution of the 5th PC at the finest scale in Figure 7 with the full text image, which is then thresholded and superimposed over the text image. As found in [1], this marks gaps between words. For this to be reproduced, further experimental modifications had to be made to the text image. Text with no large whitespace or font that was too thin failed to produce a PC that corresponded to word spacings. As mentioned above, when such a PC could be produced it was the 5th PC not the 7th as in [1].\nReScience C 6.3 (#6) \u2013 Davies and Eglen 2020 6\n3 Conclusion\nOur re-implementation of Principal Component Analysis using Sanger s\u0313 algorithm from Hancock et al. 1991 was successful in reproducing the main results and most principal components from the original paper. The main results concerning orientation dependence (Figure 4), variance (Figure 5) and scale dependence (Figure 6 and Figure 7) in [1] are reproduced well. The primary principal components are reproduced in all experiments and although there are differences in the latter principal components of text images, the re-implementation was able to extract a PC that marked word gaps (Figure 8). The description of the model in the original paper was clear and was sufficient to write a re-implementation without the source code. All model parameters are stated clearly and the method of preparing the input image samples was simple to follow. Obtaining the original natural images also allowed for results to be reproduced easily and faithfully. The main difficulty in reproduction was the creation of a new text image that recreated the desired principal components. This required some trial and error as the principal components are sensitive to font and features in the text suggesting that some of the results in [1] are particular to the images used.\n4 Acknowledgements\nThis work was financially supported by a mini science grant to CODECHECK from the Mozilla foundation. ID was also financially supported by the Faculty of Mathematics at Cambridge. We thank Professors Peter Hancock and Leslie Smith at the University of Stirling for providing their image database."}], "title": "[Re] The principal components of natural images", "year": 2020}