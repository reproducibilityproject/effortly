{"abstractText": "In this paper, we work on reproducing the results obtained in the \u2019Fairness and Bias in Online Selection\u2019 paper [1]. The goal of the reproduction study is to validate the 4 main claims made in [1]. The claims made are: (1) for the multi\u2010color secretary problem, an optimal online algorithm is fair, (2) for the multi\u2010color secretary problem, an optimal offline algorithm is unfair, (3) for the multi\u2010color prophet problem, an optimal online algorithm is fair (4) for the multi\u2010color prophet problem, an optimal online algorithm is less efficient relative to the offline algorithm. To test if the results of the secretary algorithmgeneralize to other data sets, the proposed algorithms and baselines are applied to the UFRGS Entrance Exam and GPA data set [2].", "authors": [{"affiliations": [], "name": "Diego van der Mast"}, {"affiliations": [], "name": "Soufiane Ben Haddou"}, {"affiliations": [], "name": "Jacky Chu"}, {"affiliations": [], "name": "Jaap Stefels"}, {"affiliations": [], "name": "Koustuv Sinha"}, {"affiliations": [], "name": "Sharath Chandra Raparthy"}], "id": "SP:1be9f643e5cff0c84f18546880576b31d8172d40", "references": [{"authors": ["J. Correa", "A. Cristi", "P. Duetting"], "title": "and A", "venue": "Norouzi-Fard. \u201cFairness and Bias in Online Selection.\u201d In: (2021), pp. 2112\u2013", "year": 2121}, {"authors": ["B. Castro da Silva"], "title": "UFRGS Entrance Exam and GPA Data.", "venue": "Version V1", "year": 2019}, {"authors": ["S. Moro", "P. Cortez", "P. Rita"], "title": "A Data-Driven Approach to Predict the Success of Bank Telemarketing.", "venue": "Decision Support Systems", "year": 2014}, {"authors": ["M.L. Takac"], "title": "Z\u00e1bovsk\u00fd. \u201cData analysis in public social networks.", "venue": "Trends of Innovations", "year": 2012}, {"authors": ["E. Samuel-Cahn"], "title": "Comparison of threshold stop rules and maximum for independent nonnegative random variables.", "venue": "In: the Annals of Probability", "year": 1984}, {"authors": ["D. Marx"], "title": "Proceedings of the 2021 ACM-SIAM Symposium on Discrete Algorithms (SODA)", "venue": "SIAM,", "year": 2021}, {"authors": ["J. Correa", "P. Foncea", "R. Hoeksma", "T. Oosterwijk", "T. Vredeveld"], "title": "Posted Price Mechanisms and Optimal Threshold Strategies for Random Arrivals.", "venue": "In: Mathematics of Operations Research", "year": 2021}, {"authors": ["B. Brown"], "title": "Great expectations: The theory of optimal stopping.", "venue": "Journal of the Royal Statistical Society: Series A (General)", "year": 1972}], "sections": [{"text": "R E S C I E N C E C"}, {"heading": "Replication / ML Reproducibility Challenge 2021", "text": "[Re] Replication Study of \u201dFairness and Bias in Online Selection\u201d Diego van der Mast1,2, ID , Soufiane Ben Haddou1,2, ID , Jacky Chu1,2, ID , and Jaap Stefels1,2, ID 1Informatics Institute, University of Amsterdam, Amsterdam, Netherlands \u2013 2Equal contributions\nEdited by Koustuv Sinha,\nSharath Chandra Raparthy\nReviewed by Anonymous Reviewers\nReceived 04 February 2022\nPublished 23 May 2022\nDOI 10.5281/zenodo.6574673\n1 Reproducibility Summary"}, {"heading": "Scope of Reproducibility", "text": "In this paper, we work on reproducing the results obtained in the \u2019Fairness and Bias in Online Selection\u2019 paper [1]. The goal of the reproduction study is to validate the 4 main claims made in [1]. The claims made are: (1) for the multi\u2010color secretary problem, an optimal online algorithm is fair, (2) for the multi\u2010color secretary problem, an optimal offline algorithm is unfair, (3) for the multi\u2010color prophet problem, an optimal online algorithm is fair (4) for the multi\u2010color prophet problem, an optimal online algorithm is less efficient relative to the offline algorithm. To test if the results of the secretary algorithmgeneralize to other data sets, the proposed algorithms and baselines are applied to the UFRGS Entrance Exam and GPA data set [2]."}, {"heading": "Methodology", "text": "The paper that has been reproduced includes a link to a repository containing C++ files for the algorithms that were implemented. For our experiments, we reimplemented the code in Python. Our goal was to reproduce the code in an efficient manner without altering the core logic. Using the Python code all the experiments in the paper have been replicated including some additional experiments to verify the claims made in [1]."}, {"heading": "Results", "text": "The reproduced results support all claimsmade in [1]. However, in the case of the unfair secretary algorithm (SA), some irregular results arise in the experiments due to random\u2010 ness. This irregularity is also existent in the original code."}, {"heading": "What was easy", "text": "The concepts behind the algorithms were straightforward. The existing code base pro\u2010 vided a solid reference point to verify the results of the original paper by compiling and running the provided code.\nCopyright \u00a9 2022 D.V.D. Mast et al., released under a Creative Commons Attribution 4.0 International license. Correspondence should be addressed to Diego van der Mast (diego.vandermast@student.uva.nl) The authors have declared that no competing interests exist. Code is available at https://github.com/Di-ayy-go/fact-ai \u2013 DOI 10.5281/zenodo.6518051. \u2013 SWH swh:1:dir:45176f5005ed390a349cd01e61ed37711095879e. Open peer review is available at https://openreview.net/forum?id=SNeep2MXn0K.\nNone 8.2 (#24) \u2013 Mast et al. 2022 1"}, {"heading": "What was difficult", "text": "Implementing the prophet algorithm, in comparison to the secretary algorithm, was complex. C++ is a more efficient compiler (time complexity, etc.) compared to Python. For the reproduction of the algorithms, this needed to be taken into account. While it might be possible to execute transliterated code on a powerful machine, with the avail\u2010 able resources the code would have taken over 96 hours to run. In order to tackle this problem, some of the data structures needed to be converted to NumPy arrays to de\u2010 crease computation time.\n2 Introduction\nAs more machine learning algorithms are used in decision\u2010making circumstances, it is important to ensure that social norms are not violated. The social norm that serves as the pivot of this research is fairness. Specifically \u2019fairness\u2019 in the use of selectionmodels. The importance of fairness is to avoid undesirable biases. Selection models are models that input a finite amount of agents and attempt to pick the best possible candidate (agent). The goal is to design algorithms that can fairly judge between agents regardless of any unfair bias. In some real\u2010life implementations of selection models, there is no clear overview of all agents. For example, in the online selection problem, the agents enter the algorithm sequentially. For every agent, a decision has to bemadewhether this is the best possible agent. The complexity of this task is not being able to have any knowledge on agents that might come in the future. As soon as the decision is made that an agent is the best fit, the algorithm should stop as that agent is the optimal candidate (according to the model). Multiple attempts have been made to create the most accurate algorithm for these online selection models. For this research, we reproduce the \u2019Fairness and Bias in Online Selection\u2019 paper [1]. In this paper, the authors focus on 2 main problems: the secretary problem and the prophet problem. The secretary problem is a scenario for the sequential selection prob\u2010 lem where an attempt is made to select the candidate with the highest value without knowing the value of the candidates to come. An immediate decision has to be made on the candidate, the candidate either gets picked or gets passed on. For the prophet algorithm the same assumptions are made as for the secretary algorithm, but we know the distributions the candidate values are drawn from. The probability of the candidate is based on these distributions. In the case of both problems, the goal is to stop at the best possible candidate based on the assigned probabilities. In order to include a form of fairness in these models, a concrete definition needs to be given to fairness in online selection models. Based on the [1] paper, fairness is defined as an unbiased evaluation of agents in a selection model. A selection algorithm is fair if it selects the best candidate, closely following the original probability of the best can\u2010 didate existing in that group. Along with fairness, efficiency has also been used as an evaluation metric in the original paper. Efficiency is a measure of how accurately the online algorithm picks the actual best candidate. By creating a \u2019fair\u2019 version for these problems, the authors claim to have created a fair use of sequential single item selection models. Through categorization of the agents by color, a distinction between the agents can be made. However, the qualities these agents possess might be different enough that they could be considered incomparable. So implementing a multi\u2010color version of the sequential selection models and picking the best possible candidate, taking color into account, an \u2019unfair\u2019 comparison is avoided.\nNone 8.2 (#24) \u2013 Mast et al. 2022 2\n3 Scope of reproducibility\nIn this reproduction study, we focus on the authors\u2019 claims that the use of a multi\u2010color version of the secretary and prophet problem would make the use of these algorithms fair. The authors of the paper implement these algorithms on synthetic data sets and real\u2010world data sets. For our study, we put an effort into reproducing the results given by the paper. The goal of this reproduction is to either validate or deny the claimsmade in the paper. This effort has been fulfilled by re\u2010implementing the code publicly available for the algorithm. This re\u2010implementation is done in Python in comparison to the C++ code provided by the authors. Most of the code has been written using NumPy to try and achieve about the same efficiency as the C++ code. However, the setup for the experiments corresponds to that of the authors. To show that the claims generalize well over differently distributed data sets, we run the proposed algorithms and baselines on the UFRGS Entrance Exam and GPA data set [2]. The claims made in the [1] paper are:\n\u2022 Claim 1: For themulti\u2010color secretary problem, an optimal online algorithm is fair.\n\u2022 Claim 2: For the multi\u2010color secretary problem, an optimal offline algorithm is unfair.\n\u2022 Claim 3: For the multi\u2010color prophet problem, an optimal online algorithm is fair.\n\u2022 Claim 4: For the multi\u2010color prophet problem, an optimal online algorithm is less efficient relative to the offline algorithm.\nTo test these claims we use the algorithms mentioned above on 4 types of data sets. These data sets are further discussed in section 3.3.\n4 Methodology\nIn this section, our approach to the re\u2010implementation of the experiments will be dis\u2010 cussed and an additional experiment will be proposed.\n4.1 Code The code accompanying the paper is provided in C++. As required for this study, we reproduced the work in Python, and subsequently made use of the inherent Pythonic efficiencies. The provided code allowed for a smooth initial reproduction. However, many optimisations were required to decrease computation duration.\n4.2 Model descriptions In the original paper, two types of single item selection models are considered: the sec\u2010 retary algorithm and the prophet algorithm. Candidates are partitioned into different groups which the authors refer to as colors. Every candidate has a numerical value that indicates the capabilities of that candidate. The authors refer to these indicators as values. Candidates arrive sequentially, and upon arrival, the algorithms decide whether the candidate is the best candidate overall. The best candidate is defined as the candi\u2010 date with the highest value of the sequence of candidates. For clarity, the main parts of the Methodology and Results sections are divided per model.\nNone 8.2 (#24) \u2013 Mast et al. 2022 3\nSecretary Algorithm \u2014 For the secretary algorithm, it is assumed that candidates arrive in uniformly random order. To verify the claims made by the author, we compare the optimal online algorithm as proposed by [1] to two baselines. Additionally, the algo\u2010 rithm and its baselines are applied on different data sets, either synthetically generated or composed from real\u2010word data sets. The optimal online algorithm proposed by the authors (Fair secretary algorithm) is denoted formally as:\nwhere the input t = (t1, ..., tk) is a vector of thresholds, one for each color j \u2208 [k]. The algorithmfirst checks if the candidate i arrived after the threshold of its color tc(i). If this condition is met, it accepts the candidate if its value exceeds the value of all previous candidates of color tc(i), indicating that it is the best candidate for that color. After having chosen the best candidate of each color, we are interested in selecting the best overall candidate. We denote the probabilities with which the best candidate of group j is the best among all colors by pj , which results in the vector p = (p1, ..., pk) covering all colors. We use this in our experiments to verify the claims of the author using equal, and unequal values for p among colors.\nProphet Algorithm \u2014 For the prophet algorithm, the same assumptions aremade as for the secretary algorithm, but we know the distributions Fi the candidate values are drawn from. In the paper, the authors propose two optimal online algorithms specified in figure 1, where q1, \u00b7 \u00b7 \u00b7 , qn denote the marginal probabilities that the optimal fair offline algorithm picks the candidates i = 1, \u00b7 \u00b7 \u00b7 , n. Figure 1a shows the general Fair prophet al\u2010 gorithm (Fair prophet algorithm). This algorithm does notmake any assumptions about the underlying probability distribution, it can be different for every candidate. Figure 1b shows the Fair independent and identically distributed prophet algorithm (Fair IID prophet algorithm). This algorithm assumes that the values of the candidates are drawn from the same distribution.\nNone 8.2 (#24) \u2013 Mast et al. 2022 4\n4.3 Data sets The experiments involving the SA algorithm are conducted on two synthetic data sets and two real\u2010world data sets. The data sets and their properties are summarised below:\n1. Synthetic data set, equal p values contains four different colors with 10, 100, 1000, and 10000 occurrences. The value of each element is chosen independently and uniformly at random from [0, 1].\n2. Synthetic data set, general p values contains a similar setup as 1, but with p = (0.3, 0.25, 0.25, 0.2).\n3. Feedback maximization (Bank) contains records of direct marketing campaigns (phone calls) by a Portuguese banking institution [3]. The clients are split into 5 colors by age: under 30, 31\u201040, 41\u201050, 51\u201060, and over 61 years old. The value of every client is the duration of the phone call. Moreover, an equal p of 0.2 was used for all colors.\n4. Influence maximization (Pokec) contains records of the influence of users of the Pokec social network [4]. We pre\u2010process the data by dividing the users into 5 dif\u2010 ferent colors according to their body mass index (BMI): under weighted (BMI < 18.5), normal (18.5 <= BMI < 25), over weighted (25.0 <= BMI < 30.0), obese type 1 (30.0 <= BMI < 35), and obese type 2 (BMI >= 35.0). The value is computed as the number of the followers for each user. Again, an equal p of 0.2 was used for all colors.\n4.4 Experimental setup In this subsection, the experimental evaluation performed by the authors is discussed. As before, a distinction between the two problems is made for clarity. Additionally, an extra experiment will be considered where the secretary algorithm will be evaluated on another real\u2010world data set. Secretary experiments The authors propose two different baselines to compare the Fair secretary algorithm to. Firstly, the classic secretary algorithm (SA), which does not take the colors of the candidates into account. Secondly, the single\u2010color secretary algorithm (SCSA). This algorithm picks a color proportional to the p values and then runs the classic secretary algorithmon the candidates of only that color. To evaluate the claims by the authors, the three mentioned algorithms are evaluated against the four data sets discussed earlier. The parameters of these experiments consist of the size of the data sets and the number of repetitions. For the experiments on the Synthetic data sets (equal p / general p) and the Bank data set, all available candidates were used in 20.000 repetitions. In the orig\u2010 inal paper, the authors used all \u00b1 650.000 candidates of the Pokec data set in 1000.000 repetitions. In our experiment, we had to limit these parameters due to time constraints. We only considered the first 40.000 candidates and used 40.000 repetitions."}, {"heading": "Prophet experiments", "text": "For the prophet experiments, the Fair prophet algorithm and Fair IID prophet algorithm are evaluated against three baselines: the SC algorithm [5], EHKS algorithm [6], CFHOV algorithm [7] and DP algorithm [8]. The specific works of these algorithms are described in further detail in the paper [1] section 4.2. For the experiments, two settings are implemented. In the first setting, 50 samples are taken from a uniform distribution in a range of [0, 1]. These samples function as the input stream. In the second setting, 1000 samples are taken from a binomial distribu\u2010 tion with 1000 trials and a probability of a successful single trial p = 0.5. In order to compare this method with the already existing algorithms, we assume each candidate\nNone 8.2 (#24) \u2013 Mast et al. 2022 5\nto be group of its own. For every algorithm, we repeat the experiment 50.000 times.\nExtending to other data set (UFRGS) experiments This subsection describes an experimental extension on the work of [1]. In our work, we have concluded that the secretary results claimed in the paper are reproducible. It is shown in section 5 that the Fair algorithm significantly outperforms the SCSA baseline. However, all real\u2010world data sets used to prove this claim contain the same distribution of values for every color. The distributions for the Bank and Pokec data sets are shown in Figures 2a 2b respectively. Our extension investigates the effect of applying the Fair algorithm to an unequally dis\u2010 tributed real\u2010word data set, such as the UFRGS Entrance Exam and GPA Data (UFRGS) data set. This work will show whether the claims made by the authors generalize to these types of data sets. The UFRGS contains entrance exam scores of students apply\u2010 ing to a university in Brazil (Federal University of Rio Grande do Sul), along with the students\u2019 GPAs during the first three semesters at university. The data set also includes the gender of every student (male or female). The distribution of the data set is shown in Figure 2c. This experiment is a duplication of the original secretary experiments but with the UFRGS data set as input. The gender of the students is used as color, their GPA score as values. The experiment is repeated 20.000 times.\n5 Results\nThe following paragraphs will present the results for the experiments discussed in sec\u2010 tion 4.4: (1) the secretary experiments, (2) the prophet experiments, (3) our extended work."}, {"heading": "Secretary results", "text": "The plots in Figure 3 show our reproduction work regarding the original paper on the secretary problem over the four different data sets. We find that all results are in line with the work of [1]. Due to the nature of construction of the fair algorithm proposed by the authors, and the SCSA, we find that it picks elements from each color proportional to the vector p. From this, it can be concluded that the authors\u2019 Claims 1 and 2 are valid. The authors claim that the quality of the solution of their algorithm is significantly higher than the SCSA. Table 1 shows our replication of this comparison. We find that our implementation reproduces the authors\u2019 claim that their method is superior to the SCSA. Small discrepancies in the results are found, this is due to the random nature of the algorithm. However, as mentioned earlier, after scrutinizing the distributions of the used data sets, we found that all the used data sets have similar distributions in the input. Therefore, we proceed by agreeing with the claims of the author given this restriction. Prophet results\nNone 8.2 (#24) \u2013 Mast et al. 2022 6\nNone 8.2 (#24) \u2013 Mast et al. 2022 7\nThe patterns of the results in the original paper are reflected in our reproduction as visualized in figured 4. A major difference is that the scale of their y\u2010axis is twice the size of our reproduction. Because the shown plots are a histogram of arrival positions, this could be attributed to a difference in bin size. The authors\u2019 report specifies using uniform distributions. Table 2 shows our replication of the average values chosen by each algorithm. While small differences exist, our reproduction mirrors the authors\u2019 results upon running their code closely.\nExtending to other data set (UFRGS) results Figure 5 shows the results of the experiments proposed in section 4.4. It can be noted that the pattern visible in the earlier secretary results still holds for a new, unequally distributed data set. However, when looking at Table 1, a significant decrease in per\u2010 formance can be detected. The Bank and Pokec data sets scored +37.7% and +36.8% for F\u2010Pick compared to S\u2010Pick. The UFRGS only has an increase of +19.2%. The differ\u2010 ence is even more significant when comparing F\u2010Max to S\u2010Max; Pokec and Bank have an increase of +81.2% and +81.0%, UFRGS only has an improvement of +36.4%. We can conclude that the performance increase of the Fair secretary algorithm is not as signifi\u2010 cant when using an unequally distributed data set, compared to the increasementioned in the paper.\n6 Discussion\nIn this research, we have tried to reproduce the work of [1] as closely as possible. How\u2010 ever, there are a few inconsistencies in the original code and paper, which caused com\u2010 plications. These points and our solution to them (if required) will be briefly discussed in the following paragraph. Firstly, as mentioned before, the BMI thresholds for the pre\u2010processing of the Pokec dataset were missing in the authors\u2019 work. This poses a problem as slight alterations\nNone 8.2 (#24) \u2013 Mast et al. 2022 8\nto these thresholds yield different results. This problem was solved by finding concur\u2010 ring values in other research. Secondly, to limit the computation time of our reproduc\u2010 tion, the size of the Pokec data set was limited from approximately 650.000 to 40.000 ele\u2010 ments. The number of repetitions for this experimentwas also decreased from1.000.000 to 40.000. We opted for this solution as the distributions in the results did not change from these limits onward. Thirdly, the U\u2010pick/U\u2010max values in the secretary results of the original work are inconsistent due to randomness. It seems that changing the seed value of the randomnumber generator in the C++ code heavily changes the output of the SA algorithm (U\u2010pick/U\u2010max). The SA results could therefore be cherry\u2010picked as no fur\u2010 ther explanation was provided by the authors. Lastly, some inconsistencies are present in the paper. From minor typos e.g. using the word desbribed instead of described, to more serious mistakes, such as claiming that an increase of 1.721 is equal to (+73.1%). A thorough reread of the paper would have resolved this.\n6.1 Reflection on our replication study The algorithms used in the original were clear and straightforward. The existing C++ code of the authors provided a good starting point for the verification of the results. However, our goal was to further validate these claims and generalize them to a further extent. We did this by reproducing thework of the original paper. Reproducing thework efficiently in another language, in our case Python, introduced some difficulties and took longer than expected. An execution of transliterated code resulted in an excessive run time. To tackle this problem, some of the data structures needed to be converted to NumPy arrays to decrease computation time. This requires advanced knowledge of Numpy and the use of data structures.\n6.2 Communication with original authors As certain parameters and split\u2010off values were not clearly defined in either the paper or the original code, we reached out to the authors via mail to ensure a fair assessment of the reproduction. Examples of missing split\u2010off values are the BMI category thresholds for the pre\u2010processing of the Pokec data set. These category values are not fixed in liter\u2010 ature and differ depending on age and nationality. At the time of writing this report, we had not yet heard back from the authors. We resolved this by assuming certain values and explanations, which are all documented in our paper."}, {"heading": "2. B. Castro da Silva. \u201cUFRGS Entrance Exam and GPA Data.\u201d Version V1. In: (2019). DOI: 10.7910/DVN/O35FW8.", "text": "URL: https://doi.org/10.7910/DVN/O35FW8. 3. S. Moro, P. Cortez, and P. Rita. \u201cA Data-Driven Approach to Predict the Success of Bank Telemarketing.\u201d In: Decision Support Systems 62 (2014), pp. 22\u201331. 4. L. Takac and M. Z\u00e1bovsk\u00fd. \u201cData analysis in public social networks.\u201d In: Trends of Innovations (2012), pp. 1\u20136. 5. E. Samuel-Cahn. \u201cComparison of threshold stop rules and maximum for independent nonnegative random vari-\nables.\u201d In: the Annals of Probability (1984), pp. 1213\u20131216. 6. D. Marx. Proceedings of the 2021 ACM-SIAM Symposium on Discrete Algorithms (SODA). SIAM, 2021. 7. J. Correa, P. Foncea, R. Hoeksma, T. Oosterwijk, and T. Vredeveld. \u201cPosted Price Mechanisms and Optimal\nThreshold Strategies for Random Arrivals.\u201d In: Mathematics of Operations Research (2021). 8. B. Brown. \u201cGreat expectations: The theory of optimal stopping.\u201d In: Journal of the Royal Statistical Society: Series\nA (General) 135.4 (1972), pp. 610\u2013610.\nNone 8.2 (#24) \u2013 Mast et al. 2022 10"}], "title": "[Re] Replication Study of \u201dFairness and Bias in Online Selection\u201d", "year": 2022}