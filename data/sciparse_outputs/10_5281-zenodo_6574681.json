{"abstractText": "We conducted a reproducibility study of the paper Exacerbating Algorithmic Bias through Fairness Attacks [1]. According to the paper, current research on adversarial attacks is primarily focused on targeting model performance, which motivates the need for ad\u2010 versarial attacks on fairness. To that end, the authors propose two novel data poisoning adversarial attacks, the influence attack on fairness and the anchoring attack. We aim to verify the main claims of the paper, namely that: a) the proposed methods indeed af\u2010 fect a model\u2019s fairness and outperform existing attacks, b) the anchoring attack hardly affects performance, while impacting fairness, and c) the influence attack on fairness provides a controllable trade\u2010off between performance and fairness degradation.", "authors": [{"affiliations": [], "name": "Angelos Nalmpantis"}, {"affiliations": [], "name": "Apostolos Panagiotopoulos"}, {"affiliations": [], "name": "John Gkountouras"}, {"affiliations": [], "name": "Konstantinos Papakostas"}, {"affiliations": [], "name": "Koustuv Sinha"}, {"affiliations": [], "name": "Sharath Chandra Raparthy"}], "id": "SP:d356a5234c6dc64466a6ae0571857a1b501f1e79", "references": [{"authors": ["N. Mehrabi", "M. Naveed", "F. Morstatter"], "title": "and A", "venue": "Galstyan. \u201cExacerbating Algorithmic Bias through Fairness Attacks.\u201d In: Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 35. 10.", "year": 2021}, {"authors": ["A. Chakraborty", "M. Alam", "V. Dey", "A. Chattopadhyay", "D. Mukhopadhyay"], "title": "A survey on adversarial attacks and defences.", "venue": "CAAI Transactions on Intelligence Technology", "year": 2021}, {"authors": ["G. Li", "P. Zhu", "J. Li", "Z. Yang", "N. Cao"], "title": "and Z", "venue": "Chen. Security Matters: A Survey on Adversarial Machine Learning.", "year": 2018}, {"authors": ["V. Nanda", "S. Dooley", "S. Singla", "S. Feizi"], "title": "and J", "venue": "P. Dickerson. \u201cFairness through robustness: Investigating robustness disparity in deep learning.\u201d In: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency.", "year": 2021}, {"authors": ["D. Solans", "B. Biggio"], "title": "and C", "venue": "Castillo. \u201cPoisoning Attacks on Algorithmic Fairness.\u201d In: Machine Learning and Knowledge Discovery in Databases. Ed. by F. Hutter, K. Kersting, J. Lijffijt, and I. Valera. Cham: Springer International Publishing,", "year": 2021}, {"authors": ["P.W. Koh", "J. Steinhardt", "P. Liang"], "title": "Stronger data poisoning attacks break data sanitization defenses.", "venue": "Machine Learning (Nov. 2021). DOI: 10.1007/s10994-021-06119-y. URL: https://doi.org/10.1007/s10994-02106119-y", "year": 2021}, {"authors": ["P.W. Ko"], "title": "and P", "venue": "Liang. \u201cUnderstanding Black-box Predictions via Influence Functions.\u201d In: Proceedings of the 34th International Conference on Machine Learning. Ed. by D. Precup and Y. W. Teh. Vol. 70. Proceedings of Machine Learning Research. PMLR, June", "year": 2017}, {"authors": ["M.B. Zafar", "I. Valera", "M.G. Rogriguez"], "title": "and K", "venue": "P. Gummadi. \u201cFairness Constraints: Mechanisms for Fair Classification.\u201d In: Proceedings of the 20th International Conference on Artificial Intelligence and Statistics. Ed. by A. Singh and J. Zhu. Vol. 54. Proceedings of Machine Learning Research. PMLR, 20\u201322 Apr", "year": 2017}, {"authors": ["J. Steinhardt", "P.W. Koh"], "title": "and P", "venue": "Liang. \u201cCertified Defenses for Data Poisoning Attacks.\u201d In: Proceedings of the 31st International Conference on Neural Information Processing Systems. NIPS\u201917. Long Beach, California, USA: Curran Associates Inc.,", "year": 2017}, {"authors": ["H. Hofmann"], "title": "Statlog (German Credit Data)", "venue": "UCI Machine Learning Repository.", "year": 1994}, {"authors": ["J. Angwin", "J. Larson", "S. Mattu", "L. Kirchner"], "title": "Machine bias", "venue": "May", "year": 2016}, {"authors": ["E. Fehrman", "V. Egan", "E. Mirkes"], "title": "Drug consumption (quantified)", "venue": "UCI Machine Learning Repository.", "year": 2016}, {"authors": ["C. Dwork", "M. Hardt", "T. Pitassi", "O. Reingold"], "title": "and R", "venue": "Zemel. \u201cFairness through Awareness.\u201d In: Proceedings of the 3rd Innovations in Theoretical Computer Science Conference. ITCS \u201912. Cambridge, Massachusetts: Association for Computing Machinery,", "year": 2012}, {"authors": ["M. Hardt", "E. Price", "E. Price"], "title": "and N", "venue": "Srebro. \u201cEquality of Opportunity in Supervised Learning.\u201d In: Advances in Neural Information Processing Systems. Ed. by D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett. Vol. 29. Curran Associates, Inc.,", "year": 2016}, {"authors": ["R.K.E. Bellam"], "title": "et al", "venue": "AI Fairness 360: An Extensible Toolkit for Detecting, Understanding, and Mitigating Unwanted Algorithmic Bias. Oct.", "year": 2018}], "sections": [{"text": "R E S C I E N C E C Replication / ML Reproducibility Challenge 2021\n[Re] Exacerbating Algorithmic Bias through Fairness"}, {"heading": "Attacks", "text": "Angelos Nalmpantis1,2, ID , Apostolos Panagiotopoulos1,2, ID , John Gkountouras1,2, ID , and Konstantinos Papakostas1,2, ID 1University of Amsterdam, Amsterdam, NL \u2013 2Equal contributions\nEdited by Koustuv Sinha,\nSharath Chandra Raparthy\nReceived 04 February 2022\nPublished 23 May 2022\nDOI 10.5281/zenodo.6574681"}, {"heading": "Reproducibility Summary", "text": ""}, {"heading": "Scope of Reproducibility", "text": "We conducted a reproducibility study of the paper Exacerbating Algorithmic Bias through Fairness Attacks [1]. According to the paper, current research on adversarial attacks is primarily focused on targeting model performance, which motivates the need for ad\u2010 versarial attacks on fairness. To that end, the authors propose two novel data poisoning adversarial attacks, the influence attack on fairness and the anchoring attack. We aim to verify the main claims of the paper, namely that: a) the proposed methods indeed af\u2010 fect a model\u2019s fairness and outperform existing attacks, b) the anchoring attack hardly affects performance, while impacting fairness, and c) the influence attack on fairness provides a controllable trade\u2010off between performance and fairness degradation."}, {"heading": "Methodology", "text": "We chose PyTorch Lightning to re\u2010implement all of the code required to reproduce the original paper\u2019s results. Our implementation enables the quick and easy extension of existing experiments, as well as the integration with the various development tools that come with PyTorch Lightning. All of our experiments took about 120 hours to complete on a machine equipped with an Intel Core i7 7700k CPU and an NVIDIA GeForce GTX 1080 GPU."}, {"heading": "Results", "text": "Our results slightly deviate from the ones reported by the authors. This could be at\u2010 tributed to the design choices we had to make, due to ambiguities present in the orig\u2010 inal paper. After inspecting the provided codebase along with relevant literature, we were able to replicate the experimental setup. In our experiments, we observe similar trends and hence we can verify most of the paper\u2019s claims, albeit not getting identical experimental results.\nCopyright \u00a9 2022 A. Nalmpantis et al., released under a Creative Commons Attribution 4.0 International license. Correspondence should be addressed to Angelos Nalmpantis (angelos.nalampantis@student.uva.nl) The authors have declared that no competing interests exist. Code is available at https://github.com/toliz/fairness-attacks \u2013 DOI 10.5281/zenodo.6505214. \u2013 SWH swh:1:dir:224b71f5d3c02f260427e2f7c492b6db98c65638. Open peer review is available at https://openreview.net/forum?id=rYLMJ6zX3RF.\nNone 8.2 (#28) \u2013 Nalmpantis et al. 2022 1"}, {"heading": "What was easy", "text": "The original paper is well\u2010structured and easy to follow, with the principal ideas behind the proposed algorithms being very intuitive. Additionally, the datasets used in the ex\u2010 periments are publicly available, small in size, and the authors provide their code on GitHub."}, {"heading": "What was difficult", "text": "During our study, we encountered a few unforeseen issues. Most importantly, we were not able to identify critical technical information required for the implementation of the proposed algorithms, as well as a detailed description of the models used, their training pipeline, hyperparameters, and data pre\u2010processing techniques. Furthermore, the pub\u2010 licly available code is convoluted and employs out\u2010of\u2010date libraries, making it difficult to set up the necessary environment.\nCommunication with original authors We contacted the paper\u2019s first author once to confirm our understanding of certain el\u2010 ements of the paper that were either not specific enough or missing. Although they responded fairly quickly, their answer prompted us back to the paper and the provided codebase, while not encouraging any further communication.\nNone 8.2 (#28) \u2013 Nalmpantis et al. 2022 2\n1 Introduction\nAdversarial attacks have becomepopular in themachine learning community since they allow scientists to understand and mitigate the weaknesses of the employed models. Current research is primarily focused on adversarial attacks targeting the performance of machine learning systems [2, 3], but recent studies indicate that adversarial attacks can also be used to target fairness [1, 4, 5]. In the studied paper, the authors propose two novel families of adversarial attacks \u2010 the influence attack on fairness and the anchor\u2010 ing attack \u2010 and demonstrate their effect in exacerbating algorithmic bias by evaluating them on three datasets using two well\u2010known fairness metrics. Both of the proposed methods belong to the family of data poisoning attacks, in which the adversary attempts to inject malicious data points into the training data. In par\u2010 ticular, given a \u201cclean\u201d training dataset Dc, i.e. a dataset containing only the original training samples, the adversary generates a \u201cpoisoned\u201d dataset Dp and integrates it into the original one, resulting in the final train set Dtrain = Dc \u222a Dp. The poisoned dataset Dp is generated in such a way that training with Dtrain results in a model with degraded performance or, in our case, a less fair model. The paper considers a binary classification scenario, under a common fairness setup with two demographic groups; the advantaged Dadv and the disadvantaged Ddisadv. Un\u2010 der this setting and given an adversarial loss that increaseswhen themodelmakes unfair decisions, the influence attack on fairness finds adversarial data points by performing gradient ascent on the adversarial loss. On the other hand, the anchoring attack places poisoned points in the close vicinity of two target points, one from Dadv and one from Ddisadv, with the opposite labels but the same demographic.\n2 Scope of reproducibility\nIn this reproducibility study we aim to verify the following main claims of the paper:\n\u2022 Both of the proposed attacks impact the fairness of the targeted model, outper\u2010 forming other attacks in the literature, such as Koh\u2019s basic influence attack [6] and Solan\u2019s gradient\u2010based poisoning attack [5].\n\u2022 The anchoring attack has little to no impact on the model\u2019s accuracy, making it more difficult to detect.\n\u2022 The influence attack on fairness provides a controllable trade\u2010off between the im\u2010 pact on performance and fairness via a regularization term \u03bb.\nAdditionally, we extend the evaluation set up to test whether current methods can be used to invert the inherent bias of a dataset. To this end, we re\u2010implement the entire experimental setup, and hence contribute:\n\u2022 an extensive study and evaluation of the adversarial attacks proposed by Mehrabi et al. [1].\n\u2022 amodification to the influence attack on fairness which can invert or diminish the inherent bias of a dataset.\n\u2022 a comprehensible and easily extensible codebase, which can be used both in the evaluation of current methods and as a framework for further research on adver\u2010 sarial attacks on fairness.\nNone 8.2 (#28) \u2013 Nalmpantis et al. 2022 3\n3 Methodology\n3.1 Poisoning Attacks Poisoning attacks are a category of adversarial attacks where the attacker impacts a sys\u2010 tem by injecting a small portion of engineered malicious data into its training set. In particular, we consider that the system is trained on a clean dataset Dc and evaluated on a test datasetDtest. The attacker has knowledge of both sets, as well as of the system\u2019s architecture and its training pipeline. With this information, the attacker creates a poi\u2010 soned dataset Dp, with |Dp| = \u03f5|Dc|, so that training the attacked system on Dc \u222a Dp impacts its performance, or in our case its fairness. The parameter \u03f5 controls the per\u2010 centage of poisoned points, which depends on the nature of the application. Finally, we assume that the attacked system has a defense mechanism B that possibly removes poisoned data with the use of anomaly detection techniques.\nInfluence Attack on Fairness \u2014 The Influence Attack on Fairness (IAF) is a gradient\u2010based data poisoning attack, derived from a combination of the works of Koh et al. [7], which introduces the basic influence attack, and Zafar et al. [8], which proposes a novel fair\u2010 ness loss. Themain idea is to buildDp from copies of two datapoints (x\u03031, y\u03031) and (x\u03032, y\u03032) sampled from Dc, and progressively update them to decrease model fairness, as mea\u2010 sured by an adversarial loss Ladv. The authors propose to use Ladv = Lbc +\u03bb \u00b7 Lf , where Lbc is any binary classification loss and Lf is the aforementioned fairness loss. To update (x\u03031, y\u03031) and (x\u03032, y\u03032), the paper suggests to perform gradient ascent on Ladv and then updateDp with their copies. Since Ladv depends on the trainedmodel\u2019s param\u2010 eters \u03b8\u0302, the gradient ascent follows an expectation\u2010maximization scheme, where in the expectation step the model is trained on B(Dc \u222a Dp)1and in the maximization step the points move on the gradient direction. Although this idea is very intuitive, calculating the gradient of Ladv w.r.t each adversarial point is challenging. The approach presented in [6] is to apply the chain rule as \u2202L\u2202x\u0303i = \u2202L \u2202\u03b8\u0302 \u2202\u03b8\u0302 \u2202x\u0303i , with the later derivatives calculated in Equations 1 and 2. Here, \u2113 is the model\u2019s train loss for the single data point and H\u03b8\u0302 is the Hessian of the train loss at \u03b8\u0302 w.r.t. the adversarial sample x\u0303i. More details for the derivation of these formulas, as well as how to compute them efficiently, can be found in Section 2.2 of [7] and Section 4.1.1 of [6].\ng\u03b8\u0302,Dtest def = \u2202L \u2202\u03b8\u0302 = 1 |Dtest| \u2211\n(x,y)\u2208Dtest\n\u2207\u2113(\u03b8\u0302; x, y) (1)\n\u2202\u03b8\u0302 \u2202x\u0303 = \u2212H\u22121 \u03b8\u0302 \u22022\u2113(\u03b8\u0302; x\u0303, y\u0303) \u2202\u03b8\u0302\u2202x\u0303\n(2)\nAnchoring Attack \u2014 The anchoring attack places poisoned datapoints, which act as an\u2010 chors, in the near vicinity of two target points. In particular, the attacker samples two target points xtarget\u2212, and xtarget+ from the advantaged Dadv and disadvantaged Ddisadv groups of the train dataset. Subsequently, |\u03f5n| poisoned datapoints {x\u0303i}|\u03f5n|i=1 are gener\u2010 ated in the near vicinity of the target points, placing them in the same demographic group but on opposite categories y\u0303i \u0338= ytarget. Intuitively, this aims to move the decision boundary so that more advantaged points have a positive predictive outcome and more disadvantaged points have a negative outcome, hence inducing more biased outcomes. The paper proposes two methods to sample xtarget\u2212 and xtarget+ from the dataset:\n\u2022 RandomAnchoring (RAA): xtarget is sampleduniformly for eachdemographic group. 1In the original paper, the authors mention that training is performed onDc\u222aDp, but we deem that using\nB(Dc \u222a Dp) is more sensible and congruent with the basic influence attack [6].\nNone 8.2 (#28) \u2013 Nalmpantis et al. 2022 4\n\u2022 Non\u2010RandomAnchoring (NRAA): xtarget is the point close to themost similar points given its label and demographic. This aims to affect as many points as possible when placing poisoned points within its vicinity.\nIn the latter case, the authors suggest to consider two points, x and x\u2032, as neighbors if and only if ||x \u2212 x\u2032|| < R, R \u2208 R. The choice of R and the specific norm || \u00b7 || is not defined in the paper. After careful examination of the provided code, we found that the L1 norm was used and the R values were hard\u2010coded for each dataset. To avoid manual experimentation for each dataset\u2019s R, we propose the following definition for the most popular point in a dataset X :\nxpop def = argmax\nx\u2208X \u2211 x\u2032\u2208X exp\n( \u2212d(x, x \u2032)\n\u03c32d(X )\n) (3)\nwhere d is a distance metric and \u03c32d(X ) denotes the variance of the points\u2019 distances to each other under d. Motivation for this choice and implementation details can be found in Appendix B.\n3.2 Defenses The authors use a defense mechanism B in both of the proposed attacks, along with a corresponding projection function that bypasses it, without specifying the actual type of the defense. Although this information is not crucial for the comprehension of the attacks, we deem it critical for their reproducibility. After inspecting the code and the cited literature, we found that the defensemechanism used is a combination of the L2 defense and the slab defense [9]. The L2 defense removes points far from their corresponding class\u2019 centroid according to the L2 distance:\n\u03b2y = ED[x | y], s\u03b2 = ||x\u2212 \u03b2y||2\nThe slab defense projects points onto the line between the class centroids and then re\u2010 moves the points too far from the centroids:\n\u03b2y = ED[x | y], s\u03b2 = \u2223\u2223(\u03b21 \u2212 \u03b2\u22121)\u22a4(x\u2212 \u03b2y)\u2223\u2223\nThe feasible set F\u03b2 \u2282 X \u00d7 Y encodes the defenses, as well as the constraints for the in\u2010 put\u2019s features, and contains all of the points that would not be discarded by the defender. For the L2 constraint, we apply the LP relaxation technique as described in [6] and end up with a feasible set:\nFLP = { (x, y) : E [\u2225\u2225x\u0302\u2212 \u00b5y\u2225\u222522] \u2264 \u03c42y \u2227 x \u2208 R\u22650} where \u00b5y denotes the centroid of the subset of points in class y. The parameter \u03c4y is chosen dynamically for each y, such that 90% of the points in the Dy subset satisfy the L2 constraint. For the slab constraint, we construct a feasible set:\nFslab = { (x, y) : |(\u00b51 \u2212 \u00b5\u22121)\u22a4(x\u2212 \u00b5y)| \u2264 \u03c4 \u2032y \u2227 x \u2208 R\u22650 } where \u00b51 and \u00b5\u22121 denote the centroids of classes 1 and \u22121 respectively. Once again, the parameter \u03c4 \u2032y is chosen dynamically for each y such that 90% of the points in theDy subset satisfy the slab constraint. Our final feasible set is the intersection of the feasible sets under the two constraints, plus any additional input constraints imposed by X . Projecting points ontoF\u03b2 takes the formof anoptimizationproblem, namely calculating argminx\u2208F\u03b2 \u2225x\u2212 x\u0303i\u22252, where x\u0303i denotes the poisoned point. We then simply solve the optimization problem using the library CVXPY with the SCS solver. This procedure is extensively discussed in [6], Section 3.3.\nNone 8.2 (#28) \u2013 Nalmpantis et al. 2022 5\n4 Experimental Setup\n4.1 Model and training pipeline We did not manage to find a detailed description of either the model used or its training pipeline in the original paper. The authors mention that the hinge loss was used, lead\u2010 ing us to assume that benchmarked model was a Support Vector Machine. However, after examining their code, we identified that the default model used was a Logistic Re\u2010 gression model. We also followed this choice, as it allows for an easy calculation of the fairness loss used in the influence attack on fairness. Additionally, the authors seem to use SciPy\u2019s fmin_ncg optimizer to train the model, which is a second\u2010order opti\u2010 mization algorithm that uses conjugate gradients. In our implementation, we opted for Stochastic Gradient Descent, which should be able to converge to the same parameters, as the minimization problem is convex. In our reported results, we used the average over three runs to account for any stochasticity in the pipeline.\n4.2 Datasets We carry out our experiments on the same three datasets as the original paper and con\u2010 sider \u201cgender\u201d to be the sensitive attribute. We use a pre\u2010processed version of each dataset, as provided by the authors, to have a common starting point. However, we later discovered a few issues regarding the pre\u2010processing pipeline, which we elaborate on in Appendix A. In all cases, the test set consists of 20% of the total data and there is no validation set. A short description of each dataset is presented below: German Credit Dataset2 [10]. This dataset has 1000 entries of loan applicants. Each applicant is characterized by 13 categorical and 7 numerical features describing their credit risk and is classified as either \u201cgood\u201d or \u201cbad\u201d, in terms of their ability to repay the loan. COMPAS Dataset3 [11]. This dataset has 7214 entries of criminal defendants. We utilize 8 categorical features from the dataset to predict whether a defendant will recommit a crime within 2 years. Drug Consumption Dataset4 [12]. This dataset has 1885 entries of people alongside their drug history. Each person is described by 13 numerical attributes, which can be used to infer drug usage of 18 different substances. We focused on predicting whether indi\u2010 viduals have used cocaine in their lifetime, akin to the original paper.\n4.3 Fairness Metrics We evaluate the impact of our attacks both in terms of performance and fairness. For performance, we use the accuracy error, while for fairness we use the Statistical Parity Difference (SPD) [13] and the Equality of Opportunity Difference (EOD) [14]. This eval\u2010 uation protocol matches the one in the original paper, although our implementation of EOD gives different results. We were able to verify our results\u2019 validity by compar\u2010 ing them with the AI Fairness 360 library [15]. Moreover, the original paper used the absolute values of the aforementioned metrics, which we followed for the reproduced experiments but not for our extensions, as the metrics\u2019 signs contained the necessary information. Statistical Parity Difference. Statistical parity is used to ensure that the demographic distribution of the samples being classified positively (or negatively) is similar to the distribution of the entire population. As a result, when we measure the difference in\n2https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data-numeric 3https://github.com/propublica/compas-analysis/blob/master/compas-scores-two-years.csv 4https://archive.ics.uci.edu/ml/machine-learning-databases/00373/drug_consumption.data\nNone 8.2 (#28) \u2013 Nalmpantis et al. 2022 6\nstatistical parity between the twodemographics (advantaged and disadvantaged groups), we can deduce whether a model is biased in favoring or harming one of the two groups.\nSPD = \u2223\u2223 P (ypred = +1 | x \u2208 Dadv)\u2212 P (ypred = +1 | x \u2208 Ddisadv) \u2223\u2223\nEquality of Opportunity Difference. Equality of opportunity is used to guarantee that samples with a positive ground truth label are just as likely to be classified positively, regardless of the demographic group they belong in. By measuring the difference in equality of opportunity for the two groups, we can identify whether the model is biased towards classifying positively more often for either demographic group, given that they have a positive ground\u2010truth label."}, {"heading": "EOD =", "text": "\u2223\u2223 P (ypred = +1 | x \u2208 Dadv, ylabel = +1)\u2212 P (ypred = +1 | x \u2208 Ddisadv, ylabel = +1) \u2223\u2223\n4.4 Hyperparameters For all of our experiments, we trained the models for 300 epochs with early stopping based on the train accuracy. We chose an SGD optimizer with a learning rate of 0.001, weight decay of 0.09, and batch sizes of 10, 50, and 10 for the German Credit, Drug Consumption, and COMPAS datasets respectively. Regarding the adversarial attack hy\u2010 perparameters, we used 100 iterations and a step size \u03b7 = 0.01 for the IAF, and \u03c4 = 0 for both anchoring attacks.\n4.5 Implementation Details We implemented the data poisoning attacks described above in Python, using PyTorch Lightning to train our models5. Each attack, along with its helper functions, is imple\u2010 mented in a separate file under the attacks folder. We also placed a utils.py file under the same folder, which implements essential utilities that are leveraged by all adversarial attacks. We defined two abstract classes, Dataset and Datamodule, in the corresponding files under the datamodules folder, which enable our framework to process a dataset from a given file and construct the required PyTorch DataLoader objects. Consequently, each dataset mentioned in Section 4.2 corresponds to a separate file under the same folder, deriving from the Datamodule class. Ourmodels are placed under the models folder, deriving from the LinearModel class, while the training pipeline is described in trainingmodule.py. Finally, our fairness metrics and losses are available in fairness.py. In this way, besides providing a well\u2010structured and easy\u2010to\u2010follow code, we also allow fellow researchers to extend our experiments by easily incorporating different models, attacks, datasets, and fairness metrics. To implement a new attack, one can simply create a separate file under the attack folder and leverage the implemented attack utilities, such as the projection and defense mechanisms. To test existing attacks with a different dataset, one can create a new PyTorch Lightning LightningDatamodule that extends our Datamodule class. Finally, in order to test a different model, one needs to create a PyTorch Module that extends the LinearModel class and update the BinaryClassifier class accordingly.\n4.6 Computational requirements All of our experiments required a total of 120 hours on a machine with an Intel Core i7 7700k CPU and an NVIDIA GeForce GTX 1080 GPU. We found the most computationally expensive part to be the training of the models, and hence the influence attack, which requires multiple train iterations. This makes it significantly slower than the anchoring attack. However, it is worth noting that a GPU is not strictly necessary. GPU speedups\n5Our code is available at https://github.com/toliz/fairness-attacks\nNone 8.2 (#28) \u2013 Nalmpantis et al. 2022 7\nwere in the vicinity of 20% over a CPU\u2010only setup since we only have a single\u2010layer linear model.\n5 Results\n5.1 Results reproducing the original paper In this Section, we are reporting the results for the two experiments conducted in the original paper.\nImpact of the proposed attacks on fairness \u2014 First, we evaluate the effectiveness of the pro\u2010 posed adversarial attacks on the three datasets mentioned in Section 4.2 using the met\u2010 rics discussed in Section 4.3, for varying \u03f5 values. We perform the anchoring attack, us\u2010 ing both random (RAA) and non\u2010random sampling (NRAA). We additionally reproduce Koh\u2019s influence attack [6] and Solan\u2019s attack [5], using our implementation. Our results are presented in Figure 1 and correspond to Figure 2 of the original paper.\nWe observe that the IAF is the most versatile attack on fairness, as it can raise the test error by 20% and push the SPD and EOD values close to 1. This general trend matches the results of the original paper, although it appears that the effectiveness of the attack diminishes for higher values of \u03f5. As a result, we see caseswhere the fairness is impacted less than other attacks, which is contradictory to the results of the original paper. The NRAA appears to be the second most effective fairness attack, especially for the COMPAS dataset, where it can reach the performance of the IAF, at the cost of using a significantly higher percentage of poisoned data \u03f5. However, it also appears to increase the model\u2019s test error up to 20%, which contradicts the findings of the original paper, that the NRAA attack does not affect performance. Finally, the RAA appears to be less effective when compared to the NRAA. The test er\u2010 ror was preserved, as in the original paper, but its impact on fairness was inconsistent depending on the value of \u03f5 and the dataset. It is worth mentioning that this attack exhibited the most variance in our results when using different seeds, which can be explained by the method\u2019s inherent stochasticity.\nNone 8.2 (#28) \u2013 Nalmpantis et al. 2022 8\nRegulation of the trade-off between impacting performance and fairness \u2014We evaluated the reg\u2010 ulation of the trade\u2010off between impacting fairness and performance using the IAF on the same datasets and metrics as previously. Our results are presented in Figure 2 and, apart from our extra experiment for \u03bb = 0.5, correspond to Figure 3 of the original paper.\nWe notice that the IAF drops the model\u2019s performance by 10% to 20%. The hyperparam\u2010 eters \u03bb and \u03f5 seem to not have a strong correlation with the test error, as every pair of them leave it intact. This comes in contrast to the results of the original paper, where higher \u03bb and \u03f5 values affect the performance less. We also observe that higher \u03bb values have a greater impact on fairness, which is in accordance with the original paper\u2019s re\u2010 sults. However, in the original paper, higher \u03f5 values also increase the rate at which \u03bb affects the fairness of the targeted model, while in our results very high values, such as \u03f5 = 1, seem to have the opposite effect.\n5.2 Results beyond the original paper In this section, we report our results for an additional experiment we conducted. Al\u2010 though there were many interesting directions we wanted to investigate, we focused on just one due to limited time and resources.\nInversion of the dataset\u2019s bias direction \u2014 The experiments of Section 5.1.1 made us question whether it is possible to use the principal idea behind the IAF to inverse the bias present in the datasets, instead of always exacerbating it in favor of the advantaged group. To this end, we changed the sign of \u03bb, according to the intrinsic bias of the dataset. Our results are presented in Figure 3 and indicate this approach does indeed shift the bias of the dataset towards the other extreme. An important byproduct of this technique is that it can be used to mitigate the existing bias of the datasets. We observe in Figure 3 that for \u03bb = 0.2, the fairness metrics ap\u2010 proach zero while the performance remains on the same level. Hence, tuning the value of \u03bb in a held\u2010out validation set would allow us to augment the existing datasets to be fairer without sacrificing performance. Do note that in this experiment we used the ac\u2010 tual differences of the SPD and EOD to better capture the direction of the bias. For more details, refer to Appendix C.\nNone 8.2 (#28) \u2013 Nalmpantis et al. 2022 9\n6 Discussion\nBased on the results of the first experiment, we are able to partially verify the first two claims of the paper. More specifically, both the IAF and the NRAA are indeed the most effective attacks on fairness, under most of the evaluated settings. However, the RAA performs poorly compared to the existing methods, such as Koh\u2019s and Solan\u2019s, which contradicts part of the first claim. What is more, although the RAA does not affect the performance of the targeted system, theNRAA can, which contradicts part of the paper\u2019s second claim. Similarly, the results of our second experiment suggest that although \u03bb is able to control the impact on fairness, it is not as effective in doing so with performance. Based on this, we can partially verify the third claim of the paper. In summary, although we were not able to fully verify the original claims based on our results, we can confirm the methods\u2019 effectiveness in attacking fairness.\n6.1 What was easy One of the things we found welcoming was the overall presentation of the paper which is nicely structured and has cohesive sections. The provided pseudo\u2010code condenses the principal ideas of both attacks very intuitively, and the datasets used in the paper are publicly available and small in size. The latter welcomes everyone to reproduce the results, regardless of their computational budget. Additionally, the authors provide their code on GitHub where missing details can be found easily. All these elements hint at an easy reproduction of the results.\n6.2 What was difficult As we got familiarized with the concepts behind the attacks, we identified some issues which were not apparent at first. To begin with, even though the principal ideas are in\u2010 tuitive, the notation used is not always self\u2010sufficient. The algorithms depend on other utilities (such as the projection of data in the feasible set) and non\u2010trivial calculus op\u2010 erations, which are not discussed. Additionally, information about the model, training pipeline, hyperparameters, and data pre\u2010processing used is absent. For these elements, we tried consulting the code provided by the authors, but it turned out to be convo\u2010 luted. We encountered a structure that was hard to follow, non\u2010intuitive variable names, absence of comments and docstrings, and large portions of unused code. All these ele\u2010 mentsmade the reproduction of the results challenging and required some assumptions and critical decisions on our part.\n6.3 Communication with the authors We contacted the first author with a list of questions to resolve the existing ambiguities. Although the response was fairly quick, we were prompted to check the existing code in\u2010depth, while further communication was discouraged.\nNone 8.2 (#28) \u2013 Nalmpantis et al. 2022 10\n7 Conclusion\nIn this reproduction study, we extensively reviewed the paper Exacerbating Algorithmic Bias through Fairness Attacks. We provided a clear foundation, upon which we described the proposed data poisoning attacks, namely the influence attack on fairness and the anchoring attack, as well as the experimental setup of the original paper. We filled in numerous details that we considered crucial for the reproducibility of the results. We evaluated the effectiveness of the proposed attacks both in terms of performance and fairness, and even though we did not manage to get the exact results of the original paper, our experiments show similar trends. Hence, we can verify the superiority of the proposed methods compared to the rival ones. Finally, we examined the regulation of the trade\u2010off between impacting fairness and performance and found that while the impact on performance cannot be directly controlled, the impact in fairness can be. These findings suggest that although the original paper is not reproducible, its claims are valid."}, {"heading": "Appendix", "text": "A List of inconsistencies and assumptions\nAfter studying the original paper and the provided code, we spotted a few inconsisten\u2010 cies between the two. In order to deal with them, we had to make some assumptions that better aligned with the methods presented in the original paper. Regarding the influence attack on fairness, Koh et al. [6] suggest that the train set during the attack is not Dc \u222a Dp, but B(Dc \u222a Dp), i.e. the set that passes from the defense mechanism B. Hence, we assume that when the authors mention that they update the feasible setF\u03b2 \u2190 B(Dc\u222aDp), theymean that they update the parameters \u03b2 of the feasible set. Additionally, pre\u2010computingH\u22121\n\u03b8\u0302 is computationally expensive and is avoided in the\nauthors\u2019 code. Instead the computational trick introduced in Koh et al. [7] is used. Regarding the anchoring attack, we noticed two issues in the paper and the accompa\u2010 nied code. The anchoring attack with non\u2010random sampling is deterministic and thus each iteration of attack will result in the same poisoned dataset Dp discarding the need to have multiple iterations. Moreover, the anchoring attack with random sampling is a stochasticmethod, yet in the existing implementation, the randomnumber generator is seeded with the same number in every iteration, resulting in the same poisoned dataset Dp. As a result, the attack\u2019s output will be deterministically generated as the method\u2019s stochasticity is discarded with the iterations being redundant. Regarding the helper functions for both attacks and defenses, it seems that the authors use the LP relaxation technique implemented in [6] by default in their experiments. However, we could not find an explicit mention of this in paper. Additionally, we did not find any suggestion for choosing the neighbor cutoff radius \u03c3, which seems to be hard\u2010 coded for every dataset. Finally, the choice of radii for the L2 constraint and slab cutoff are not discussed in the paper, although the authors seem to use similar techniques to the ones discussed in 3.2. Regarding the pre\u2010processing pipeline applied to the original data, we noticed it is nei\u2010 ther mentioned in the paper nor provided in the GitHub repository of the authors. Af\u2010 ter contacting them, they pointed us to another repository that included a similar pre\u2010 processing pipeline to the one applied for the paper. Observing the code, we noticed two issues. Categorical data were converted to one\u2010hot encoded and then standardized with the quantitative features, which is not the most efficient technique. Also, the test data were normalized along with the train data, allowing information from the test set to be utilized for training. Regarding the experimental setup, the reported results in the paper are the output of a single seed for the random generator. As a consequence, there was only a single split of the data between training and testing leading to results with high variance.\nB Finding the most popular point in a dataset\nLet x1, x2, . . . , xn \u2208 Rm be points in a dataset X . Our goal is to define the most popular point xpop in a meaningful way, such that it is dataset agnostic, i.e. it does not require manual input of parameters, such as a manually defined radius for each dataset. We mainly experimented with two methods.\n\u2022 Method A: Percentile Radius: We define the most popular point\nxpopA def = argmax\nx\u2208X CountN (x, R) (4)\nwhere CountN(x, R) is a function that returns the number of points xi \u2208 X such that d(x, xi) \u2264 R, R \u2208 R+ \u2200xi \u2208 X for some distance metric d. The problem of\nNone 8.2 (#28) \u2013 Nalmpantis et al. 2022 13\npicking a fitting radiusR is not trivial as the radius has to be neither too small nor too big as either all or no points would be considered neighbors, respectively. The method we propose is to pick a radius R such that at least \u03b1% of x \u2208 X satisfy ||x \u2212 \u00b5|| \u2264 R, where \u00b5 the centroid of X . In our experiments, \u03b1 = 15 has proved to be decent for all three datasets.\n\u2022 Method B: Exponentially decayed distances: We define the most popular point\nxpopB def = argmax\nx\u2208X \u2211 x\u2032\u2208X exp\n( \u2212d(x, x \u2032)\n\u03c32d(X )\n) (5)\nwhere d is a distance metric and \u03c32d(X ) denotes the variance of all the distances of the points in the dataset to each other under d. We define \u03c32d(X ) def = Var (vec(d(X ))),\nwhere [d(X )]ij := d ( [X ]i: , [X ]j: ) .\nIn Method A, we still define neighbors based on balls surrounding datapoints. Even though we still have to pick an \u03b1, the choice is easier, as we don\u2019t have to manually check the distances in the dataset. InMethod B, we discard the idea of neighbors based on radii around points and we turn our focus on finding a datapoint in a very dense area of the dataset. To ensure that the sum is higher for points with a lot of other points in their close vicinity, we exponentially decay the distances. This forces points close to our point in question to contribute more to the sum. We also need the method to be dataset agnostic, thus we need to scale the wideness of the exponential kernel. If the variance6of the distances is high, we need to widen the kernel such that points further away still contribute to the sum. In contrast, if distances have low varianceweneed to sharpen the exponential kernel tomake sure that only points close enough to the point in question contribute to the sum. We define the variance of the dataset X as \u03c32d(X ) = Var (vec(d(X ))), where [d(X )]ij := d ( [X ]i: , [X ]j: ) . We opted for this method since it requires the least amount of arbitrary assumptions about the dataset. Preliminary experiments hinted towardsmethod B achieving slightly better results in our task, but this wasn\u2019t pursued further. In the Anchoring Attack, we need to sample a negative sample xtarget\u2212 from the advan\u2010 taged class Dadv and a positive sample xtarget+ from the disadvantaged class Ddisadv. In the non\u2010random sampling setting (NRAA), we simply calculate the most popular point in the negative but advantaged class Dadv \u2229 D\u2212 \u2282 D and the most popular point in the positive but disadvantaged class Ddisadv \u2229 D+ \u2282 D.\nC Data Augmentation\nAs it has been demonstrated through experimental evaluation, the IAF can deteriorate a model\u2019s fairness. However, we argue that the same approach can be applied for data augmentation to increase a model\u2019s fairness resulting in an unbiased classifier. The use of the fairness metrics with absolute values, as described in Section 4.3, fails to highlight the bias direction. However, by using the actual differences of the metrics, we canutilize this information. Therefore, knowing the initial bias of the data by inspecting the sign of P (ylabel | x \u2208 Dadv) \u2212 P (ylabel | x \u2208 Ddisadv), we can assume that the model\u2019s bias will be in the same direction, i.e., the SPD and EOD will have the same sign. To this end, to direct a model\u2019s bias towards zero, we have to use the opposite sign of the aforementioned quantity for the values of \u03bb.\n6The mean of the dataset or some other statistic could also be used, which intuitively makes more sense. Basic experiments hinted that dividing by the variance performed better, but the mean method can not be completely discarded as we didn\u2019t conduct thorough experiments due to time constraints.\nNone 8.2 (#28) \u2013 Nalmpantis et al. 2022 14\nMoreover, as the altered method is used for augmentation, the test dataset Dtest should not be utilized, in contrast with the IAF. Finally, we could use a validation set to halt the data augmentation process in order to find the optimal value of \u03bb where the SPD and EOD would be close to zero.\nNone 8.2 (#28) \u2013 Nalmpantis et al. 2022 15"}], "title": "[Re] Exacerbating Algorithmic Bias through Fairness Attacks", "year": 2022}