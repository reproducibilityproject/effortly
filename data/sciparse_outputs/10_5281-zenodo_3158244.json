{"abstractText": "DOI 10.5281/zenodo.3158244 Welcome to this special issue of the ReScience C journal, which presents results of the 2019 ICLR Reproducibility Challenge (2nd edition). One of the challenges in machine learning research is to ensure that published results are sound and reliable. Reproducibility, that is obtaining similar results as presented in a paper, using the same code and data (when available), is a necessary step to verify research findings. Reproducibility is also an important step to promote open and accessible research, thereby allowing the scientific community to quickly integrate new findings and convert ideas to practice. Reproducibility also promotes use of robust experimentation workflows, which can potentially reduce unintentional errors.", "authors": [{"affiliations": [], "name": "Joelle Pineau"}, {"affiliations": [], "name": "Koustuv Sinha"}, {"affiliations": [], "name": "Genevieve Fried"}, {"affiliations": [], "name": "Rosemary Nan Ke"}, {"affiliations": [], "name": "Hugo Larochelle"}, {"affiliations": [], "name": "Nicolas Rougier"}], "id": "SP:5d171b1243bcda5255358c06362fc1f36c2ed302", "references": [], "sections": [{"text": "R E S C I E N C E C"}, {"heading": "Editorial", "text": ""}, {"heading": "ICLR Reproducibility Challenge 2019", "text": "Joelle Pineau1,2,3, Koustuv Sinha1,2,3, ID , Genevieve Fried1, Rosemary Nan Ke2,3, and Hugo Larochelle4 1School of Computer Science, McGill University, Montreal, Canada \u2013 2Montreal Institute of Learning Algorithms (Mila), Montreal, Canada \u2013 3Facebook AI Research (FAIR), Montreal, Canada \u2013 4Google Brain, Montreal, Canada \u2013 5Polytechnique Montr\u00e9al, Montreal, Canada\nEdited by Nicolas Rougier ID\nReceived 04 May 2019\nPublished 22 May 2019\nDOI 10.5281/zenodo.3158244\nWelcome to this special issue of the ReScience C journal, which presents results of the 2019 ICLR Reproducibility Challenge (2nd edition). One of the challenges in machine learning research is to ensure that published results are sound and reliable. Reproducibility, that is obtaining similar results as presented in a paper, using the same code and data (when available), is a necessary step to verify research findings. Reproducibility is also an important step to promote open and accessible research, thereby allowing the scientific community to quickly integrate new findings and convert ideas to practice. Reproducibility also promotes use of robust experimentation workflows, which can potentially reduce unintentional errors.\nThe Challenge \u2014 In support of this, the goal of this challenge was to investigate reproducibility of empirical results submitted to the 2019 International Conference on Learning Representations (ICLR). Primarily, the aimwas to assess if the experiments reported in a paper are reproducible, and to determine if the conclusions of the paper are supported by the findings of the reproducibility report. The role of each challenge participant was to be an inspector verifying the validity of the experimental results and conclusions of the paper. Entry into the challenge was open to all, either individually or in a team. Several graduate-level machine learning courses in universities around the world incorporated this challenge as a final course project for their students. In total, 90 teams from 31 universities and 4 companies participated in the challenge, and 26 teams, from 10 universities, submitted reports investigating 26 ICLR submissions.\nReplicability and Reproducibility \u2014 Reproduction of a computational study means running the same code, using the same input data, and then checking if the results are the same, or at least \u201cclose enough\u201d when it comes to numerical approximations. This is most easily achieved when the code and data are openly shared. Alternately, the methods described can also be implemented/re-implemented according to the description in the paper, which promotes Replicability. This is a higher bar than reproducibility, and may be helpful in detecting anomalies in the code, or shedding light on aspects of the implementation that affect results. In the absence of code, several aspects of a paper can influence the ease with which the results can be replicated, as listed in the Reproducibility Checklist.\nBaselines need attention \u2014 It is sometimes not feasible to reproduce all the experiments in a paper: factors such as private datasets, extensive training time, requirement of extensive or non-standard computing infrastructure can all limit reproducibility. In those cases, challenge participants were encouraged to reproduce results from baselinemethods. It is sometimes the case that baseline methods are not properly implemented, or\nCopyright \u00a9 2019 J. Pineau et al., released under a Creative Commons Attribution 4.0 International license. Correspondence should be addressed to Koustuv Sinha (koustuv.sinha@mail.mcgill.ca) The authors have declared that no competing interests exists.\nReScience C 5.2 (#5) \u2013 Pineau et al. 2019 1\nhyper-parameter search is not done with sufficient care, leading to poor comparison of alternative methods. Reproducing the baselines can be as impactful as reproducing the main technical contributions of a paper, and therefore this was encouraged in this challenge.\nRelationship with Authors \u2014 Authors of research papers have as much to gain from this challenge as the participants. We encouraged participants to communicate with the authors to clarify various nuances of the open source implementation or to communicate the choice of hyperparameters in their algorithmic implementation. In the 1st edition of the challenge, we found that this helped several authors improve the quality of their work and paper. During the review period, communication between authors and challenge participants was done through the open comment platform on Open Review.\nPublication medium \u2014 Challenge participants were encouraged to prepare a written report of their reproducibility study, for submission to this special issue. ReScience C provides the perfect platform for publication of reproducibility efforts. ReScience C lives on GitHub where each new implementation of a computational study is made available together with comments, explanations and tests. This exactly aligns with our philosophy and goal from the challenge, which also lives in Github on its own repository consisting of submissions and the reviewer comments. We received 26 submissions in total, of which 4 reports are chosen to be published in this journal, following a single-blind review process.\nContent \u2014 The 4 reports in this special issuewere selected for their high standard of scholarship, including clear explanation of the scope and objectives, care in themethodology, clarity of explanations, thoroughness of results, and insightfulness of the findings. In the report on Learning Neural PDE Solvers with Convergence Guarantees, the authors perform robust experiments on the proposed approach and provide a well documented codebase and Jupyter notebooks for quick replication. In the report onVariational Sparse Coding, the authors extendupon the codebase released alongwith the original paper and perform rigorous experiments, while validating the hyperparameters used by effective communication with the authors of the main paper through OpenReview, the conference management system used by ICLR. In the report on H-detach, the authors went one step further to propose their own faster implementation based on low level CUDA binaries in order to improve training. Finally, the report on Meta learning with differentiable closed form solvers, the authors provide thoughtful discussions on the repercussions on reproducibility and fairness of comparisonwith prior literature of the choice varying the number of classes at training time, which points to the care and attention to detail employed by the authors in this work.\nConclusion \u2014 Reproducibility in machine learning has recently garnered a considerable amount of attention and momentum thanks to key efforts by top researchers. Conferences such as ICLR, AAAI, ICML have organized dedicated workshops on the topic. The premier conference in the field, NeurIPS, has recently adopted the pledge of reproducibility as part of their submission process. We hope our endeavour will similarly spur more efforts in reproducing existing ideas and papers, and in turn promote open, accessible and sound machine learning research.\nReviewers \u2014Many thanks to all our reviewers who spent their precious time to critically review the reports. We acknowledge your hard effort and hope that you will keep supporting us in this endeavour in the future!\n\u2022 Andrew Jaegle, University of Pennsylvania\nReScience C 5.2 (#5) \u2013 Pineau et al. 2019 2\n\u2022 Arna Ghosh, McGill University\n\u2022 Chaochao Lu, University of Cambridge\n\u2022 Ishan Durugkar, University of Texas at Austin\n\u2022 Jiahui Yu, University of Illinois, Urbana Champaign\n\u2022 Joelle Pineau, Facebook AI Research / McGill University\n\u2022 Joey Bose, Mila / McGill University\n\u2022 Koustuv Sinha, Mila / McGill University / Facebook AI Research\n\u2022 Lovedeep Gondara, Simon Fraser University\n\u2022 Maneesh K Singh, Verisk\n\u2022 Martin Jaggi, \u00c9cole polytechnique f\u00e9d\u00e9rale de Lausanne\n\u2022 Malik Altakrori, Mila / McGill University\n\u2022 Melanie Fernandez Pradier, Harvard University\n\u2022 Michela Paganini, Facebook AI Research\n\u2022 Mido Assran, Facebook AI Research / McGill University\n\u2022 Nicolas Gontier, Mila / Element AI\n\u2022 Noe Casas, Universitat Politecnica de Catalunya\n\u2022 Olexa Bilaniuk, Mila / Universit\u00e9 de Montr\u00e9al\n\u2022 Pablo Samuel Castro, Google Brain\n\u2022 Peter Henderson, Stanford University\n\u2022 Rosemary Nan Ke, Mila / Facebook AI Research\n\u2022 Ryan Lowe, Mila / McGill University / Facebook AI Research\n\u2022 Shagun Sodhani, Mila / Universit\u00e9 de Montr\u00e9al\n\u2022 Seungjae Ryan Lee, END-TO-END AI, Princeton University\n\u2022 Xavier Giro, Universitat Politecnica de Catalunya\nReScience C 5.2 (#5) \u2013 Pineau et al. 2019 3"}], "title": "ICLR Reproducibility Challenge 2019", "year": 2019}