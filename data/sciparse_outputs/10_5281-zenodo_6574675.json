{"abstractText": "Kalantzis et al. [1] present a method to update the rank\u2010k truncated SVD of matrices where the matrices are subject to periodic additions of rows or columns. The main claim of the original paper states that the presented algorithms outperform other state\u2010 of\u2010the\u2010art approaches in terms of accuracy and speed. However, no results were given comparing the proposed methods to other state\u2010of\u2010the\u2010art methods. Accordingly, we reproduce their results and compare it to the state\u2010of\u2010the\u2010art FrequentDirections streaming algorithm [2].", "authors": [{"affiliations": [], "name": "Andy Chen"}, {"affiliations": [], "name": "Shion Matsumoto"}, {"affiliations": [], "name": "Rohan Sinha Varma"}, {"affiliations": [], "name": "Koustuv Sinha"}, {"affiliations": [], "name": "Sharath Chandra Raparthy"}], "id": "SP:9030aa19f16c9ca4bb23d868d0986da38a79fa6c", "references": [{"authors": ["V. Kalantzis", "G. Kollias", "S. Ubaru", "A.N. Nikolakopoulos", "L. Horesh", "K.L. Clarkson"], "title": "Projection techniques to update the truncated SVD of evolving matrices with applications.", "venue": "Proceedings of the 38th International Conference on Machine Learning", "year": 2021}, {"authors": ["M. Ghashami", "E. Liberty", "J.M. Phillips", "andD"], "title": "P.Woodruff. \u201cFrequent Directions: Simple andDeterministicMatrix Sketching.", "venue": "SIAM Journal on Computing", "year": 2016}, {"authors": ["H. Zha", "H.D. Simon"], "title": "Timely communication on updating problems in latent semantic indexing.", "venue": "Society for Industrial and Applied Mathematics", "year": 1999}, {"authors": ["D.P. O\u2019Leary"], "title": "The block conjugate gradient algorithm and related methods.", "venue": "Linear Algebra and its Applications 29 (Feb", "year": 1980}, {"authors": ["C.R. Harris"], "title": "Array programming with NumPy", "venue": "Sept. 2020. DOI: 10.1038/s41586-020-2649-2", "year": 2020}, {"authors": ["P. Virtanen"], "title": "SciPy 1.0: fundamental algorithms for scientific computing in Python.", "venue": "Nature Methods 17.3 (Mar", "year": 2020}, {"authors": ["F. Pedregosa"], "title": "Scikit-learn: Machine Learning in Python.", "venue": "Journal of Machine Learning Research", "year": 2011}, {"authors": ["J.D. Hunter"], "title": "Matplotlib: A 2D Graphics Environment.", "venue": "Computing in Science Engineering", "year": 2007}, {"authors": ["N. Halko", "P.G. Martinsson", "J.A. Tropp"], "title": "Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions.", "venue": "SIAM Review", "year": 2011}, {"authors": ["D. Cai", "X. He", "J. Han"], "title": "Document clustering using locality preserving indexing.", "venue": "IEEE Transactions on Knowledge and Data Engineering", "year": 2005}, {"authors": ["D. Cai", "X. He", "W.V. Zhang", "J. Han"], "title": "Regularized locality preserving indexing via spectral regression.", "venue": "Proceedings of the sixteenth ACM conference on Conference on information and knowledge management - CIKM \u201907", "year": 2007}, {"authors": ["D. Cai", "Q. Mei", "J. Han", "C. Zhai"], "title": "Modeling hidden topics on document manifold.", "venue": "Proceeding of the 17th ACM conference on Information and knowledge mining - CIKM \u201908", "year": 2008}, {"authors": ["D. Cai", "X.Wang", "andX. He"], "title": "Probabilistic dyadic data analysiswith local and global consistency.", "venue": "In:Proceedings of the 26th Annual International Conference on Machine Learning - ICML \u201909", "year": 2009}, {"authors": ["F.M. Harper", "J.A. Konstan"], "title": "The movielens datasets: History and context.", "venue": "ACM Transactions on Interactive Intelligent Systems 5.4 (Dec", "year": 2015}], "sections": [{"text": "R E S C I E N C E C Replication / ML Reproducibility Challenge 2021\n[Re] Projection-based Algorithm for Updating the"}, {"heading": "TruncatedSVD of Evolving Matrices", "text": "Andy Chen1, ID , Shion Matsumoto1, ID , and Rohan Sinha Varma1, ID 1University of Michigan, Ann Arbor, Michigan, USA\nEdited by Koustuv Sinha,\nSharath Chandra Raparthy\nReviewed by Anonymous Reviewers\nReceived 04 February 2022\nPublished 23 May 2022\nDOI 10.5281/zenodo.6574675"}, {"heading": "Reproducibility Summary", "text": ""}, {"heading": "Scope of Reproducibility", "text": "Kalantzis et al. [1] present a method to update the rank\u2010k truncated SVD of matrices where the matrices are subject to periodic additions of rows or columns. The main claim of the original paper states that the presented algorithms outperform other state\u2010 of\u2010the\u2010art approaches in terms of accuracy and speed. However, no results were given comparing the proposed methods to other state\u2010of\u2010the\u2010art methods. Accordingly, we reproduce their results and compare it to the state\u2010of\u2010the\u2010art FrequentDirections streaming algorithm [2]."}, {"heading": "Methodology", "text": "We re\u2010implemented the algorithm in Python and evaluated the performance on five datasets. All experiments were run on a MacBook Pro and the code is available on GitHub1. The accuracy of the methods were evaluated using the same metrics as in the paper."}, {"heading": "Results", "text": "We successfuly reproduced the task\u2010agnostic experiments of the original paper, finding our results to stronglymatch with the original results. We also carried out a comparison with FrequentDirections but found the evaluation metrics of the original paper to be ill\u2010suited to compare \u2010 setting up for further work on developing fair comparisons."}, {"heading": "What was easy", "text": "The benchmark algorithm was fairly simple to implement. Furthermore, running the experiments did not place any computational resource burden as all experiments could be run on a laptop.\n1https://github.com/andyzfchen/truncatedSVD\nCopyright \u00a9 2022 A. Chen, S. Matsumoto and R.S. Varma, released under a Creative Commons Attribution 4.0 International license. Correspondence should be addressed to Andy Chen (andych@umich.edu) The authors have declared that no competing interests exist. Code is available at https://github.com/andyzfchen/truncatedSVD. \u2013 SWH swh:1:dir:4116fecf6ec4ac207cdad025ec62b25839a75678. Open peer review is available at https://openreview.net/forum?id=HN2xWpMQ30K.\nReScience C 8.2 (#25) \u2013 Chen, Matsumoto and Varma 2022 1"}, {"heading": "What was difficult", "text": "The most difficult part of the reproduction study was understanding the justification underlying the construction of the algorithm as it involved several complex proofs from numerical linear algebra to provide bounds on the accuracy. Demystifying the specifics of constructing the projection matrix for the main algorithm the author\u2019s propose was also initially difficult until we gained access to their code.\nCommunication with original authors\nWe contacted one of the authors by email and received their data and MATLAB imple\u2010 mentation of the algorithm and experiments.\nReScience C 8.2 (#25) \u2013 Chen, Matsumoto and Varma 2022 2\n1 Introduction\nThe singular value decomposition (SVD) remains a fundamental dimensionality reduc\u2010 tion technique inmachine learning and continues to be used in a variety of applications. In a traditional formulation, the entirety of the matrix to be decomposed is available at the time of application of the SVD. However, certain applications, such as latent se\u2010 mantic indexing (LSI) and recommender systems, have matrices that are subject to the periodic addition of new rows and/or columns. A na\u00efve solution is to recalculate the SVD each time the matrix is updated, but such an approach quickly becomes impracti\u2010 cal when updates are frequent. For this reason, algorithms that exploit information on the previous SVD of the matrix to calculate the SVD of the updated matrix are crucial. Such schemes have been proposed for both the full SVD and rank\u2010k SVD. The algorithm presented in [1], which is the focus of our study, is for the rank\u2010k truncated SVD case. Following the notation introduced in [1], the problem of updating the rank\u2010k truncated SVDof anupdatedmatrix is as follows. LetB \u2208 Cm\u00d7n be amatrix forwhich a rank\u2010k SVD Bk = Uk\u03a3kV H k = \u2211k j=1 \u03c3ju\n(j)(v(j))H where Uk = [u(1), . . . , u(k)], Vk = [v(1), . . . , v(k)], and \u03a3k = diag(\u03c31, . . . , \u03c3k) where \u03c31 \u2265 \u03c32 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03c3k > 0 is known. The goal is to approximate the rank\u2010k SVD Ak = U\u0302k\u03a3\u0302kV\u0302 Hk = \u2211k j=1 \u03c3\u0302j u\u0302\n(j)(v\u0302(j))H of the updated matrix\nA = ( B E ) , or A = ( B E ) whereE \u2208 Cs\u00d7n orE \u2208 Cm\u00d7s is thematrix containing the newly added rows or columns, respectively. We focus on the row\u2010update case in this study as is the case in [1]. The remainder of this study is outlined as follows. In Section 2, we introduce the central claim of the original paper that we tested in our study. Following that, in Section 3, we introduce the necessary background prior to describing the proposed algorithm. In Section 4, we describe the experimental setup: our implementation of the algorithm, datasets used, and experiments run. We present the experimental results in Section 5 along with our interpretation of the results and thoughts on the overall study in Section 6.\n2 Scope of reproducibility\nIn this study, we aimed to verify the central claim of the original paper, which stated that the proposed algorithm outperforms other state\u2010of\u2010the\u2010art approaches at calculat\u2010 ing the truncated SVD of evolving matrices. In particular, they claimed that the method had especially high accuracy for the singular triplets with the largest modulus singular values. We sought to verify this claim by evaluating two metrics using our implemen\u2010 tation of the method as well as with FrequentDirections, a state\u2010of\u2010the\u2010art matrix sketching and streaming algorithm [2]:\n1. Relative approximation error rel_err of leading k singular values of A (Equa\u2010 tion 1) is smaller when using the proposed algorithm compared to previous meth\u2010 ods.\nrel_err = \u2223\u2223\u2223\u2223 \u03c3\u0302i \u2212 \u03c3i\u03c3i \u2223\u2223\u2223\u2223 (1) 2. Scaled residual norm res_norm of leading k singular triplets {u\u0302(i), v\u0302(i), \u03c3\u0302i} (Equa\u2010\ntion 2) is smaller when using the proposed algorithm compared to previous meth\u2010 ods. res_norm = \u2225\u2225Av\u0302(i) \u2212 \u03c3\u0302iu\u0302(i)\u2225\u22252\n\u03c3\u0302i (2)\nAdditionally, we also sought to verify the original paper\u2019s claims about the runtime per\u2010 formance of the proposed algorithm.\nReScience C 8.2 (#25) \u2013 Chen, Matsumoto and Varma 2022 3\n3 Projection-based update algorithm\nIn the following sections, we first introduce the original zha-simon algorithm, then introduce the proposed projection\u2010based update algorithm. Note that there are two im\u2010 plementations to the proposed algorithm: one which uses the same projection matrix as the zha-simon algorithm (Algorithm 2.1) and another that uses an enhanced projec\u2010 tion matrix (Algorithm 2.2).\n3.1 Zha-Simon algorithm As motivated in the introduction, an update algorithm that uses prior knowledge re\u2010 garding the SVD of the matrix is crucial for it to be useful in practice. The algorithm proposed in [1] is based on an algorithm proposed in [3], the latter of which we will re\u2010 fer to as the zha-simon algorithm (Algorithm 1). Using zha-simon in the row\u2010update case A = ( B E ) , the QR decomposition of the row space of E that is not captured by the\nrange of the right singular vectors Vk can be expressed as (I \u2212 VkV Hk )EH = QR. Using this result and the previously known rank\u2010k SVD Bk = Uk\u03a3kV Hk , the updated matrix A can be decomposed approximately as follows:\nA = ( B E ) \u2248 ( Uk\u03a3kV H k E ) = ( Uk\nIs )( \u03a3k EVk R H )( V Hk QH ) (3)\nIf we let F\u0398GH be the compact SVD of (\n\u03a3k EVk R H\n) , then Equation 3 can be further\ndecomposed as follows: A \u2248 ( Uk\nIs\n)( F\u0398GH )(V Hk QH ) = (( Uk\nIs\n) F ) \u0398 (( Vk Q ) G )H (4)\nThe key here is to notice that the approximation of the rank\u2010k truncated SVD of A us\u2010 ing the zha-simon algorithm does not require access to the previous matrix B \u2013 only the rank\u2010k SVD Bk = Uk\u03a3kV Hk of the matrix from the previous iteration is needed. We can further simplify Equation 4 and see that it approximates the SVD of A as A \u2248\n(ZF )\u0398(WG)H where Z = ( Uk\nIs\n) and WH = ( Vk Q )H are orthonormal matrices with ranges that approximately capture range(U\u0302k) and range(V\u0302 Hk ), respectively.\nAlgorithm 1 zha-simon algorithm Require: A,E,Uk,\u03a3k, Vk, k\n1: Z \u2190 ( Uk\nIs ) 2: [Q,R]\u2190 qr(I \u2212 VkV Hk )EH 3: W \u2190 ( Vk Q\n) 4: [Fk,\u0398k, Gk]\u2190 svd(ZHAW, k) 5: Uk \u2190 ZFk 6: \u03a3k \u2190 \u0398k 7: V k \u2190WGk\nEnsure: Uk \u2248 U\u0302k,\u03a3k \u2248 \u03a3\u0302k, V k \u2248 V\u0302k\nAlgorithm 2 Proposed row\u2010update algo\u2010 rithm Require: B,E, k 1: [Uk,\u03a3k, Vk]\u2190 svd(B, k) 2: Construct projection matrix Z 3: [Fk,\u0398k] \u2190 svd(ZHA, k) where A =(\nB E ) 4: Uk \u2190 ZFk 5: \u03a3k \u2190 \u0398k 6: V k \u2190 AHUk\u03a3 \u22121 k\nEnsure: Uk \u2248 U\u0302k,\u03a3k \u2248 \u03a3\u0302k, V k \u2248 V\u0302k\n3.2 Proposed row-update algorithm In practice, computing the rank\u2010k truncated SVDofAusingAlgorithm1 is expensive due to the QR (Step 2) and SVD (Step 4) steps and possibly inaccurate based on the structure\nReScience C 8.2 (#25) \u2013 Chen, Matsumoto and Varma 2022 4\nof A [1]. The cost of the QR decomposition can be mitigated by setting W = In by observing that v\u0302(i) \u2286 range(In) for i = 1, . . . , n. Therefore, ZHAW in Step 4 can be replaced with ZHA and the QR decomposition in Step 2 can be eliminated. With these modifications, we have the newproposed row\u2010update algorithm (Algorithm2). Note that Step 2 has intentionally not been specified as the authors proposed two options for the construction of the projection matrix Z. The first option (Algorithm 2.1) uses the same Z matrix as in Algorithm. Although the construction of Z andZHA are presented in two separate steps in Algorithm 2, ZHA for Step 3 is directly computed as 1. Below are the expressions forZ andZHA for Algorithm 2.1.\nZ =\n( Uk\nIs\n) (5a)\nZHA =\n( \u03a3kV H k\nE\n) (5b)\nIn the casewhere the rankofB is larger than k and the singular values\u03c3k+1, . . . , \u03c3min(m,n) are not small, the approximation returned by Algorithm 2.1 can be of poor accuracy. Al\u2010 gorithm 2.2 addresses this by using an enhanced version of the projection matrix by adding a term \u2212B(\u03bb)BEH in the Z matrix such that\nZ =\n( Uk \u2212B(\u03bb)BEH\nIs\n) (6)\nSetting X = \u2212B(\u03bb)BEH , the additional term is equal to the matrix X that satisfies the equation\n\u2212(BBH \u2212 \u03bbIm)X = (Im \u2212 UkUHk )BEH , (7)\nwhich can be computed using the block conjugate gradient (BCG)method [4]. To ensure that the matrix \u2212(BBH \u2212 \u03bbIm) is positive definite for BCG, a lower bound of \u03bb > \u03c321 is imposed. The leading singular value canbe estimatedusing a few iterations of truncated SVD. However, to reduce the number of columns in X and keep Z manageable, the randomized rank\u2010r SVD ofX can be taken so that\n\u2212B(\u03bb)BEHR \u2248 X\u03bb,rS\u03bb,rY H\u03bb,r (8)\nwhere R is a matrix with at least r columns whose entries are i.i.d. Gaussian random variables with zero mean and unit variance. WithX\u03bb,r, the Z and ZHAmatrices can be calculated as\nZ =\n( Uk X\u03bb,r\nIs\n) (9a)\nZHA = \u03a3kV HkXH\u03bb,rB E  (9b) For more detailed explanations and derivations of the algorithms and their associated proofs, we refer readers to [1].\n4 Methodology\nProfessor Vassilis Kalantzis, who we contacted via email, generously provided us with the relevant MATLAB code and data; however, we chose to re\u2010implement the algorithm from scratch in Python with standard packages (NumPy [5], SciPy [6], and scikit\u2010learn [7]) and used the MATLAB code to confirm our implementation. We compared the per\u2010 formance of Algorithms 2.1 and 2.2 with FrequentDirections [2], a state\u2010of\u2010the\u2010art streaming algorithm. Experiments were conducted on a MacBook Pro with a 2.3 GHz\nReScience C 8.2 (#25) \u2013 Chen, Matsumoto and Varma 2022 5\nDual\u2010Core Intel Core i5 processor with 16 GB of RAM, and the code is publicly available on GitHub2. All plots were generated using Matplotlib [8].\n4.1 Implementation We chose to implement the three truncated SVD update algorithms as methods of an EvolvingMatrix class, which we will refer to as EM from here on out. With each ex\u2010 periment, the EM class was initialized with various parameters (initial matrix, matrix to be appended, number of batches, etc.) and updates were carried out using one of the up\u2010 date methods. A simplified version of the experiment is shown in Listing 1. Algorithms 2.1 and 2.2 were written based on the pseudo\u2010code presented in Algorithm 2, where the Z and ZHAmatrices were calculated using their respective formulas.\n# Initialize EM object with initial matrix, number of batches, and desired rank model = EM(initial_matrix, n_batches, k_dim)\n# Set entire matrix to be appended model.set_append_matrix(E)\n# Update over specified number of batches for i in range(n_batches):\nmodel.evolve() # append rows to matrix model.update_svd() # update truncated SVD\n# Calculate metrics for pre-selected updates if model.phi in phis2plot:\nmodel.calculate_true_svd() model.save_metrics()\nListing 1. Simplified experiment structure\nAlgorithm 2.1 The Z and ZHA matrices were constructed as in Equations 5a and 5b, respectively.\nAlgorithm 2.2 The main difficulty in implementing Algorithm 2.2 was in the calcula\u2010 tion of X\u03bb,r. We chose to solve for X in Equation 7 using the block Conjugate Gradi\u2010 ent method (BCG) [4] as recommended in [1]. Though [1] specified, at maximum, one iteration of BCG, we found that the MATLAB code set the limit to two iterations. As the additional iteration did not greatly increase the computational cost, we chose to run BCG a maximum of two iterations as well. Once X was calculated, we calculated X\u03bb,r as per Equation 8 using randomized SVD [9]. For this, we used the scikit\u2010learn randomized_svd implementation [7]. Based on the description for calculating X\u03bb,r in [1], we set n_components= r, n_oversamples= 2r, and n_iter= 0. The X\u03bb,r returned was then used to calculate Z and ZHA as in Equations 9a and 9b, respectively.\nFrequent Directions Amodified version of FrequentDirections3 was incorporated as an update method into the EM class. Since FrequentDirections is a line\u2010by\u2010line updatemethod as opposed to a batch updatemethod, the updatemethod in the EM class was constructed to receive a matrix E containing the rows to be added and performs the FrequentDirections algorithms for each row of theE. Any form of error metric\n2https://anonymous.4open.science/r/truncatedSVD\u20100162/ 3https://github.com/edoliberty/frequent\u2010directions\nReScience C 8.2 (#25) \u2013 Chen, Matsumoto and Varma 2022 6\ncalculation or subsequent update is performed only after the entire matrix E has been processed using the line\u2010by\u2010line update method. Since the updatedmatrixB for theFrequentDirectionsmethodhas constant dimen\u2010 sions throughout the update process, the residual norm error calculation is modified to measure the error betweenB andA\u2032 whereA\u2032 is a truncated version ofA that only holds the first 2l singular vectors and values of A and where 2l is the number of rows in B.\n4.2 Datasets In total, we conducted experiments on five datasets. MED, CRAN, CISI, and Reuters\u2010 21578 are term\u2010document matrices from latent semantic indexing applications [10, 11, 12, 13, 14] and ML1M is a movie rating dataset from MovieLens [15]. Table 1 lists the dimensions of the matrices as well as the average number of nonzero (nnz) entries per row and Figure 1 shows the leading 100 singular values for each matrix. It should be noted that the matrices used for CISI, CRAN, and MED in [1] had slightly different di\u2010 mensions compared to what was listed on [10]. We received these datasets along with the MATLAB code and chose to use their versions of the data for ease of comparison; as we were interested in the accuracy of singular value reconstruction we determined that somewhat corrupted data merely introduced a different set of singular values to recon\u2010 struct. Furthermore, as the Reuters and ML1M datasets were intact, we used them as controls against the corruption of the other sets.\nReScience C 8.2 (#25) \u2013 Chen, Matsumoto and Varma 2022 7\n4.3 Experiments We conducted two sets of experiments: one to confirm the results of [1] in a series of re\u2010 producibility studies and another to furthermeasure the performance of the algorithms using two additional metrics as well as observing the effect of the number of batches on the runtime and performance.\nUpdate method comparison As a first step, we sought to reproduce the results in Fig\u2010 ures 3 and 4 of [1]. To do this, we conducted the sequence updates experiment. The initial matrix B \u2261 A(0) was set equal to the first \u00b5 rows of A \u2208 Cm\u00d7n and the remain\u2010 ing m \u2212 \u00b5 rows of A were appended to the initial matrix over a sequence of \u03d5 updates, each with \u03c4 = \u230a(m \u2212 \u00b5)/\u03d5\u230b rows. Following the notation of [1], the i\u2010th update would\nyield A(i) = (\nB \u2261 A(i\u22121) E \u2261 A(\u00b5+ (i\u2212 1)\u03c4 + 1 : \u00b5+ i\u03c4, :)\n) with the exception of the last update\nwhich is likely to have fewer rows in E. After each update, the rank\u2010k truncated SVD was calculated by one of the three algorithms. The parameters used in [1], and thus in our experiments as well were \u00b5 = \u2308m/10\u2309 rows, \u03d5 = 10 updates, and rank k = 50. The relative errors and residual norms were reported for the k = 50 leading singular triplets for \u03d5 = 1, 5, 10. For Algorithm 2.2, we set the coefficient \u03bb = 1.01\u03c3\u030221 and r = 10.\nAlgorithm 2.2 r parameter study Next, we varied the r parameter in Algorithm 2.2 to evaluate its effect on the accuracy as was presented in Table 4 by [1]. For this, we set \u00b5 = \u2308m/10\u2309, \u03d5 = 10, and k = 50 for all three update methods as with the previous experiment and set r = 10, 20, 30, 40, 50 for Algorithm 2.2.\nRuntime comparison We compared the runtimes of the algorithms for the CRAN, CISI, and MED as a function of the rank k = 25, 25, 50, 75, 100, 125 and the total number of updates \u03d5 = 2, 4, 6, 8, 10 (Figure 2 left and middle plots in [1]).\nVarying number of batches and desired rank In addition to the experiments that we replicated based on [1], we also varied the number of batches \u03d5 = 2, 4, 6, 8, 10 and the desired rank k = 25, 50, 75, 100, 125 of the truncated SVD and evaluated the performance of each of the update methods to further observe the effects of each of these parameters on the methods\u2019 performances.\n5 Results\nRelative error and residual norms of singular triplets The relative error and residual norm of the leading k = 50 singular triplets for the CRAN dataset at \u03d5 = 1, 5, 10 us\u2010 ing Algorithms 2.1, 2.2, and FrequentDirections are shown in Figure 2. Due to the large number of figures, the complete set of plots for the standard experiments are pre\u2010 sented in Sections A to E in the Supplementary Materials. When comparing the relative error and residual norm plots for Algorithm 2.1 on CRAN, CISI, and MED, our results matched those of [1] exactly. For Algorithm 2.2, the plots did not match exactly, though the differences never exceeded half an order of magnitude and are attributable to the randomness inherent in Algorithm 2.2. Our comparison of the relative error and residual norm of the k = 50\u2010th singular triplet for Algorithm 2.2 with various values of r revealed a similar result to [1] \u2013 across the three methods, Algorithm 2.2 had the lowest errors, and within variations of Algorithm 2.2, larger values of r yielded higher accuracy.\nReScience C 8.2 (#25) \u2013 Chen, Matsumoto and Varma 2022 8\nRuntime For all three of the datasets which wemeasured runtimes on, we found Algo\u2010 rithm 2.2 to require a substantially longer amount of time to complete all of its updates. Algorithm 2.1 and FrequentDirections required a similar length of time, though Algorithm 2.1 was consistently faster than FrequentDirections by a small margin. The runtime plots for the standard experiments are shown in Section F of the Supple\u2010 mentary Materials.\nNumber of batches and rank Due to space\u2010related constraints, we chose to only in\u2010 clude two examples from the array of plots generated (Figure 4). Despite the large varia\u2010 tion in the parameters, we can see that the residual norm for overlapping update num\u2010 bers and k share very similar values.\nReScience C 8.2 (#25) \u2013 Chen, Matsumoto and Varma 2022 9\n6 Discussion\nUltimately, the reproduced results confirm the original results. Specifically, Table 2 verifies that Algorithm 2.2 outperforms Algorithm 2.1 in terms of accuracy. Further\u2010 more, Figure 3 clearly demonstrates that Algorithm 2.1 far outperforms Algorithm 2.2 with respects to wall clock speed. However, as there were no benchmarks, we viewed the comparison with FrequentDirections as a much stronger barometer. At first glance, Table 2 and Figures 2c and 2f suggest that both Algorithm 2.1 and 2.2 outperform FrequentDirections in terms of accuracy. However, upon considering the steps in\u2010 volved in FrequentDirections (namely the step involving the thresholding of the sin\u2010 gular values), we realize that the relative error and residual normof singular tripletsmay not be an applicable metric for FrequentDirections. This is further demonstrated by the irregular profile of the residual norm as a function of the singular value index (Figure 2f)). Thus it cannot conclusively be said that FrequentDirections is signif\u2010 icantly under\u2010performing the paper\u2019s proposed algorithms. Consequently, the overall conclusion becomes that while the results presented in the paper are sound, there is still need for further benchmarking to determine where the proposed algorithms stand relative to the state\u2010of\u2010the\u2010art in the field.\nReScience C 8.2 (#25) \u2013 Chen, Matsumoto and Varma 2022 10\n6.1 Future Work We believe a weakness of the paper to be the lack of benchmarking \u2010 and as discussed above, our results do not conclusively resolve this. However, they do motivate the need for metrics that will allow for a fair comparison between the proposed algorithm and state\u2010of\u2010the\u2010art algorithms such as FrequentDirections.\n6.2 What was easy Algorithm 1.1 was quite simple to understand and implement, and was exactly repro\u2010 duced quite early on. Once we received code, implementation of Algorithm 2.2 and the evaluation metrics was simplified.\n6.3 What was difficult In addition to the challenges constructingX\u03bb,r forAlgorithm2.2, another challenging/time\u2010 consuming aspect was designing the experiments as sweeping through various combi\u2010 nations of the parameters required thorough planning for data management."}], "title": "[Re] Projection-based Algorithm for Updating the TruncatedSVD of Evolving Matrices", "year": 2022}