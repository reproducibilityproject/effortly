{"abstractText": "In this study, we present our results and experience during replicating the paper titled \u201dSpatialAdaptive Network for Single Image Denoising\u201d. This paper proposes novel spatial-adaptive denoising architecture for efficient noise removal by leveraging the deformable convolutions to adapt spatial information (i.e. edges and textures). We have implemented the model from scratch in PyTorch framework, and then have conducted real and synthetic noise experiments on the corresponding datasets. We have achieved to reproduce the results qualitatively and quantitatively.", "authors": [{"affiliations": [], "name": "Sami Mente\u015f"}, {"affiliations": [], "name": "Furkan K\u0131nl\u0131"}, {"affiliations": [], "name": "Bar\u0131\u015f \u00d6zcan"}, {"affiliations": [], "name": "Furkan K\u0131ra\u00e7"}], "id": "SP:6d3ff2202bfb35126d5534f22dfb7b8bcbd5bf91", "references": [{"authors": ["T. Pl\u00f6tz", "S. Roth"], "title": "Neural Nearest Neighbors Networks.", "venue": "Advances in Neural Information Processing Systems", "year": 2018}, {"authors": ["Y. Zhou", "J. Jiao", "H. Huang", "Y. Wang", "J. Wang", "H. Shi", "T. Huang"], "title": "When AWGN-Based Denoiser Meets Real Noises.", "venue": "Proceedings of the AAAI Conference on Artificial Intelligence", "year": 2020}, {"authors": ["S. Guo", "Z. Yan", "K. Zhang", "W. Zuo", "L. Zhang"], "title": "Toward convolutional blind denoising of real photographs.", "venue": "IEEE Conference on Computer Vision and Pattern Recognition", "year": 2019}, {"authors": ["S. Anwar", "N. Barnes"], "title": "Real Image Denoising with Feature Attention.", "venue": "IEEE International Conference on Computer Vision (ICCV-Oral)", "year": 2019}, {"authors": ["M. Chang", "Q. Li", "H. Feng", "Z. Xu"], "title": "Spatial-Adaptive Network for Single Image Denoising.", "venue": "Cham: Springer International Publishing,", "year": 2020}, {"authors": ["A. Paszke"], "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library.", "venue": "Advances in Neural Information Processing Systems 32", "year": 2019}, {"authors": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "title": "Deep Residual Learning for Image Recognition.", "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "year": 2016}, {"authors": ["S. Ioffe", "C. Szegedy"], "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift.", "venue": "Proceedings of the 32nd International Conference on Machine Learning", "year": 2015}, {"authors": ["L. Biewald"], "title": "Experiment Tracking with Weights and Biases", "venue": "Software available from wandb.com", "year": 2020}, {"authors": ["X. Zhu", "H. Hu", "S. Lin", "J. Dai"], "title": "Deformable ConvNets V2: More Deformable, Better Results.", "year": 2019}, {"authors": ["A.L. Maas", "A.Y. Hannun", "A.Y. Ng"], "title": "Rectifier nonlinearities improve neural network acoustic models.", "venue": "ICML Workshop on Deep Learning for Audio, Speech and Language Processing", "year": 2013}, {"authors": ["A. Abdelhamed", "S. Lin", "M.S. Brown"], "title": "A High-Quality Denoising Dataset for Smartphone Cameras.", "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "year": 2018}, {"authors": ["J. Anaya", "A. Barbu"], "title": "RENOIR - A Benchmark Dataset for Real Noise Reduction Evaluation.", "year": 2014}, {"authors": ["T. Pl\u00f6tz", "S. Roth"], "title": "Benchmarking Denoising Algorithms with Real Photographs.", "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "year": 2017}, {"authors": ["E. Agustsson", "R. Timofte"], "title": "NTIRE 2017 Challenge on Single Image Super-Resolution: Dataset and Study.", "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops", "year": 2017}, {"authors": ["D.P. Kingma", "J. Ba"], "title": "Adam: A method for stochastic optimization.", "venue": "In: International Conference on Learning Representations (ICLR)", "year": 2015}, {"authors": ["X. Glorot", "Y. Bengio"], "title": "Understanding the difficulty of training deep feedforward neural networks.", "venue": "AISTATS", "year": 2010}, {"authors": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification.", "venue": "IEEE International Conference on Computer Vision (ICCV)", "year": 2015}, {"authors": ["K. Dabov", "A. Foi", "V. Katkovnik", "K. Egiazarian"], "title": "Color Image Denoising via Sparse 3D Collaborative Filtering with Grouping Constraint in Luminance-Chrominance Space.", "venue": "IEEE International Conference on Image Processing. Vol", "year": 2007}, {"authors": ["K. Zhang", "W. Zuo", "Y. Chen", "D. Meng", "L. Zhang"], "title": "Beyond a Gaussian denoiser: Residual learning of deep CNN for image denoising.", "venue": "IEEE Transactions on Image Processing", "year": 2017}, {"authors": ["Y. Tai", "J. Yang", "X. Liu", "C. Xu"], "title": "MemNet: A Persistent Memory Network for Image Restoration.", "venue": "IEEE International Conference on Computer Vision (ICCV)", "year": 2017}, {"authors": ["K. Zhang", "W. Zuo", "L. Zhang"], "title": "FFDNet: Toward a Fast and Flexible Solution for CNN based Image Denoising.", "venue": "IEEE Transactions on Image Processing", "year": 2018}, {"authors": ["Y. Zhang", "K. Li", "B. Zhong", "Y. Fu"], "title": "Residual Non-local Attention Networks for Image Restoration.", "venue": "In: International Conference on Learning Representations", "year": 2019}], "sections": [{"text": "R E S C I E N C E C Replication / ML Reproducibility Challenge 2020\n[Re] Spatial-Adaptive Network for Single Image"}, {"heading": "Denoising", "text": ""}, {"heading": "Sami Mente\u015f1, ID , Furkan K\u0131nl\u01311, ID , Bar\u0131\u015f \u00d6zcan1, ID , and Furkan K\u0131ra\u00e71, ID", "text": "1Video, Vision and Graphics Lab, \u00d6zye\u011fin University, Istanbul, Turkey\nEdited by Koustuv Sinha\nReviewed by Anonymous Reviewers\nReceived 29 January 2021\nPublished 27 May 2021\nDOI 10.5281/zenodo.4834672"}, {"heading": "Reproducibility Summary", "text": "In this study, we present our results and experience during replicating the paper titled \u201dSpatialAdaptive Network for Single Image Denoising\u201d. This paper proposes novel spatial-adaptive denoising architecture for efficient noise removal by leveraging the deformable convolutions to adapt spatial information (i.e. edges and textures). We have implemented the model from scratch in PyTorch framework, and then have conducted real and synthetic noise experiments on the corresponding datasets. We have achieved to reproduce the results qualitatively and quantitatively."}, {"heading": "Scope of Reproducibility", "text": "The original paper proposes an encoder-decoder structure exploiting a residual spatialadaptive block and a context block to capture multi-scale information for achieving the state-of-the-art on real and synthetic noise removal."}, {"heading": "Methodology", "text": "We have implemented themodel, namely SADNet, from scratch in PyTorch as described in the paper, and also adopted the training loop and proposed blocks from the author s\u0313 code. Since theweight initialization of proposed blocks was not implicitly defined in the paper, we have decided to use the default initialization method for convolutional layers in PyTorch (i.e. Kaiming). Experiments have been completed on a single RTX 2080 Ti in 3 days for each, and it requires \u223c3GB GPUmemory for training, and \u223c8GB CPUmemory for loading the data, due to the file structure of datasets."}, {"heading": "Results", "text": "We have achieved to reproduce the results qualitatively and quantitatively on synthetic and noise removal tasks. SADNet has the capacity to learn to remove the synthetic and real noise in images, and it produces visually-plausible outputs even after a few epochs. Moreover, we have employed SSIM and PSNRmetrics tomeasure the quantitative performance for all settings. The quantitative results on both tasks are on-par when compared to the reported results in the paper.\nCopyright \u00a9 2021 S. Mente\u015f et al., released under a Creative Commons Attribution 4.0 International license. Correspondence should be addressed to Sami Mente\u015f (sami.mentes@ozu.edu.tr) The authors have declared that no competing interests exist. Code is available at https://github.com/sami-automatic/SADNet_Replication. \u2013 SWH swh:1:dir:1c60d43a0fe927c1f1287adefd252804c2f273b9. Open peer review is available at https://openreview.net/forum?id=yiAI9QN9nYt&noteId=SMFjCY6qG8.\nReScience C 7.2 (#12) \u2013 Mente\u015f et al. 2021 1"}, {"heading": "What was easy", "text": "The code was open-source, and implemented in PyTorch, hence adopting the training loop and proposed blocks to our implementation facilitated our reproduction study. The loss function is straightforward and the architecture has a U-Net-like structure, so that we could achieve to implement the architecture in a fair time."}, {"heading": "What was difficult", "text": "Due to the lack of compatibility with the current versions of PyTorch and TorchVision and the dependency on an external CUDA implementation of deformable convolutions, we have encountered several issues during our implementation. Then, we have considered to re-implement residual spatial-adaptive block and context block from scratch for deferring these dependencies, however, we could not achieve it just by referring to the paper in limited time. Therefore, we have decided to directly use the provided blocks as in the author s\u0313 code.\nCommunication with original authors We did not make any contact with the authors since we achieved to solve the issues encountered during the implementation of SADNet by examining the author s\u0313 code.\n1 Introduction\nRecent works [1, 2, 3, 4] have shown that the previous assumption of an identicallydistributed additive white Gaussian noise (AWGN) is not an accurate representation of the real noise occurring in images. Traditional denoiser architectures lack the ability to adapt textures and edges, and thus miss the details while denoising, due to the oversmoothing behaviour of CNNs. A workaround to this problem is implementing a deeper network, however, such a practice introduces a more complex model with its computational burden. In the original paper [5], an encoder-decoder architecture consisting a residual spatialadaptive block, namely RSAB, is proposed for removing spatially-variant and channeldependent noise while processing larger regions in each step by utilizing deformable convolutions. As themain claim of the paper, thismethod produces better performance than the compared methods in given benchmark, and also for the synthetic noise removal task. In this reproducibility report, we studied SADNet architecture for both real and synthetic noise removal in detail, which contains implementing the architecture described in the paper, running the experiments, reporting the important details about certain issues encountered during reproducing, and comparing the obtained results with the ones reported in the original paper.\n2 Scope of reproducibility\nThe main idea of the paper is to present a spatial-adaptive architecture with encoderdecoder structure which captures the relevant features from the complex image content while removing real noise appearing in images. Residual spatial-adaptive block (RSAB) makes it possible to achieve this in an efficient manner. The proposed model, namely SADNet, claims to outperform the state-of-the-art performances in SSIM and PSNR metrics with a moderate run-time. To validate these claims, we try to investigate the following questions:\nReScience C 7.2 (#12) \u2013 Mente\u015f et al. 2021 2\n\u2022 Is the implementation details described in the paper and provided code sufficient for replicating the quantitative results reported in the paper?\n\u2022 Are the qualitative results visually-plausible?\n\u2022 Are the replicated quantitative results competitive enough?\n\u2022 Could our replication obtain a proximate denoising duration compared to the reported results in the original paper?\n3 Methodology\nWe have implemented the model, namely SADNet, from scratch in PyTorch [6], as described in the paper by adopting RSAB, Context block and Offset block from the author s\u0313 code. The implementation of residual blocks (ResBlock) in the author s\u0313 code differs from the common residual block implementation [7] by not using the output activation. In contrast to the common practice of applying a nonlinear activation function to the output, their ResBlock implementation directly forwards its output to the next level layers. At this point, the authors handle those activations at the model scope. We also removed Batch Normalization [8] from the residual blocks as proposed by the original paper. To enhance the readability of the model structure in our implementation, we imported those activation functions back in to ResBlock. Thedeformable convolutions inRSABare implemented inCUDA,henceweusedNVIDIA GPUs with the relevant CUDA driver. For validating the reported results on real noisy images, we have implemented the data loaders, which are missing in the author s\u0313 code. Furthermore, we integrated WandB [9] library to the training loop in order to track our experiments during training.\n3.1 SADNet SADNet is an encoder-decoder architecture with skip connections which favors spatial adaptability and large receptive field over deeper networks for the well-studied denoising task. The proposed model aims to achieve the state-of-the-art denoising performance while maintaining the computational complexity by exploiting residual spatialadaptive block (RSAB), Context Block and Offset Block. The visual representation of SADNet architecture is shown in Figure 1, and also the structural details about our implementation of SADNet can be seen in Table 1.\nReScience C 7.2 (#12) \u2013 Mente\u015f et al. 2021 3\nRegions with the sharp texture changes in an image, typically edges and corners, raise difficulties for training the regular convolutions, due to its fixed-size weighting mechanism. Such regions, where different textures co-occur in a particular receptive field of the regular convolutions, are simply ignored during the weighting process, due to the fixed size kernels. To address this issue, self-similarity weighting is attained via modulated deformable convolutions [10] in RSAB. The kernels of the deformable convolutions have a learnable offset for each location in an image, and thus it has the capacity to adapt to the spatial texture changes. The formula of modulated deformable convolutions can be seen as follows\ny(p) = \u2211\npi\u2208N(p)\nwi \u00b7 x(pi +\u2206pi) \u00b7\u2206mi (1)\nwhere \u2206pi denotes the learnable offset for location pi, and \u2206mi is the extra degree of freedom for adjusting the modulation scalar between [0, 1]. The nature of the decoder architectures enforces to transform the feature maps from coarse to fine at each scale. For learning the offsetsmore accurately in RSABs, the offsets \u2206ps\u22121 and themodulation scalars\u2206ms\u22121 from theprevious scale are further transferred into the current scale s with the help of Offset Blocks. The offset transfer is formulated as\n(\u2206ps,\u2206ms) = Foffset ( x, Fup (( \u2206ps\u22121,\u2206ms\u22121 ))) (2)\nwhere Fup denotes the up-sampling operation. RSAB receive the extracted features and the reconstructed features from the previous scale conveyed by the Offset Block. The inputs are then fused through a modulated deformable convolution layer with a subsequent regular convolution layer. Moreover, a skip connection similar to ResBlock is employed to enhance the information transferring. At this point, RSAB can be formulated as,\nFRSAB(x) = Fcn(Fact(Fdcn(x))) + x (3)\nwhere Fcn and Fdcn denote regular convolution and modular deformable convolution, respectively. Lastly, Fact stands for the leaky ReLU activation function [11] with a negative slope of 0.2. Another introduced block is the Context Block, which resides at the bottleneck of the model. To increase the size of the receptive fields while preserving the spatial resolution, Context Block is employed for the model, just between the encoder and decoder structures. Furthermore, unlike the common implementation of Context Block, Batch Normalization layer is removed in the published code, and only four dilation rates are used, which are 1, 2, 3 and 4. Following the original paper, we usedL1 loss for training ourmodel on real-noise image datasets, and L2 loss for training on synthetic image datasets.\n3.2 Datasets The original paper uses SIDDMedium [12] and RENOIR [13] datasets during training for denoising real noisy images and reports the qualitative and quantitative results on DND [14] and SIDD validation [12] datasets. Since the author s\u0313 code only provides the data loader for synthetic image datasets, we have integrated our SIDD validation data loader implementation and the DND test script provided by TU Darmstadt [14] to our pipeline. For synthetic noise removal, additive white Gaussian noise with standard deviation of 30, 50 and 70 have been added to DIV2K dataset [15], which consists 800 high resolution images. To validate the performance of SADNet on synthetic noise removal task, the models are tested on BSD68 [16] and Kodak24 datasets processed with the same noise addition mechanism.\nReScience C 7.2 (#12) \u2013 Mente\u015f et al. 2021 4\nBoth test and validation data for all settings are composed of high resolution images. Therefore, they are fed to themodel as 128x128 patches cropped by fixed coordinates, as described in the paper. We have applied 90\u25e6 rotation, horizontal and vertical flipping to the images during training, following the practice in the paper.\n3.3 Hyperparameters In our replication study, we used the ADAM optimizer [17] with \u03b21 = 0.9, \u03b22 = 0.999, and \u03f5 = 1e\u2212 8, with an initial learning rate of 1e\u2212 4 during training, as described in the paper. The provided code initializes the weights of the convolutional layers in all blocks with Xavier Uniformmethod [18]. Since this choice has not been discussed in the paper, we left each convolution layer initialized by the default weight initialization method in PyTorch (i.e. Kaiming [19]) in our experiments.\n3.4 Experimental setup In this study, we have followed the same training procedures for all setting, and employed SSIM and PSNR values as performance metrics, as described in the original paper. The parameters for all training settings can be found in the configuration file in our GitHub repository. Our implementation and the trained weights are open-sourced, and can be accessed at https://github.com/sami-automatic/SADNet_Replication.\n3.5 Computational requirements The experiments have been conducted on a single RTX 2080Ti for approximately 3 days, and only requires highGPUmemory,mostly due tomodulated deformable convolutions. It requires \u223c3GB GPUmemory for training, and \u223c8GB CPUmemory for loading the data, due to the file structure of datasets.\n4 Results\nWehave implemented themodel fromscratchby following the descriptions presented in the original paper, and then achieved to replicate the claimed results by referring to the published code. Overall, our implementation of SADNet achieved on-par performances\nReScience C 7.2 (#12) \u2013 Mente\u015f et al. 2021 6\nin SSIM and PSNR metrics on test datasets, and we also validated the results on both denoising tasks by examining their qualitative results. As shown in Table 2, our quantitative results on SIDD sRGB validation dataset has 39.41 PSNR value, which is only 0.12% less than the one reported in the original paper. Moreover, the average duration of a single inference of SADNet is 26.7 ms. according to the paper, while our implementation of SADNet completes the single inference on 25.9 ms. The visual comparisons of real noise removal of the images are shown in Figure 2 and Figure 3. The samples are from SIDD validation dataset, according to the ones reported in the paper. The first two rows in these figures are directly taken from the original paper for further comparison with our replication results, which can be seen in the third row. On the results from the original paper, SADNet mainly differs from the compared methods by generating a distinct clear continuous stripe texture on the background while preserving the object surface appearance. Our replication clearly shares the similar behaviour. Therefore, we can state that the replicated quantitative results on real noise removal task are competitive enough, and also supports the main claim in the original paper. Similarly, Table 3 demonstrates that the results of our SADNet implementation achieves on-par PSNR values with the ones reported in the paper for different noise levels (i.e. \u03c3 \u2208 {30, 50, 70}) on BSD68 and Kodak24 datasets. Particularly, we have obtained better results than all other compared methods and the reported SADNet results on Kodak24 dataset for all noise levels. Moreover, the replicated SADNet model imitates the qualita-\nReScience C 7.2 (#12) \u2013 Mente\u015f et al. 2021 7\ntive results of the original model on both datasets. Although the images from Kodak24 and BSD68 are heavily exposed to the synthetic noise, SADNet has the ability to remove noise, and to generate well-defined textures when compared to the recent works. As shown in Figure 4, all other compared methods have smoothed the texture and swept away the feather details, meanwhile the original implementation of SADNet and ours achieve to generate more plausible feather-like texture. Similar to the previous example, in Figure 5, the clothing details are significantly preserved, especially pilling on the top-left part of the cloth and the vertical texture details on the cloth. The ground truth of DND validation set is private, and thus it is not possible to locally validate the results on this dataset. Despite of several attempts to submit our results to DND online validation system, we could not obtain SSIM and PSNR results, due to the server error. We have tried to contact with DND Team, but we could not get any advice for solving this issue.\n5 Discussion\nThe qualitative results generatedwith our replication strongly resemble to the presented results, and differs from the other compared studies. According to these results, we can state that our implementation of SADNet consistently yields visually-plausible results on both real and synthetic noisy images, and supports the claims of the original paper. In addition, our experiments firmly correlates with the reported PSNR values. Overall, the paper and the provided codewas sufficient for replicating the results on real and synthetic noise removal. For re-implementing themodel from scratch, wehave only referred to the paper, and ended up on-par performance with the ones in the paper on all settings. Lastly, to provide an insight for run-time on different hardware, our replication has 25.9 ms. inference run-time on real noise removal task, whereas the reported run-time duration is 26.7 ms. Note that we used a single RTX 2080Ti GPU during our experiments, while a single GTX 1080Ti GPU is used in the original study, and we assume that this is the reason of this difference.\n5.1 What was easy The code was open-source, and implemented in PyTorch, hence adopting the training loop and proposed blocks to our implementation facilitated our reproduction study. The loss function is straightforward and the architecture has a U-Net-like structure, so that we could achieve to implement the architecture in a fair time.\n5.2 What was difficult Due to the lack of compatibility with the current versions of PyTorch and TorchVision and the dependency on an external CUDA implementation of deformable convolutions, we have encountered several issues during our implementation. Then, we have considered to re-implement residual spatial-adaptive block and context block from scratch for deferring these dependencies, however, we could not achieve this just by referring\nReScience C 7.2 (#12) \u2013 Mente\u015f et al. 2021 9\nto the paper. Therefore, we have decided to directly use the provided blocks as in the author s\u0313 code.\n5.3 Communication with original authors We did not make any contact with the authors since we achieved to solve the issues encountered during the implementation of SADNet by examining the author s\u0313 code."}], "title": "[Re] Spatial-Adaptive Network for Single Image Denoising", "year": 2021}