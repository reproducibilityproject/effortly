{"abstractText": "Partial differential equations (PDEs) are differential equationswhich contain a-priori unkown multivariable functions and their partial derivatives. They are used to model various physical phenomena, such as heat, fluid dynamics or quantum mechanics. There are several numerical methods to solve PDEs. A common one is the finite-difference method (FDM), which approaches the differential equation by discretizing the problem space and converting the PDE to a system of linear equations. The obtained linear system can be solved using an iterative procedure which updates the solution until convergence is reached. The original paper proposes to use machine learning techniques in order to find high performing update rules instead of designing them by hand [1], while still guaranteeing convergence. In order to fulfill these requirements the learned solver is an adapted existing standard solver, from which the convergence property is inherited by enforcing that a fixed point of the original solver is a fixed point for the trained solver as well. We stress that the goal is not to find a new solver, but to optimize an existing one. To be precise the learned part operates with the residuals after applying the standard solver. This construction allows application to other existing linear iterative solvers of equivalent design. Since a linear iterative solver can be expressed as a product of convolutional operations, it is not far fetched to use the similar techniques used in deep learning in order to find such an optimal operator. In order to test this approach a solver was trained to solve a 2D Poisson equation on a square-shaped geometry with Dirichlet boundary conditions. This solver is then tested on larger geometries of two shapes and different boundary values. No significant loss of performance was observed; generalization is thus reached. For more information we kindly refer to the original paper [1].", "authors": [{"affiliations": [], "name": "Francesco Bardi"}, {"affiliations": [], "name": "Samuel von Baussnern"}, {"affiliations": [], "name": "Emiljano Gjiriti"}, {"affiliations": [], "name": "Koustuv Sinha"}], "id": "SP:dcdbb8bb1f0ef9080fcdbed5279791c4db6bd6fa", "references": [{"authors": ["J.-T. Hsieh", "S. Zhao", "S. Eismann", "L. Mirabella", "S. Ermon"], "title": "Learning Neural PDE Solvers with Convergence Guarantees.", "venue": "In: International Conference on Learning Representations", "year": 2019}, {"authors": ["D. Gilbarg", "N. Trudinger"], "title": "Elliptic Partial Differential Equations of Second Order", "year": 2001}, {"authors": ["J. Thomas"], "title": "Numeric Partial Differential Equations", "venue": "Finite Difference Methods. Springer,", "year": 1995}, {"authors": ["R. LeVeque.Finite"], "title": "DifferenceMethods for Ordinary andPartial Differential Equations: Steady-state and Timedependent Problems", "year": 2007}, {"authors": ["M.D. Zeiler"], "title": "ADADELTA: An Adaptive Learning Rate Method.", "year": 2012}], "sections": [{"text": "R E S C I E N C E C"}, {"heading": "Replication / Machine Learning", "text": "[Re] Learning Neural PDE Solvers with Convergence Guarantees ICLR Reproducibility Challenge 2019\nFrancesco Bardi1, Samuel von Baussnern1,, and Emiljano Gjiriti1, 1\u00c9cole polytechnique f\u00e9d\u00e9rale de Lausanne (EPFL), Lausanne, Switzerland\nEdited by Koustuv Sinha ID\nReviewed by Anonymous reviewers\nReceived 04 May 2019\nPublished 22 May 2019\nDOI 10.5281/zenodo.3162890\n1 Introduction\nPartial differential equations (PDEs) are differential equationswhich contain a-priori unkown multivariable functions and their partial derivatives. They are used to model various physical phenomena, such as heat, fluid dynamics or quantum mechanics. There are several numerical methods to solve PDEs. A common one is the finite-difference method (FDM), which approaches the differential equation by discretizing the problem space and converting the PDE to a system of linear equations. The obtained linear system can be solved using an iterative procedure which updates the solution until convergence is reached. The original paper proposes to use machine learning techniques in order to find high performing update rules instead of designing them by hand [1], while still guaranteeing convergence. In order to fulfill these requirements the learned solver is an adapted existing standard solver, from which the convergence property is inherited by enforcing that a fixed point of the original solver is a fixed point for the trained solver as well. We stress that the goal is not to find a new solver, but to optimize an existing one. To be precise the learned part operates with the residuals after applying the standard solver. This construction allows application to other existing linear iterative solvers of equivalent design. Since a linear iterative solver can be expressed as a product of convolutional operations, it is not far fetched to use the similar techniques used in deep learning in order to find such an optimal operator. In order to test this approach a solver was trained to solve a 2D Poisson equation on a square-shaped geometry with Dirichlet boundary conditions. This solver is then tested on larger geometries of two shapes and different boundary values. No significant loss of performance was observed; generalization is thus reached. For more information we kindly refer to the original paper [1].\n2 Background\nIn this section, we give a short introduction to the Poisson problem and iterative solvers, which will help to understand the justification of using a convolutional neural network to obtain a solver.\n2.1 Poisson Equation The Poisson equation is a second order linear partial differential equation (PDE). In order to guarantee the existence and uniqueness of a solution, appropriate boundary con-\nCopyright \u00a9 2019 F. Bardi, S.V. Baussnern and E. Gjiriti, released under a Creative Commons Attribution 4.0 International license. Correspondence should be addressed to Francesco Bardi (francesco.bardi@epfl.ch) The authors have declared that no competing interests exists. Code is available at https://github.com/francescobardi/pde_solver_deep_learned \u2013 DOI 10.5281/zenodo.2660730. Open peer review is available at https://github.com/reproducibility-challenge/iclr_2019/pull/136.\nReScience C 5.2 (#3) \u2013 Bardi, Baussnern and Gjiriti 2019 1\nditions needs to be prescribed [[2]]. In this paper only Dirichlet boundary conditions were considered. The Poisson problem hence reads:\nFind u : \u2126 = \u2126 \u222a \u2202\u2126\u2192 R s.t\n{ \u22072u = \u2211 i \u22022\n\u2202x2i = f(x) in \u2126\nu = b(x) on \u2202\u2126 (1)\nWhere \u2126 \u2282 Rk is a bounded domain with boundary \u2202\u2126. More specifically we consider \u2126 = [0, 1]2.\n2.2 Finite Difference Method In order to solve complex, real-world PDEs a numerical approach must be used, as analytic solutions can be seldom found. As a first step the problem is discretized by transforming the solution space from u : Rk \u2192 R to uh : Dk \u2192 R, where Dk is a discrete subset of Rk. In this paper k = 2 and denoting by N the domain size, we introduce a regular grid \u2126h \u2282 Dk on \u2126:\n\u2126h = {xi,j = (ih, jh) i, j = 0, ..., N \u2212 1} \u2126h = {xi,j = (ih, jh) i, j = 1, ..., N \u2212 2}\n\u2202\u2126h = \u2126h \\ \u2126h with h = 1/(N \u2212 1) and denoting by \u2126h the interior points, and by \u2202\u2126h the boundary points. Equation 1 can be approximated as follows, discretizing and approximating\u22072:\nFind uh : \u2126h \u2192 R s.t. { 1 h2 (ui\u22121,j + ui+1,j + ui,j\u22121 + ui,j+1 \u2212 4ui,j) = fi,j in \u2126h ui,j = bi,j in \u2202\u2126h\n(2) It can be shown that the discrete approximation in equation 2 is stable and that ||u \u2212 uh||L2 \u2264 ch2 with c being a constant ([3]). Introducing a matrix A \u2208 RN\n2\u00d7N2 and a vector f \u2208 RN2 problem definition 2 can be written as a linear system:\nAu = f (3)\nWithA being a pentadiagonal matrix:\nAi,j =  1 if i = j, \u2212 14 else if j \u2208 {i\u00b1 1, i\u00b1N}, 0 else\nand defining i\u22c6 = \u230ai/N\u230b, j\u22c6 = (i mod N) we have:\nfi = h2\n4 f(xi\u22c6,j\u22c6)\nIn order to prescribe the boundary conditions we introduce a reset operator G:\nG(u, b) = Gu+ (I \u2212G)b\nwhereG \u2208 RN2\u00d7N2 is a diagonal matrix and b \u2208 RN2 is the boundary values vector:\nGi,i = 1, bi = 0 xi\u22c6,j\u22c6 \u2208 \u2126h Gi,i = 0, bi = b(xi\u22c6,j\u22c6) xi\u22c6,j\u22c6 \u2208 \u2202\u2126h\nWe note that the proposed approach to enforce boundary conditions is restricted to iterative methods solving linear systems equivalent to equation 3. Moreover we have not investigated how this approach can be generalized to other type of boundary conditions other than Dirichlet or to different iterative methods such as the Gauss-Seidel method.\nReScience C 5.2 (#3) \u2013 Bardi, Baussnern and Gjiriti 2019 2\n2.3 Iterative Solvers A linear iterative solver finds the solution of a linear system by iteratively updating an initial solution guess u0. The updating step can be expressed as:\nuk+1 = Tuk + c\nWhere T is a constant update matrix and c is a constant vector. A common approach to build T and c is to splitA intoA = M \u2212N and by rewritingAu = f asMu = Nu+f the following updating rule naturally arises:\nuk+1 = M\u22121Nuk +M\u22121f\nFor more details we refer readers to [4] or to [1].\nJacobi method \u2014 SettingM = diag(A) leads to the so called Jacobimethod. In the case of the Poisson problemM = I and T = I\u2212A, hence relying on the previously introduced reset operator the Jacobi method reads:\nuk+1 = \u03a8(uk)\n= G((I \u2212A)uk + f , b) = G((I \u2212A)uk + f) + (I \u2212G)b = G((I \u2212A)uk + f \u2212 b) + b\nThe Jacobi method can also be implemented by convolution and point-wise operations, as we explain in the following. We define by \u03c9J \u2217u the 2D convolutionwith zero padding of the kernel \u03c9J and u \u2208 RN\u00d7N , with:\n\u03c9J =\n( 0 1/4 0\n1/4 0 1/4 0 1/4 0 ) We can also define a new reset operator G denoting by \u25e6 the Hadamard product:\nG(u, b) = G \u25e6 u+ b\nwhereG, b \u2208 RN\u00d7N :\nGi,j = 1, bi,j = 0 xi,j \u2208 \u2126h Gi,j = 0, bi,j = b(xi,j) xi,j \u2208 \u2202\u2126h\nFinally the Jacobi method can be written as\nuk+1 = \u03a8(uk)\n= G(\u03c9J \u2217 uk + f , b) = G \u25e6 (\u03c9J \u2217 uk + f) + b\n3 Learning Process\nWewant to find an operatorH to optimize the convergence of the Jacobi method for the Poisson problem of the form:\nuk+1 = \u03a6H(u k)\n= \u03a8(uk) +H(\u03a8(uk)\u2212 uk)\nReScience C 5.2 (#3) \u2013 Bardi, Baussnern and Gjiriti 2019 3\nWe defineH as the composition ofK operations:\nH(w) = HK ...(H3(H2(H1(w))))...) Hi(w) = G \u25e6 (\u03c9i \u2217w)\nAs in the Jacobi method \u03c9i \u2217 w represents a 2D convolution with zero padding and no bias term of a 3 \u00d7 3 kernel \u03c9i with w. The operation with G ensures that the residuals are always zero at the boundary points.\n3.1 Interpretation ofH The operator H can also be expressed as a matrix vector multiplication. We call H \u2208 RN2\u00d7N2 the equivalent matrix:\nH = GHKGHK\u22121...GH1\nHi is a banded matrix which is obtained from the corresponding 3 \u00d7 3 kernel \u03c9i as follows:\nHi,i\u2212N\u22121 = \u03c90,0 Hi,i\u2212N = \u03c90,1 Hi,i\u2212N+1 = \u03c90,2\nHi,i\u22121 = \u03c91,0 Hi,i = \u03c91,1 Hi,i+1 = \u03c91,2\nHi,i+N\u22121 = \u03c92,0 Hi,i+N = \u03c92,1 Hi,i+N+1 = \u03c92,2\nSo the new method can be written using only matrix multiplications as:\nuk+1 = \u03a6H(u k) = \u03a8(uk) +H(\u03a8(uk)\u2212 uk)\nThis interpretation is useful because if the following holds:\n\u03c1(GT +H(GT \u2212 I)) < 1 (4)\nthen the method is guaranteed to convergence to a fixed point. Which can be used during training time to enforce the convergence requirement.\n4 Training and Generalization\n4.1 Training In order to find the optimal operatorH the corresponding linear neural network is created. Each 2D convolutional layer has a kernel size 3 \u00d7 3 and zero bias, without any activation function. The training phase is done on a set of Poisson problem instancesD. A problem instance is uniquely defined byG, f , and b. We set f = 0 andwe use a square domain with a 16\u00d7 16 grid. Each side exhibits a different but constant boundary value chosen fromauniformdistribution on the interval [-1, 1]. For each problem instance the error between the ground truth solution u\u22c6(G,f , b) and the computed solution using \u03a6H with k iterations contributes to the loss function. The ground truth solution is obtained using the Jacobi method operator\u03a8with a sufficiently high number of iterations k = 2000. The optimization objective is then defined as:\nmin H \u2211 G,b,f\u2208D;k\u2208DU(1,20) \u2225\u2225\u2225\u03a6kH(u0,G,f , b)\u2212 u\u22c6(G,f , b)\u2225\u2225\u22252 2\n(5)\nReScience C 5.2 (#3) \u2013 Bardi, Baussnern and Gjiriti 2019 4\nWith k \u2208 DU(1, 20) we denote the sampling of k from a discrete uniform distribution on the interval [1, 20]. The initial guess u0 is sampled from a Gaussian distribution: u0 \u223c N (0, 1). We have not enforced the any constraint to guarantee that the obtained operator\u03a6H converges to a fixed point. Since it is not possible to express analytically the spectral radius in Inequality 4 it is not clear how a regularization term could be added to the objective function. A possible solution would be to check the spectral radius at each iteration and if > 1 under-relax the weights of the convolutional kernels \u03c9i. However this technique is highly computationally expensive since it requires to compute the eigenvalues of a N2 \u00d7N2 matrix at each iteration. We showed however that empirically, without explicitly enforcing this constraint, the optimization yields an operator \u03a6H which indeed converges for the tested problems.\nOptimizer \u2014We are using Adadelta as the optimizer of our model, because of its ability to adapt over time and its minimal computational overhead. The method requires no manual adjustment of a learning rate and is robust to various selection of hyperparameters. Adadelta adjusts the learning rate by slowing down learning around a local optima, when the accuracy changes by a small margin. Adadelta also uses the idea of momentum to accelerate progress along dimensions in which the gradient consistently point in the same direction. This idea is implemented by keeping track of the previous parameter update and applying an exponential decay with a decay factor of \u03c1 = 0.9 ([5]).\nThe training was done with batch optimization of size |B| = 10. At each epoch the set of problem instancesD is randomly split in \u2308|D|/|B|\u2309 subsets. The loss for these batches is defined as the sum over all losses in the batch. The pseudo code for our training process is given in Algorithm 1.\n4.2 Hyper Parameter Search In order to find the optimal number of layers and learning rate a simple grid search is performed. As a first step we fix the number of layersK = 3 and compare the loss evolution for different learning rates \u03b3. From Figure 1 it is evident the the loss decay is highly dependent on the choice of the learning rate. For \u03b3 small the loss tends to converge to what probably is a local minimum while for high values it can lead to divergence problems; note that in Figure 1 the loss for \u03b3 = 1e\u2212 4 is not displayed since the optimization diverged. We hence decided to use the Adadelta optimization method for its ability to adapt to the specific problem. We report in Table 1 the parameters used for the training process. The number of layers K chosen was from 1 to 5. Figure 2 compares the loss evolution for the different models. It is evident that the improvement on the total loss at convergence diminishes withK increasing, in particular it seems that there is not a substantial difference when k > 3.\nReScience C 5.2 (#3) \u2013 Bardi, Baussnern and Gjiriti 2019 5\nParameter : ConvNetH Data :G,b,f Result :Optimal ConvNetH for {G,f , b} \u2208 D do\nCompute u\u22c6(G,f , b) Randomly sample ki from DU(1, 20) Sample u0 from a Gaussian with \u00b5 = 0 and \u03c3 = 1\nend repeat D\u22c6\u2190 randomly split D in \u2308|D|/|B|\u2309 subsets for B \u2208 D\u22c6 do\nlossbatch \u2190 \u2211\np\u2208B \u2225\u2225\u2225\u03a6kH(p)\u2212 u\u22c6(p)\u2225\u2225\u22252 2\nCompute the gradient of the loss function Update weights ofH\nend lossepoch \u2190 \u2211\np\u2208D \u2225\u2225\u2225\u03a6kH(p)\u2212 u\u22c6(p)\u2225\u2225\u22252 2\nuntil \u2225lossepoch\u22121 \u2212 lossepoch\u2225 < Tolerance; Algorithm 1: Training Process\n5 Experiments & Results\nThe hypothesis of the original paper is that a general solver can be found by training on simple domains. The simplest Laplace equation \u22072u = 0 on a square boundary shape was therefore chosen as training data. The model was trained on 16 \u00d7 16 grid, and evaluated on grids of size 32 \u00d7 32 and 64 \u00d7 64 for both a square and an L-shaped domain. The L-shaped domain is created by removing a smaller square from one of the edges. Each side exhibits a different but constant boundary value chosen from a uniform distribution on the interval [-1, 1]. Thus an L-shaped domain has 6 different boundary values. The ground truth solution is obtained using the Jacobi method with a sufficient number of iterations k = 5\u00d7 104. See Figure 4 for an example solution. In Figure 3 we shows how the error w.r.t the ground truth solution evolves with the number of iterations k for the obtained solvers (K = {1, 2, 3, 4, 5}) and the Jacobi method. The learned solvers clearly outperform the Jacobi method, however we need better metrics in order to fairly compare the different models. Both solvers were evaluated on three metrics: the number of iterations, ratio of FLOPS and ratio of CPU-time until required tolerance is reached. The number of flops were calculated assuming both solvers would be implemented using convolutional operators. This results in 4 multiply-add operations for each element in the grid for the Jacobi iteration, whereas the learned solvers exhibit 4 + 9K multiply-add operations. This is the same measurement as reported in the original paper, which is an estimation of the FLOPS taken. In addition to the paper we measured the CPU-time, which deemed us to be a less error-prone and more reliable measure, nevertheless both ratios gave comparable results. As can be seen in Table 2 the trained solver was considerably faster than the existent solver, showing a much quicker conversion than the baseline model. Thus replicating the given results in the original paper. The highest speed-up is achieved by the 5-layer network.\nReScience C 5.2 (#3) \u2013 Bardi, Baussnern and Gjiriti 2019 6\nReScience C 5.2 (#3) \u2013 Bardi, Baussnern and Gjiriti 2019 7\n6 Related Work\nRecently, there have been several works on applying deep learning to solve the Poisson equation. However, to the best of our knowledge, previous works used deep networks to directly generate the solution; they have no correctness guarantees and are not generalizable to arbitrary grid sizes and boundary conditions. This is the reason why our work was focused on reproducing the results of [1], and on empirically proving the generalization of their model to arbitrary shapes and grid sizes.\n7 Conclusion & Future work\nWecould partially confirm the results reported in the original paper, not every resultwas reproducible either through lack of time or certainty in how these results were achieved or measured. The trained solver was able to generalize well to the presented different sizes, geometries and boundary values, while using less resources compared to the standard solver. In the futureworkwewould like to improve the design of the solver and the experiments in order to gain more confidence in the presented approach. For example H is fixed for each iteration, one could imagine a solver with different H for different iterations up to a certain threshold. We did not have the opportunity to test the solver using the MultiGrid method, nor the square-Poisson problem. It is not clear how the cylinder domain was implemented in a finite difference framework, whether radial coordinates or a non uniform grid were used. We estimate that investigating how this approach can be generalized to other type of boundary conditions other than Dirichlet or to different iterative methods such as the Gauss-Seidel method would lead to interesting results and a more applicable approach in general, as well as trying to solve different PDEs."}, {"heading": "1. J.-T. Hsieh, S. Zhao, S. Eismann, L. Mirabella, and S. Ermon. \u201cLearning Neural PDE Solvers with Convergence", "text": "Guarantees.\u201d In: International Conference on Learning Representations. 2019. 2. D. Gilbarg and N. Trudinger. Elliptic Partial Differential Equations of Second Order. Springer, 2001. 3. J. Thomas. Numeric Partial Differential Equations: Finite Difference Methods. Springer, 1995. 4. R. LeVeque.Finite DifferenceMethods for Ordinary andPartial Differential Equations: Steady-state and Time-\ndependent Problems. 2007. 5. M. D. Zeiler. \u201cADADELTA: An Adaptive Learning Rate Method.\u201d In: CoRR abs/1212.5701 (2012). arXiv: 1212.5701.\nReScience C 5.2 (#3) \u2013 Bardi, Baussnern and Gjiriti 2019 9"}], "title": "[Re] Learning Neural PDE Solvers with Convergence Guarantees ICLR Reproducibility Challenge 2019", "year": 2019}