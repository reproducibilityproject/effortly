{"abstractText": "The authors claim that their proposed method is able to, given an ensemble of deep neural networks, capture the uncertainty estimation and decomposition capabilities of the ensemble into a singlemodel. The authors also claim that this only results in a small reduction in classification performance compared to the ensemble. We examine these claims by reproducing most of the authors\u02bc experiments on the CIFAR-10 dataset.", "authors": [{"affiliations": [], "name": "Toomas Liiv"}, {"affiliations": [], "name": "Einar Lennel\u00f6v"}, {"affiliations": [], "name": "Aron Nor\u00e9n"}, {"affiliations": [], "name": "Koustuv Sinha"}, {"affiliations": [], "name": "Jessica Zosa Forde"}], "id": "SP:99e5ace09ffb6989dda411cea1fe0d58bf5afe40", "references": [{"authors": ["B. Lakshminarayanan", "A. Pritzel", "C. Blundell"], "title": "Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles.", "venue": "Advances in Neural Information Processing Systems", "year": 2017}, {"authors": ["A. Malinin", "B. Mlodozeniec", "M. Gales"], "title": "Ensemble Distribution Distillation.", "venue": "In: International Conference on Learning Representations (ICLR)", "year": 2020}, {"authors": ["A. Malinin", "M. Gales"], "title": "Predictive Uncertainty Estimation via Prior Networks.", "venue": "Advances in Neural Information Processing Systems", "year": 2018}, {"authors": ["G. Hinton", "O. Vinyals", "J. Dean"], "title": "Distilling the Knowledge in a Neural Network.", "venue": "NIPS Deep Learning and Representation Learning Workshop", "year": 2015}, {"authors": ["A. Krizhevsky"], "title": "Learning Multiple Layers of Features from Tiny Images.", "venue": "University of Toronto (May", "year": 2012}, {"authors": ["F. Yu", "Y. Zhang", "S. Song", "A. Seff", "J. Xiao"], "title": "LSUN: Construction of a Large-scale Image Dataset using Deep Learning with Humans in the Loop.", "year": 2015}], "sections": [{"text": "R E S C I E N C E C"}, {"heading": "Replication / ML Reproducibility Challenge 2020", "text": "[Re] A Reproduction of Ensemble Distribution Distillation\nToomas Liiv1,2, ID , Einar Lennel\u00f6v1,2, ID , and Aron Nor\u00e9n1,2, ID 1Equal contribution \u2013 2KTH Royal Institute of Technology, Stockholm, Sweden\nEdited by Koustuv Sinha, Jessica Zosa Forde\nReviewed by Anonymous Reviewers\nReceived 29 January 2021\nPublished 27 May 2021\nDOI 10.5281/zenodo.4834516"}, {"heading": "Reproducibility Summary", "text": ""}, {"heading": "Scope of Reproducibility", "text": "The authors claim that their proposed method is able to, given an ensemble of deep neural networks, capture the uncertainty estimation and decomposition capabilities of the ensemble into a singlemodel. The authors also claim that this only results in a small reduction in classification performance compared to the ensemble. We examine these claims by reproducing most of the authors\u02bc experiments on the CIFAR-10 dataset."}, {"heading": "Methodology", "text": "Theproposedmethodwas re-implemented intf.keras. The surroundingdata pipelines, pre-processing, and experimentation code were also re-implemented. As in the original paper, the models were based on VGG-16 networks trained from scratch with random initialization. Training and evaluation was done on two consumer-grade GPUs, for a total of 273 hours."}, {"heading": "Results", "text": "Our findings support the authors\u02bc central claims. In terms of uncertainty estimation our EnD2 achieved (99 \u00b1 1) % of the AUC-ROC of our ensemble on the OOD-detection task. The corresponding value in the original paper was (100\u00b11)%. In terms of classification our EnD2 had (16 \u00b1 1)% higher error than our ensemble. The corresponding values in the original paper was (11\u00b1 6)%. Other metrics showed similar agreement, but, significantly, in the OOD-detection task our EnD performed at least as well as our EnD2. This is in stark contrast with the original paper. We also took a novel approach to visualizing the uncertainty decomposition by plotting the resulting distributions on a simplex, offering a visual explanation to some surprising results in the original paper, whilemostly supporting the authors\u02bc intuitive justifications for the model."}, {"heading": "What was easy", "text": "The original paper features a thoroughmathematical formulation of themethod, aiding conceptual understanding. The datasets used by the authors are publicly available. The\nCopyright \u00a9 2021 T. Liiv, E. Lennel\u00f6v and A. Nor\u00e9n, released under a Creative Commons Attribution 4.0 International license. Correspondence should be addressed to Toomas Liiv (toomasl@kth.se) The authors have declared that no competing interests exist. Code is available at https://github.com/lennelov/endd-reproduce. \u2013 SWH swh:1:dir:2c366708175b2ed7c83ce6b33a80dd43c8aad915. Open peer review is available at https://openreview.net/forum?id=p1BXNUcTFsN.\nReScience C 7.2 (#10) \u2013 Liiv, Lennel\u00f6v and Nor\u00e9n 2021 1\nuse of the simpler datasets also meant that it was computationally feasible for us to reproduce these results. The base model used is well known with several implementation available, allowing us to focus on the novel aspects of the method."}, {"heading": "What was difficult", "text": "While the theoretical explanations of themethod are excellent, we initially found it hard to translate this into an implementation. Our difficulty was likely caused by our inexperience with the subject matter. Nonetheless, a pseudocode, such as the one we have provided, wouldhavee simplified the re-implementation. Wewerenot able to reproduce the results on some of the datasets due to limited computational resources.\nCommunication with original authors We did not contact the original authors directly, but we did refer to a public GitHub and blog post created by one of the authors. At the same time as submitting this report to the ML Reproducibility Challenge 2020 we also sent a copy to the authors and asked for their feedback.\nReScience C 7.2 (#10) \u2013 Liiv, Lennel\u00f6v and Nor\u00e9n 2021 2\n1 Introduction\nUncertainty estimation can help to make deep learning safer and more usable by allowing the model to identify cases it is not suitable to handle. There are different kinds of uncertainty, however, and it is especially interesting to separate uncertainty caused by ambiguities or contradictions in the data from the uncertainty that arises when a model faces a situation it has not been trained for. Ensemble-based methods of uncertainty estimation are capable of making this distinction but suffer from computational requirements at the evaluation phase [1]. The authors ofEnsembleDistributionDistillation (EnD2) [2] address this issue by using the output of an ensemble to train a so-called Prior Network (PN) [3], distilling the ensemble down to a single model while also preserving its uncertainty decomposition abilities. This can be contrasted with regular ensemble distillation models [4] (EnD), which are not able to decompose uncertainty. The reproduced paper was accepted to ICLR2020.\n2 Scope of reproducibility\nWe consider the setting of using CIFAR10 [5] as an in-distribution dataset, and LSUN [6] as an out-of-distribution dataset. Our supplementarymaterial also examines the setting of using a synthetic dataset in R2 for visualization. The claims from the original article that this reproduction is testing are as follows:\n1. Classification performance: In terms of error rate, prediction rejection rate, and negative log-likelihood EnD2has worse performance than the ensemble, but similar performance to EnD and PriorNet, and better performance than the individual model. In terms of expected calibration error, EnD2 has worse performance than the ensemble, but better performance than the other methods. On CIFAR-10 in particular, EnD2 has the best expected calibration error of all models. This claim corresponds to Table 3 in the original paper.\n2. Out-of-distribution detection performance: In terms of AUC-ROC on CIFAR-10 vs. LSUN, EnD2 without auxiliary dataset performs worse than the ensemble and the PriorNet, similar to the individual model, and better than EnD. With the auxiliary dataset, however, EnD2 performs as well as the ensemble, almost as well as PriorNet, and better than EnD. Using knowledge uncertainty as opposed to total uncertainty on CIFAR-10 vs. LSUN does not yield an improved AUC-ROC. This claim corresponds to Table 4 in the original paper.\n3. Dependency on ensemble size: Using 20 models in the ensemble does better than using 5 models, but there is no conclusive gain when using more than 20 models.\n4. Dependency on temperature: It is necessary to use temperature of at least 5 to successfully distribution-distill the ensemble. Using higher initial temperatures do not result in conclusive improvement.\n5. Uncertainty decomposition: EnD2 trained with an auxiliary dataset is able to reconstruct the uncertainty decomposition made possible by ensembles.\nWe reproduce all experiments of the main article and most of the appendix, except for the use of CIFAR100 and Tiny Imagenet datasets. Some of these results can be found in our supplementary material. From their appendix, we do not reproduce Table 7 in appendix B.We did not recreate the OOD-detection plots when reproducing the ablation study.\nReScience C 7.2 (#10) \u2013 Liiv, Lennel\u00f6v and Nor\u00e9n 2021 3\nThesemodels are all based on almost identical VGG16 architectures [7], adapted toCIFAR10 data as in [3] by adding dropout, batch normalization and reducing the size of the fully connected layers. The only exception being that batch normalization is not used for PN.\n3.2 Dataset The training set of CIFAR-10 was used as the primary training dataset. The training set of CIFAR-100 was used as an auxiliary dataset. For evaluating the classification task the test set of CIFAR-10 was used. For evaluating the out-of-distribution detection task the CIFAR-10 test set was used as in-domain dataset, while the LSUN test set was used as the out-of-domain dataset. Information about the datasets is listed in Table 1. Each image x was normalized according to x\u2032 = x/127.5 \u2212 1 where the operations are elementwise, causing all values to lie in the range (-1, 1). The LSUN images were also scaled down to 32x32. Furthermore, dataset augmentation was used for all models, consisting of rotations with 15\u25e6 range, horizontal flips, width and height shifts of up to 4 pixels in each direction, and using nearest-neighbour interpolation.\n3.3 Hyperparameters The models were trained with the hyperparameters listed in Table 2.\n3.4 Experimental setup and code Using thesemodels and dataset we ran a number of experiments, as detailed below. The full code is available on https://github.com/lennelov/endd-reproduce. Our implementation was made in TensorFlow Keras, as opposed to the original implementation which was made in PyTorch. Classification: The classification task was evaluated on the test set of CIFAR-10. We use the same fourmetrics as in the original paper, ERR, PRR, ECE, andNLL. ERR is themean\nReScience C 7.2 (#10) \u2013 Liiv, Lennel\u00f6v and Nor\u00e9n 2021 4\nclassification error. PRR is the prediction rejection area ratio introduced in Appendix B of [2]. ECE is the expected calibration error1. Finally, NLL is the negative log-likelihood. This experiment tests Claim 1. Out-of-distribution detection: The OOD-detection task was evaluated with the CIFAR-10 test set as the in-domain set, and the LSUN test set as the out-of-domain set. The AUCROC was computed both when total uncertainty and when only knowledge uncertainty is used to make rejection decisions. This experiment tests Claim 2. Ensemble size ablation study: Our examinationof the effect of ensemble size goes slightly beyond the original authors. We extend the error analysis to also consider the sensitivity of EnD2 to variations in the underlying ensemble. We began by training a set of 400 VGG16 models on CIFAR-10. Next, we sampled randomly from this set to create 4 different sets, each consisting of 100 models. For each N \u2208 {1, 2, 3, 4, 6, 8, 10, 13, 16, 20, 25, 30, 45, 60, 75, 100} we trained four EnD2 models on an ensemble consisting of the first N models in the first of the four sets, corresponding to what was done in the original study. We also trained one model on an ensemble consisting of the firstN models for each of the three remaining sets, capturing the sensitivity of EnD2 to changes in the underlying ensemble. All ensemble and EnD2 models were then evaluated on the classification task. This experiment tests Claim 3. Temperature ablation study: We reproduce the temperature ablation study by training EnD2 models for various initial temperatures. For each T \u2208 {1, 2, 3, 4, 5, 7.5, 10, 15, 20} we trained three EnD2 models with initial temperature T on an ensemble consisting of 100 VGG16 models. The EnD2 models were then evaluated on the classification task. In this experiment, we have chosen to use a slightly finer spacing between the temperatures than what the original authors used. This experiment tests Claim 4. Simplex visualization: A key motivation for EnD2 is the idea that an ensemble can distinguish between knowledge uncertainty and data uncertainty, and that this distinction is retained by the EnD2 model. This is communicated using a schematic figure showing ensemble predictions on a simplex. A similar schematic figure can be found in [3], depicting a Dirichlet PDF of a PriorNet on a simplex. We recreated these figures using experimental data in order to examine Claim 5 from a novel perspective. A new training set was created, consisting of all images from the CIFAR10 train set with one of three labels chosen for their similarity: \u02bcdeer ,\u0313 \u02bchorse ,\u0313 and \u02bcdog .\u0313 The remaining images were reserved as out-of-distribution dataset for testing. CIFAR-100 was used as auxiliary data. An ensemble and EnD2 was then trained on this data using the same architecture and processed as before. We then selected various images from the test set and visualized the ensemble predictions as well as the PDF of the EnD2 model. The simplex visualization was created using open source code 2.\n1We used the open-source implementation in https://github.com/google/uncertainty-metrics. 2http://blog.bogatron.net/blog/2014/02/02/visualizing-dirichlet-distributions/\nReScience C 7.2 (#10) \u2013 Liiv, Lennel\u00f6v and Nor\u00e9n 2021 5\nThe OOD-detection results are shown in Table 4. The results suggest plain EnD2 performs worse than ENSM, but that the addition of an auxiliary dataset brings the performance up to at least the level of ENSM. More surprising, perhaps, is that EnD2 seems to perform worse than EnD. In both metrics PN+AUX has a significant lead. Using knowledge uncertainty instead of total uncertainty decreases the effectiveness of all tested models. The supplemental material contains histograms showing the distribution of estimated total and knowledge uncertainty over the images.\n4.3 Ensemble size ablation study Figure 1 shows the results of the ensemble size ablation study. The lines \u02bcENSM Paper\u02bc and \u02bcEnD2 Paper\u02bc show the results of the original paper. The bands indicate two standard deviations. Two bands surround the \u02bcEnD2+AUX\u02bc line, representing the two types of variation we have examined. The purple band represents the variation of four EnD2 models each trained on a different ensemble. The orange band represents the variation of four EnD2 models all trained on the same ensemble. The band surrounding the \u02bcEnD2+AUX Paper\u02bc line corresponds to the latter type of variation.\nReScience C 7.2 (#10) \u2013 Liiv, Lennel\u00f6v and Nor\u00e9n 2021 6\nThere appears to be a trend of small improvement when the number of models is increased, but the high level of uncertainty makes it difficult to draw conclusions from the remaining points. Nonetheless, the results seem to generally indicate that EnD2 is not particularly sensitive to ensemble size.\n4.4 Temperature ablation study The results of our temperature ablation study are shown in Figure 2, along with the results of the original paper. For initial temperature equal to 1 and 2 our models fail to converge, resulting in poor classification performance. Raising the initial temperature to 3 allows the model to converge. Increasing the initial temperature further has no significant effect. It is worth noting the negative PRR values for T = 2. The original authors mention this possibility when they propose the metric, and offer the interpretation that this means that the model is increasing the classification error by rejecting samples, performing worse than simply rejecting at random.\n4.5 Simplex visualization Predictions for four images are visualized in Figure 3. These four images were selected from the CIFAR10 dataset for respectively having the lowest total uncertainty, highest data uncertainty, highest knowledge uncertainty, and highest total uncertainty, as measured by the ensemble. The third row shows the Dirichlet PDF of EnD2. There is a strong tendency towards extremely sharp distributions, even when the ensemble has high spread, making comparison difficult. For this reason the fourth row plots the PDF after being transformed by the transformation log(x+ 1). It is now possible to see that the PDF is adapting to the distribution of the ensembles.\nReScience C 7.2 (#10) \u2013 Liiv, Lennel\u00f6v and Nor\u00e9n 2021 7\nReScience C 7.2 (#10) \u2013 Liiv, Lennel\u00f6v and Nor\u00e9n 2021 8\nWe also plot randomly selected images from the in, out, and auxiliary datasets respectively. The PDF has again been transformed using log(x + 1). Figure 4 shows images from the in-domain dataset, and Figure 5 shows images from the out-of-domain dataset. The PDF appears to follow the ensemble fairly well, but it is noteworthy that the ensembles show such a low degree of spread despite encountering samples onwhich they have not been trained.\n5 Discussion\n5.1 Comparison with original paper We now revisit the six claims which we specified in Section 2.\n1. Classification performance: When compared to the original table we see overall worse performance. This is likely rooted in the fact that we were unable to achieve as high accuracy on our base VGG16 as in the original article. We therefore instead consider the relative performance between the models. Our supplementary material contains a table allowing for easy comparison with the original results. For example, we find that our EnD2 has 112.5% of the classification error of the ensemble, while in the original paper this figure is 117.7%. The absolute difference is the same in both papers, 1.1 percentage units. Our results generally agree well, with those of the authors. There are some discrepancies in expected calibration error, but our extremely high ECE for the individual model suggests that there might be an issue in our computations of this metric. Overall our findings support Claim 1.\n2. Out-of-distribution detection performance: For the most part, our results agree with Claim 2. For instance, we found that using total uncertainty EnD2 without auxiliary data had 98.1% of the AUC-ROC of the ensemble, while the corresponding figure with auxiliary data was 100.0%. In the original paper, these figures were 96.8% and 99.8% respectively. There is one very significant discrepancy, however. With auxiliary dataset, our EnD2had 99.6% of the AUC-ROC of our EnD, while in the original paper this figure is 106.5%. A similar relationship exists without the auxiliary dataset. It is worth noting that in the original paper EnD performs worse than even the individual model, and the authors themselves note that this is odd. Since EnD2 is designed to overcome certain shortcomings of EnD in terms of uncertainty estimation we believe that this warrants further investigation.\n3. Dependency on ensemble size: For prediction error and negative log-likelihood, our results confirm the relative performance between ensembles and EnD2+AUX, with increased resolution. For expected calibration error, the relative performance is confirmed for a large number of models, but for a small number of models, we get contradictory results. Their results seem to suggest that smaller ensembles have worse calibration, which is not expected, as per [1]. Our results confirm this expectation. In their paper, they state this expectation, but we see no comment\nReScience C 7.2 (#10) \u2013 Liiv, Lennel\u00f6v and Nor\u00e9n 2021 9\nfor this discrepancy. For prediction rejection rate, we confirm the relative performance, and also show that it starts to drop rapidly below their tested range.\n4. Dependency on temperature annealing: Our results diverge heavily from the results in the paper for temperatures 1 and 2. While the original authors are able to train working but sub-par models with these temperatures, we are unable to get the models to converge at all. We re-did the experiments with a new ensemble, and experimented with the smoothing factor and auxiliary data, but were unable to find any explanation for this difference. Nevertheless, these findings support the claim that temperature annealing is essential for successful use of the EnD2 method. The authors suggested temperature 5 as a minimum value beyond which larger values make no difference. Our findings support this as well, although our increased resolution reveals that the minimum value for the CIFAR-10 dataset is closer to 3 than 5.\n5. Uncertainty decomposition: Based on the description in [3] an image with a high knowledge uncertainty should produce a Dirichlet PDF with a close to uniform spread. Our simplex visualizations on the 3-class+AUX dataset shows that this is not the case. This is not too surprising, given that high knowledge uncertainty correlates with small alphas, and this in turn produces convex as opposed to flat probability density surfaces. Overall, these plots suggest that EnD2can capture the uncertainty decomposition of the ensemble. The plots also show an interesting behaviour in the ensemble. The ensembles agree to a surprising extent on the out-of-domain samples. In fact, when they do disagree it normally takes the form of data uncertainty as opposed to knowledge uncertainty. This could perhaps shed some light on the observation that knowledge uncertainty does not seem to be useful for OOD-detection on CIFAR-10. The original authors explain this as essentially being a property of the dataset. We feel, based on the visualizations, that another possibility might be that the ensemble models simply are not diverse enough to provide a useful measure of knowledge uncertainty.\n5.2 What was difficult Although the general idea of the paper is well formulated in mathematical terms, the original paper does not providemany hints regarding how to implement themethod. In our case, this imposed a significant barrier to immediately reproducing the work, since our inexperience meant that we\u02bcre unable to immediately see how it could be implemented in a modern deep learning framework. There is some code available in a public repository hosted by one of the authors but this is not mentioned in the paper, and so we could not treat it as an official implementation. We have provided a pseudocode in our supplemental material, in order to hopefully assist future reproducers. There are also some missing details regarding the models used. Most importantly, the authors mention that they have used a modified VGG model, but do not specify what these modifications are. The authors also do not specify the min and max value of the cyclic LR. These details may explain the consistently worse performance of our models despite the attempt of replication.\n5.3 What was easy The synthetic dataset was fairly easy to reconstruct, and the other datasets are well known and publicly available. The data augmentation was straightforward and easy to incorporate into a training pipeline. The base model (VGG16) used in most of the experiments is well known and was computationally feasible to train. Similarly, the datasets are not excessively demanding in terms of computation, although in our case training\nReScience C 7.2 (#10) \u2013 Liiv, Lennel\u00f6v and Nor\u00e9n 2021 10\ntime did become a limiting factor due to the amount of time we spent on implementation and experimentation. The mathematical formulation of the model is very good, helping the conceptual understanding.\n5.4 Communication with original authors We did not communicate with the authors while reproducing their work, although we did refer to some resources which one of the authors has made publicly available, including an repository 3 made for [3] containing an implementation of EnD2. At the same time as submitting this report, we also sent a copy to the authors and asked for their comments."}, {"heading": "1. B. Lakshminarayanan, A. Pritzel, and C. Blundell. \u201cSimple and Scalable Predictive Uncertainty Estimation using", "text": "Deep Ensembles.\u201d In: Advances in Neural Information Processing Systems 30. 2017. 2. A. Malinin, B. Mlodozeniec, and M. Gales. \u201cEnsemble Distribution Distillation.\u201d In: International Conference on Learning Representations (ICLR). 2020. 3. A. Malinin and M. Gales. \u201cPredictive Uncertainty Estimation via Prior Networks.\u201d In: Advances in Neural Information Processing Systems 31. 2018. 4. G. Hinton, O. Vinyals, and J. Dean. \u201cDistilling the Knowledge in a Neural Network.\u201d In: NIPS Deep Learning and Representation Learning Workshop. 2015. 5. A. Krizhevsky. \u201cLearning Multiple Layers of Features from Tiny Images.\u201d In: University of Toronto (May 2012). 6. F. Yu, Y. Zhang, S. Song, A. Seff, and J. Xiao. \u201cLSUN: Construction of a Large-scale Image Dataset using Deep\nLearning with Humans in the Loop.\u201d In: CoRR abs/1506.03365 (2015). arXiv:1506.03365. URL: http://arxiv.org/ abs/1506.03365. 7. K. Simonyan and A. Zisserman. \u201cVery Deep Convolutional Networks for Large-Scale Image Recognition.\u201d In: International Conference on Learning Representations (ICLR). 2014.\n3https://github.com/KaosEngineer/PriorNetworks/tree/master/prior_networks\nReScience C 7.2 (#10) \u2013 Liiv, Lennel\u00f6v and Nor\u00e9n 2021 11\nA The EnD2 Algorithm\nThe original paper features an excellent description of themathematical formulation of the EnD2 model, but we did not find it immediately obvious how to translate this into an implementation in a modern deep learning framework. For this reason, we will now briefly describe it from an algorithmic-centred perspective using pseudocode and plain English. The process of training an EnD2 model is described in Algorithm 1. In practice, the optimization in line 7 can easily be achieved using the standard \u201dfit\u201d method of frameworks such as Keras and PyTorch, by constructing an intermediate dataset and using a custom loss function with a callback for annealing the temperature. The intermediate dataset is constructed by first adding any auxiliary images to the training images, and then passing the extended image set as input to the ensemble. The ensemble should output an array of logits as described in line 5 of Algorithm 1. The new dataset is then formed by matching each image to its corresponding ensemble logits, using the latter as the target. The custom loss function is described in Algorithm 2. This formulation includes temperature annealing. This loss function is the onlymodification necessary to adapt a general classificationmodel into an EnD2 model, providing it is then trained on an intermediate dataset as described in the previous paragraph. Note that this formulation assumes that the model outputs logits. This output can be converted into Dirichlet probabilities by applying the standard softmax operation.\nAlgorithm 1: Training algorithm for EnD2 given an ensemble Input :Ensemble En outputting logits, training data X (same as the ensemble is trained on), (optional) Out of distribution dataXOOD Output :Trained EnD2 model\n1 if XOOD not None then 2 X = [X,XOOD] // append OOD data to training set 3 end 4 \u03d5 = En.predict(X) // exp(\u03d5) are the labels for EnD2 5 // \u03d5 is a tensor of logits corresponding to the true distribution, each row\ncorresponds to a model and each column a class. Each matrix corresponds to one image\n6 model\u03b8 \u2190 classifier //create a new classifier model with weights \u03b8, with logits as output 7 EnD2 = argmin\u03b8{LossEnD2(\u03d5,model\u03b8(X))} //train model backpropagation 8 return EnD2\nB Experiments on Synthetic Data\nB.1 Methodology The goal with these experiments is to provide qualitative justification for Claim 5 and illustrate the inner workings of EnD2. We also provide some new experiments on temperature annealing and the size of the auxiliary dataset, to visualize their effect.\nDataset \u2014 To illustrate the model, Malinin et. al. use a synthetic dataset in R2. Our rendering of this dataset can be seen in Figure 6. This is advantageous since it enables plotting both knowledge and data uncertainty over the entire data manifold, giving a qualitative understanding of whether the algorithm works or not, in contrast to higher\nReScience C 7.2 (#10) \u2013 Liiv, Lennel\u00f6v and Nor\u00e9n 2021 12\nAlgorithm 2: loss for EnD2\nInput :Ensemble logits: \u03d5, predicted logits: z, temperature: T = T (t), annealing Output :cost: C\n1 \u03f5 = 10\u22128 // Smoothing factor 2 \u03b4 = 1\u2212 10\u22123 // Central smoothing factor 3 \u03b1 = ez/T (t) // elementwise exponential 4 M = #models 5 N = #classes 6 for i\u2190 1 to M do 7 \u03b10,i = \u2211 j \u03b1i,j // sum over the classes to produce the precision factor\n8 end 9 PEn = softmax(\u03d5/T (t)) // softmax over classes 10 PEn = \u03b4(PEn \u2212 1N ) + 1 N\n11 TIT = \u2211N\ni (log(\u0393(\u03b1i + \u03f5)))\u2212 log(\u0393(\u03b10 + \u03f5)) // target independent term, where log(\u0393(x)) = log((x\u2212 1)!)\n12 A = 1M \u2211M\ni (log(PEni + \u03f5) // mean over ensemble 13 TDT = \u2212 \u2211N i ((\u03b1i \u2212 1)Ai) // target dependent term, sum over classes 14 return (TDT + TIT )T (t)2\ndimensional data (images, etc.) that cannot be plotted. The dataset itself looks like a spiral, divided into three classes shaped as spiralling arms of increasing radius. The spirals are centred and almost symmetric around the origin. Furthermore, they have increased noise and overlap with radius, which leads us to believe that uncertainty should vary as well. In addition to the spiral data an OOD data-set, referred to as the AUX data-set is also used, which takes the form of a ring slightly outside the spiral. For the experiments, 1000 samples per ID class are used, both for training and test. The number of AUX samples was also 1000. This is the same setting as the original paper. The generation of the data uses the original paper s\u0313 code, but the hyperparameters were not specified. Our hyperparameters can be found in our code. We manually searched for hyperparameters, so that our plot would look as close to theirs, but the exact correspondence is probably not achieved.\nModel description and hyperparameters \u2014 The original paper does not specify what type of neural network was used for classification. We were also unable to find it in the (unofficial) code. Instead, we chose to use a simple DNNwith four hidden layers, each of width\nReScience C 7.2 (#10) \u2013 Liiv, Lennel\u00f6v and Nor\u00e9n 2021 13\n64 with ReLu-activation functions, trained by minimizing the categorical cross-entropy using the Adam-optimizer, all with standard tf.keras settings, for 85 epochs. EnD and EnD2 used the same base model but was instead trained for 500 epochs.\nExperimental setup and code \u2014 On the output of an ensemble of 100 models, all differently randomly initialized, we train EnD and EnD2 both with and without auxiliary data, using an initial temperature of 1, as in the paper. Doing this, we observed that the training diverges for many initialisations, mainly for EnD2+AUX. Thus, we also used an initial temperature of T = 2.5, both with and without annealing. The annealing schedule was T = 2.5 between epoch 0 and 200, linearly decreasing to 1 between epoch 200 and 400 and 1 between epoch 400 and 500. Additionally, we also trained amodel EnD2+AUX20, with only 20 samples from the auxiliary dataset. All 7 models were trained 20 times, with different random initialisations. To make sure they converged, the test error was calculated. In cases test error was above 10%, it was deemed as non-convergence, and not taken into account. Among the converged ones, the mean error and the 95%-confidence interval around the mean is calculated, assuming a normal distribution. Thismeans that for cases with fewer samples, the confidence interval is larger. The main goal of this experiment is to visually show the total uncertainty, the data uncertainty and the knowledge uncertainty. They were calculated as specified in [2] and [3], for the grid [\u22122000, 2000] \u00d7 [\u22122000, 2000] at all coordinates divisible by four, for a total of 106 points. The full code is available at https://github.com/lennelov/endd-reproduce.\nComputational requirements \u2014 The experiments were run on the CPU of a normal laptop (2.7 GHz Dual-Core i5). The total time to reproduce the ensemble of 100 models and all 20 repetitions of all 7 tested distillation methods, is around 5 to 6 hours.\nB.2 Results\nClassification accuracy \u2014 In Table 5, the classification accuracy from our experiment and the original paper is reported. We see that\n\u2022 the ensemble outperforms the individualmodels, and that all distillationmethods perform closer to the ensemble, than an individual model.\n\u2022 the best performance is achieved by EnD with auxiliary data.\n\u2022 using annealing or not when starting at T = 2.5 does not affect the final classification accuracy.\nVisualization of uncertainty \u2014 The total, data and knowledge uncertainty is plotted in Figure 7 for a grid of 106 points. In contrast to the original paper, we fix the scale of the colour bar for better comparability between plots. We observe that\nReScience C 7.2 (#10) \u2013 Liiv, Lennel\u00f6v and Nor\u00e9n 2021 14\nIn this section, we report the computational resources used for this reproduction. The running time of the major experiments on CIFAR-10 is expressed in time on an RTX 2070. For easier comparison, we also report the equivalent cost when running on a V100 GPU on Google Cloud for $2.48 per hour, given a relative performance of 2.89 versus an RTX 20704. Note that these figures represent the time to reproduce only the final experiments. We estimate that the total GPU time used for this reproduction, including experimentation and bug-hunting, to be 3 to 5 times as long. The full data can be seen in Table 6.\nD Histograms\nTo compare ensembles, EnD2 andEnD2+AUX on theCIFAR-10 and 3-classCIFAR-10 datasets, we provide histograms of data and knowledge uncertainty for in- and out-of-domaindistribution, in Figure 8 and 9.\nE Relative performance of EnD2 compared to ensemble and original article\nIn Tables 3 and 4 of the main report we report several measures for the 7 different models tested. For better comparability, we here also provide the values normalized to the ensembles\u02bc performance, both for our experiments, and for the original paper, in Table 8 and 7.\n4Benchmark taken from https://timdettmers.com/2020/09/07/which-gpu-for-deep-learning/\nReScience C 7.2 (#10) \u2013 Liiv, Lennel\u00f6v and Nor\u00e9n 2021 15\nReScience C 7.2 (#10) \u2013 Liiv, Lennel\u00f6v and Nor\u00e9n 2021 16\nFigure 8. Data/knowledge uncertainty-distributions for ensemble, EnD2 and EnD2+AUX.\nFigure 9. Data/knowledge uncertainty-distributions for ensemble and EnD2+AUX on the 3-class CIFAR10 dataset\nReScience C 7.2 (#10) \u2013 Liiv, Lennel\u00f6v and Nor\u00e9n 2021 17"}], "title": "[Re] A Reproduction of Ensemble Distribution Distillation", "year": 2021}