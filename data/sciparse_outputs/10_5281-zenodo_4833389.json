{"abstractText": "The central claims of the paper are two-fold: (1) The properties of the activation functions are carried over to the neural networks. Atanhnetworkwill be smooth and extrapolates to a constant function, while ReLU extrapolates in a linear way. Standard neural networks with conventional activation functions are insufficient for extrapolating periodic functions. (2) The proposed activation function manages to learn periodic functions while being able to optimize as well as conventional activation functions. While both experimental proof and theoretical justifications are provided for the claims, we shall only be concerned with testing the claims via experimental means.", "authors": [{"affiliations": [], "name": "Mayur Arvind"}, {"affiliations": [], "name": "Mustansir Mama"}, {"affiliations": [], "name": "Koustuv Sinha"}, {"affiliations": [], "name": "Sasha Luccioni"}], "id": "SP:2a5543a1c87da3d14d4bb19324db170feb28ccf7", "references": [{"authors": ["L. Ziyin", "T. Hartwig"], "title": "and M", "venue": "Ueda. Neural Networks Fail to Learn Periodic Functions and How to Fix It.", "year": 2020}, {"authors": ["M. Leo", "A. Furnari", "G.G. Medioni", "M. Trivedi", "G.M. Farinella"], "title": "Deep Learning for Assistive Computer Vision", "venue": "Ed. by L. Leal-Taix\u00e9 and S. Roth. Cham,", "year": 2019}, {"authors": ["L. Deng"], "title": "Deep learning: from speech recognition to language and multimodal processing. 2016", "year": 2015}, {"authors": ["G. Melis", "C. Dyer"], "title": "and P", "venue": "Blunsom. On the State of the Art of Evaluation in Neural Language Models.", "year": 2017}, {"authors": ["A. Silvescu"], "title": "Fourier Neural Networks", "venue": "July", "year": 2000}, {"authors": ["A. Zhumekenov", "M. Uteuliyeva", "O. Kabdolov", "R. Takhanov", "Z. Assylbekov"], "title": "and A", "venue": "J. Castro. Fourier Neural Networks: A Comparative Study.", "year": 2019}, {"authors": ["V. Sitzmann", "J.N.P. Martel", "A.W. Bergman", "D.B. Lindell"], "title": "and G", "venue": "Wetzstein. Implicit Neural Representations with Periodic Activation Functions.", "year": 2020}, {"authors": ["P. Ramachandran", "B. Zoph"], "title": "and Q", "venue": "V. Le. Swish: a Self-Gated Activation Function.", "year": 2017}, {"authors": ["V. Nair", "G.E. Hinton"], "title": "Rectified Linear Units Improve Restricted Boltzmann Machines", "venue": "Ed. by J. F\u00fcrnkranz and T. Joachims.", "year": 2010}, {"authors": ["D.-A. Clevert", "T. Unterthiner"], "title": "and S", "venue": "Hochreiter. Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs).", "year": 2016}, {"authors": ["B. Xu", "N. Wang", "T. Chen"], "title": "and M", "venue": "Li. Empirical Evaluation of Rectified Activations in Convolutional Network.", "year": 2015}, {"authors": ["G. Parascandolo", "H. Huttunen"], "title": "and T", "venue": "Virtanen. Taming the waves: sine as activation function in deep neural networks.", "year": 2017}, {"authors": ["K. He", "X. Zhang", "S. Ren"], "title": "and J", "venue": "Sun. Deep Residual Learning for Image Recognition.", "year": 2016}, {"authors": ["L. Ziyin", "Z.T. Wang"], "title": "and M", "venue": "Ueda. LaProp: Separating Momentum and Adaptivity in Adam.", "year": 2020}, {"authors": ["R. Pascanu", "T. Mikolov"], "title": "and Y", "venue": "Bengio. On the difficulty of training Recurrent Neural Networks.", "year": 2013}, {"authors": ["A. Radford", "L. Metz"], "title": "and S", "venue": "Chintala. Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks.", "year": 2016}], "sections": [{"text": "R E S C I E N C E C Replication / ML Reproducibility Challenge 2020\n[Re] Neural Networks Fail to Learn Periodic Functions and How to Fix It Mayur Arvind1, ID and Mustansir Mama2, ID 1BITS Pilani, Goa, India \u2013 2BITS Pilani, Pilani, India\nEdited by Koustuv Sinha, Sasha Luccioni\nReviewed by Anonymous Reviewers\nReceived 29 January 2021\nPublished 27 May 2021\nDOI 10.5281/zenodo.4833389"}, {"heading": "Reproducibility Summary", "text": ""}, {"heading": "Scope of Reproducibility", "text": "Neural Networks Fail to Learn Periodic Functions and How to Fix It [1] demonstrates experimentally that standard activations such as ReLU, tanh, sigmoid and their variants all fail to learn how to extrapolate simple periodic functions. The original paper goes on to propose a new activation, which the authors name the snake function.\nThe central claims of the paper are two-fold: (1) The properties of the activation functions are carried over to the neural networks. Atanhnetworkwill be smooth and extrapolates to a constant function, while ReLU extrapolates in a linear way. Standard neural networks with conventional activation functions are insufficient for extrapolating periodic functions. (2) The proposed activation function manages to learn periodic functions while being able to optimize as well as conventional activation functions. While both experimental proof and theoretical justifications are provided for the claims, we shall only be concerned with testing the claims via experimental means."}, {"heading": "Methodology", "text": "While one of the authorswas contacted to clarify certain difficulties, the reproduction of all experiments was completed using only the information provided in the paper. With one exception, the links to all datasets used were also provided in the original paper. This allowed us to implement most experiments from scratch."}, {"heading": "Results", "text": "We were able to successfully replicate experiments supporting the central claim of the paper, that the proposed snake non-linearity can learn periodic functions. We also analyze the suitability of the snake activation for other tasks like generative modeling and sentiment analysis."}, {"heading": "What was easy", "text": "Manyexperiments includeddescriptions of theneural network architectures and graphs showcasing performance, giving us a clear benchmark to compare our results against.\nCopyright \u00a9 2021 M. Arvind and M. Mama, released under a Creative Commons Attribution 4.0 International license. Correspondence should be addressed to Mayur Arvind (f20160603@goa.bits-pilani.ac.in) The authors have declared that no competing interests exist. Code is available at https://github.com/mayurak47/Reproducibility_Challenge \u2013 DOI 10.5281/zenodo.4696245. \u2013 SWH swh:1:dir:8887a5d45dac9427457c6b5869728e1ba3cfd37c. Open peer review is available at https://openreview.net/forum?id=ysFCiXtCOj.\nReScience C 7.2 (#3) \u2013 Arvind and Mama 2021 1\nLinks to datasets for all experiments, barring one, were also included in the paper itself."}, {"heading": "What was difficult", "text": "Data for the human body temperature experiment was not available. Proper implementation details were not given for initializing the weights in neural networks with snake and using snake with RNNs.\nCommunication with original authors Liu Ziyin, one of the authors, was contacted to provide the dataset used for the human body temperature experiment, elaborate upon the implementation of variance correction and provide the implementation of RNNs using snake. Liu provided the GitHub link to the authors\u02bc original code for the human body temperature, market index, and extrapolation experiments. Liu also provided an explanation on how to implement variance correction. While the code for the RNN implementation using the snake activation was not made public, a screenshot of the same was provided. We thank the authors for their assistance.\n1 Introduction\nDeep neural networks are playing an increasingly prominent role in fields as diverse as computer vision [2], speech recognition [3], and language modeling [4]. However, while neural networks are excellent tools for interpolating between existing data, standard versions of these networks are not suited for extrapolation beyond the training range. This causes them to struggle at making predictions in problems with a periodic component. Previous attempts at addressing neural networks\u02bc inability to learn periodic functions have included using periodic activation functions [5, 6]. For example, using sin(x) as the activation function for implicit neural representations has been successful at representing complex natural signals and their derivatives [7]. However in more general cases, experimental results suggest that using sin as the non-linearity cannot compete against ReLU-based activation functions [8, 9, 10, 11] on standard tasks [12].\nThe original paper: (1) studies the extrapolation properties of a neural network beyond a bounded region; (2) shows that neural networkswith standard activation functions are insufficient to learn periodic functions outside the bounded region where data points are present; (3) proposes a solution for this problem in the form of a novel activation function and its variants, and showcases its performance on toy examples and realworld tasks. We have tested the claimsmade in the original paper, replicating the experiments displaying the failure of standard activation functions to learn periodic functions as well as the results of the novel activation function on toy and real-world tasks. We have also conducted experiments of our own to understand how viable the proposed activation function is at replacing existing standards such as ReLU and tanh.\n2 Scope of reproducibility\nThe authors make two key claims:\n\u2022 Standard neural networks with standard activation functions are insufficient to learn periodic functions outside the bounded region where data points are present.\n\u2022 The proposed novel activation function can learn periodic functions while maintaining the favorable optimization property of the ReLU-based activations. The novel activation is referred to as \u201csnake\u201d:\nReScience C 7.2 (#3) \u2013 Arvind and Mama 2021 2\nsnakea(x) := x+ 1\na sin2(ax)\nwhere a is treated as a fixed parameter in initial experiments, and as a learnable parameter in a few experiments. Snake is shown to outperform standard activation functions ReLU,tanh,LeakyReLU [9], as well as more recently proposed functions such as swish [8], and sin [7, 12].\nDue to the broad and far-reaching consequences of the two claims, the original paper supports them via both theoretical justification and an extensive list of experiments which range from testing performance on toy datasets to real world applications. We have exhaustively replicated the original list of experiments, and have conducted a few additional experiments of our own, using the proposed activation function in a Deep Convolutional Generative Adversarial Network (DCGAN) to generate images of handwritten digits and in a Long Short TermMemory (LSTM) network for sentiment analysis.\n3 Methodology\nThe code used by the authors had not been made public at the time we started working on re-implementing the paper. That meant we reproduced all the results in the paper from scratch relying on the descriptions of the neural network architecture and a link or description of the dataset. The descriptions were brief but sufficient such as \u201cfeedforward neural network with 2 hidden layers, both with 64 neurons\u201d for the Body Temperature Prediction experiment and \u201c4-layer feedforward network with 1 \u2192 64 \u2192 64 \u2192 1 hidden neurons\u201d for the Financial Data Prediction experiment. In the case of experiments that utilized large standard networks such as ResNet18, the PyTorch library implementation of the model was used, with snake substituted in place of the default activation functions. Besides the model implementations, we were also required to make a a learnable parameter in snake for a few experiments.\n3.1 Model descriptions Models used in the original paper included fully-connected, feed-forward neural networks with different architectures for the various experiments. Larger standard models such as ResNet18 were also used. The authors of the original paper had initially not made their code available and we had to implement most models ourselves.\n3.2 Datasets The data used in the extrapolation experiments are directly sampled from periodic functions such as sin(x). Some experiments dealt with standard datasets such MNIST and CIFAR-10. Data for the real-life datasets had to be downloaded:\n\u2022 Daily data from 1995-1-1 to 2020-1-31 of Wilshire 5000 Total Market Full Cap Index: Downloaded from link provided in the original paper: https://www.wilshire.com/ indexes\n\u2022 Average weekly temperature evolution in Minami-Tori-shima, an island south of Tokyo (longitude: 153.98, latitude: 24.28) after April 2008: Downloaded from link provided in the original paper: https://join.fz-juelich.de/access\n\u2022 Patient body temperature: Made available by the authors upon request\n\u2022 IMDBReviewsDataset used for our additional sentiment analysis experiment: Downloaded from https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\nReScience C 7.2 (#3) \u2013 Arvind and Mama 2021 3\n3.3 Hyperparameters Different experiments included varying levels of detail with respect to hyperparameters. Many experiments provided an overview of the neural network architecture (e.g. \u201c4-layer fully connected neural network\u201d) but not other hyperparameters and relevant details, such as batch size, loss function, or learning rate. In cases where information was missing, assumptions had to be made, with some trial-and-error required to obtain a close approximation of the original result. This trial-and-error involved a grid search over the architecture (number of layers, number of neurons in each layer), number of epochs (100 to 5000), batch size (16 to 512), optimizer (Adam, SGD, RMSProp), learning rate (0.001 to 0.1) and value of a in networks with the snake activation (1 to 30).\n3.4 Experimental setup The entire codebase has been uploaded to GitHub and is publicly available: https://github. com/mayurak47/Reproducibility_Challenge. The experiments were run locally as well as on GPU enabled sessions on Google Colab. All the models and experiments were coded using the PyTorch library.\n3.5 Computational requirements Many of the experiments, particularly those relating to regressing different functions and datasets, could be run locally on a MacBook Air with an Intel i5 CPU and 8 GB of RAM, not requiring more than a few minutes to train. The more demanding experiments required the use of GPUs. Training a ResNet18 on CIFAR-10 with six activation functions for 100 epochs took roughly 12 hours on a Tesla T4 GPU on Google Colab. Our additional experiments on training a GAN and an LSTM required roughly 2 hours each on the same hardware.\n4 Results\nWherever possible, the claims of the original papers were tested and in each case, we were able to reproduce the original results. The list of experiments that we reproduced is listed below.\n4.1 Extrapolation experiments on analytic functions\nReScience C 7.2 (#3) \u2013 Arvind and Mama 2021 4\nNeural networks with a single hidden layer consisting of 512 neurons are trained on data sampled from four different analytic functions using the ReLU and tanh activation functions. The training data is obtained by sampling from [-5, 5] with a gap in [-1, 1]. It is observed in Fig. 1 that the extrapolation of neural networks depends on the activation function used. When ReLU is used, the extrapolation diverges to \u00b1\u221e. When tanh is used, the extrapolation levels off. The authors formally prove these observations and conclude that neural networks using these activation functions cannot learn to extrapolate periodic functions.\n4.2 Applicability of proposed method It is first demonstrated that the snake activation function is easier to optimize than other commonly used baseline periodic activation functions like sin(x) and x+sin(x). Fully-connected neural networks with 3 hidden layers (512 neurons each) are trained on the MNIST dataset. This is a 10-way classification problem, and the training crossentropy losses for the different networks can be observed in Fig. 2, with the snake network achieving the lowest training loss.\nIt is then shown that snake is able to regress the periodic function sin(x). While all activation functions learn the training data (Fig. 1), only snake is able to capture the periodic behavior of sin(x) (Fig. 3). The extrapolation diverges from the underlying sin function due to the limited training data used.\nReScience C 7.2 (#3) \u2013 Arvind and Mama 2021 5\n4.3 Applications Multiple experiments are conducted to illustrate the performance of snake on a range of tasks.\nResNet18 [13], with 10M parameters, is trained on the CIFAR-10 dataset. This is a 10-way image classification task. The ReLU layers in the architecture are replaced with the specified activation, and the network is trained for 100 epochs. The LaProp optimizer 1 [14] is used; the learning rate is 4 \u00d7 10\u22124 for the first 50 epochs and 4 \u00d7 10\u22125 for the next 50. A test accuracy of 93-94% is achieved by the snake network (Fig. 4), in line with that of the other standard activation functions. This suggests that snake is suitable for large-scale image classification problems, and may be used as a straightforward alternative to other activation functions.\nThe core utility of snake is shown via two real-life problems. The two tasks are predicting the evolution of temperature in Minami-Tori-shima island\nin Japan (Fig. 5), and the modeling the body temperature of a patient (Fig. 6). The architectures used are 1 \u2192 100 \u2192 100 \u2192 1 and 1 \u2192 64 \u2192 64 \u2192 1 respectively, as in the original paper. In the Minami-Tori-shima weather experiment, the parameters a were made learnable; in the body temperature experiment, a = 30. In both cases, snake is the only activation function that makes meaningful extrapolation and predictions. It can also be seen in Fig. 5b that snake is the only activation function that is able to learn the training data - the other non-linearities are unable to fit the training points, irrespective of the number of epochs the models are trained for.\nThe snake network correctly learns the periodicity of the atmospheric temperature dataset, even though the amplitude is slightly off, and correctly infers that body temperature is roughly 37\u00b0C.\n1Code taken from https://github.com/Z-T-WANG/LaProp-Optimizer\nReScience C 7.2 (#3) \u2013 Arvind and Mama 2021 6\nAnother regression problem the authors used to demonstrate the working of snake is that of financial data prediction (Fig. 7). The data used is from the Wilshire 5000 Total Market Full Cap Index, considered representative of the worldwide economic trend. The snake network ( 1 \u2192 64 \u2192 64 \u2192 1, a = 30), which was trained using data from1995 to 2020-1-31, before COVID-19 impacted the world economy, predicted an economic slowdown in 2020. This might be due to the cyclic na-\nture of world markets, which the model was able to capture. As in the previous regression experiments, snake performs better than conventional non-linearities (Table 1).\nThe authors, in an additional experiment described in the appendix, use this dataset to gain insights into how the snake activation function learns (Fig. 8). Observing the predictions made at various points in the training process, we notice that at first, the features learned are mostly linear, low frequency features are then learned, and high-\nReScience C 7.2 (#3) \u2013 Arvind and Mama 2021 7\nfrequency features are learned at the later stages of training.\nThe performance of a snake feedforward network (two hidden layers of 64 neurons each, a = 30) and a recurrent neural network (single recurrent layer, 64 features in hidden state), typically used for time-series prediction, are compared in Fig. 9. The task is to learn the function sin(0.1x), with Gaussian noise \u03c3 added, for T = 300 timesteps. The first 200 are used for training, while the last 100 are used for testing.\nIt is seen that because of the noisy training data, even the predictions of the RNN are noisy, with a high generalization loss. The feedforward network, on the other hand, almost perfectly learns the underlying function with the right frequency and amplitude. Further, RNNs learn by backpropagation through time (BPTT), which has a prohibitively high computation cost, and can result in the exploding/vanishing gradient problem [15]. As a result the time taken by the snake network to regress the function is roughly 2 orders of magnitude lower than the time taken by the RNN (Fig. 10). This suggests that snake networks may be more effective in modeling data that is known beforehand to be periodic in nature.\n4.4 Effect of a In a series of experiments, the authors depict the effect the parameter a has on the learning process. We reproduce one of these experiments for brevity. Simple neural networks ( 1 \u2192 64 \u2192 64 \u2192 1) are trained on the sinusoidal function sin(x) + sin(4x)/4. It is seen in Fig. 11 that larger a encourages the model to learn features with higher frequency. With a = 1, the higher frequency modulation is considered noise, while the a = 16 model learns both the signals. This tendency can be taken into account while working with data known to be periodic, with a well-chosen a speeding up training.\nReScience C 7.2 (#3) \u2013 Arvind and Mama 2021 8\n4.5 Results beyond original paper The original paper demonstrated the ability of neural networks with the snake activation function to learn periodic functions and that the performance on everyday tasks like image classification is similar to that of neural networks employing conventional activation functions. We extend this study to more sub-fields of deep learning.\nWe train a deep convolutional generative adversarial network (DCGAN) 2 [16] to generate samples of theMNIST dataset. All the activations in the generator and discriminator sub-networks are replaced with the specified non-linearity. We see that while the initial training is slow for the snake GAN (Fig. 13a), it eventually generates realistic samples\n2Code adapted from https://github.com/eriklindernoren/PyTorch-GAN\nReScience C 7.2 (#3) \u2013 Arvind and Mama 2021 9\n(Fig. 12a), which are qualitatively indistinguishable from those output by a typical GAN using the LeakyReLU non-linearity (Fig. 12b). a was a learnable parameter in this experiment.\nFinally, we use the snake activation function in a Long Short Term Memory (LSTM) network for sentiment analysis 3 on the IMDB movie reviews dataset. This is a binary classification problem, attempting to predict whether amovie review is positive or negative. The typical tanh activation used to output the value ht = ot \u2217tanh(Ct) in an LSTM is replaced by the snake activation, so that ht = ot \u2217 snake(Ct). We observed that the snake LSTM network did not perform very well on this task (Fig. 14) and convergence was much more gradual. A single epoch of training the snake LSTM took twice as long as training the tanh LSTM. Also, in many cases, the snake network got stuck in local minima, necessitating a restart of training.\nA possible explanation for this is that the snake function is not bounded like tanh, causing an increase in the magnitudes of ht and leading to instability. The results of the experiment do not mean that snake cannot be used in sequence models, only that the application is not as straightforward as in the previous experiments, and further modifications in the architectures might be necessary.\n5 Discussion\nAs the authors had not initially made their code available and only included brief descriptions of the network architectures used in their experiments, exact replication of their experimental results was not possible. However, the qualitative nature of the papermeant that only the relative performance ofsnake in comparison to other activation functions on the specified problems was of interest, as opposed to the exact architectural details or loss values achieved. For example, the losses observed in Table 1 and Fig. 5b are orders of magnitude different from those in the original paper, likely due to varying normalization techniques and hyperparameters, even though the overall results observed in Fig. 5a and Fig. 7 are similar to those observed in the original paper. We were able to uphold the claim that neural networks with standard activation functions are insufficient to learn periodic functions outside the training range. We were also able to verify that the proposed activation function performs as well as standard activation functions, ReLU,tanh,LeakyReLU, over a wide range of tasks (with the exception of the LSTM experiment), by replicating the experiments in the original paper and conducting some additional ones ourselves. Future work could focus upon providing theoretical justifications for the behavior of snake and developing more suitable optimization algorithms.\n3Code adapted from https://www.kaggle.com/arunmohan003/sentiment-analysis-using-lstm-pytorch and https://github. com/piEsposito/pytorch-lstm-by-hand\nReScience C 7.2 (#3) \u2013 Arvind and Mama 2021 10\n5.1 What was easy A detailed description of the neural network architectures used for experiments such as training on the MNIST dataset and human body temperature was provided, allowing us to replicate the experiments closely. Links to datasets for all experiments, barring one, were also included in the paper itself. An extensive appendix sections listed additional experiments comparing the performance of snake with different a. Every experiment was supported by graphs showcasing the performance of snake with other activation functions, giving us a clear metric against which we could compare the results of our reproductions.\n5.2 What was difficult The original source codewasnot provided initially andwehad to rely on the descriptions of architectures and hyperparameters (which were absent in many cases) and educated guesswork while attempting to replicate the results. Data for the human body temperature experiment was not available. Theoretical justification for variance correction and the results of this variance correction using ResNet101 on CIFAR-10 were provided, but implementation details were not included. The section on Comparison with RNN on Regressing a Simple Periodic Function simply states that snake was deployed on a feedforward network, without any additional details of the hyperparameters used. The dataset for the experiment had to be inferred from the graphs of the results, and since white noise had been added to the data, exact replication of the experimental setup was not possible.\n5.3 Communication with original authors Liu Ziyin, one of the authors, was contacted to provide the dataset used for the human body temperature experiment, elaborate upon the implementation of variance correction and provide the implementation of RNNs using snake. Liu provided the GitHub link to the authors\u02bc original code4 for the human body temperature, market index, and extrapolation experiments. Liu also provided an explanation on how to implement variance correction. While the code for the RNN implementation using snake activation was not made public, a screenshot of the same was provided. The provided code was incomplete and not fully documented but was nonetheless valuable in giving us a rough idea about the hyperparameters used. The provided repository also contains the human body temperature dataset within the codebase, which is not available in the original paper. We thank the authors for their assistance."}], "title": "[Re] Neural Networks Fail to Learn Periodic Functions and How to Fix It", "year": 2021}