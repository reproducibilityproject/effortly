{"abstractText": "Koustuv Sinha1,2,4, ID , Jesse Dodge6, Sasha Luccioni2,3, ID , Jessica Zosa Forde5, ID , Robert Stojnic4,7, ID , and Joelle Pineau1,2,4, ID 1School of Computer Science, McGill University, Montreal, Canada \u2013 2Montreal Institute of Learning Algorithms (Mila), Canada \u2013 3Universit\u00e9 de Montr\u00e9al, Canada \u2013 4Facebook AI Research, Montreal, Canada \u2013 5Brown University, USA \u2013 6Allen Institute for AI, USA \u2013 7PapersWithCode, USA", "authors": [{"affiliations": [], "name": "Koustuv Sinha"}, {"affiliations": [], "name": "Jesse Dodge"}, {"affiliations": [], "name": "Sasha Luccioni"}, {"affiliations": [], "name": "Jessica Zosa Forde"}, {"affiliations": [], "name": "Robert Stojnic"}, {"affiliations": [], "name": "Joelle Pineau"}], "id": "SP:e1af07de9fccdcc960b4738773c19c6dcc82c0e3", "references": [], "sections": [{"text": "R E S C I E N C E C"}, {"heading": "Editorial", "text": ""}, {"heading": "ML Reproducibility Challenge 2020", "text": "Koustuv Sinha1,2,4, ID , Jesse Dodge6, Sasha Luccioni2,3, ID , Jessica Zosa Forde5, ID , Robert Stojnic4,7, ID , and Joelle Pineau1,2,4, ID 1School of Computer Science, McGill University, Montreal, Canada \u2013 2Montreal Institute of Learning Algorithms (Mila), Canada \u2013 3Universit\u00e9 de Montr\u00e9al, Canada \u2013 4Facebook AI Research, Montreal, Canada \u2013 5Brown University, USA \u2013 6Allen Institute for AI, USA \u2013 7PapersWithCode, USA\nEdited by Nicolas P. Rougier\nReceived 21 May 2021\nPublished 27 May 2021\nDOI 10.5281/zenodo.4833117\n1 Introduction\nReproducibility is a key ingredient for an impactful scientific discovery, which allows future practitioners to build on the shoulders of publishedwork. Reproducibility is also an important step to promote open and accessible research, allowing the scientific community to quickly integrate new findings and convert ideas to practice more seamlessly. In the spirit of promoting a culture of reproducible science in the Machine Learning community, we have hosted the fourth iteration of theMLReproducibility Challenge in 2020. Unlike previous years, in this challenge we increased the scope to include a broad range of top Machine Learning conferences, including NeurIPS, ICML, ICLR, CVPR, ECCV, ACL and EMNLP. The goal of this challenge was to investigate the reproducibility of accepted papers published in these top conferences, and in-turn contribute to the better understanding of their central claims. In this special issue of ReScience C Journal, we present the peer-reviewed accepted papers of the 2020 ML Reproducibility Challenge.\n2 Challenge\nThe goal of the challenge was to reproduce the central claims of papers published in top Machine Learning conferences of the year. Unlike the last iteration (NeurIPS 2019), in this year we focus on the central claim of the papers, and participants were open to choose to work on either all claims or partial claims depending on the complexity of the project. Participants were also free to reuse authors\u02bc code when available, while being encouraged to explore beyond simply running the code provided to verify reproducibility. The challenge involved a \u201cClaim paper\u201d step, where early on participants were encouraged to submit a claim on the paper they wished to work on using the OpenReview portal. The objective of the claiming process was to help participants narrow down their task by writing a short summary of items they wished to explore in reproducing the papers.\nAs in the last iteration, participants were free to claim multiple papers, and multiple teams could claim the same paper. In this year s\u0313 iteration, a total of 244 claims were submitted, which is a 41% increase over last year. However, the total number of final submissions was slightly lower at 82 papers (vs 84 in previous year). We had participation from 48 institutions (47 universities and 1 industry organization). Top participating\nCopyright \u00a9 2021 K. Sinha et al., released under a Creative Commons Attribution 4.0 International license. Correspondence should be addressed to Koustuv Sinha (koustuv.sinha@mail.mcgill.ca) The authors have declared that no competing interests exist.\nReScience C 7.2 (#1) \u2013 Sinha et al. 2021 1\ninstitutes consisted of University of Amsterdam, Netherlands, Indian Institute of Technology Gandhinagar, India, University of Waterloo, Canada, San Jose State University, USA. In these cases (and several others), a high participation rate occurred when a professor at the university used this challenge as a final course project.\nIt is also worth noting that this iteration of the challenge witnessed a significant jump in the quality of the reproducibility reports. After extensive peer review, in this special issue we present the top 23 accepted reports, selected from 82 submissions, thus driving up the acceptance rate from 11% last year to 28% this year.\n3 Reproducibility Summary and Template\nThe substantial increase in the quality of the submitted reports is attributed to two key decisions: reducing the scope of the challenge to cover central claims of the paper, and introducing the Reproducibility Summary Template to help authors to communicate their results and findings clearly and concisely, given that scientific communication is challenging. While there are many different types of papers, there are also common elements across ML, NLP, and vision. In a reproduction report, the main emphasis is on good reporting. This year we introduced a first-page summary and optional template. This template:\n\u2022 Has a place for reporting all items on the reproducibility checklist 1\n\u2022 Acts as guide for researchers\n\u2022 Shows that they understood themain claims, and the evidence that supports those claims.\n\u2022 Allows readers to quickly look up what they\u02bcre interested in \u2013 readers know what sections to check\n4 Going beyond\nAnother crucial factor contributing to the increase in quality of the reports is that this year s\u0313 edition sawmany authors going above and beyond the original paper, running additional experiments and analyses and converting code between frameworks (e.g. Tensorflow \u2192 PyTorch). This was particularly encouraging since it is an indicator of the evolution of the Reproducibility Challenge from a simple replication of the initial results to a broader scope in terms of depth of engagement with reproducing research. We are impressed with the work of this year s\u0313 authors and look forward to seeing further developments of the challenge!\n5 Platforms\nThis challenge is conducted with the support of PapersWithCode2 and OpenReview3. PapersWithCode is an open, collaborative platform to discover latest trending machine learning research papers with their codebases, which enables rapid re-usability and reproducibility of published works. PapersWithCode enabled us to reach a wide audience of students and researchers who participated in the competition. As last time, OpenReview provided crucial logistic support by providing an unique platform to claim and\n1https://www.cs.mcgill.ca/ jpineau/ReproducibilityChecklist.pdf 2https://paperswithcode.com/rc2020 3https://openreview.net/group?id=ML_Reproducibility_Challenge/2020\nReScience C 7.2 (#1) \u2013 Sinha et al. 2021 2\nsubmit reproducibility reports. After submission, all reports went through a thorough peer review process consisting of hundreds of reviewers from the Machine Learning community, and OpenReview provided an easy-to-use platform for managing reviews and administrative processes. Finally, we used a public Github repository to perform the final editorial process of converting accepted papers into ReScience format, and thereby publish 23 high quality reports in this special issue.\n6 Conclusion\nReproducibility of central claims of papers published in Machine Learning conferences has been a center of considerable attention over the past several years. Conferences such as NeurIPS, ICLR, AAAI, ICML, EMNLP have routinely included reproducibility workshops and challenges to cultivate the culture of reproducible science in the community. Several conferences have also introduced code submission policies and Reproducibility Checklists to further advance the cause and build momentum of reproducible science. We hope our continued endeavour of hosting regular reproducibility challenges and publishing high-quality peer-reviewed reproducibility reports will contributemore information about the existing published papers, and help strengthen their core contributions in the process, while also promoting open, accessible and soundmachine learning research.\n7 Acknowledgements\nWe thank the board and program committee of NeurIPS, ICML, ICLR, ACL, EMNLP, CVPR and ECCV for partnering with us in this massive initiative and supporting the challenge. We thank the OpenReview team (in particular Andrew McCallum, Parag Pachpute, Melisa Bok, Celeste Martinez Gomez, Pam Mandler and Mohit Uniyal) for their constant support in hosting and building the customized portal used in our challenge. We thank Robert Stojnic, Ross Taylor and Elvis Saravia from PapersWithCode for hosting and supporting the challenge along with its logistics. We thank the ReScience board (in particular Nicolas Rougier, Konrad Hinsen, Olivia Guest and Beno\u00eet Girard) for presenting the accepted reports in their esteemed journal. Finally, we thank all of our participants who dedicated time and effort to verify results that were not their own, to help strengthen our understanding of the concepts presented in the papers. A special thank you to Ana Lucic (University of Amsterdam), who instructed and supported several of the student teams whose reports are featured in this issue.\n8 Reviewers\nOur reviewers need a special section dedicated to thank them for their tireless efforts in screening and providing valuable feedback to the Area Chairs (Jesse Dodge, Sasha Luccioni, Jessica Zosa Forde andKoustuv Sinha) to select the best papers. Wewere fortunate enough to attract a large pool of reviewers, who spent their precious time to critically review the reports. We would like to specifically acknowledge our Emergency reviewers (marked in *) who responded to our call for help to review some additional reports at the last minute. We hope that our reviewer base will keep supporting us in this endeavour in future.\nAbhinav Agarwalla\nAkshita Gupta\nAli H\u00fcrriyeto\u011flu\nAndreas Ruttor\nAndrew Drozdov\nAnis Zahedifard\nArna Ghosh\nAzin Shamshirgaran\nBharathi Srinivasan *\nReScience C 7.2 (#1) \u2013 Sinha et al. 2021 3\nChao Qin\nCharbel Sakr\nChuan Li\nClement Laroche\nDavid Arbour\nDi He\nDmitriy Serdyuk\nDonghyeon Cho\nDylan Hadfield-Menell\nEmmanuel Bengio\nErnest K. Ryu\nFan Feng\nFatemeh Koochaki\nFernando Mart\u00ednez-Plumed\nGagana B\nGeorgios Leontidis\nHaitian Sun\nHanna Suominen\nHao He\nHeng Fang\nHuaibo Huang\nHuseyin Coskun\nIshani Vyas\nJiakai Zhang\nJiangwen Sun\nJie Fu\nJitong Chen\nJohn Frederick Wieting\nKanika Madan\nKatherine Lee\nKaushy Kularatnam\nKoustuv Sinha *\nLeo M Lahti\nLeonid Kholkine\nLevent Sagun\nLi cheng\nLijun Wu\nLinh Tran *\nLluis Castrejon\nMahzad Khoshlessan\nManeesh Kumar Singh\nMani A,Marija Stanojevic\nMaria Maistro\nMarija Stanojevic\nMarin Misur\nMassimiliano Mancini\nMatthew Kyle Schlegel\nMatthew Ryan Krause\nMaxime Wabartha\nMaxwell D Collins\nMd Imbesat Hassan Rizvi\nMelanie F. Pradier\nMichal Drozdzal\nMingrui Liu\nMonjoy Saha\nNeal Fultz\nNikolaos Vasiloglou\nOlga Isupova *\nOlivier Delalleau\nOpeyemi Osakuade\nOtasowie Owolafe *\nPablo Robles-Granda\nPascal Lamblin\nPatrick Philipp\nPaul Tylkin\nPeter Henderson\nPrasad Sudhakara Murthy\nPraveen Narayanan\nRadha Chitta\nRajanie Prabha\nSadid A. Hasan\nSamira Shaikh\nSandhya Prabhakaran\nSatya Prakash Dash\nSeohyun Kim\nSepehr Janghorbani\nShuai Kyle Zheng\nSiwei Wang\nSteffen Udluft\nSunnie S. Y. Kim\nSwetha Sirnam *\nTammo Rukat\nTaniya Seth\nTobias Uelwer\nUjjwal Verma\nVibha Belavadi\nV\u00edctor Campos\nWenbin Zhang\nWenhao Yu\nXavier Bouthillier\nXavier Sumba\nXiang Zhang\nXiao Zhang\nXin Guo\nYufei Han\nYuntian Deng\nZhangjie Cao\nReScience C 7.2 (#1) \u2013 Sinha et al. 2021 4"}], "title": "ML Reproducibility Challenge 2020", "year": 2021}