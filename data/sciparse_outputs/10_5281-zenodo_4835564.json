{"abstractText": "For a fixed parameter count and compute budget, the proposed algorithm (RigL) claims to directly train sparse networks thatmatch or exceed the performance of existing denseto-sparse training techniques (such as pruning). RigL does so while requiring constant Floating Point Operations (FLOPs) throughout training. The technique obtains state-ofthe-art performance on a variety of tasks, including image classification and characterlevel language-modelling.", "authors": [{"affiliations": [], "name": "Varun Sundar"}, {"affiliations": [], "name": "Rajat Vadiraj Dwaraknath"}, {"affiliations": [], "name": "Koustuv Sinha"}, {"affiliations": [], "name": "Jesse Dodge"}], "id": "SP:cfbecc6175e0f64108d32423a6497deea71d32f6", "references": [{"authors": ["S. Han", "X. Liu", "H. Mao", "J. Pu", "A. Pedram", "M.A. Horowitz", "W.J. Dally"], "title": "EIE: efficient inference engine on compressed deep neural network.", "venue": "ACM SIGARCH Computer Architecture News", "year": 2016}, {"authors": ["M. Ashby", "C. Baaij", "P. Baldwin", "M. Bastiaan", "O. Bunting", "A. Cairncross", "C. Chalmers", "L. Corrigan", "S. Davis", "N. van Doorn"], "title": "Exploiting Unstructured Sparsity on Next-Generation Datacenter Hardware.", "year": 2017}, {"authors": ["S. Srinivas", "A. Subramanya", "R. Venkatesh Babu"], "title": "Training Sparse Neural Networks.", "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops", "year": 2017}, {"authors": ["U. Evci", "T. Gale", "J. Menick", "P.S. Castro", "E. Elsen"], "title": "Rigging the Lottery: Making All Tickets Winners.", "venue": "Proceedings of Machine Learning and Systems (ICML)", "year": 2020}, {"authors": ["D.C. Mocanu", "E. Mocanu", "P. Stone", "P.H. Nguyen", "M. Gibescu", "A. Liotta"], "title": "Scalable Training of Artificial Neural Networkswith Adaptive Sparse Connectivity inspired byNetwork Science.", "venue": "In:Nature Communications", "year": 2018}, {"authors": ["L.T. Dettmers"], "title": "Zettlemoyer.SparseNetworks fromScratch: Faster Trainingwithout", "venue": "Losing Performance", "year": 2020}, {"authors": ["M. Zhu", "S. Gupta"], "title": "To Prune, or Not to Prune: Exploring the Efficacy of Pruning for Model Compression.", "venue": "Proceedings of the International Conference on Learning Representations (ICLR)", "year": 2018}, {"authors": ["M. Abadi", "P. Barham", "J. Chen", "Z. Chen", "A. Davis", "J. Dean", "M. Devin", "S. Ghemawat", "G. Irving", "M. Isard"], "title": "Tensorflow: A system for large-scale machine learning.", "venue": "{USENIX} Symposium on Operating Systems Design and Implementation ({OSDI}", "year": 2016}, {"authors": ["A. Paszke"], "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library.", "venue": "Advances in Neural Information Processing Systems", "year": 2019}, {"authors": ["L. Biewald"], "title": "Experiment Tracking with Weights and Biases", "venue": "Software available from wandb.com", "year": 2020}, {"authors": ["G.H. Alex Krizhevsky"], "title": "Learning multiple layers of features from tiny images", "venue": "Tech. rep", "year": 2009}, {"authors": ["S. Zagoruyko", "N. Komodakis"], "title": "Wide Residual Networks.", "venue": "Proceedings of the British Machine Vision Conference (BMVC)", "year": 2016}, {"authors": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "title": "Deep Residual Learning for Image Recognition.", "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "year": 2016}, {"authors": ["P. Goyal", "P. Doll\u00e1r", "R. Girshick", "P. Noordhuis", "L. Wesolowski", "A. Kyrola", "A. Tulloch", "Y. Jia", "K. He"], "title": "Accurate, large minibatch sgd: Training imagenet in 1 hour.", "year": 2017}, {"authors": ["S. Ioffe", "C. Szegedy"], "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift.", "venue": "In: International Conference on Machine Learning (ICML). July", "year": 2015}, {"authors": ["T. He", "Z. Zhang", "H. Zhang", "J. Xie", "M. Li"], "title": "Bag of Tricks for Image Classification with Convolutional Neural Networks.", "venue": "In:Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "year": 2019}, {"authors": ["T. Akiba", "S. Sano", "T. Yanase", "T. Ohta", "andM. Koyama"], "title": "Optuna: A Next-generation Hyperparameter Optimization Framework.", "venue": "Proceedings of the 25rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining", "year": 2019}, {"authors": ["J. Frankle", "M. Carbin"], "title": "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks.", "venue": "Proceedings of the International Conference on Learning Representations (ICLR)", "year": 2018}, {"authors": ["J. Bergstra", "R. Bardenet", "Y. Bengio", "B. K\u00e9gl"], "title": "Algorithms for Hyper-Parameter Optimization.", "venue": "Advances in Neural Information Processing Systems", "year": 2011}, {"authors": ["O. Russakovsky"], "title": "ImageNet Large Scale Visual Recognition Challenge.", "venue": "Journal of Computer Vision (IJCV)", "year": 2015}, {"authors": ["S. Gray", "A. Radford", "D.P. Kingma"], "title": "Gpu kernels for block-sparse weights.", "venue": "arXiv preprint arXiv:1711.09224", "year": 2017}], "sections": [{"text": "R E S C I E N C E C Replication / ML Reproducibility Challenge 2020\n[Re] Rigging the Lottery: Making All Tickets Winners\nVarun Sundar1, ID and Rajat Vadiraj Dwaraknath2, ID 1University of Wisconsin Madison, Wisconsin, USA \u2013 2Stanford University, California, USA\nEdited by Koustuv Sinha, Jesse Dodge\nReviewed by Anonymous Reviewers\nReceived 29 January 2021\nPublished 27 May 2021\nDOI 10.5281/zenodo.4835564"}, {"heading": "Reproducibility Summary", "text": ""}, {"heading": "Scope of Reproducibility", "text": "For a fixed parameter count and compute budget, the proposed algorithm (RigL) claims to directly train sparse networks thatmatch or exceed the performance of existing denseto-sparse training techniques (such as pruning). RigL does so while requiring constant Floating Point Operations (FLOPs) throughout training. The technique obtains state-ofthe-art performance on a variety of tasks, including image classification and characterlevel language-modelling."}, {"heading": "Methodology", "text": "We implement RigL from scratch in Pytorch using boolean masks to simulate unstructured sparsity. We rely on the description provided in the original paper, and referred to the authors\u02bc code for only specific implementation detail such as handling overflow in ERK initialization. We evaluate sparse training using RigL for WideResNet-22-2 on CIFAR-10 and ResNet-50 on CIFAR-100, requiring 2 hours and 6 hours respectively per training run on a GTX 1080 GPU."}, {"heading": "Results", "text": "We reproduce RigL s\u0313 performance on CIFAR-10 within 0.1% of the reported value. On both CIFAR-10/100, the central claim holds\u2014given a fixed training budget, RigL surpasses existing dynamic-sparse training methods over a range of target sparsities. By training longer, the performance can match or exceed iterative pruning, while consuming constant FLOPs throughout training. We also show that there is little benefit in tuning RigL s\u0313 hyper-parameters for every sparsity, initialization pair\u2014the reference choice of hyperparameters is often close to optimal performance.\nGoing beyond the original paper, we find that the optimal initialization scheme depends on the training constraint. While the Erdos-Renyi-Kernel distribution outperforms Randomdistribution for a fixed parameter count, for a fixed FLOP count, the latter performs better. Finally, redistributing layer-wise sparsity while training can bridge the performance gap between the two initialization schemes, but increases computational cost.\nCopyright \u00a9 2021 V. Sundar and R.V. Dwaraknath, released under a Creative Commons Attribution 4.0 International license. Correspondence should be addressed to Varun Sundar (vsundar4@wisc.edu) The authors have declared that no competing interests exist. Code is available at https://github.com/varun19299/rigl-reproducibility. \u2013 SWH swh:1:dir:0707870fafa16ef60dc64071e1aed482e373e75f. Open peer review is available at https://openreview.net/forum?id=riCIeP6LzEE.\nReScience C 7.2 (#21) \u2013 Sundar and Dwaraknath 2021 1"}, {"heading": "What was easy", "text": "The authors provide code for most of the experiments presented in the paper. The code was easy to run and allowed us to verify the correctness of our re-implementation. The paper also provided a thorough and clear description of the proposed algorithmwithout any obvious errors or confusing exposition."}, {"heading": "What was difficult", "text": "Tuning hyperparameters involved multiple random seeds and took longer than anticipated. Verifying the correctness of a few baselines was tricky and required ensuring that the optimizer s\u0313 gradient (or momentum) buffers were sparse (or dense) as specified by the algorithm. Compute limits restricted us from evaluating on larger datasets such as Imagenet.\nCommunication with original authors We had responsive communication with the original authors, which helped clarify a few implementation and evaluation details, particularly regarding the FLOP counting procedure.\nReScience C 7.2 (#21) \u2013 Sundar and Dwaraknath 2021 2\n1 Introduction\nSparse neural networks are a promising alternative to conventional dense networks\u2014 having comparatively greater parameter efficiency and lesser floating-point operations (FLOPs) (Han et al., Ashby et al., Srinivas, Subramanya, and Venkatesh Babu1,2,3). Unfortunately, present techniques to produce sparse networks of commensurate accuracy involve multiple cycles of training dense networks and subsequent pruning. Consequently, such techniques offer no advantage over training dense networks, either computationally or memory-wise.\nIn the paper Evci et al.4, the authors propose RigL, an algorithm for training sparse networks from scratch. The proposedmethod outperforms both prior art in training sparse networks, as well as existing dense-to-sparse training algorithms. By utilising dense gradients only during connectivity updates and avoiding any global sparsity redistribution, RigL canmaintain a fixed computational cost and parameter count throughout training.\nAs a part of the ML Reproducibility Challenge, we replicate RigL from scratch and investigate if dynamic-sparse training confers significant practical benefits compared to existing sparsifying techniques.\n2 Scope of reproducibility\nIn order to verify the central claims presented in the paper we focus on the following target questions:\n\u2022 Does RigL outperform existing sparse-to-sparse training techniques\u2014such as SET (Mocanu et al.5) and SNFS (Dettmers and Zettlemoyer6)\u2014and match the accuracy of dense-to-sparse training methods such as iterative pruning (Zhu and Gupta7)?\n\u2022 RigL requires two additional hyperparameters to tune. We investigate the sensitivity of final performance to these hyperparameters across a variety of target sparsities (Section 5.3).\n\u2022 How does the choice of sparsity initialization affect the final performance for a fixed parameter count and a fixed training budget (Section 6.1)?\n\u2022 Does redistributing layer-wise sparsity during connection updates (Dettmers and Zettlemoyer6) improve RigL s\u0313 performance? Can the final layer-wise distribution serve as a good sparsity initialization scheme (Section 6.2)?\n3 Methodology\nThe authors provide publicly accessible code1 written in Tensorflow (Abadi et al.8). To gain a better understanding of various implementation aspects, we opt to replicate RigL in Pytorch (Paszke et al.9). Our implementation extends the open-source code2 ofDettmers and Zettlemoyer6 which uses a boolean mask to simulate unstructured sparsity. Our source code is publicly accessible on Github3 with training plots available on WandB4 (Biewald10).\n1https://github.com/google-research/rigl 2https://github.com/TimDettmers/sparse_learning 3https://github.com/varun19299/rigl-reproducibility 4https://wandb.ai/ml-reprod-2020\nReScience C 7.2 (#21) \u2013 Sundar and Dwaraknath 2021 3\nMask Initialization \u2014 For a network with L layers and total parameters N , we associate each layer with a randombooleanmask of sparsity sl, l \u2208 [L]. The overall sparsity of the network is given by S = \u2211 l slNl N , where Nl is the parameter count of layer l. Sparsities sl are determined by the one of the following mask initialization strategies:\n\u2022 Uniform: Each layer has the same sparsity, i.e., sl = S \u2200l. Similar to the original authors, we keep the first layer dense in this initialization.\n\u2022 Erdos-Renyi (ER): Following Mocanu et al.5, we set sl \u221d ( 1\u2212 Cin+CoutCin\u00d7Cout ) , where\nCin, Cout are the in and out channels for a convolutional layer and input and output dimensions for a fully-connected layer.\n\u2022 Erdos-Renyi-Kernel (ERK):Modifies the sparsity rule of convolutional layers in ER initialization to include kernel height and width, i.e., sl \u221d ( 1\u2212 Cin+Cout+w+hCin\u00d7Cout\u00d7w\u00d7h ) , for\na convolutional layer with Cin \u00d7 Cout \u00d7 w \u00d7 h parameters.\nWedonot sparsify either bias or normalization layers, since these have anegligible effect on total parameter count.\nMask Updates \u2014 Every\u2206T training steps, certain connections are discarded, and an equal number are grown. Unlike SNFS (Dettmers and Zettlemoyer6), there is no redistribution of layer-wise sparsity, resulting in constant FLOPs throughout training.\nPruning Strategy \u2014 Similar to SET and SNFS, RigL prunes f fraction of smallest magnitude weights in each layer. As detailed below, the fraction f is decayed across mask update steps, by cosine annealing:\nf(t) = \u03b1\n2\n( 1 + cos ( t\u03c0\nTend\n)) (1)\nwhere, \u03b1 is the initial pruning rate and Tend is the training step after whichmask updates are ceased.\nGrowth Strategy \u2014 RigL s\u0313 novelty lies in how connections are grown: during every mask update, k connections having the largest absolute gradients among current inactive weights (previously zero + pruned) are activated. Here, k is chosen to be the number of connections dropped in the prune step. This requires access to dense gradients at each mask update step. Since gradients are not accumulated (unlike SNFS), RigL does not require access to dense gradients at every step. Following the paper, we initialize newly activated weights to zero.\n4 Experimental Settings\n4.1 Model descriptions\nFor experiments onCIFAR-10 (AlexKrizhevsky11), weuse aWideResidualNetwork (Zagoruyko and Komodakis12) with depth 22 and width multiplier 2, abbreviated as WRN-22-2. For experiments on CIFAR-100 (Alex Krizhevsky11), we use a modified variant of ResNet-50 (He et al.13), with the initial 7\u00d7 7 convolution replaced by two 3\u00d7 3 convolutions (architecture details provided in the supplementary material).\nReScience C 7.2 (#21) \u2013 Sundar and Dwaraknath 2021 4\nOn both datasets, we train models for 250 epochs each, optimized by SGD with momentum. Our training pipeline uses standard data augmentation, which includes random flips and crops. When training on CIFAR-100, we additionally include a learning rate warmup for 2 epochs and label smoothening of 0.1 (Goyal et al.14). We also initialize the last batch normalization layer (Ioffe and Szegedy15) in each BottleNeck block to 0, following He et al.16.\n4.3 Hyperparameters RigL includes two additional hyperparameters (\u03b1,\u2206T ) in comparison to regular dense network training. In Sections 5.1 and 5.2, we set \u03b1 = 0.3,\u2206T = 100, based on the original paper. Optimizer specific hyperparameters\u2014learning rate, learning rate schedule, and momentum\u2014are also set according to the original paper. In Section 5.3, we tune these hyperparameters with Optuna (Akiba et al.17). We also examine whether indivdually tuning the learning rate for each sparsity value offers any significant benefit.\n4.4 Baseline implementations\nWe compare RigL against various baselines in our experiments: SET (Mocanu et al.5), SNFS (Dettmers and Zettlemoyer6), and Magnitude-based Iterative-pruning (Zhu and Gupta7). We also compare against two weaker baselines, viz., Static Sparse training and Small-Dense networks. The latter has the same structure as the dense model but uses fewer channels in convolutional layers to lower parameter count. We implement iterative pruning with the pruning interval kept same as the masking interval for a fair comparison.\n4.5 Computational requirements We run our experiments on a SLURM cluster node\u2014equipped with 4 NVIDIA GTX1080 GPUs and a 32 core Intel CPU. Each experiment on CIFAR-10 and CIFAR-100 consumes about 1.6 GB and 7 GB of VRAM respectively and is run for 3 random seeds to capture performance variance. We require about 6 and 8 days of total compute time to produce\nReScience C 7.2 (#21) \u2013 Sundar and Dwaraknath 2021 5\nall results, including hyper-parameter sweeps and extended experiments, on CIFAR-10 and CIFAR-100 respectively.\n5 Results\nGiven a fixed training FLOP budget, RigL surpasses existing dynamic sparse training methods over a range of target sparsities, on both CIFAR-10 and 100 (Sections 5.1, 5.2). By training longer, RigLmatches ormarginally outperforms iterative pruning. However, unlike pruning, its FLOP consumption is constant throughout. This a prime reason for using sparse networks, and makes training larger networks feasible. Finally, as evaluated on CIFAR-10, the original authors\u02bc choice of hyper-parameters are close to optimal for multiple target sparsities and initialization schemes (Section 5.3).\n5.1 WideResNet-22 on CIFAR-10 Results on the CIFAR-10 dataset are provided in Table 2. Tabulatedmetrics are averaged across 3 random seeds and reported with their standard deviation. All sparse networks use random initialization, unless indicated otherwise.\nWhile SET improves over the performance of static sparse networks and small-dense networks, methods utilizing gradient information (SNFS, RigL) obtain better test accuracies. SNFS can outperform RigL, but requires a much larger training budget, since it (a) requires dense gradients at each training step, (b) redistributes layer-wise sparsity during mask updates. For all sparse methods, excluding SNFS, using ERK initialization improves performance, but with increased FLOP consumption. We calculate theoretical FLOP requirements in a manner similar to Evci et al.4 (exact procedure is described\nReScience C 7.2 (#21) \u2013 Sundar and Dwaraknath 2021 6\nin the appendix).\nFigure 1 contains test accuracies of selectmethods across two additional sparsity values: (0.5, 0.95). At lower sparsities (higher densities), RigL matches the performance of the dense baseline. Performance further improves by training for longer durations. Particularly, training RigL (ERK) twice as long at 90% sparsity exceeds the performance of iterative pruning while requiring similar theoretical FLOPs. This validates the original authors\u02bc claim that RigL (a sparse-to-sparse training method) outperforms pruning (a dense-to-sparse training method).\n5.2 ResNet-50 on CIFAR100\nWesee similar trendswhen training sparse variants of ResNet-50 on theCIFAR-100 dataset (Table 3,metrics reported as in Section 5.1). We also include a comparison against sparse networks trained with the Lottery Ticket Hypothesis (Frankle and Carbin18) in Table 3\u2014we obtain tickets with a commensurate performance for sparsities lower than 80%.\nReScience C 7.2 (#21) \u2013 Sundar and Dwaraknath 2021 7\nFinally, the choice of initialization scheme affects the performance and FLOP consumption by a greater extent than the method used itself, with the exception of SNFS (groups 1 and 2 in Table 3).\n5.3 Hyperparameter Tuning\n(\u03b1,\u2206T ) vs Sparsities \u2014 To understand the impact of the two additional hyperparameters included in RigL, we use a Tree of Parzen Estimator (TPE sampler, Bergstra et al.19) via Optuna to tune (\u03b1,\u2206T ). We do this for sparsities (1 \u2212 s) \u2208 {0.1, 0.2, 0.5}, and a fixed learning rate of 0.1. Additionally, we set the sampling domain for \u03b1 and\u2206T as [0.1, 0.6] and {50, 100, 150, ..., 1000} respectively. We use 15 trials for each sparsity value, with our objective function as the validation accuracy averaged across 3 random seeds.\nReScience C 7.2 (#21) \u2013 Sundar and Dwaraknath 2021 8\nTable 4 shows the test accuracies of tuned hyperparameters. While the reference hyperparameters (original authors, \u03b1 = 0.3,\u2206T = 100) differ from the obtained optimal hyperparameters, the difference in performance is marginal, especially for ERK initialization. This in agreement with the original paper, which finds \u03b1 \u2208 {0.3, 0.5},\u2206T = 100 to be suitable choices. We include contour plots detailing the hyperparameter trial space in the supplementary material.\nLearning Rate vs Sparsities \u2014We further examine if the final performance improves by tuning the learning rate (\u03b7) individually for each sparsity-initialization pair. We employ a grid search over \u03b7 \u2208 {0.1, 0.05, 0.01, 0.005} and (\u03b1,\u2206T ) \u2208 {(0.3, 100), (0.4, 200), (0.4, 500), (0.5, 750)}. As seen in Figure 3, \u03b7 = 0.1 and \u03b7 = 0.05 are close to optimal values for a wide range of sparsities and initializations. Since these learning rates also correspond to good choices for the Dense baseline, one can employ similar values when training with RigL.\n6 Results beyond Original Paper\n6.1 Sparsity Distribution vs FLOP Consumption\nWhile ERK initialization outperforms Random initialization consistently for a given target parameter count, it requires a higher FLOPbudget. Figure 4 compares the two initialization schemes across fixed training FLOPs. Theoretical FLOP requirement for Random initialization scales linearly with density (1 \u2212 s), and is significantly lesser than ERK s\u0313 FLOP requirements. Consequently, Random initialization outperforms ERK initialization for a given training budget.\n6.2 Effect of Redistribution One of themain differences ofRigL over SNFS is the lack of layer-wise redistribution during training. We examine if using a redistribution criterion can be beneficial and bridge the performance gap between Random and ERK initialization. Following Dettmers and Zettlemoyer6, during every mask update, we reallocate layer-wise density proportional to its average sparse gradient or momentum (RigL-SG, RigL-SM).\nTable 5 shows that redistribution significantly improves RigL (Random), but not RigL (ERK). We additionally plot the FLOP requirement against training steps and the final sparsity distribution in Figure 5. The layer-wise sparsity distribution largely becomes constant within a few epochs. The final distribution is similar, but more \u201cextreme\u201d than ERK\u2014wherever ERK exceeds/falls short of Random, redistribution does so by a greater\nReScience C 7.2 (#21) \u2013 Sundar and Dwaraknath 2021 9\nextent.\nBy allocating higher densities to 1 \u00d7 1 convolutions (convShortcut in Figure 5), redistribution significantly increases the FLOP requirement\u2014and hence, is not a preferred alternative to ERK. Surprisingly, initializing RigLwith the final sparsity distribution in a manner similar to the Lottery Ticket Hypothesis results in subpar performance (group 3, Table 5).\n7 Discussion\nEvaluated on image classification, the central claims of Evci et al.4 hold true\u2014RigL outperforms existing sparse-to-sparse training methods and can also surpass other denseto-sparse training methods with extended training. RigL is fairly robust to its choice of hyperparameters, as they can be set independent of sparsity or initialization. We find that the choice of initialization has a greater impact on the final performance and compute requirement than the method itself. Considering the performance boost obtained by redistribution, proposing distributions that attain maximum performance given a FLOP budget could be an interesting future direction.\nReScience C 7.2 (#21) \u2013 Sundar and Dwaraknath 2021 10\nFor computational reasons, our scope is restricted to small datasets such asCIFAR-10/100. RigL s\u0313 applicability outside image classification\u2014in Computer Vision and beyond (machine translation etc.) is not covered here.\nWhat was easy \u2014 The authors\u02bc code covered most of the experiments in their paper and helped us validate the correctness of our replicated codebase. Additionally, the original paper is quite complete, straightforward to follow, and lacked any major errors.\nWhat was difficult \u2014 Implementation details such as whether momentum buffers were accumulated sparsely or densely had a substantial impact on the performance of SNFS. Finding the right \u03f5 for ERK initialization required handling of edge cases\u2014when a layer s\u0313 capacity is exceeded. Hyperparameter tuning (\u03b1,\u2206T ) involved multiple seeds and was compute-intensive.\nCommunication with original authors \u2014We acknowledge and thank the original authors for their responsive communication, which helped clarify a great deal of implementation and evaluation specifics. Particularly, FLOP counting for various methods while taking into account the changing sparsity distribution. We also discussed experiments extending the original paper\u2014as towhether the authors had carried out a similar study before."}], "title": "[Re] Rigging the Lottery: Making All Tickets Winners", "year": 2021}