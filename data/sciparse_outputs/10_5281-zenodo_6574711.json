{"abstractText": "In this paper we attempt to reproduce the results found in \u201dDECAF: Generating Fair Syn\u2010 thetic Data Using Causally\u2010Aware Generative Networks\u201d by Breugel et al [1]. The goal of the original paper is to create a model that takes as input a biased dataset and out\u2010 puts a debiased synthetic dataset that can be used to train downstreammodels to make unbiased predictions both on synthetic and real data.", "authors": [{"affiliations": [], "name": "Velizar Shulev"}, {"affiliations": [], "name": "Paul Verhagen"}, {"affiliations": [], "name": "Shuai Wang"}, {"affiliations": [], "name": "Jennifer Zhuge"}, {"affiliations": [], "name": "Koustuv Sinha"}, {"affiliations": [], "name": "Sharath Chandra Raparthy"}], "id": "SP:50fc82f519801be26e1d4b3df140edadc58c85b8", "references": [{"authors": ["B. van Breugel", "T. Kyono", "J. Berrevoets", "M. van der Schaar"], "title": "DECAF: Generating Fair Synthetic Data Using Causally-Aware Generative Networks.", "venue": "Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems", "year": 2021}, {"authors": ["N. Mehrabi", "F. Morstatter", "N. Saxena", "K. Lerman", "A. Galstyan"], "title": "A Survey on Bias and Fairness in Machine Learning.", "venue": "ACM Comput. Surv", "year": 2021}, {"authors": ["W.P. O\u2019Hare"], "title": "Who Is Missing? Undercounts and Omissions in the U.S. Census.", "venue": "SpringerBriefs in Population Studies Differential Undercounts in the U.S. Census", "year": 2019}, {"authors": ["G.M. Johnson"], "title": "Algorithmic bias: on the implicit biases of social technology.", "venue": "Synthese", "year": 2020}, {"authors": ["D. Xu", "S. Yuan", "L. Zhang", "X. Wu"], "title": "Fairgan: Fairness-aware Generative Adversarial Networks.", "venue": "IEEE International Conference on Big Data (Big Data)", "year": 2018}, {"authors": ["U. A\u0131\uffffvodji", "F. Bidet", "S. Gambs", "R.C. Ngueveu", "A. Tapp"], "title": "Local Data Debiasing for Fairness Based on Generative Adversarial Training.", "venue": "In: Algorithms", "year": 2021}, {"authors": ["P. Sattigeri", "S.C. Hoffman", "V. Chenthamarakshan", "K.R. Varshney"], "title": "Fairness gan: Generating datasets with fairness properties using a generative Adversarial Network.", "venue": "IBM Journal of Research and Development", "year": 2019}, {"authors": ["K.T. Rodolfa", "H. Lamba", "R. Ghani"], "title": "Empirical observation of negligible fairness\u2013accuracy trade-offs in Machine Learning for Public Policy.", "venue": "Nature Machine Intelligence", "year": 2021}, {"authors": ["H. Edwards", "A. Storkey"], "title": "Censoring Representations with an Adversary.", "venue": "(Nov", "year": 2015}, {"authors": ["T. Kamishima", "S. Akaho", "H. Asoh", "J. Sakuma"], "title": "Fairness-aware classifier with prejudice remover regularizer.", "venue": "In:Machine Learning and Knowledge Discovery in Databases", "year": 2012}, {"authors": ["D. Dua", "C. Graff"], "title": "UCI Machine Learning Repository. 2017", "venue": "URL: http://archive.ics.uci.edu/ml. ReScience C", "year": 2022}], "sections": [{"text": "R E S C I E N C E C"}, {"heading": "Replication / ML Reproducibility Challenge 2021", "text": "[Re] Replication Study of DECAF: Generating Fair"}, {"heading": "Synthetic Data Using Causally-Aware Generative", "text": ""}, {"heading": "Networks", "text": "Velizar Shulev1, ID , Paul Verhagen1, ID , Shuai Wang1, ID , and Jennifer Zhuge1, ID 1Universiteit van Amsterdam, Amsterdam, the Netherlands\nEdited by Koustuv Sinha,\nSharath Chandra Raparthy\nReceived 04 February 2022\nPublished 23 May 2022\nDOI 10.5281/zenodo.6574711\n1 Reproducibility Summary\n1.1 Scope of reproducibility In this paper we attempt to reproduce the results found in \u201dDECAF: Generating Fair Syn\u2010 thetic Data Using Causally\u2010Aware Generative Networks\u201d by Breugel et al [1]. The goal of the original paper is to create a model that takes as input a biased dataset and out\u2010 puts a debiased synthetic dataset that can be used to train downstreammodels to make unbiased predictions both on synthetic and real data.\n1.2 Methodology We built upon the (incomplete) code provided by the authors to repeat the first experi\u2010 ment of [1] which involves removing existing bias from real data, and the second exper\u2010 iment where synthetically injected bias is added to real data and then removed.\n1.3 Results We reproduced most of the data utility results reported in the first experiment for the Adult dataset. However, while the fairness metrics generally match the original paper they are numerically not comparable in absolute or relative terms. For the second exper\u2010 iment, we were unsuccessful in reproducing results found by the authors. We note how\u2010 ever that we made considerable changes to the experimental setup, which may make it difficult to perform a direct comparison of the results.\n1.4 What was easy The smaller size and tabular format of both datasets allowed for quick training and model modifications.\n1.5 What was difficult There are several possible interpretations of the paper on both a methodological and conceptual level. Reproducing the experiments required rewriting or adding large sec\u2010\nCopyright \u00a9 2022 V. Shulev et al., released under a Creative Commons Attribution 4.0 International license. Correspondence should be addressed to Velizar Shulev (velizar.shulev@student.uva.nl) The authors have declared that no competing interests exist. Code is available at https://github.com/ShuaiWang97/UvA_FACT2022 \u2013 DOI 10.5281/zenodo.6515893. \u2013 SWH swh:1:dir:78ac8ec89fd95397fd635e8d9e885e1d5ac6c039. Open peer review is available at https://openreview.net/forum?id=SVx46hzmhRK.\nReScience C 8.2 (#43) \u2013 Shulev et al. 2022 1\ntions of code. Given these multiple interpretations it is difficult to be confident in the reproduction. In addition, several results found by the authors appear to be counterin\u2010 tuitive, such as algorithms debiasing without being designed to do so and sometimes outperforming debiasing algorithms on the same dataset.\n1.6 Communication with original authors We sent two emails to the authors describing our issues. We received a reply with a few extra files, but no direct answer to content questions.\nReScience C 8.2 (#43) \u2013 Shulev et al. 2022 2\n2 Introduction\nIt is broadly acknowledged that real world data contains bias. Despite efforts to make data collection more equitable and representative, a myriad of challenges remain. The importance of addressing bias and fairness more broadly is gaining awareness [2], as biased data can lead to the under\u2010representation of particular demographics, such as the case of political representation in theUnited States Census[3]. As technology progressed to the emergence of machine learning (ML) models, the same challenges persist as ML models inherit the biases of the data and humans who created them. Models trained on biased data can pass bias downstream to various other applications, a phenomenon referred to as algorithmic bias[4]. Suchmodels have potential to not only perpetuate but exacerbate social inequality. Hence, there is a clear and present need for methods that can utilize biased data to produce unbiased results.\n3 Background\nThe notion of using Generative Adversarial Networks (GAN) to increase fairness within artificial intelligence is broadly supported by the literature. Various models exists such as FairGAN[5], GANSAN[6], and Fairness GAN [7] to name but a few. Notably, fairness ef\u2010 forts have typically recognized a fairness\u2010accuracy trade\u2010off assumption, where a fairer algorithm comes at the cost of accuracy. However, recent work has challenged these assumptions, finding that the accuracy cost of fairness is negligible in some circum\u2010 stances[8]. Nonetheless, given the increased awareness of the nefarious effects of data bias, many research efforts have been directed towards the debiasing of data and other attempts to create fairer artificial intelligence[2].\n3.1 DECAF premise One such effort, and the subject of the present study, is DEbiasing CAusal Fairness (DE\u2010 CAF) [1]. DECAF takes a distinct approach to debiasing data, explicitly approaching fairness from a causal standpoint with a goal of downstream model fairness. There are three broad approaches to fairness that may be identified, (1) the preprocessing ap\u2010 proach, where the characteristics of the input data are changed to suppress undesirable biases [1], (2) the algorithmicmodification approach, where the learning algorithm itself is adapted to reduce bias [9], and (3) the postprocessing approach, where the output of a model is manipulated to obtain the desired level of fairness[10]. The DECAF approach falls in the first category of preprocessing because it attempts to remove bias from the input data and subsequently from all downstream models. The DECAF model is a generative adversarial network (GAN) that utilizes the causal structure of directed acyclical graphs (DAGs) to remove bias from real data. The three critical assumptions of the DECAF method are (1) the data generating process is rep\u2010 resented by a DAG, (2) the DAG is causally sufficient, and (3) the DAG is known for a given dataset. DAGs are central to the method, as it is through edge manipulation that debiasing is performed. The model may be separated into two stages. During the training phase, the model learns the causal conditionals of the dataset from its DAG. In the inference phase, the data is debiased through DAG modification. Each fairness level defines a unique set of edge removals from the original DAG, resulting in a new, intervened DAG. These inter\u2010 vened DAGs are given to the model to generate synthetic, fair datasets from the original data. The synthetic datasets have similar distributions to the original data, but avoid bias. Because the method debiases at inference time, retraining the model is not re\u2010 quired when using different fairness measures, thus providing inference\u2010time fairness.\nReScience C 8.2 (#43) \u2013 Shulev et al. 2022 3\nOnce DECAF generates a synthetic and unbiased dataset, a simple multilayer percep\u2010 tron (MLP) is trained on this synthetic data to create an unbiased classifier that can be used both on the original data and in other settings. Because the data used for training theMLP has already been debiased, the authors claim that theMLP or any chosen down\u2010 streammodel is guaranteed to be fair since it does not incorporate any of the bias from the original training data; this is a hallmark of the preprocessing approach to fairness.\n3.2 Fairness standards Three definitions of algorithmic fairness are used in the paper, each corresponding to a unique modified DAG. The most lenient standard is the commonly used Fairness Through Unawareness (FTU) definition, which entails that the protected variable, A, is not explicitly used by the model to predict the label, Y\u0302 . While widely used because of its conceptual simplicity and the fact it avoids direct discrimination, FTU nonetheless fails to eliminate indirect discrimination. A more stringent definition of fairness is Demographic Parity (DP), which declares that classification probability must be independent of classes, i.e. if the protected attribute is gender, all gender classes have the same success rate. The DP definition is considered to be very strict because it potentially under\u2010utilizes feature differences between groups in the process of blocking indirect discrimination. Conditional Fairness (CF) lies in the middle ground between the first two definitions by presuming that the selection rate between groups segregated by the protected attribute must be the same when conditioned on some explanatory variable(s) determined by prior knowledge. Each of these standards corresponds to a variation of DECAF, respec\u2010 tively DECAF\u2010ND (no debiasing), DECAF\u2010FTU, DECAF\u2010CF, and DECAF\u2010DP. The fairness of each model is tested against FTU and DP metrics.\n4 Scope of reproducibility and claims\nThe authors claim that DECAF allows for the generation of unbiased synthetic data from biased real data and that their method does so with minimal loss in data utility com\u2010 pared to other approaches. Furthermore, they identify five characteristics of fair syn\u2010 thetic data that their method achieves: (1) allows post\u2010hoc distribution changes, (2) pro\u2010 vides fairness, (3) supports causal notions of fairness, (4) allows inference\u2010time fairness, and (5) requires minimal assumptions. Additionally, they claim that DECAF is the only method to achieve all of the five listed characteristics. The authors identify three main contributions of their work:\n(i) DECAF, a causal GAN\u2010based model that can use a biased datasetX to generate an equivalent synthetic unbiased dataset X with minimal loss of data utility\n(ii) A flexible causal approach for modifying DECAF to generate fair data\n(iii) Guarantee that downstream models trained on the generated synthetic data will make unbiased predictions on both synthetic and real\u2010life (biased) data\nWe aim to evaluate claims (i) and (iii) by replicating the two experiments of [1]. We will focus on the narrow interpretation of reproducibility, namely whether the experiment can be reproduced by independent researchers with the same setup rather than testing against the more general standard of replicatability on different datasets. Despite the availability of code, there were considerable problems with running the models even with instructions given, meaning that we limited our scope to direct reproducibility. As the authors have done, we will evaluate the data utility of the DECAFmethod with preci\u2010 sion, recall, and area under the receiver operation characteristic (AUROC); fairness will be evaluated with Fairness Through Unawareness (FTU) and Demographic Parity (DP) measures.\nReScience C 8.2 (#43) \u2013 Shulev et al. 2022 4\n5 Methodology\nWhile code from the creators of the DECAF method is available 1, the documentation leaves room for interpretation and the instructions given for running the code do not reproduce the results as presented. In addition, there are several possible discrepancies between the method described in the paper and the code provided. Thus, we made the assumption that the paper leads and adjusted the code accordingly to match.\n5.1 Methodological Code Changes Though the DECAF class code was working, several components of the experimental setup codewas eithermissing or not fully explained. Thus, we had to extrapolate heavily to produce results. The major code changes required are listed below:\n(i) Preprocessing: the original paper mentions standardizing continuous variables, however, following the procedure given in the paper generates uninterpretable re\u2010 sults. As a solutionwe standardize all variables, including categorical ones though we question the conceptual validity of this decision. After standardizing with Stan\u2010 dardScaler, we still do not obtain results as high as the reportedmetrics, so we nor\u2010 malize with MinMaxScaler which produces matching results in data utility. The DECAF class employs a final sigmoid layer that converts all generated data to a range between 0 and 1. We suspect this is the reason why the run_example.py script will only predict labels of one class and why using a Scaler allows us to ob\u2010 tain meaningful predictions.\n(ii) DAGs: There appears to be a mismatch with the dags provided, as neither con\u2010 tain all of the variables in the datasets. In addition the code provided utilized a toy graph. The authors state that they used Tetrad to generate the DAG for the dataset, so we attempted to generate a full causal graph for the Adult dataset, but our generated graphs do notmatch Figure 6 and 7 of [1]. Hence, wemanually input the graphs from the paper.\n(iii) Label Generation: The paper instructs that the labels for synthetic data should be generated by the model as they are part of the causal dependencies graph. The original code does not generate the labels for the synthetic dataset, but instead generates only the x values and then predicts the labels from those generated x values using the baseline model. The code seems to omit the target variable from the GAN input, but we feel this would leave out valuable causal information con\u2010 tained in the edges from the explanatory variables to the target variable. Thus, we decided to include the target variable in the DAG, which improves our results. In the end, we were forced to generate labels for experiment 1, while predicting labels for experiment 2 in order to obtain interpretable results.\n(iv) Downstream Classifer: The papermentions anMLP from sklearn, but the exam\u2010 ple code uses an XGBClassifier as the downstream classifier which caused instal\u2010 lation issues. We followed the paper by using an MLP.\n5.2 Dataset\nFor the first experiment, we use the Adult dataset 2 [11] collected from the 1994 United States Census. The dataset contains about 45,000 data points, and 2,000 data points are set aside for the test set as specified by [1]. The protected attribute is sex, and the target\n1The DECAF code is available at: https://github.com/vanderschaarlab/DECAF 2The Adult dataset is available at http://archive.ics.uci.edu/ml/index.php\nReScience C 8.2 (#43) \u2013 Shulev et al. 2022 5\nvariable is income with roughly 75% in the \u2019<=50k\u2019 class and the remaining 25% belong\u2010 ing to the \u2019>50k\u2019 class. This makes sense considering the average earnings of Americans at the time, but does make our data skewed towards one class. We manually input the DAG from Figure 6 of [1] and use the preprocessing steps described in the previous sec\u2010 tion. For the second experiment, we use the Credit Approval dataset [11] of credit card appli\u2010 cations. This dataset is considerably smaller than the first dataset with only 678 data points. The original paper does not specify how large the test set is, so we choose a typical 80%/20% split for training and testing. The protected attribute is ethnicity and the target variable is application approval. About 55% of the applicationswere approved while the rest were rejected, so this dataset is considerablymore balanced than the other. Again, we have to manually input the graph from Figure 7 of the original paper. Since the protected attribute here, ethnicity, is not binary, we first convert the variable to be binary with 0 corresponding to \u2019not discriminated against\u2019 and 1 to \u2019discriminated against\u2019. Then we use the same preprocessing steps as in the first experiment.\n5.3 Hyperparameters A hyperparameter search is not necessary for our experiments. We use the DECAF class as given with the parameters set by the authors\u2019 code. The onlymodification wemake is changing thedag_seed parameter from the provided toy graph to the respective graphs for each dataset presented on Page 28 of [1]. The DECAF generator is instantiated with d, the number of features, sub\u2010networks with shared hidden layers. The generator and discriminator both use 2 hidden layers with 2d neurons. The generator is updated once for every 10 discriminator updates. Adam is used as the optimizer with a learning rate of 0.001. The other GANs used for comparison are also given default parameters and settings from their respective packages because no settings are specified by the authors. An MLP with default parameters from sklearn is used. The default settings are 100 neurons with ReLU activation functions and Adam with a learning rate of 0.001. A Soft\u2010 max activation and binary cross entropy loss is used for the output layer.\n5.4 Experimental setup and code In this study, we aim to replicate the experiments of the original paper, Debiasing Cen\u2010 sus Data (experiment 1) and Fair Credit Approval (experiment 2), to evaluate the perfor\u2010 mance of DECAF when generating unbiased synthetic data from real, biased data from the Adult dataset. We train each model listed in Table 2 of the original paper, four DECAF GANs and three other GANs for comparison for 50 epochs. A synthetic dataset is generated from each model that is then used to train an MLP to classify a test set of 2,000 unmodified data points from the original dataset. We compare these predictions with the ground truth labels from the original data to evaluate performance and fairness. This process is re\u2010 peated ten times to obtain average metrics over multiple runs as specified by the au\u2010 thors. To mimic the DECAF paper, precision, recall, and AUROC are used to measure the per\u2010 formance of the models, while FTU and DP are used to measure the fairness of the mod\u2010 els. Precision, recall, and AUROC are given by sklearn.metrics, and higher scores indicate better performance. Lower FTU and DP scores indicate less bias. To calculate FTU, we set all the labels of the protected attribute to one class and predict the labels; repeat with the remaining class (for binary variables), and compare the difference of the means of the two prediction sets, such that |PA=0(Y\u0302 |X)\u2212PA=1(Y\u0302 |X)|. Then for DP, we segregate the dataset into datapoints with one class label and datapoints with the other label (for binary variables), and again predict the labels of each set and compare the dif\u2010 ference of themeans of the two prediction sets, such that |P (Y\u0302 |A = 0)\u2212P (Y\u0302 |A = 1)|. To\nReScience C 8.2 (#43) \u2013 Shulev et al. 2022 6\ncompare our replication against the original experiments of the authors, we compare both the absolute difference and the relative difference (as a ratio) with our findings. Our code and more details can be found on our Github repository3.\n5.5 Computational requirements Because the datasets used are small and tabular, the computational requirements are minimal. No GPU is necessary; all models were run on an Intel Core i7\u20108750h CPU. It takes six minutes to train DECAF models on the Adult dataset [11] for 50 epochs, and five seconds to generate synthetic data. The total runtime is approximately four hours for experiment 1 and approximately two hours for experiment 2.\n6 Results\nWe are able to reproduce some results in experiment 1, but we can not get similar results on the second experiment. Table 1 shows our result with synthetic data that is generated\n3Our Github repository: https://anonymous.4open.science/r/DECAF-CF0A/\nReScience C 8.2 (#43) \u2013 Shulev et al. 2022 7\nusing each benchmark method, after which a separate MLP is trained on each dataset for computing the metrics, and Table.2 is the result from the original paper. Section 5.4 details how we obtained the relevant metrics. We can see DECAF does have the effect of debiasing and there is improvement comparable with FairGAN. Like the original pa\u2010 per, DECAF\u2010ND performs the best among all methods in terms of data quality. Methods DECAF\u2010FTU, DECAF\u2010CF, and DECAF\u2010DP have relatively lower scores on data quality but perform better on fairness. Figure 1 shows DECAF results for experiment 2 in which we remove synthetically in\u2010 jected bias. These results do not match the Figure 3 of original paper. This mismatch is not surprising because the second experiment is based on the first experiment where we suspect our setup already significantly diverges from that of the authors.\n7 Discussion\nOverall, we have been able to produce the results found by the authors. That being said, there aremultiple interpretations of the results and overall saliency is relatively low. For the purpose of this paper, we will focus primarily on the fairness metrics since the data utility metrics are closer to the findings of the authors and fairness is the primary goal of the method. Though the order of the fairness of various models of our results match with the original results from the paper, our numerical figures do notmatch the authors\u2019 results with a satisfactory level of precision. Several observations are further pursued as plausible explanations for this phenomenon.\n7.1 Interpretation of the results As shown in Tables 1 and 2, we obtained interpretable results for all models tested in experiment 1. For the most part, we find effects similar to the authors, but they deviate significantly in numerical terms. More specifically, we do find that as the model varia\u2010 tions move from least strict to most strict definition of fairness, the fairness increases and data utility decreases. However, there are notable deviations from the authors re\u2010 sults, specifically concerning the fairness metrics of the GAN. In addition, we find that DECAF\u2010ND increases the level of bias compared to the original dataset which matches the authors. However, we find a higher DP of 0.353 and a FTU of 0.114 compared to the authors DP of 0.198 and FTU of 0.152. These results run counter to our expectations. The results found in the Credit dataset also show the directional correctness of DECAF in reducing bias. However, direct comparison is difficult because our results differ sig\u2010 nificantly from the authors\u2019 findings. In particular, we find the FTU and DP scores is maximized at, 0 and minimized at 1. In addition, the authors find relatively stable data utility metrics, whereas we find a significant decrease between bias 0.25 and 0.75. The results for bias 1 and 0 do reflect the average value found by the authors, with the excep\u2010 tion of recall which is significantly lower. Furthermore, the authors did not directly interpret their chosen metrics. The original paper designated FTU and DP measures for fairness and reported figures, but does not explain the actual meaning of the numbers and magnitude of changes seen. For ex\u2010 ample, most of the reported fairness metrics are very small, but we do not have any guidance on the significance of a .001 decrease in the FTU metric. Thus, we feel the paper lacked explainability. Additionally, the fairness definitions themselves, the in\u2010 structions for calculating the fairness measures, and the given FTU and DP code are somewhat contradictory. Calculating FTU and DP based on our interpretation of the au\u2010 thors\u2019 method does not reproduce their results. Using the FTU andDP calculations from an extra code file we received still does not produce matching results. One possibility is that the authors\u2019 final fairness metrics calculation code are not contained in the files we have access to and does not match any of the implementations we attempted.\nReScience C 8.2 (#43) \u2013 Shulev et al. 2022 8\n7.2 What was easy One aspect that eased our investigation into the reproduceability of [1] was the tabular format and small size of the datasets we used. Training and modifying the model was not computationally expensive or time consuming, thus we could test many different strategies to find the closest solution.\n7.3 What was difficult Wewere originally under the impression that the DECAF code repository was fully func\u2010 tional as a basis for extension. Upon further examination, we found that it was not working and did not reproduce the published results. Thus, we had to pivot from ex\u2010 tending their code to replicating the results with our own code which was challenging in itself. While attempting to reproduce the experiments, we found that the instructions given were incomplete and contradictory to the code provided. There are multiple obstacles to replicating the experiments as described, which can broadly be separated into conceptual and methodological issues. On the former, there are many important research decisions that are not fully articulated, as well as results that appear counterintuitive. For example, the authors found that their application of GAN, a method that does not do explicit debiasing, had significantly improved fairness metrics compared to the original dataset. One would expect that all the methods that do not debias, namely original data, GAN, WGAN\u2010GP, and DECAF\u2010ND would perform in the same order of magnitude in terms of fairness, but this is not the case in the au\u2010 thor\u2019s initial findings. Moreover, while the DECAF models do reduce bias in line with the level of fairness required, DECAF\u2010ND actually makes the dataset more biased com\u2010 pared to the original dataset. Our reproduction of GAN does match the expected results, with original data, GAN, andWGAN all returning roughly the same fairness metrics. As discussed, we successfully reproduced the overall impact of DECAF, namely higher fair\u2010 ness and lower data utility for more stringent definitions of fairness. However, DECAF\u2010 ND exhibits considerably higher bias than the original dataset and no clear intuition is given on why this may be the case. In addition to the conceptual challenges, there are multiple methodological issues. Fol\u2010 lowing the instructions provided by the authors results in numerous compatibility warn\u2010 ings and failed tests. As described in section 5.1, several substantial changes are needed to generate any interpretable results. Further compounding these issues, there are in\u2010 consistencies in the applied method, as the code utilized in the example explicitly devi\u2010 ates from the approach described in the experimental setup. Wewere forced to generate labels for experiment 1, while predicting labels for experiment 2. Attempts to use gener\u2010 ated labelsmade experiment 2 uninterpretable, as all key performance indicatorswould become zero otherwise. This methodological inconsistency between experiments fur\u2010 ther problematizes the reproducibility of DECAF.\n7.4 Overall reproducibilty Due to the number of possible conceptual and methodological interpretations with the code, modifications were needed as described in section 5.1. While we were successful in producing results that could be interpreted, the numerical variations and method\u2010 ological deviations are so substantial that further research would be needed to assess the overall accuracy of the authors claims. We found evidence that supports the nar\u2010 row interpretation of the claims made by the author, namely that DECAF reduces bias in downstream models, and allows for the generation of debiased synthetic data. How\u2010 ever, the authors claim that the approach allows for minimal data utility loss. Without a further explanation on what is considered minimal data utility loss, it is difficult to eval\u2010 uate this claim, especially with amount of deviation found between the authors results and ours. While our findings on the first experiment are in line with the authors, the\nReScience C 8.2 (#43) \u2013 Shulev et al. 2022 9\nresults of the second experiment are in direct contradiction to their findings. Since any fundamental issues in experiment 1 are likely to carry over to experiment 2 we focus our recommendations on experiment 1. Overall, we find that the results are reproducible but difficult to interpret and compare. Fruitful avenues of further investigation would be to re\u2010evaluate the fairness metrics. Another hypothesis is that there is a more functional issue with the DECAF model itself that would lend itself to further investigation.\n7.5 Communication with original authors We sent two emails to the authors of DECAF detailing the aforementioned code issues. One author did respond with a few extra code files, but unfortunately did not directly address our content questions. However, several of the interpretations we made were retroactively confirmed by the extra code files.\n8 Conclusion\nDuring our investigation, we faced multiple significant challenges in reproducing the results of the original paper. The biggest challenges stemmed from the number of pos\u2010 sible interpretations of the code and method. While we were not able to reproduce the results in full, we believe methods like DECAF have great potential for extension. The relevance of unbiased downstream classifiers and the evident need for bias removal in real data will likely remain a societally relevant area of research. For instance, the Adult dataset[11] we studied is nearing 30 years old. Perhaps an intriguing next phase could be to pull this year\u2019s Census data to investigate how bias has changed over time and if DECAF is still applicable for removing likelymore nuanced and hidden bias that persists through the increased awareness of bias and techniques for counteracting bias that exist today."}, {"heading": "10. T. Kamishima, S. Akaho, H. Asoh, and J. Sakuma. \u201cFairness-aware classifier with prejudice remover regularizer.\u201d", "text": "In:Machine Learning and Knowledge Discovery in Databases (2012), pp. 35\u201350. DOI: 10.1007/978-3-642-33486- 3_3. 11. D. Dua and C. Graff. UCI Machine Learning Repository. 2017. URL: http://archive.ics.uci.edu/ml.\nReScience C 8.2 (#43) \u2013 Shulev et al. 2022 11\nAppendix\n8.1 Additional results\nAbsolute difference is calculated as the value found by the authors minus the value found in our reproduction.\nRelative performance is calculated as the ratio between the original data and the perfor\u2010 mance of the selected model on the same variable."}], "title": "[Re] Replication Study of DECAF: Generating Fair Synthetic Data Using Causally-Aware Generative Networks", "year": 2022}