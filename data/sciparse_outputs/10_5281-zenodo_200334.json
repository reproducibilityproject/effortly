{"abstractText": "The reference paper [2] introduces a new reinforcement learning model called AttentionGated MEmory Tagging (AuGMEnT). The results presented suggest new approaches in understanding the acquisition of tasks requiring working memory and attentional feedback, as well as biologically plausible learning mechanisms. The model also improves on previous reinforcement learning schemes by allowing tasks to be expressed more naturally as a sequence of inputs and outputs. A Python implementation of the model is available on the author\u2019s GitHub page [1] which helped to verify the correctness of the computations. The script written for this replication also uses Python along with NumPy.", "authors": [{"affiliations": [], "name": "Erwan Le Masson"}, {"affiliations": [], "name": "Fr\u00e9d\u00e9ric Alexandre"}, {"affiliations": [], "name": "Julien Vitay"}, {"affiliations": [], "name": "Etienne Roesch"}], "id": "SP:3062bbcdbcacc28d6dd8d7b65470b62d79c46c6a", "references": [{"authors": ["J. Rombouts", "S. Bohte", "P. Roelfsema"], "title": "How Attention Can Create Synaptic Tags for the Learning of Working Memories in Sequential Tasks", "venue": "PLoS Computational Biology", "year": 2015}, {"authors": ["J. Rombouts", "P. Roelfsema", "S. Bohte"], "title": "Learning Resets of Neural Working Memory", "venue": "European Symposium on Artificial Neural Networks, Computational Intelligence And Machine Learning", "year": 2014}, {"authors": ["J. Rombouts", "P. Roelfsema", "S. Bohte"], "title": "Neurally Plausible Reinforcement Learning of Working Memory Tasks", "venue": "Advances in Neural Information Processing Systems", "year": 2012}, {"authors": ["J. Vitay", "H. Dinklebach", "F. Hamker"], "title": "ANNarchy: a code generation approach to neural simulations on parallel hardware", "venue": "Frontiers Neuroinformatics", "year": 2015}, {"authors": ["M.T. Yang"], "title": "Shalden. \u201cProbabilistic reasoning by neurons", "venue": "Nature", "year": 2007}, {"authors": ["D. Zambrano", "P. Roelfsema", "S. Bohte"], "title": "Continuous-time on-policy neural Reinforcement Learning of working memory tasks", "venue": "In: International Joint Conference on Neural Networks", "year": 2015}], "sections": [{"text": "[Re] How Attention Can Create Synaptic Tags for the"}, {"heading": "Learning of Working Memories in Sequential Tasks", "text": "Erwan Le Masson1, 2, 3 and Fr\u00e9d\u00e9ric Alexandre2, 1, 3\n1 LaBRI, Universit\u00e9 de Bordeaux, Bordeaux INP, CNRS, UMR 5800, Talence, France 2 INRIA Bordeaux Sud-Ouest, 200 Avenue de la Vieille Tour, 33405 Talence, France 3 IMN, Universit\u00e9 de Bordeaux, CNRS, UMR 5293, Bordeaux, France\nfrederic.alexandre@inria.fr\nEditor Olivia Guest\nReviewers Julien Vitay Etienne Roesch\nReceived Aug, 9, 2016 Accepted Dec, 1, 2016 Published Dec, 9, 2016\nLicence CC-BY\nCompeting Interests: The authors have declared that no competing interests exist.\n Article repository\n Code repository\n Data repository\nA reference implementation of\n\u2192 How Attention Can Create Synaptic Tags for the Learning of Working Memories in Sequential Tasks, J. Rombouts, M. Bohte and P. Roelfsema (2015), PLoS Computational Biology 11.3, e1004060. DOI: 10.1371/journal.pcbi.1004060"}, {"heading": "Introduction", "text": "The reference paper [2] introduces a new reinforcement learning model called AttentionGated MEmory Tagging (AuGMEnT). The results presented suggest new approaches in understanding the acquisition of tasks requiring working memory and attentional feedback, as well as biologically plausible learning mechanisms. The model also improves on previous reinforcement learning schemes by allowing tasks to be expressed more naturally as a sequence of inputs and outputs.\nA Python implementation of the model is available on the author\u2019s GitHub page [1] which helped to verify the correctness of the computations. The script written for this replication also uses Python along with NumPy."}, {"heading": "Methods", "text": ""}, {"heading": "Model", "text": "The model is composed of three layers: sensory, association and motor or Q-value. The association layer has specialized memory units keeping trace of the sensory signal variation to build an internal state of the environment. By implementing the SARSA(\u03bb) algorithm, the model is capable of predicting the reward as a function of all its possible actions. Attentional feedback is used during learning, meaning only synapses participating in the decision are updated. All computations are done locally at the unit\u2019s level, which is a strong argument to present AuGMEnT as biologically plausible.\nThe initial intention was to implement the model using an artificial neural network simulator. The simulation tool ANNarchy [5] was considered for its ability to simulate rate-coded networks. Unfortunately, there were several incompatibilities with AuGMEnT. The fixed order of evaluation between entities, i.e. connections then populations, and the unspecified order of evaluation between different populations make it difficult to implement cascading evaluations. The use of ANNarchy was abandoned and it was instead decided to write a custom script to simulate the network.\nReScience | rescience.github.io 1 Dec 2016 | Volume 2 | Issue 1\nThe paper\u2019s description of the model details update functions for all populations and connections and is relatively straight forward to implement. Some informations are however missing for equation 17: the initial value for qa(t\u2212 1) is not provided and the article omits that qa\u2032(t) needs to be set to 0 when receiving the end-trial signal, only mentioning a \u03b4 that reflects the transition to the terminal state. This information can actually be found in the 2012 conference paper about AuGMEnT [4]. Also, when it is said that Q-values are set to zero at the end of a trial, qa(t\u22121) is the only value which needs to be reset. It might also be useful to clarify the nature of the feedback weights w\u2032 in equations 14 and 16: once an action is selected, only the feedback synapses leaving the corresponding selected Q-value unit are activated to update tags, more precisely: w\u2032ij = wij \u00d7 zi. The model could also have dedicated feedback connections but the simpler method is to use the feedforward synapses\u2019 weights.\nTo offer some discussion about the model and its limits, the first point to bring forward would be its artificial time management. The extreme discretization of time and explicit signals such as trial begin and end make it difficult to consider real-time simulation or even realistic environments implementations. These constraints became apparent when trying to use ANNarchy as it is not designed to work with large time steps. In fact, the authors have published a \u201ccontinuous\u201d version of AuGMEnT [7], as well as a \u201clearning to reset\u201d version [3] to address theses issues. As a whole, we have found these mechanisms for artificial time management rather misleading when reproducing the model, and providing ad hoc solutions, not robust nor generic enough. We would consequently recommend to use them with parsimony and only with a strong justification.\nAnother possible weakness worth noting is the ambiguity of some memory traces. Because the traces in memory units are defined as the sums of changes in input, there exist sequences the model would be incapable of distinguishing. For example, the sequences ((0, 0), (1, 0), (1, 1)) and ((0, 0), (0, 1), (1, 1)) have the same memory traces (1, 1)."}, {"heading": "Tasks", "text": "The descriptions of the tasks used to test the network are somewhat minimal and it was necessary to refer to other resources for more informations. In this section, some details of implementation are exposed.\nFor the fixation tasks , i.e. saccade/anti-saccade and probabilistic decision making, the sequence of phases is as listed:\n1. Begin: blank screen for one step. 2. Fixation: fixation point on screen for a maximum of 8 steps. Once the network\nhas fixated the point, it has to maintain fixation for an additional step before moving on to the next phase with a potential reward. 1\n3. Cues: all visual cues are displayed (over several steps when there are multiple shapes).\n4. Delay: only the fixation points is on screen for two steps. 5. Go: the screen appears blank for a maximum of 10 steps. The network has to\nchoose a direction to look at, if it chooses the intended target, it is rewarded. 6. End: extra step to give the final reward and signal the end of the trial with a\nblank display.\nAdditional informations were found in the author\u2019s implementation of the saccade/antisaccade task: once the network has fixated the point in the Fixation phase, it has to\n1In the code, this phase is split in a Wait phase to wait for the network to fixate once and Fixate phase to ensure it maintains fixation.\nReScience | rescience.github.io 2 Dec 2016 | Volume 2 | Issue 1\nmaintain fixating until the Go signal when the screen turns off, otherwise the experiment is failed. Moreover, during the Go phase, the gaze can only be chosen once, if it is not the target, the trial fails.\nThe provided code did not implement the probabilistic decision making task but, fortunately, the original experiment\u2019s article [6] provided a more thorough methodology description. The shaping strategy for the probabilistic decision making task consists in gradually increasing the difficulty of the task. Table 3 in the article describes all 8 levels of difficulty. The column # Input Symbols is the size of the subset of shapes. The network is not presented all shapes immediately: first, the two shapes with infinite weights are used, then shapes with the smallest absolute weights are added as the difficulty increases. The column Sequence Length is the number of shapes shown during a trial. The more shapes there are on screen, the more difficult it is to determine which target should be chosen. A number of settings is randomized, such as which shapes should appear, their order of apparition, but also their locations around the fixation point. The first shape can appear in any of the 4 locations, the second in any of the remaining 3 locations, etc. If the total weight of the input symbols is infinite the corresponding target is guaranteed to give the reward. If the total weight is 0, meaning both targets are equally likely to be rewarded, the network can look in either direction for the trial to be successful, but only one random target gives a reward. Finally, the triangle and heptagon, shapes with infinite weights, cancel each other in the computation of the total weight."}, {"heading": "Results", "text": ""}, {"heading": "Implementations Comparison", "text": "As both codes use NumPy and identical data structures, they function in very similar way. The main difference is their structures since the replication extensively uses object oriented paradigms for readability. The computations in [1] variate in two points: the initial value of qa(t \u2212 1) is set to qa\u2032(t) whereas the replication script simply uses 0 and the Q-values are rounded to 5 decimals. From our tests these modifications do not yield better results. The replication offers a 40% speedup, mostly from the way it handles bias weights: they are created alongside the units\u2019 activities instead of being concatenated before every computation. A profiling of the author\u2019s code indicates most of its execution time is spent inside the function hstack."}, {"heading": "Replicated Data", "text": "Only the saccade/anti-saccade task and the probabilistic decision making task were implemented. For the probabilistic decision making task, the results are very similar. However, the saccade/anti-saccade task results are slightly worse than announced in the original article. The results presented in table 1 were obtained using the same parameters as in tables 1 and 2 of the reference article. Success designates the ratio of networks which successfully learned the task and Convergence the median number of trials necessary to learn it over 10,000 networks for saccade/anti-saccade and 100 networks for probabilistic decision making. Since the results are fairly sensible to the task\u2019s protocol, it is possible the differences for the saccade tasks come from undocumented changes in the experiments. Qualitative results such as the use of shaping strategy to obtain better performances are confirmed by this replication. See also figures 1 and 2 for the replicated activity traces of figures 2D and 4C in the reference article.\nReScience | rescience.github.io 3 Dec 2016 | Volume 2 | Issue 1"}, {"heading": "Conclusion", "text": "The results obtained are comparable to those announced in the article. Ambiguities in the experiments\u2019 descriptions could be the cause for worse performances, but do not contradict the article\u2019s overall conclusion.\nReScience | rescience.github.io 4 Dec 2016 | Volume 2 | Issue 1\nReScience | rescience.github.io 5 Dec 2016 | Volume 2 | Issue 1"}], "title": "[Re] How Attention Can Create Synaptic Tags for the Learning of Working Memories in Sequential Tasks", "year": 2017}