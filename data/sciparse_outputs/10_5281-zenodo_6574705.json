{"abstractText": "Combating bias in NLP requires bias measurement. Bias measurement is almost always achievedbyusing lexicons of seed terms, i.e. sets ofwords specifying stereotypes or dimen\u2010 sions of interest. This reproducibility study focuses on Antoniak and Mimno1\u2019s main claim that the rationale for the construction of these lexicons needs thorough checking before usage, as the seeds used for bias measurement can themselves exhibit biases. The study aims to evaluate the reproducibility of the quantitative and qualitative results presented in the paper and the conclusions drawn thereof.", "authors": [{"affiliations": [], "name": "Jille van der Togt"}, {"affiliations": [], "name": "Lea Tiyavorabun"}, {"affiliations": [], "name": "Matteo Rosati"}, {"affiliations": [], "name": "Giulio Starace"}, {"affiliations": [], "name": "Koustuv Sinha"}, {"affiliations": [], "name": "Sharath Chandra Raparthy"}], "id": "SP:28a04b554e5109a4fc6b7e5bd5ae023ebcb3fb76", "references": [{"authors": ["M. Antoniak", "D. Mimno"], "title": "Bad Seeds: Evaluating Lexical Methods for Bias Measurement.", "venue": "Online: Association for Computational Linguistics,", "year": 2021}, {"authors": ["T. Bolukbasi", "K.-W. Chang", "J.Y. Zou", "V. Saligrama", "A.T. Kalai"], "title": "Man Is to Computer Programmer asWoman Is to Homemaker? Debiasing Word Embeddings.", "venue": "Advances in Neural Information Processing Systems", "year": 2016}, {"authors": ["A. Caliskan", "J.J. Bryson", "A. Narayanan. \u201cSemantics derived automatically from language corpora contain human-like biases.\u201d EN. In"], "title": "Science (Apr", "venue": "2017). Publisher: American Association for the Advancement of Science. DOI: 10.1126/science.aal4230. URL: https : / /www.science .org /doi /abs /10 .1126/science .aal4230", "year": 2022}, {"authors": ["R. Rudinger", "C. May", "B. Van Durme. \u201cSocial Bias in Elicited Natural Language Inferences.\u201d en. In"], "title": "Proceedings of the First ACL Workshop on Ethics in Natural Language Processing", "venue": "Valencia, Spain: Association for Computational Linguistics, 2017, pp. 74\u201379. DOI: 10.18653/v1/W17-1609. URL: http://aclweb.org/anthology/W171609", "year": 2022}, {"authors": ["K. Joseph", "W. Wei", "K.M. Carley"], "title": "Girls Rule, Boys Drool: Extracting Semantic and Affective Stereotypes from Twitter.", "venue": "Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing. CSCW \u201917", "year": 2017}, {"authors": ["K. Ethayarajh", "D. Duvenaud", "G. Hirst"], "title": "Understanding undesirable word embedding associations.", "venue": "arXiv preprint arXiv:1908.06361", "year": 2019}, {"authors": ["S. Merity", "C. Xiong", "J. Bradbury", "R. Socher"], "title": "Pointer Sentinel Mixture Models.", "venue": "CoRR abs/1609.07843", "year": 2016}, {"authors": ["M. Wan", "J. McAuley"], "title": "Item Recommendation on Monotonic Behavior Chains.", "venue": "Proceedings of the 12th ACM Conference on Recommender Systems. RecSys \u201918", "year": 2018}, {"authors": ["M. Wan", "R. Misra", "N. Nakashole", "J. McAuley"], "title": "Fine-Grained Spoiler Detection from Large-Scale Review Corpora.", "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. Florence, Italy: Association for Computational Linguistics,", "year": 2019}, {"authors": ["M. Honnibal", "I. Montani", "S. Van Landeghem", "A. Boyd"], "title": "spaCy: Industrial-strength Natural Language Processing in Python", "year": 2020}, {"authors": ["R. Rehurek", "P. Sojka"], "title": "Software framework for topic modelling with large corpora.", "venue": "Proceedings of the LREC 2010 workshop on new challenges for NLP frameworks. Citeseer", "year": 2010}, {"authors": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "title": "Distributed representations of words and phrases and their compositionality.", "venue": "Advances in neural information processing systems", "year": 2013}, {"authors": ["A. Paszke"], "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library.", "venue": "Advances in Neural Information Processing Systems 32", "year": 2019}], "sections": [{"text": "R E S C I E N C E C"}, {"heading": "Replication / ML Reproducibility Challenge 2021", "text": "[Re] Badder Seeds: Reproducing the Evaluation of"}, {"heading": "Lexical Methods for Bias Measurement", "text": "Jille van der Togt1,2, ID , Lea Tiyavorabun1,2, ID , Matteo Rosati1,2, ID , and Giulio Starace1,2, ID 1University of Amsterdam, Amsterdam, the Netherlands \u2013 2Equal contributions\nEdited by Koustuv Sinha,\nSharath Chandra Raparthy\nReviewed by Anonymous Reviewers\nReceived 04 February 2022\nPublished 23 May 2022\nDOI 10.5281/zenodo.6574705\n1 Reproducibility Summary"}, {"heading": "Scope of Reproducibility", "text": "Combating bias in NLP requires bias measurement. Bias measurement is almost always achievedbyusing lexicons of seed terms, i.e. sets ofwords specifying stereotypes or dimen\u2010 sions of interest. This reproducibility study focuses on Antoniak and Mimno1\u2019s main claim that the rationale for the construction of these lexicons needs thorough checking before usage, as the seeds used for bias measurement can themselves exhibit biases. The study aims to evaluate the reproducibility of the quantitative and qualitative results presented in the paper and the conclusions drawn thereof."}, {"heading": "Methodology", "text": "We re\u2010implement the entirety of the approaches outlined in the original paper. We train a skip\u2010gram word2vec model with negative sampling to obtain embeddings for four cor\u2010 pora. This does not require particular computing requirements beyond standard con\u2010 sumer personal computers. Additional code details can be found in our linked reposi\u2010 tory."}, {"heading": "Results", "text": "We reproduce most of the results supporting the original authors\u2019 general claim: seed sets often suffer from biases that affect their performance as a baseline for bias metrics. Generally, our results mirror the original paper\u2019s. They are slightly different on select occasions, but not inways that undermine the paper\u2019s general intent to show the fragility of seed sets."}, {"heading": "What was difficult", "text": "The significant difficulties encounteredwere due to a lack of publicly available code and documentation to clarify missing information in the paper. For this reason, many algo\u2010 rithms that ultimately turned out to be quite simple required lengthy clarifications with authors or trial and error. Lastly, the research was quite data\u2010intensive, which caused some implementations to be non\u2010trivial to account for memory management.\nCopyright \u00a9 2022 J.V.D. Togt et al., released under a Creative Commons Attribution 4.0 International license. Correspondence should be addressed to Giulio Starace (giulio.starace@gmail.com) The authors have declared that no competing interests exist. Code is available at https://github.com/thesofakillers/badder-seeds \u2013 DOI 10.5281/zenodo.6480966. \u2013 SWH swh:1:dir:13ff45fd249e765a221d49f701c32d45b64ee675. Open peer review is available at https://openreview.net/forum?id=HcIxA3Mm2CF.\nReScience C 8.2 (#40) \u2013 Togt et al. 2022 1"}, {"heading": "What was easy", "text": "Once understood, the methods proposed by the authors were relatively easy to imple\u2010 ment. The mathematics involved is quite straightforward. Communication was also reasonably accessible. The authors\u2019 emails were readily available, and the responses came quickly and were always helpful.\nCommunication with original authors Wemaintained a lengthy email correspondence throughout the replication of the paper with one author, Maria Antoniak. We contacted her to clarify extensive aspects of the paper\u2019s methodology. Specifically, this concerned summarizing the data processing ap\u2010 proach, explaining missing hyperparameters, and outlining the aggregation of metrics across different bootstrapped models. None of the original code was disclosed.\nReScience C 8.2 (#40) \u2013 Togt et al. 2022 2\n2 Introduction\nThe emergence of bias quantification in Natural Language Processing (NLP) methods has given rise to two use cases, referred to as downstream and upstream. In the former, biasmeasurements are used to debias or correct biases in word representations to avoid encoded biases trickling downwhen applying these NLPmodels [2, 3]. In the latter, bias measurements are used onmodels trained on small corpora to quantify the bias present and compare them. This use case has endowed social scientists with the quantitative foundation to answer political and social questions about bias across corpora in an em\u2010 pirical manner [4, 5]. Crucially, most bias quantification methods depend on lexicons of seed terms that specify the bias dimensions of interest. The selection of seed terms varies considerably across the literature, and seed sets themselves may exhibit social and cognitive biases [1]. It is not clear whether it is possible to re\u2010use seed set across corpora (thereby interfering with upstream use cases), and elements such as seed term frequency have been shown to affect bias measurements, and thus downstream uses [6]. We seek to replicate the Antoniak and Mimno1 paper, hereafter referred to as \u201dthe orig\u2010 inal paper/work\u201d. In it, the authors seek to 1) qualitatively explore seed selection and their sources, 2) demonstrate that features of seed sets such as pairing order, set simi\u2010 larity, and frequency can cause instability in bias measurements, and 3) make recom\u2010 mendations for the testing and justifying of seed sets in future work. We have replicated the experiments showing the fragility of seed sets, thus verifying the claims of a need for better justification and analysis of them in future literature. We have also built a public toolkit to reproduce these measures on arbitrary seed sets and trained embeddings.\n3 Scope of reproducibility\nThis reproducibility study focuses on the authors\u2019 main claim that seed lexicons need thorough checking before usage to measure bias, as seeds themselves can be biased and induce instabilities in measurement. The authors conducted a literature review on prior works to gather many seed sets. They subsequently evaluated the gathered seed sets with a series of bias measurement metrics proposed by Bolukbasi et al., Caliskan, Bryson, and Narayanan2,3, and themselves. Our work consists of two interconnected efforts: code replication, given the absence of pre\u2010existing code for the original paper, and reproducing the main results. The latter goal is the main focus of our work and entails reproducing the outcomes that support the paper\u2019s central claims, which can be summarized as follows:\n1. Bias subspaces generated from common bias subspace metrics (e.g., WEAT, PCA) can help capture the difference represented by the seed set pairs.\n2. Bias subspaces suffer from instability due to the following factors:\n(a) The ordering and pairing of the seed sets. (b) The selection of seeds that are members of the seed sets. (c) The degree of semantic similarity between seeds.\n3. Methods of sourcing seed sets are inconsistent, with disparate strategies being used across NLP literature.\n4 Methodology\nThe code from the original paper was not made publicly available. We, therefore, re\u2010 implemented the entire approach from the description in the original paper. The fol\u2010\nReScience C 8.2 (#40) \u2013 Togt et al. 2022 3\nlowing section will summarize the resources and methodology used to reproduce the original paper accurately.\n4.1 Code As mentioned above, the code from the original paper is not publicly available. We fully re\u2010implement all the code, which can be found on GitHub1. We closely follow the original paper\u2019s methodology to achieve accurate reproduction. The reproduction is performed step by step, from downloading and preprocessing the data to training the models and visualizing the results.\n4.2 Documentation Unfortunately, there was little to no documentation in the original work besides the content of the original paper. This occasionally lacked crucial information to reproduce the results or was vague on implementation details. In addition to the original paper, Antoniak and Mimno1 published a Github repository that contained a JSON with the metadata on seed sets gathered from prior works2.\n4.3 Model descriptions We train several bootstrapped skip\u2010gram word2vec models with negative sampling on unigrams on each dataset. This model attempts to predict whether a particular word is a valid context (where the context window size is a hyperparameter) for a given other word using a single fully connected hidden layer. The first step in training this model is creating a vocabulary of the entire training dataset. With this vocabulary, each word can be represented as a one\u2010hot vector. The network output is then a measure of the probability that the word is a valid context. The trained weights from this hidden layer are then used to obtain word embedding vectors for each term in the training set vocab\u2010 ulary.\n4.4 Datasets The original paper used four datasets and one pretrained model: New York Times ar\u2010 ticles from April 15th\u2010June 30th, 20163; high\u2010quality WikiText articles, using the com\u2010 pleteWikiText\u2010103 training set [7]; Goodreads book reviews for the romance and history and biography genres sampled from the UCSD book Graph [8, 9]; and the pretrained word2vec GoogleNewsmodel4. We use these same corpora for our research, preprocess\u2010 ing them as closely as possible to the original paper. This consists of grouping the text into documents, filtering relevant documents, lowercasing and removing special char\u2010 acters. We then use spaCy [10] for tokenization and POS\u2010tagging. Because the work is not concerned with model performance, this study makes no use of train/dev/test splits. The WikiText\u2010103 dataset, however, is pre\u2010split, so like in the original work, we work with the training split. Links to all these datasets can be found in our Github repository. Preprocessing statistics of our work and the original paper can be found in Table A.1. We find general agreement in our numbers regarding the total number of documents per dataset. There are minor discrepancies in the Goodreads datasets, most likely due to implementation differences. We also count slightly fewer total words than the origi\u2010 nal paper in all cases, but the orders of magnitude generally match. We are, however, unable to reproduce vocabulary size accurately. We tried many strategies in the replica\u2010 tion process to obtain these numbers, but none were successful. Furthermore, looking\n1https://github.com/thesofakillers/badder\u2010seeds 2https://github.com/maria\u2010antoniak/bad\u2010seeds 3https://www.kaggle.com/nzalake52/new\u2010york\u2010times\u2010articles 4https://github.com/mmihaltz/word2vec\u2010GoogleNews\u2010vectors\nReScience C 8.2 (#40) \u2013 Togt et al. 2022 4\nat the official dataset statistics, for example for WikiText [7], it is clear that our repro\u2010 duced vocabulary size is a lot closer to the ground truth than the one by Antoniak and Mimno1. Lastly, mean document length values of each dataset are accurately repro\u2010 duced, with the WikiText values suffering the most. The subsections below will discuss each dataset in more detail.\nNewYork Times This dataset contains 165,900 paragraphs from 8,888 articles from the New York Times published between April 15th and June 30th 2016. The articles cover a broad range of sections, including but not limited to movies, sports, technology, busi\u2010 ness, books, science, and fashion.\nWikiText\u2010103 This dataset contains 28,472manually verified articles fromWikipedia.org. The entire training dataset is used, in which lists, HTML errors, math, and code have already been removed. Furthermore, we removed all formulas still present in the text.\nGoodreads The entire Goodreads dataset containsmillions of reviews. This study uses just the Romance and the History/Biography genres. Five hundred book reviews per book are sampled for each genrewhile filtering out all bookswith fewer than 500 reviews and all reviews containing fewer than 20 characters.\nGoogleNews Google\u2019s pretrained word2vec model is trained on ca.100 billion words from the GoogleNews dataset (4). Our use of this model was limited to replicating the results outlined below for additional robustness.\nSeed Set Dataset Part of the contributions of the original workwas creating a catalogue of 178 seed sets gathered from eighteen highly\u2010cited prior works on bias measurements. We refer to this catalogue as the gathered seeds. Each element of the catalogue comprises a seed set, the category it represents, a justification, the source categorization, a link, and a unique ID. It is readily available on the original author\u2019s GitHub2. A brief statistical overview can be found in Fig. A.1. We process the catalogue by lower\u2010casing the seeds and removing bigrams to use them with our models. We also filter seed sets containing less than two seeds as we argue that a single seed would not be sufficient to form a set.\n4.5 Experimental setup and code An environment containing all necessary packages is included in the publicly available repository and can be quickly set up. To mirror the original paper\u2019s setup, we used the gensim [11] implementation of skip\u2010gram with negative sampling [12] to train the vector embeddings for all datasets. We used this library to train ourmodels as that is the frame\u2010 work used by the original paper and to avoid noise due to different implementations (the investigation of which would be outside the scope of this paper). Several PyTorch [13] implementations are also available on GitHub if that is preferred5,6. We reproduce the original paper\u2019s results by focusing on two popular seed\u2010based bias metrics to measure bias in corpus\u2010derived embeddings: WEAT and PCA. These met\u2010 rics are used to produce a bias subspace vector given a pair of seed sets that specifies a bias dimension of interest. The WEAT method, introduced in Caliskan, Bryson, and Narayanan3, produces a vector based on the difference between themean vectors of the two target sets. The PCA method, described in Bolukbasi et al.2, instead requires that each seed term in one of the seed sets be paired with one seed term from the other seed set. The subspace vector is then the first principal component resulting from the PCA\n5https://github.com/theeluwin/pytorch\u2010sgns 6https://github.com/ddehueck/skip\u2010gram\u2010negative\u2010sampling\nReScience C 8.2 (#40) \u2013 Togt et al. 2022 5\nof a matrix constructed by, for each pair of seeds, taking the two half vectors from the pair\u2019s mean to the two pair members and using them as two columns of the matrix. We also reproduce the original paper\u2019s coherence metric, which aims to quantify the robustness of the bias subspace. This metric is calculated as the absolute value of the difference in mean ranks of the terms in two seed sets when all the model\u2019s vocabulary is ranked by cosine similarity to the bias subspace. Anothermetric used is set similarity, the cosine similarity between the average vectors of two seed sets. Finally, when aggregating embeddings of a specific word across bootstrapped models, we take the average of the embedding vectors in each model that includes the word. Given a particular pair of seed sets for coherence aggregation, we only average coher\u2010 ence scores for models containing every seed term in the two sets to avoid aggregating coherence based on different seed sets.\n4.6 Hyperparameters 100\u2010dimensional embeddings were trained for five epochs on all four datasets, with a five\u2010word negative context sampling rate and a window size of five. We trained embed\u2010 dings with aminimumword count of 0, 10, and 100 due to variation in the original paper. This process was repeated for 20 bootstrapped samples of each dataset (with the sample size equal to the number of documents in the dataset), resulting in 20 separate models. The bootstrapping provided the stochasticity required for robustness. To ensure this reproducibility, we use a random seed of 42 throughout.\n4.7 Computational requirements The execution of the reproduced code does not take excessive computing power. This study used no GPUs or computing clusters. We ran the experiments on an Intel I9 9900k and 32GB of 3200MHz RAM running Ubuntu 20.04.3 LTS. Table A.3 shows peak RAM usage and time in seconds to completion for every subprocess of the replication.\n5 Results\n5.1 Quantitative Results We started by confirming that the bias subspace does capture the difference or bias that the seed pairs are intended to represent. For this, we reproduced an experiment by An\u2010 toniak andMimno1 ranking the cosine similarity between the first Principal Component (PC) of the bias subspace and all words in the corpus. The top and bottom ten words for each bias subspace are shown in Fig. 1a. In the shown words of the gender pair subspace and the shuffled gender pair subspace gender\u2010related words are found, whereas none are present in the randompair subspace. However, only the gender pair subspace divides nicely betweenmale and female terms. We extended this by calculating the cosine similarity of the top and bottom ten words from the ordered bias subspace for the shuffled bias subspace. The results in Fig. A.2 show she and his as the two highest\u2010ranked words, which are not split along the intended bias subspace. Fig. 2 shows that the first PC has almost always a very high explained variance ratio for the bias subspace of ordered pairs, which drops off quickly for the subsequent PCs. Instead, the explained variance ratio per PC drops more smoothly for the shuffled pairs. Fig. A.2 shows this behavior by computing the top and bottom ten words by cosine similarity against the second PC of the gender subspace. We can observe that the bias subspace of the ordered pairs does not contain gender words anymore. In contrast, the shuffled subspace does have gender words such as her, thereby replicating the trend ob\u2010 served in Fig. 2. It is also important to note that in Fig. 2 there are exceptional cases where shuffled seed sets produce the first PC with a higher explained variance than the\nReScience C 8.2 (#40) \u2013 Togt et al. 2022 6\nordered seed sets. In general, these results replicate the trends of the original experi\u2010 ments.\nFig 3 shows that bias measurement is highly inconsistent across seed sets with the same seed category sourced from different papers. We used the cosine similarity between female seed sets and the word unpleasantness as a biasmeasurement. The cosine similarity varies greatly between seed sets, replicating the same trends as the original paper.\n0.6 0.4 0.2 0.0 0.2 0.4 0.6 cosine similairty to unpleasentness\nfemale-Kozlowski_et_al_2019\nfemale_1-Caliskan_et_al_2017\ndefinitional_female-Bolukbasi_et_al_2016\nfemale_singular-Hoyle_et_al_2019\nfemale_definition_words_2-Zhao_et_al_2018\nfemale_stereotype_words-Zhao_et_al_2018\nse ed\ns et\nromance history and biography\nFigure 3. Reproduction of Fig. 2. Displaying the cosine similarity between the averaged vector of unpleasantness across all 20 boot\u2010 strapped models and different seeds sets of the category female.\nFig. 4 explores the rela\u2010 tionship between set sim\u2010 ilarity and the robust\u2010 ness of the bias subspace. The relationshipbetween set similarity and the ex\u2010 plained variance of the PCA\u2010derivedbias subspace vector is plotted for each dataset and frequency thresh\u2010 olds. The original pa\u2010\nper shows this relation\u2010 ship only for the Wiki\u2010 Text dataset, and we find a similar negative corre\u2010\nlation between set similarity and explained variance for that dataset. Table A.2 qualitatively explores this relationship, ranking both gathered and generated\nReScience C 8.2 (#40) \u2013 Togt et al. 2022 7\nsets by coherence. More semantically dissimilar seed sets score higher in coherence than more similar sets. In the gathered sets, seed sets related to names have extremely low coherence due to their semantics being very similar and the set pairs containing duplicate terms (see \u201dnames black\u201d and \u201dnames white\u201d). In the generated sets, we see that very different terms (such as those relating to careers and those related to lower body clothing/parts) have high coherence. In contrast, sets such as food terms score much lower. We observe a similar pattern when using the PCA algorithm as a basis for coherence. These results show the replicability of the original paper, as they are almost identical.\n5.2 Qualitative Results The original paper gathered 178 seed sets of eighteen highly\u2010cited prior work on bias measurement. These seeds are both embedding\u2010based and non\u2010embedding\u2010based bias detection methods, often overlapping. The seeds are chosen in a multitude of ways. Only unigram seeds are selected, and words that do not appear in the training corpus are omitted. We have validated the accuracy of Table 3 in the original paper by review\u2010 ing each of the eighteen papers and determining which methods the authors used. We briefly summarize them below:\nBorrowed from social sciences Select seed sets are borrowed from prior psychology and other social sciences work.\nCrowd\u2010Sourced Crowd\u2010based annotation can create custom seed sets. This method can aid in gathering contemporary associations and stereotypes. However, controlling crowd demographics often poses a problem. This can lead to stereotypes being hard\u2010 coded into the seeds.\nPopulation\u2010Derived Seeds can also be derived from government\u2010collected population datasets. These datasets are usually names and occupations common to specific demo\u2010\nReScience C 8.2 (#40) \u2013 Togt et al. 2022 8\ngraphic groups. A significant problemwith thismethod is that the data tends to be often US\u2010centric and thus gives a distorted view of the rest of the world.\nAdapted from Lexical Resources Researchers can also draw seeds from existing dic\u2010 tionaries, lexicons and other public resources. The advantage is that these seeds have already undergone a round of validation.\nCorpus\u2010Derived This quantitativemethod is used to extract seeds terms from a corpus. It has the advantage of ensuring high\u2010frequency words are selected but suffers from similar risks as crowd\u2010sourced seeds.\nCurated Seed hand\u2010selection by authors often yields high precision seeds but is slow and relies on unbiased authors.\nRe\u2010used The last method relies on prior bias measurement research for seed terms. The advantage is that the seeds have already been used, but researchers should not use them without validation.\n5.3 Results beyond original paper Set Similarity and Bias Subspace in Additional Datasets We extended the original pa\u2010 per\u2019s set similarity versus bias subspace explained variance analysis to cover all datasets (beyondWikiText) in Fig. 4. The negative trend is still present with the NYT corpus, but not in the Goodreads corpora, where the trend is almost absent or slightly positive. In addition, the positions of the highlighted seed set pairs are variable across corpora. We also extended this work to examine the relationship between seed pair coherence versus set similarity, where the inverse relationship is present in all datasets. Notice that the requirement that coherence is calculated only for models that contain all seed terms (as described in Section 4.5) makes specific pairs of seed sets be ignored, as seen from the lack of the two highlighted set pairs for select datasets.\nTesting Minimum Frequency Filter Due to inconsistencies both in the paper and in communication with the author in the reported minimum frequency filter for the skip\u2010 gram models, we experimented with minimum frequencies \u00b5 \u2208 {0, 10, 100}. These en\u2010 abled us to see results across the whole vocabulary in the case of \u00b5 = 0 and reduce noise from rare words in the case of \u00b5 = 10. We also used \u00b5 = 100 to generate Fig. 1 as the original paper.\nSeed Toolkit and Pairing Seed Set Data. Other than extending the experiments of the original paper, we have two additional contributions. For the sake of reproducibility, we make our code publicly available and design our repository as an open Python package that can be used to obtain bias subspace vectors and assess seed set robustness. This toolkit can help future researcherswho aim to evaluate their seeds carefully. Our second contribution is an augmentation of the seed dataset provided by Antoniak and Mimno1. We provided additional annotations regarding pairing, i.e. we identify which seeds to pair together along standard bias dimensions in a queriable .csv format.\n6 Discussion\nOverall, our results replicate the data reported in the original paper. This replication lends strong support to the general claim of the original paper that seed sets incorporate\nReScience C 8.2 (#40) \u2013 Togt et al. 2022 9\nstrong inductive biases that affect their performance as grounding for bias metrics and that researchers should be more cognizant of these limitations. Instability in bias subspaces can be introduced by selecting seeds in seed sets, as stated in claim 2b. Our results in Fig. 3 support this as they reproduce the original work. The same bias measurement varies across seed sets selected by different authors who as\u2010 signed it to the same category. In addition, the dependence of the bias subspace on seed set selection is further supported by Fig. 4. The two highlighted seed sets (black vs white roles/names) are generally distinct in position for each corpus, despite theoret\u2010 ically attempting to define similar bias dimensions. Another source of instability claimed by Antoniak and Mimno1 is the ordering and pair\u2010 ing of seed sets. In Fig. 2 we show that the explained variance ratio for the ordered bias subspaces can behave very differently from the shuffled bias subspaces, supporting claim 2a. Our work in Fig. 1a also supports this claim. While the ordered subspace success\u2010 fully splits the top words along the intended subspace of male and female, the first PC of the shuffled bias subspace has words such as mother and boyfriend both ranked on the same end. This shows that while the subspace still picks up on gender words, it does not represent the intended subspace. Supporting claim 2a that bias subspaces can become less meaningful with a shuffled seed pairing. We could further confirm this behavior by calculating the cosine similarity of the top words of the ordered subspace for the shuffled subspace in Fig. 1b. These results show that she and his are ranked next to each other at the top and not split along the intended bias subspace. These experiments lend strong support to claim 2a that the order of seed pairs can substantially influence the meaningfulness of the bias subspace and, consequently, the bias metrics. Finally, bias subspaces suffer instability due to semantically overlapping seeds being less distinguishable in the bias subspace, as stated in claim 2c. Our results in Table A.2 and Fig. 4 demonstrate that bias subspace vectors are less robust when the seed sets are semantically similar or overlapping. This relationship lends strong credence to claim 2c. However, our results did show that this inverse relationship is not conserved across a minority of corpora (e.g., the Goodreads datasets) for the explained variance metric. More broadly, however, this still shows that the reliability of seed selection is quite variable. While similar seed sets may generate robust bias subspaces for more semantically equivalent seed pairs for some corpora, that is not guaranteed. Therefore, while this inverse relationshipmay beminimized for specific corpora, extensive corpus\u2010 specific seed set investigations are still required.\nWhat was easy. The original paper clearly described the algorithms used to obtain bias metrics. Additionally, it carefully cited the papers that first proposed them, which specified further details. This aided our understanding of the underlying concepts and accelerated the implementation of the frameworks. Model training and embedding generation was also facilitated by the pre\u2010existing gensim framework. This permitted greater focus on reproducing the details of the experiments than choosing between al\u2010 ternative implementations of skip\u2010gram word2vec. In addition, responsive authors per\u2010 mitted quick clarifications through email communication when important details were not clear.\nWhatwas difficult. The original paper did notmake code publicly available and largely lacked documentation. Only the gathered seeds were provided via GitHub (2). This made it necessary to reproduce all the code from scratch. In select instances, the paper crucially omitted important information, making us re\u2010 liant on communicationwith the authors. This wasmost pronouncedwhen aggregating embeddings or other metrics across the bootstrapped model sampling, where vocabu\u2010 lary sizes were different. This meant that not all models had good embeddings for all seed terms. We had to consider several different approaches before settling on the av\u2010 eraging criteria described in Section 4.5.\nReScience C 8.2 (#40) \u2013 Togt et al. 2022 10\nFinally, preprocessing the data was more difficult than initially imagined. The tokeniza\u2010 tion pipeline in the original paper was vaguely specified, and differences in our imple\u2010 mentation caused the slight discrepancies in Table A.3. The POS taggingwith spaCywas imperfect, resulting in the incorrect tagging of several proper nouns as common nouns, making it hard to control for POS in random seed generation.\nCommunication with original authors. While the authors did not disclose any code, we maintained a lengthy email correspondence with them. One author, Maria Anto\u2010 niak, was contacted to clarify hyperparameters of the word2vec model, the methodol\u2010 ogy for generating random seeds across bootstrapped models, and which bias metrics (PCA orWEAT)were used for different results. She also described her dataset processing pipeline, as there were many alternate ways to process the corpora before training.\n7 Conclusion\nOverall, our results replicate the ones reported in the original paper. This lends strong support to the general claim of the original paper that seed sets incorporate significant inductive biases that affect their performance as grounding for bias metrics and that researchers should be more cognizant of these limitations. Aside from confirming the danger of blindly using seed sets, we also provide additional contributions. First of all, all code used to replicate the original paper is publicly available. This code can obtain bias subspace vectors and assess seed set robustness. Secondly, we extended the original paper\u2019s set similarity versus bias subspace explained variance analysis to cover all datasets. Furthermore, we implement multiple numbers of minimum frequencies that further enable results across the entire vocabulary. Lastly, we provide an additional annotation pairing of the original seed dataset. We have highlighted a need for carefully justifying the use of particular sets through em\u2010 pirical means, but a theoretically sound and systematic method for doing so is still in its infancy. Further workmay explore what criteria seed sets should satisfy to demonstrate robustness. In addition, future researchersmaywant to extend this work to bigram seed terms and embeddings to explore the limitations of more expressive seeds and bias di\u2010 mensions."}], "title": "[Re] Badder Seeds: Reproducing the Evaluation of Lexical Methods for Bias Measurement", "year": 2022}