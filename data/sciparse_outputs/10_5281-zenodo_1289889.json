{"abstractText": "Reinforcement learning represents a fundamental cognitive process: learning by trial and error to maximize rewards and minimize punishments. Current and most influential theoretical models of reinforcement learning assume a unique learning rate parameter, independently of the outcome valence (Sutton and Barto [14], O\u2019Doherty et al. [10], Behrens et al. [1]). However human participants were shown to integrate differently positive and negative outcomes (Frank, Seeberger, and O\u2019Reilly [3], Frank et al. [4], Sharot, Korn, and Dolan [13]). This motivated the reference article to implement a modified version of the reinforcement learning model, with two distinct learning rates for positive and negative outcomes (Caz\u00e9 and Meer [2]). They have shown that although differential learning rates shifted reward predictions and could thus be seen as a maladaptive bias, this model can outperform the classical reinforcement learning model on tasks with specific outcome probabilities. Following Caz\u00e9 and Meer [2]\u2019s predictions, a subsequent empirical article have modeled human behavior on these specific tasks (Gershman [7]). The question is still an active research area, as various articles have further investigated the difference learning rates bias (Garrett and Sharot [5], Moutsiana et al. [9], Shah et al. [12], Garrett and Sharot [6], Lefebvre et al. [8], Palminteri et al. [11]). A link to the pdf version of the reference article was posted on the last author\u2019s laboratory website (http://www.vandermeerlab.org/publications.html), but the corresponding code was not available (https://github.com/vandermeerlab/papers/tree/ master/Caze_vanderMeer_2013). We believe that an openly available code repository replicating the results of Caz\u00e9 and Meer [2]\u2019s paper can be helpful to the scientific community. We therefore implemented the model and analysis scripts using Python, with numpy, random and matplotlib libraries.", "authors": [{"affiliations": [], "name": "Sophie Bavard"}, {"affiliations": [], "name": "H\u00e9lo\u00efse Th\u00e9ro"}], "id": "SP:10fac7946925cd6b10096767f2b599e9c6986de1", "references": [{"authors": ["Timothy EJ Behrens"], "title": "Learning the value of information in an uncertain world", "venue": "Nature neuroscience", "year": 2007}, {"authors": ["Romain D Caz\u00e9", "Matthijs AA van der Meer"], "title": "Adaptive properties of differential learning rates for positive and negative outcomes", "venue": "Biological cybernetics", "year": 2013}, {"authors": ["Michael J Frank", "Lauren C Seeberger", "Randall C O\u2019Reilly"], "title": "By carrot or by stick: cognitive reinforcement learning in parkinsonism", "venue": "Science", "year": 2004}, {"authors": ["Michael J Frank"], "title": "Genetic triple dissociation reveals multiple roles for dopamine in reinforcement learning", "venue": "Proceedings of the National Academy of Sciences", "year": 2007}, {"authors": ["Neil Garrett", "Tali Sharot"], "title": "How robust is the optimistic update bias for estimating self-risk and population base rates?", "venue": "PLoS One", "year": 2014}, {"authors": ["Neil Garrett", "Tali Sharot"], "title": "Optimistic update bias holds firm: Three tests of robustness following Shah et al.", "venue": "Consciousness and cognition", "year": 2017}, {"authors": ["Samuel J Gershman"], "title": "Do learning rates adapt to the distribution of rewards?", "venue": "Psychonomic bulletin & review", "year": 2015}, {"authors": ["Germain Lefebvre"], "title": "Behavioural and neural characterization of optimistic reinforcement learning", "venue": "Nature Human Behaviour", "year": 2017}, {"authors": ["Christina Moutsiana"], "title": "Human frontal\u2013subcortical circuit and asymmetric belief updating", "venue": "Journal of Neuroscience", "year": 2015}, {"authors": ["John O\u2019Doherty"], "title": "Dissociable roles of ventral and dorsal striatum in instrumental conditioning", "year": 2004}, {"authors": ["Stefano Palminteri"], "title": "Confirmation bias in human reinforcement learning: Evidence from counterfactual feedback processing", "venue": "PLoS computational biology", "year": 2017}, {"authors": ["Punit Shah"], "title": "A pessimistic view of optimistic belief updating", "venue": "Cognitive Psychology", "year": 2016}, {"authors": ["Tali Sharot", "Christoph W Korn", "Raymond J Dolan"], "title": "How unrealistic optimism is maintained in the face of reality", "venue": "Nature neuroscience", "year": 2011}, {"authors": ["Richard S Sutton", "Andrew G Barto"], "title": "Introduction to reinforcement learning", "year": 1998}], "sections": [{"text": "[Re] Adaptive properties of differential learning rates for positive and negative outcomes Sophie Bavard1 and H\u00e9lo\u00efse Th\u00e9ro1\n1 Laboratoire de Neurosciences Cognitives Computationnelles (ENS - INSERM), D\u00e9partement d\u2019\u00c9tudes Cognitives, \u00c9cole Normale Sup\u00e9rieure, PSL Research University, 29 rue d\u2019Ulm, 75005 Paris, France sophie.bavard@gmail.com, thero.heloise@gmail.com\nEditor Olivia Guest\nReviewers Xavier Hinaut Beno\u00eet Girard\nReceived Feb, 20, 2018 Accepted Jun, 14, 2018 Published Jun, 14, 2018\nLicence CC-BY\nCompeting Interests: The authors have declared that no competing interests exist.\n Article repository\n Code repository\nA reference implementation of\n\u2192 Caz\u00e9, R. D., & van der Meer, M. A. (2013). Adaptive properties of differential learning rates for positive and negative outcomes. Biological cybernetics, 107(6), 711-719. https://doi.org/10.1007/s00422-013-0571-5"}, {"heading": "Introduction", "text": "Reinforcement learning represents a fundamental cognitive process: learning by trial and error to maximize rewards and minimize punishments. Current and most influential theoretical models of reinforcement learning assume a unique learning rate parameter, independently of the outcome valence (Sutton and Barto [14], O\u2019Doherty et al. [10], Behrens et al. [1]). However human participants were shown to integrate differently positive and negative outcomes (Frank, Seeberger, and O\u2019Reilly [3], Frank et al. [4], Sharot, Korn, and Dolan [13]). This motivated the reference article to implement a modified version of the reinforcement learning model, with two distinct learning rates for positive and negative outcomes (Caz\u00e9 and Meer [2]).\nThey have shown that although differential learning rates shifted reward predictions and could thus be seen as a maladaptive bias, this model can outperform the classical reinforcement learning model on tasks with specific outcome probabilities. Following Caz\u00e9 and Meer [2]\u2019s predictions, a subsequent empirical article have modeled human behavior on these specific tasks (Gershman [7]). The question is still an active research area, as various articles have further investigated the difference learning rates bias (Garrett and Sharot [5], Moutsiana et al. [9], Shah et al. [12], Garrett and Sharot [6], Lefebvre et al. [8], Palminteri et al. [11]).\nA link to the pdf version of the reference article was posted on the last author\u2019s laboratory website (http://www.vandermeerlab.org/publications.html), but the corresponding code was not available (https://github.com/vandermeerlab/papers/tree/ master/Caze_vanderMeer_2013). We believe that an openly available code repository replicating the results of Caz\u00e9 and Meer [2]\u2019s paper can be helpful to the scientific community. We therefore implemented the model and analysis scripts using Python, with numpy, random and matplotlib libraries."}, {"heading": "Methods", "text": "We first implemented our scripts on Matlab, as we were more familiar with this language, and then adapted them on Python.\nReScience | rescience.github.io 5 - 1 Jun 2018 | Volume 4 | Issue 1\nWe used the modeling description of the reference article to implement our replication. They used standard Q-learners with a softmax action selection rule (Sutton and Barto [14]), and their precise description enabled us to implement them with low difficulty. But we found four ambiguities in the simulation procedure.\nFirst, the authors described their analytical results to be valid for \u201cQ0 \u0338= {\u22121, 1}\u201d in section 2, but did not specify what value of Q0 they used in all the following simulations. We chose to use Q0 = 0, as this initial value is the middle point between the two possible outcomes (i.e., -1 and 1). As we replicated all the original figures, even the dynamics in the beginning of the learning curves (see Figures 2 A, 3 and 4 B), we believe the reference article must have used similar initial Q-values.\nSecond, regarding the parameter setting for Figure 1\u2019s simulations, the ratio of \u03b1+ over \u03b1\u2212 was said to be either 0.25, 1 or 4, but they did not specify what were the exact values of \u03b1+ and \u03b1\u2212 used. We thus set them according to the following description of the pessimistic, rational and optimistic agents in section 3, i.e.,:\n\u2022 \u03b1+ = 0.1 and \u03b1\u2212 = 0.4 for the ratio of 0.25 \u2022 \u03b1+ = 0.1 and \u03b1\u2212 = 0.1 for the ratio of 1 \u2022 \u03b1+ = 0.4 and \u03b1\u2212 = 0.1 for the ratio of 4\nThird, the number of iterations made to generate Figures 3 and 4 were not indicated, and we assumed the authors used the same number as in Figures 1 and 2 (i.e., 5,000 runs).\nFinally, in the reinforcement learning framework, the probabilities to choose each action are computed, then used to select an action through a pseudo-random generator. In the reference article, it was sometimes unclear whether the analyses were performed on the probabilities of choice, or rather the proportions of implemented choices. For example Figure 2\u2019s legend indicated: \u201cMean probability of choosing the best arm\u201d, suggesting that the probabilities themselves were used. However, when commenting the figure in section 3, the authors appeared to say that the actual choices were rather used: \u201cthe optimistic agent learns to take the best action significantly more than the rational agent\u201d. For our analyses, we started by using the probabilities of choice, as this would lead to more clear, less noise-corrupted results. However we then obtained very smooth learning curves, and were unable to reproduce the spikiness of the original Figures 2, 3 and 4. We thus computed the proportions of implemented choices for all our figures."}, {"heading": "Results", "text": "We numbered our figures in the same way as the reference article.\nAll our figures reproduced the patterns of the original results. We were even able to replicate the fine-grained details of the learning curves, like the early bumps in performance in the high-reward task (Figures 2 A, 3 and 4 B, right panels, around 50-100 trials). In Figure 1, the mean and the variance of the Q-values were also very similar as the ones in the original figure.\nThe only discrepancy we found was in Figure 4 A. Although the general pattern was replicated, our learning curves appeared smoother than in the reference article. As the number of simulations were not explicitly specified for this figure, we cannot know if this is due to us running a higher number of simulations than the reference article, or from another difference in model implementation."}, {"heading": "Conclusion", "text": "All the figures in Caz\u00e9 and Meer [2] have been successfully reproduced with high fidelity, and we confirm the validity of their simulations. Overall the whole replication\nReScience | rescience.github.io 5 - 2 Jun 2018 | Volume 4 | Issue 1\nReScience | rescience.github.io 5 - 3 Jun 2018 | Volume 4 | Issue 1\nReScience | rescience.github.io 5 - 4 Jun 2018 | Volume 4 | Issue 1\nprocedure was smooth: the models were implemented with low difficulty, and the simulations were quite straightforward apart from a few obscure details. We hope this replication can foster future research in the domain."}], "title": "[Re] Adaptive properties of differential learning rates for positive and negative outcomes", "year": 2018}