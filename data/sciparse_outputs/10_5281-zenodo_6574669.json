{"abstractText": "The presented study evaluates \u201dExacerbating Algorithmic Bias through Fairness Attacks\u201d by Mehrabi et al. [1] within the scope of theML Reproducibility Challenge 2021. We find it not possible to reproduce the original results from sole use of the paper, and difficult even in possession of the provided codebase. Yet, we managed to obtain similar findings that supported three out of the five main claims of the publication, albeit using partial re-implementations and numerous assumptions. On top of the reproducibility study, we also extend the work of the authors by implementing a different stopping method, which changes the effectiveness of the proposed attacks.", "authors": [{"affiliations": [], "name": "Matteo Tafuro"}, {"affiliations": [], "name": "Andrea Lombardo"}, {"affiliations": [], "name": "Tin Had\u017ei Veljkovi\u0107"}, {"affiliations": [], "name": "Lasse Becker-Czarnetzki"}, {"affiliations": [], "name": "Koustuv Sinha"}, {"affiliations": [], "name": "Sharath Chandra Raparthy"}], "id": "SP:458aeeffb143c0afc43dae93c5222444198e98c7", "references": [{"authors": ["N. Mehrabi", "M. Naveed", "F. Morstatter"], "title": "and A", "venue": "Galstyan. Exacerbating Algorithmic Bias through Fairness Attacks.", "year": 2020}, {"authors": ["P.W. Koh", "J. Steinhardt", "P. Liang"], "title": "Stronger Data Poisoning Attacks Break Data Sanitization Defenses.", "venue": "arXiv e-prints,", "year": 2018}, {"authors": ["D. Solans", "B. Biggio"], "title": "and C", "venue": "Castillo. Poisoning Attacks on Algorithmic Fairness.", "year": 2020}, {"authors": ["N. Mehrabi", "M. Naveed", "F. Morstatter", "A. Galstyan"], "title": "Exacerbating Algorithmic Bias through Fairness Attacks", "venue": "https://github.com/Ninarehm/attack. Accessed on: 23-01-2022.", "year": 2020}, {"authors": ["P.W. Koh", "J. Steinhardt", "P. Liang"], "title": "Stronger Data Poisoning Attacks Break Data Sanitization Defenses", "venue": "https: //github.com/kohpangwei/data-poisoning-journal-release. Accessed on: 23-01-2022.", "year": 2018}, {"authors": ["D. Dua", "C. Graff"], "title": "UCI Machine Learning Repository", "venue": "Last accessed on: 23-01-2022.", "year": 2017}, {"authors": ["J. Larson", "M. Roswell", "V. Atlidakis"], "title": "Compas analysis", "venue": "https://github.com/propublica/compas-analysis/. Last accessed on: 23-01-2022.", "year": 2016}, {"authors": ["E. Fehrman", "A.K. Muhammad", "E.M. Mirkes", "V. Egan"], "title": "and A", "venue": "N. Gorban. The Five Factor Model of personality and evaluation of drug consumption risk.", "year": 2017}, {"authors": ["N. Mehrabi"], "title": "Fairness- Statistical Equity: A Fairness Classification Objective", "venue": "https : / /github .com/Ninarehm/ Fairness. Accessed on: 23-01-2022.", "year": 2020}, {"authors": ["C. Dwork", "M. Hardt", "T. Pitassi", "O. Reingold"], "title": "and R", "venue": "Zemel. \u201cFairness through awareness.\u201d In:", "year": 2012}, {"authors": ["M. Hardt", "E. Price", "N. Srebro"], "title": "Equality of Opportunity in Supervised Learning.", "venue": "CoRR abs/1610.02413", "year": 2016}], "sections": [{"text": "R E S C I E N C E C"}, {"heading": "Replication / ML Reproducibility Challenge 2021", "text": "[Re] Exacerbating Algorithmic Bias through Fairness"}, {"heading": "Attacks", "text": "Matteo Tafuro1,2, ID , Andrea Lombardo1,2, ID , Tin Had\u017ei Veljkovi\u01071,2, ID , and Lasse Becker-Czarnetzki1,2, ID 1M.Sc. Artificial Intelligence, University of Amsterdam, 1012 WX Amsterdam, The Netherlands \u2013 2Equal contributions\nEdited by Koustuv Sinha,\nSharath Chandra Raparthy\nReviewed by Anonymous Reviewers\nReceived 04 February 2022\nPublished 23 May 2022\nDOI 10.5281/zenodo.6574669"}, {"heading": "Reproducibility Summary", "text": "The presented study evaluates \u201dExacerbating Algorithmic Bias through Fairness Attacks\u201d by Mehrabi et al. [1] within the scope of theML Reproducibility Challenge 2021. We find it not possible to reproduce the original results from sole use of the paper, and difficult even in possession of the provided codebase. Yet, we managed to obtain similar findings that supported three out of the five main claims of the publication, albeit using partial re-implementations and numerous assumptions. On top of the reproducibility study, we also extend the work of the authors by implementing a different stopping method, which changes the effectiveness of the proposed attacks."}, {"heading": "Scope of Reproducibility", "text": "The paper presents twonovel kinds of adversarial attacks against fairness: the IAF attack and the anchoring attacks. Our goal is to reproduce the five main claims of the paper. The first claim states that using the novel IAF attack we can directly control the trade\u2010 off between the test error and fairness bias metrics when attacking. Claims two to five suggest a superior performance of the novel IAF and anchoring attacks over the two baseline models. We also extend the work of the authors by implementing a different stopping method, which changes the effectiveness of some attacks."}, {"heading": "Methodology", "text": "To reproduce the results, we use the open\u2010source implementation provided by the au\u2010 thors as the main resource, although many modifications were necessary. Additionally, we implement the two baseline attacks which we compare to the novel proposed attacks. Since the assumed classifier model is a support vector machine, it is not computation\u2010 ally expensive to train. Therefore, we used a modern local machine and performed all of the attacks on the CPU."}, {"heading": "Results", "text": "Due tomanymissing implementation details, it is not possible to reproduce the original results using the paper alone. However, in a specific setting motivated by the authors\u2019 code (more details in section 3), we managed to obtain results that support 3 out of 5 claims. Even though the IAF and anchoring attacks outperform the baselines in certain\nCopyright \u00a9 2022 M. Tafuro et al., released under a Creative Commons Attribution 4.0 International license. Correspondence should be addressed to Matteo Tafuro (matteo.tafuro@student.uva.nl) The authors have declared that no competing interests exist. Code is available at https://github.com/imandrealombardo/FACT-AI. \u2013 SWH swh:1:dir:b775237e47e9de16827cb9cae83423d090faa4f8. Open peer review is available at https://openreview.net/forum?id=H4lzChGmhCK.\nReScience C 8.2 (#22) \u2013 Tafuro et al. 2022 1\nscenarios, our findings suggest that the superiority of the proposed attacks is not as strong as presented in the original paper."}, {"heading": "What was easy", "text": "The novel attacks proposed in the paper are presented intuitively, so even with the lack of background in topics such as fairness, we managed to easily grasp the core ideas of the paper."}, {"heading": "What was difficult", "text": "The reproduction of the results requires much more details than presented in the pa\u2010 per. Thus, we were forced to make many educated guesses regarding classifier details, defense mechanisms, and many hyperparameters. The authors also provide an open\u2010 source implementation of the code, but the code uses outdated dependencies and has many implementation faults, which made it hard to use as given.\nCommunication with original authors Contact was made with the authors on two occasions. First, we asked for some clarifica\u2010 tions regarding the provided environment. They promptly repliedwith lengthy answers, which allowed us to correctly run their code. Then, we requested additional details con\u2010 cerning the pre\u2010processing of the datasets. The authors pointed at some of their previ\u2010 ous projects, where we could find further information on the processing pipeline.\n1 Introduction\nMachine Learning models have shown impressive performance in countless domains in the last decade. However, it has been demonstrated that an adversary can input carefully\u2010crafted perturbations to subvert the predictions of these models. The area of Adversarial Machine Learning has emerged to study vulnerabilities of machine learn\u2010 ing approaches in adversarial settings and to develop techniques that make them robust against malicious attacks. Most of the research has focused on studying malign interventions that degrade the accuracy of a system: imagine, for example, the consequences of inducing wrong pre\u2010 dictions in an autonomous driving system. Only recently, fairness has become a rising concern for the performance of machine learning models, especially for sensitive fields such as criminal justice and loan decisions. Along these lines, \u201cExacerbating Algorith\u2010 mic Bias through Fairness Attacks\u201d [1] proposes two families of poisoning attacks that inject malicious points into the models\u2019 training sets and intentionally target the fair\u2010 ness of a classification model. The first, the influence attack, extends the optimization\u2010based technique introduced by Koh, Steinhardt, and Liang [2] by incorporating in the loss function a constraint for fair classification. An attacker can hence harm both accuracy and fairness simultaneously, with a trade\u2010off regularized via a parameter \u03bb. The second type of attack, the anchoring attack, affects solely fairness and aims to place poisoned data points to bias the decision boundary without modifying the attacker loss. Depending on whether the target point is chosen at random, anchoring attacks are classified as random or non-random.\n2 Scope of reproducibility\nThis report investigates the reproducibility of the original paper by Mehrabi et al. [1] and aims to verify its main claims. Since these heavily rely on the datasets and metrics\nReScience C 8.2 (#22) \u2013 Tafuro et al. 2022 2\nused by the authors, the reader is invited to consult Sections 3.2 and 3.3 \u2013 respectively \u2013 for a refresh of such concepts. Then, the main claims can be summarized as follows: \u2013 Influence Attack on Fairness (IAF):\n\u2022 Claim 1: Increasing the parameter \u03bb results in stronger attacks against fairness. Contrarily, for lower values themodel acts similarly to the original influence attack [2] targeted towards accuracy;\n\u2022 Claim2: Theproposed IAFoutperforms the attack of Koh, Steinhardt, andLiang [2] in affecting both fairness metrics (SPD and EOD), on all three datasets;\n\u2022 Claim 3: The proposed IAF also outperforms the attack based on the loss function proposed by Solans, Biggio, and Castillo [3] in affecting SPD and EOD, on all tested datasets.\n\u2013 Anchoring Attack:\n\u2022 Claim 4: Both randomandnon\u2010randomanchoring attacks (RAA andNRAA, respec\u2010 tively) outperform Koh, Steinhardt, and Liang [2] in degrading the SPD and EOD of the classification model, on all three datasets;\n\u2022 Claim 5: On the German and Drug Consumption datasets, RNAA and NRAA have a greater impact on fairnessmetrics (SPD and EOD) compared to the attack based on Solans, Biggio, and Castillo [3]. However, the latter outperforms the proposed an\u2010 choring attack in affecting fairness when classification is performed on the COM\u2010 PAS dataset.\n3 Methodology\nThe authors provided an open\u2010source implementation of their code on GitHub [4]. Un\u2010 fortunately, the repository has several issues: dependencies are not sufficiently speci\u2010 fied, and simply running the code in the given environment results in conflicts. Further\u2010 more, the code does not provide an option to run baseline methods used in the paper, nor does it include the essential hyperparameter \u03bb, which is used in the experiments. The majority of the code is based on Koh, Steinhardt, and Liang [2]\u2019s public implemen\u2010 tation [5], and a code coverage analysis revealed that more than 50% is not used for running experiments related to this paper1. Moreover, the repository comes with pre\u2010 processed datasets and while this may sound advantageous, there is no mention of the processing procedure in the paper nor on GitHub. Finally, the code is generally complex and hard to understand due to insufficient comments and documentation. Therefore, we used the codebase provided by the authors and customized it for our pur\u2010 poses. First, to aid maintainability and scalability, as well as to ensure future repro\u2010 ducibility of the original experiments, the code was modernized and made compatible with the latest version of every dependency. This involved major changes to migrate from Tensorflow 1.12.0 to 2.6.2 and to update CVXpy from version 0.4.11 to 1.1.182. Secondly, datasets were downloaded from the original sources [7, 8] and pro\u2010 cessed from scratch. The procedure is thoroughly reported in Section 3.2. Furthermore, the codewas trimmed down to the essential, and the userwas given the option to choose any of the available models and the corresponding parameters. Lastly, we added com\u2010 prehensive documentation to make the code more interpretable.\n3.1 Model descriptions It appears that the authors of the original paper donot specify themodel that they use for the given classification task. From the implementation details given in Koh, Steinhardt,\n1The coverage.py tool [6] was used to measure code coverage, and the study was performed considering all possible attacks\u2010datasets combinations.\n2In our repository we provide a YAML configuration file to quickly set up the required environment.\nReScience C 8.2 (#22) \u2013 Tafuro et al. 2022 3\nand Liang [2], as well as from [1]\u2019s codebase, we assume the use of a Support Vector Machine (SVM) trained with a smooth hinge loss and L2 regularization (refer to [2] for further details). Additionally, the optimization algorithm is not indicated; we assumed it to beNewtons Conjugate Gradient (Newton\u2010CG)method, as suggested by the codebase. Such a method is used for both the minimization of the parameters on the training set and the update step of the poisoned points (for attacks utilizing an adversarial loss). The gradient is computed using the full datasets, i.e., without using mini\u2010batches. Although hardly recognizable, this follows the implementation of the original paper: from our interpretation of the code, it seems that the authors define a variable containing the size of the mini\u2010batch size and the necessary functionality, but then never use it. Our base algorithmic setup for the IAF, RAA, and NRAA attacks is described in theMethods section of the original paper. However, the authors omitted important details that we consequently had to assume based on more or less concrete evidence. First, an ad\u2010 vantaged and disadvantaged group for the sensitive attribute (i.e., gender, as per the original work) has to be specified for all attacks. Since the rationale behind this choice does not seem to be included in the paper, we infer from the codebase that the authors did it automatically and deduced it from the datasets. More specifically, we assume that the advantaged group is chosen as the group with the highest ratio of data points with positive label (y = 1), regardless of the actual class label it corresponds to. This method is simple yet fallacious: for instance, itmeans that the group taking on the label \u201dlikely to perform a crime soon\u201d more often (in the context of the COMPAS dataset) is considered \u201dadvantaged\u201d in terms of the algorithm. Secondly, for the computation of the feasible set using an anomaly detector B, we as\u2010 sume that the intersection of the Slab defense and the L2 defense was originally em\u2010 ployed, as described in Koh, Steinhardt, and Liang [2]. For reprojecting poisoned data points into the feasible set, we again use the approach of [2], which incorporates LP rounding for discrete variables. Moreover, we implement twobaselines. The three proposed attacks are compared against the original accuracy\u2010targeting attack proposed by Koh, Steinhardt, and Liang [2], and another attack that uses a loss function proposed by Solans, Biggio, and Castillo [3], which targets fairness3. Lastly, themodel\u2010specific changes/improvements are presented below:\nIAF. As mentioned before, we modified the code to include the hyperparameter \u03bb which controls the trade\u2010off between the accuracy and the fairness loss in the adver\u2010 sarial loss.\nKoh attack. We were not able to find a way of running this baseline attack using the given codebase. Wehave decided to implement it from scratch, treating it as the limiting case of the IAF attack when \u03bb = 0 (meaning no fairness loss in the adversarial loss function). Consequently, it is not exactly as presented in [2]: in the original Koh attack sampling, the initial poisoned points are not drawn from advantaged and disadvantaged groups, contrary to the IAF attack. However, we argue that equalizing the sampling method provides a stronger comparison between the two methods, as we alleviate the issue of the missing inductive bias from the original Koh influence attack.\nSolans attack. This attack serves as the second baseline. We could not find it in the codebase, thus we implemented it by replacing the adversarial loss in the IAF attack with a weighted sum loss, as presented in [3]. Implementing this change posed a bigger issue than expected, due to the inflexibility of the TensorFlow\u2010based implementation. Thus, major revisions were required.\n3For simplicity, we will refer to the influence attack presented in [2] as the Koh attack, and we will also refer to the attack presented in [3] as the Solans attack.\nReScience C 8.2 (#22) \u2013 Tafuro et al. 2022 4\n3.2 Datasets The authors provide compressed npz files of the three real\u2010world datasets used for their experiments \u2013 the German Credit Dataset [7], the COMPAS Dataset [8] and the Drug Con\u2010 sumption Dataset [7]. However, these are already pre\u2010processed, and the processing procedure is not reported nor documented in the code. This constitutes an important re\u2010 producibility barrier, because raw datasets4 are not directly usable with the given code\u2010 base. In this section, we present our pre\u2010processing pipeline, which was mainly determined by reverse engineering of the given files. Like the authors, we provide a set of npz files containing already\u2010processed data to run our implementation, but we also include the scripts used to pre\u2010process each dataset in the Custom_data_preprocessing di\u2010 rectory. Lastly, to run the attacks, we assume that the advantaged and disadvantaged groups are males and females respectively. We accordingly map them to 0 and 1 to cre\u2010 ate the group_label binary array. In the rest of this section, we outline our dataset\u2010specific details of the pre\u2010processing pipeline and the assumptions that were made for the sake of reproducibility of the orig\u2010 inal results.\nGermanCredit Dataset. The dataset contains the credit profile of 1000 individuals with 20 attributes associated with each person. In our experiments, we use all of them, as in [1]. The attributes are both numerical and categorical, and we assumed the original authors used one-hot representations to encode the latter. The assumptionwas based on an extensive study of the provided datasets, with particular attention to their shapes. We then autonomously standardize the data, as it is common practice inMachine Learning, and split the data into an 80\u201020 train and test split, as indicated in the original paper.\nCOMPAS Dataset. ProPublica\u2019s COMPAS dataset [8] contains information about 7214 defendants from Broward County. We use the features specified in Table 1 of [1]. In this case, based on the provided dataset, we concluded that the authors must have used numerical label encoding to represent the categorical attributes. Finally, we standardize the data and split it into an 80\u201020 train and test split.\nDrugConsumptionDataset. Thedataset contains information about the drug consump\u2010 tion of 1885 individuals [9]. We use the attributes indicated in Table 1 of the original pa\u2010 per. The pre\u2010processing procedure is as follows: first, we binarize the categorical data linked to cocaine consumption into users and non-users. Intuitively, non\u2010users should be mapped to 0 (and 1 in the opposite case), but an inspection of the provided npz file sug\u2010 gests that the authors reversed the mapping. We decided to adhere to their choice for the sake of reproducibility. Moreover, we suspect that the dataset was shuffled before splitting it into training and test sets5. By doing so, we obtain similar results in the ex\u2010 periments. Finally, we standardize the data. The original processing of this dataset was particularly difficult to replicate, because contrary to what was reported in the paper, the authors did not follow an exact 80\u201020 train and test split. Rather, the two contained 1500 and 385 data points respectively. To conclude, it is noteworthy that even the pre\u2010processed datasets provided by the au\u2010 thors are not immediately usable: the position (specified as index) of the sensitive fea\u2010 ture (i.e., gender) is different for each dataset and is only given for the German dataset in the running instructions. To account for this unnecessary confusion, our custom pre\u2010processing procedure includes the moving of the gender column to the 0th index,\n4The German Credit Dataset and the Drug Consumption Dataset can be downloaded from the UCImachine learning repository [7], while the COMPAS can be found in the corresponding GitHub repository[8].\n5The main author followed a similar pre\u2010processing procedure in another project that is publicly available on their GitHub [10].\nReScience C 8.2 (#22) \u2013 Tafuro et al. 2022 5\nwhich is taken as default by the main function. In this way, we simplify the running instructions and make them coherent across datasets. Still, the user is given the ability to pass the sensitive feature index as an argument, to facilitate future experiments on different and untested data.\n3.3 Metrics The attacks are evaluated in terms of accuracy and fairness. Along with classification (test) error, the original paper uses two importantmetrics to evaluate the attack in terms of fairness: Statistical Parity Difference and Equality of Opportunity Difference.\nStatistical Parity Difference. Statistical Parity Difference (SPD) was first introduced by Dwork et al. [11] and is used to capture the predictive outcome differences between different (advantaged and disadvantaged) demographic groups. The mathematical for\u2010 mulation is reported in Equation 1.\nSPD = \u2223\u2223\u2223 p(Y\u0302 = +1 | x \u2208 Da)\u2212 p(Y\u0302 = +1 | x \u2208 Dd)\u2223\u2223\u2223 (1)\nwhere Da denotes the advantageous group and Dd denotes the disadvantageous group.\nEquality of Opportunity Difference. Equality of Opportunity Difference (EOD) (Hardt, Price, and Srebro [12]) captures differences in the true positive rate between different (advantaged anddisadvantaged) demographic groups. It is defined as shown inEquation 2.\nEOD = \u2223\u2223\u2223 p(Y\u0302 = +1 | x \u2208 Da, Y = +1)\u2212 p(Y\u0302 = +1 | x \u2208 Dd, Y = +1)\u2223\u2223\u2223 (2)\n3.4 Experimental setup and hyperparameters All experiments shown in this paper can easily be reproduced using our code, which is publicly available on GitHub6. There we also provide technical details on how to run experiments and test different attacks in various settings. In this section, however, we list some additional details necessary to replicate the exact setup.\n\u2022 The original code constrains themaximum iterations of an attack to 10000 anduses early stopping to interrupt training if the accuracy on the test set does not decrease for a specific number of iterations, which is hardcoded to be 2. We follow this strategy but adapt it for our experiments. First, we implement early stopping on both accuracy and fairness, meaning that the user can also choose to stop training in the absence of changes in fairness. Weutilize average fairness (SPD+EOD)/2 as the stopping criteria7 since the twometrics have similar behavior and equal range [0, 1]. Then, we set the early stopping patience as a controllable hyperparameter.\n\u2022 It is unclear from the paper how the best\u2010performing model was selected by the authors. The code suggests theusage of themodel after the last attack iteration and training of themodel parameters. Instead, we decided to save the best\u2010performing model on the test set according to the chosen stopping metric (average fairness or accuracy), to better reflect the actual best performance. By selecting the best model based on fairness, we hope to choose more relevant states of the poisoned data affecting the fairness metrics. We compare the results in Section 4.\n\u2022 The computation of the feasible set and the reprojection of poisoned points onto it is handled as a convex optimization problem (see [2]). Since we upgraded CVXpy\n6https://github.com/imandrealombardo/FACT-AI 7In the rest of the paper, we might refer to it simply as the fairness stopping metric.\nReScience C 8.2 (#22) \u2013 Tafuro et al. 2022 6\nto its newest version, we can let the library select the most appropriate solver for the given problem, instead of specifying one (the authors of [1] seem to have used the SCS solver).\n\u2022 Following the original implementation, we utilize the fmin_ncg optimizer of the scipy library [13] for theNewton\u2010CG optimization. We complywith the choices of the authors and set the convergence threshold of the fmin optimizer to 10\u22128, and the maximum number of iterations to 100. We follow the implementation details specified in [2] for computing the inverse Hessian\u2010vector.\n\u2022 During training, the temperature of the smooth hinge loss is chosen to be 0.001, as found hardcoded in the original implementation. The value for the weight decay is set to 0.09 for all datasets (apart from the code of the authors, this assumption is also backed up by the main experiments of Koh, Steinhardt, and Liang [2]). The step size utilized in the IAF algorithm (and thus also in the Koh and Solans attack) is set to 0.1 for all experiments, as found in the codebase.\n\u2022 The threshold of the anomaly detector (see [2]) is controlled by a hyperparameter named \u201dpercentile\u201d, which specifies the percentage of the data left after apply\u2010 ing the anomaly detector. We first experimented with a value of 95 as suggested by Koh, Steinhardt, and Liang [2] but, as this seemed to lead to some training failings, we settled on 90 (the default value given in the codebase).\n\u2022 The number of injected poisoned points is proportional to the number of clean data points, such that |Dp| = \u03f5|Dp| (where Dc and Dp are the set of clean and poisoned data points respectively). The authors control such quantity by using the proportionality factor \u03f5 as a changeable parameter. Accordingly, we do the same and also make \u03bb a controllable parameter.\n\u2022 After careful inspection and testing of the authors\u2019 code, the EOD metric calcula\u2010 tionwas found to be faulty andwas consequently re\u2010implemented. Our adaptation is based on the paper that originally proposed it [12] and inspired by the implemen\u2010 tation found in the AIF360 library [14].\n\u2022 Finally, the distance to original points in anchoring attacks \u03c4 was set to 0 for all experiments, as in the original paper.\n\u2022 The random seed in all experiments was set to 1.\n3.5 Computational requirements To give a complete overview of our experimental setup, we collect the average runtimes per iteration for different datasets and types of attacks. These are presented in Table 1. All models have been trained on a local machine with an AMD Ryzen 5 5600x CPU (6 cores, Base clock 3.7 GHz). Since the datasets are small, there is no need for more than 4Gb of RAM. In this sense, training should be virtually possible on any entry\u2010level PC.\n4 Results\n4.1 Results reproducing original paper As stated in Section 2, five main claims were identified in the original paper. In our specific setting, we were able to reproduce three of these, as summarized in Table 2. In this section we elaborate on our reproduction results: first, in section 4.1.1 we show the effect of the hyperparameter \u03bb on variousmetrics (Claim 1). In section 4.1.2 we compare the newly proposed attacks and the baselines (Claims 2-5).\nReScience C 8.2 (#22) \u2013 Tafuro et al. 2022 7\n0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0 Lambda ( )\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTe st\nE rro\nr\nGerman Dataset = 0.0 = 0.1 = 0.5 = 1.0\n0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0 Lambda ( )\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSt at\nist ica\nl P ar\nity D\niff er\nen ce\n(S PD\n)\nGerman Dataset = 0.0 = 0.1 = 0.5 = 1.0\n0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0 Lambda ( )\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nEq ua\nlit y\nof O\npp or\ntu ni\nty D\niff er\nen ce\n(E OD\n)\nGerman Dataset = 0.0 = 0.1 = 0.5 = 1.0\n0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0 Lambda ( )\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTe st\nE rro\nr\nCompas Dataset\n= 0.0\n= 0.1\n= 0.5\n= 1.0\n0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0 Lambda ( )\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSt at\nist ica\nl P ar\nity D\niff er\nen ce\n(S PD\n)\nCompas Dataset\n= 0.0\n= 0.1\n= 0.5\n= 1.0\n0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0 Lambda ( )\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nEq ua\nlit y\nof O\npp or\ntu ni\nty D\niff er\nen ce\n(E OD\n)\nCompas Dataset\n= 0.0\n= 0.1\n= 0.5\n= 1.0\n0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0 Lambda ( )\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTe st\nE rro\nr\nDrug Dataset = 0.0 = 0.1 = 0.5 = 1.0\n0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0 Lambda ( )\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSt at\nist ica\nl P ar\nity D\niff er\nen ce\n(S PD\n)\nDrug Dataset = 0.0 = 0.1 = 0.5 = 1.0\n0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0 Lambda ( )\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nEq ua\nlit y\nof O\npp or\ntu ni\nty D\niff er\nen ce\n(E OD\n)\nDrug Dataset = 0.0 = 0.1 = 0.5 = 1.0\nStopping metric: accuracy\nFigure 1. Influence of \u03bb on the different metrics for different \u03f5 on the German dataset, using accu\u2010 racy as the stopping criteria during training.\nEffect of\u03bb on the differentmetrics \u2014 To verifyClaim1, we conducted the same experiment as the authors. We run an IAF attack for each dataset using different \u03f5 values and increas\u2010 ing \u03bb, to recreate Figure 3 of the original paper (see Appendix B.3, Fig. 8). However, compared to the original experiment we test a larger range of \u03bb values (from 0 0 to 2.0) to gain better insights into its effects. As depicted in Figure 1, increasing \u03bb does result in stronger attacks against fairness. Here we use the German dataset and accuracy as the stopping metric, but similar trends were observed on the other datasets and using fairness for early stopping. The plots are included in Appendix B.1 for the sake of com\u2010 pleteness. Therefore in this specific setup, we were able to reproduce the claim.\nComparison between the proposed attacks and the baselines \u2014 To investigate Claims 2-5 we de\u2010 sign an experiment that is heavily inspired by the work of the authors. We perform each attack on each dataset, fixing \u03bb = 1 and gradually increasing \u03f5 (from 0.0 to 1.0, with steps of 0.1), and repeat this procedure for each stopping metric. The results essentially repli\u2010 cate Figure 2 of the original paper (as seen in Appendix B.3, Fig. 7) and are collected in Figures 5 and 6 of Appendix B.2. However, to facilitate a comparative study between the proposed attacks and the baselines, we average the metrics over the \u03f5 values and report the results in Table 3. In this way, we can base our observations on quantifiable measures instead of solely using visual inspection. Assuming that the authors used accuracy as the early stopping criteria, the correspond\u2010 ing values in the table reveal that \u2013 in this specific setting:\n\u2022 Claim 2 is reproducible. On average, IAF has a much stronger influence on SPD and EOD compared to Koh\u2019s attack, on all three datasets.\n\u2022 Claim 3 is not reproducible, because Solan\u2019s attack outperformed IAF in affecting the EOD on the Compas dataset.\n\u2022 Claim4 is reproducible. NRAAandRAAwere found to degrade the fairnessmetrics (SPD and EOD) more than Koh\u2019s attack, on all three datasets.\nReScience C 8.2 (#22) \u2013 Tafuro et al. 2022 8\n\u2022 Claim 5 is not reproducible. Solans\u2019 attack had a greater impact on the SPD than NRAA on the German and greater impact than NRAA on both SPD and EOD on the Compas dataset. It also has a greater impact on the EOD than the RAA attack on the Compas dataset.\n4.2 Results beyond the original paper: using fairness as the early stopping metric While the original codebase seems to use accuracy as the early stopping metric (and hence for selecting and saving the best model), we investigate the change in the results if fairness is used instead. The main motivation behind such an experiment lies in the assumption that interrupting training based on the fairnessmeasures supposedly yields more relevant states of the poisoned data, effectively resulting in more efficient attacks against fairness. Since the SPD and EOD have similar behavior and equal range [0, 1], we employ average fairness (SPD + EOD)/2 for the task at hand. Figure 2 depicts the test accuracy and the average fairness over epochs for two differ\u2010 ent dataset\u2010attack combinations. An analysis of the curves confirms that the maximum achievable average fairness is much greater than the same measure at the point of min\u2010 imal accuracy (see Table 4). The same phenomenon is observed for any dataset\u2010attack combinations, as reported in Table 3: fairness undergoes a stronger degradation if av\u2010 erage fairness is used to interrupt the training process and save the best model. This is reflected in the corresponding values of the fairness measures, which appear much higher compared to when accuracy is used.\n5 Discussion\nOur reproduction reveals that although the proposed methods represent valid novel at\u2010 tacks against the fairness of a model, they are not always superior to other methods\nReScience C 8.2 (#22) \u2013 Tafuro et al. 2022 9\nin the literature. IAF showed important performance in terms of SPD and EOD degra\u2010 dation, but anchoring attacks were outperformed by the baseline models on multiple occasions. This result conflicts with the findings of the main paper (see Appendix B.3, Fig.7)where the baselines are generally inferior to the proposed attacks. Wehad tomake several assumptions to solve issues and inconsistencies between the original paper and corresponding implementation (many of which have already been mentioned through\u2010 out the report, but we systematically collect them in Appendix A). These assumptions are, by definition, uncertain andmight have been the cause of the discrepant results. To better understand the source of discrepancy, we initially planned to perform an ablation study, which would have also unveiled more information regarding the model\u2019s behav\u2010 ior. This was ultimately not possible, given the time constraints and the contingencies encountered in the reproduction process. In the remainder of this section, we elaborate on the main claims and our ability to reproduce them. We then present some personal reflections on the overall execution of the work and conclude with a summary and look into future works.\n5.1 Discussion of the results The first claim was found to be reproducible under our experimental setup, as we ex\u2010 pected. The parameter \u03bb is specifically designed to control the trade\u2010off between accu\u2010 racy and fairness, hence a rejection of the claim would have implied a major flaw in the core idea of the paper. The other claims focused on the comparison with the two baselines and, while the results presented in Section 4.1.2 are explicative enough, some remarks are still noteworthy. In general, better statistics of the results would give us a clearer insight into the relative performance of the models. However, only four weeks were allocated for this project and we were unable to re\u2010run the experiments with multiple seeds. For example, the Solans attack outperformed the IAF attack in terms of EODmetric on theCompas dataset (when using accuracy as the stopping method) and led to the non\u2010reproducibility of Claim 3. Yet, this difference is relatively small and a measure of uncertainty could po\u2010 tentially reverse our decision. Furthermore, it was shown that the final fairness metrics can highly vary depending on the chosen stopping method. This is especially prominent for Claim 4, which was accepted under the assumption that accuracy was used for stopping and saving the best model. In reality, Koh attack outperforms NRAA on both Compas and Drug datasets in the terms of SPD/EODmetrics, if fairness is used instead. Since the validity of the claim depends on the stopping metric of choice, we argue that the claim is much weaker than originally proposed. Similarly, compare the IAF and the Koh attack in terms of fairness measures, using accuracy as the stopping criteria. On the Drug dataset, IAF\u2019s SPD/EOD metrics are respectively 2.89\u00d7/2.62\u00d7 higher than Koh\u2019s. This gap tightens if fairness is used: IAF\u2019s SPD/EOD metrics become 1.022\u00d7/1.024\u00d7 higher. Although these numbers indicate the same result, we find the claim to be weaker than proposed, as the superior performance of the IAF attack is diminished by the use of a different stopping metric. Finally it is important to notice the different behavior of the test accuracy and the aver\u2010 age fairness (Fig. 2) used as stopping criteria. While the latter has a relatively high vari\u2010 ance, the former is pretty constant, meaning that using fairness as the stopping metric does not result in significant variations in the model\u2019s accuracy. Contrarily, as empiri\u2010 cally proved by our experiments, it can be highly beneficial for the fairness measures.\n5.2 Reflection: What was easy? What was difficult? The new methods presented in the paper were described both intuitively and formally, with a clear mathematical structure. The authors also provided figures to aid the in\u2010 tuition on how new attacks can affect decision boundaries, which allowed us to easily understand the core novel ideas presented in the publication.\nReScience C 8.2 (#22) \u2013 Tafuro et al. 2022 10\nHowever, it was not trivial to re\u2010implement the proposed methods, because many de\u2010 tails required for the implementation do not appear in the paper. The provided open\u2010 source implementationwas ultimately hard to follow due to its convoluted organization, lack of documentation, poorly named functions/variables, and abundance of unused code. Even setting up a working environment using the authors\u2010given dependencies took longer than one would expect, prompting us to get help from the authors. Eventu\u2010 ally, the hope to aid future experiments motivated the decision to make the code com\u2010 patible with up\u2010to\u2010date dependencies. This was one of the biggest struggles because the codebase heavily relies on packages that underwent major updates (e.g. TensorFlow and CVXpy). The authors also provided pre\u2010processed datasets. We spent a considerable amount of time trying to replicate their exact pipeline through reverse\u2010engineering of the given files. Additionally, after recognizing some imperfections in the code and inconsisten\u2010 cies with the paper, we verified all of the existing implementation details to make sure that no further errors were made. This was a daunting task, given the complete lack of documentation and intuitive variable use.\n5.3 Communication with original authors To reiterate, we have initially contacted the main author to aid us with the dependency issues, who helped us with setting up a working environment. We then had additional contacts regarding the dataset pre\u2010processing procedure. The author provided us with some indications on the pipeline and pointed at some useful resources. Eventually, we decided to gain a better understanding of the datasets through reverse\u2010engineering.\n5.4 Conclusion In this paper, we have presented a reproducibility study of \u201dExacerbating Algorithmic Bias through Fairness Attacks\u201d, whereon we can draw some conclusions. Due to all the mentioned issues and inconsistencies (collected in Appendix A), we find it not possi\u2010 ble to reproduce the original results from sole use of the paper, and difficult even in possession of the provided codebase. Yet, we managed to obtain similar findings that supported three out of the five main claims of the publication, albeit using partial re\u2010 implementations and numerous assumptions. Ascertaining the validity of such assump\u2010 tions is therefore important for future works. Moreover, further studies could extend the classifier to work with multiple demographic groups and investigate the results us\u2010 ing different fairness metrics."}, {"heading": "Appendix", "text": "A Table of issues\nIssue Our contribution\nRunning the code in the given environment results in conflicts Codemodernized andmade compatible with the lat\u2010 est version of every dependency\nThe code is generally complex and hard to under\u2010 stand due to insufficient comments and documen\u2010 tation as well as leftover code Trimmed down the code to the essential, included option to choose any of the availablemodels and the corresponding parameters. Added comprehensive documentation tomake the codemore interpretable\nIt appears that the pre\u2010processing pipeline of the given datasets is not specified Made the scriptsweused to pre\u2010process eachdataset available as well as a detailed description\nIt appears that the position (i.e. index) of the sensi\u2010 tive feature for the COMPAS and Drug Consumption datasets is not indicated, posing a challenge to re\u2010 produce the authors\u2019 results Moved the sensitive feature (i.e. gender) of every dataset to the 0th index, which is taken as default by the main function\nThe advantaged and disadvantaged groups for the sensitive attribute (gender) has not be specified for any attack Assumed from the codebase that the authors did this automatically and inferred it from the dataset (the advantaged group is chosen as the group with a higher ratio of datapoints with the positive label (y=1), regardless of the actual class label it corre\u2010 sponds to) to be specified for all attacks\nThe code does not provide an option to run baseline methods used in the paper, nor does it include the hyperparameter \u03bb Included option to run baseline methods (Koh at\u2010 tack, Solans attack) and to include \u03bb in IAF attack\nThe code implements a deterministic point sam\u2010 pling in the anchoring attacks (RAA, NRAA) due to the same seed being reset in every attack iteration. Thus the sampling yields the same point every iter\u2010 ation not properly applying the randomness Fixed the issue so that randomness takes effect\nThe code makes use of a faulty EOD metric calcula\u2010 tion Re\u2010implemented the EOD metric calculation to fix the issue\nThe paper specifies the feasible set computation to be done on the union of the clean dataset and the initial poisoned points. The original code however does this on the clean data only when using the run\u2010 ning commands given by the authors Implemented the feasible set as specified in the pa\u2010 per\nIt appears that the model used for the given classifi\u2010 cation task is not specified Assumed they used a Support VectorMachine (SVM) trained with a smooth hinge loss and L2 regulariza\u2010 tion\nThe optimization algorithm is not indicated Assumed it to be Newtons Conjugate Gradient (Newton\u2010CG) method, as suggested by the codebase\nIt is unclear how the best performing model was se\u2010 lected Saved best performing model on the test set accord\u2010 ing to the chosen stopping metric\nB Additional figures\nHere we collect additional figures that support the results discussed above.\nB.1 Effect of \u03bb on the different metrics Figure 3 shows the influence of \u03bb on the different metrics when accuracy is used as the stopping criteria. The experiment is repeated using average fairness as the stopping met\u2010 ric, and the results are collected in Figure 4. These results support Claim 1 of Section 2,\nReScience C 8.2 (#22) \u2013 Tafuro et al. 2022 13\neffectively proving it.\nB.2 Comparative study between the proposed attacks and the baselines We report the results of the experiment designed to support Claims 2-5 of Section 2. We perform each attack (IAF, NRAA, RAA, Koh, Solans) on each dataset (German, Compas, Drug), fixing\u03bb = 1 and gradually increasing \u03f5 from0.0 to 1.0, with steps of 0.1. We repeat this procedure for each stoppingmetric (average fairness and accuracy). The results are respectively collected in Figures 4 and 5.\nB.3 Figures of the original paper For the sake of self\u2010containedness of this reproducibility study, we report the two main figures of the original paper. Figures 7 and 8 correspond \u2013 respectively \u2013 to Figures 2 and 3 of \u201dExacerbating Algorithmic Bias through Fairness Attacks\u201d [1].\nReScience C 8.2 (#22) \u2013 Tafuro et al. 2022 14\nReScience C 8.2 (#22) \u2013 Tafuro et al. 2022 15\nReScience C 8.2 (#22) \u2013 Tafuro et al. 2022 16\nmeasures on three different datasets (German Credit, COMPAS, and Drug Consumption) with different \u270f values.\neffective since more poisoned points may be needed in order\nto achieve the goal of infecting many points and shifting the decision boundary. In our experiments we set \u2327 = 0.\nInfluence Attack (Koh et al.) This is a type of attack that is targeted only toward affecting accuracy (Koh, Steinhardt, and Liang 2018; Koh and Liang 2017). The reason we include this type of attack along with attacks targeted toward fairness is that it can help us understand how attacks targeting only accuracy affect fairness measures. Attacks of this nature can also serve as a good comparison because they show the effect of attacks on accuracy; because this attack is specifically designed to target accuracy, it can be a strong method to compare against.\nPoisoning Attack Against Algorithmic Fairness (Solans et al.) In (Solans, Biggio, and Castillo 2020), the authors propose a loss function that claims to target fairness measures. We utilized the loss introduced in this paper as depicted below in equation (3) in the influence attack from (Koh, Steinhardt, and Liang 2018; Koh and Liang 2017) and compared it to our proposed attacks. The goal of (Solans, Biggio, and Castillo 2020) was to incorporate the loss in (3) into an attack strategy that would maximize the loss;\nthus, we incorporated this loss into the influence attack (Koh,\nSteinhardt, and Liang 2018; Koh and Liang 2017), which we found to be a strong attack strategy in maximizing the loss and also the same attack strategy used in our influence attack on fairness. In our experiments, we utilized the same value as proposed in (Solans, Biggio, and Castillo 2020) to balance the class priors.\nLadv(\u2713\u0302;Dtest) = pX\nk=1\n`(\u2713\u0302;xk, yk)\n| {z } disadvantaged\n+ mX\nj=1\n`(\u2713\u0302;xj , yj)\n| {z } advantaged\nwhere = p\nm . (3)\nResults The results in Figure 2 demonstrate that the influence attack (Koh et al.), although performing remarkably well in attacking accuracy, does not attack fairness well. The results also confirm that our influence attack on fairness method outperforms (Solans et al.) (Solans, Biggio, and Castillo 2020) in affecting fairness measures, and anchoring attack outperforms (Solans et al.) (Solans, Biggio, and Castillo 2020) in\nFigure 3: Results obtained for different lambda values for the IAF attack with regards to different fairness (SPD and EOD) and\naccuracy (test error) measures on three different datasets (German Credit, COMPAS, and Drug Consumption) with different \u270f.\naffecting fairness measures in most of the cases. One can observe that influence attack on fairness is the most effective amongst all the attacks in attacking fairness measures.\nDue to the nature of our influence attack on fairness loss function and its controlling parameter on accuracy and fairness, it can be utilized in scenarios where the adversary wants to maliciously harm the system in terms of accuracy, or fairness, or both. On the other hand, anchoring attacks can be utilized in places where the adversary wants to subtly harm accuracy with an effective harm on fairness. These types of attacks can be used by, e.g., adversaries who would want to gain profit off of biasing decisions for their benefit; thus, to remain less detectable they do not harm accuracy. Although it is possible that anchoring attack can harm accuracy to a higher degree, as shown empirically in our results, it is less likely that anchoring attack is able to degrade accuracy by a large amount in practice for real world datasets.\nIn addition, in Figure 3 we demonstrate the effect of our regularized loss in the influence attack on fairness. The results show that with the increase of lambda the attack affects fairness measures more as expected from the loss; however, for the lower lambda values the attack acts similar to the orig-\ninal influence attack targeted towards accuracy. The results also show that higher epsilon values highlight the behavior of the loss more as expected such that for high epsilon value of 1 the changes are more significant with modifications to the lambda value in the loss function, while less subtle for lower epsilon values such as 0.1.\nRelated Work\nHere, we cover related work from both fair machine learning as well as adversarial machine learning research.\nAdversarial Machine Learning Research in adversarial machine learning is mostly focused on designing defenses and attacks against machine learning models (Steinhardt, Koh, and Liang 2017; Chakraborty et al. 2018; Li et al. 2018). Ultimately, the goal is for machine learning models to be robust toward malicious activities designed by adversaries. Thus, it is important to consider both sides of the spectrum in terms of designing the attacks and defenses that can overcome the attacks. In adversarial machine learning, different types of attacks, such as data poisoning and evasion attacks, exist. In evasion attacks, the goal is to\nFigure 8. R ults of the original paper ob ined for diff rent \u03bb values fo the IAF a tackwith regards to different fairness (SPD and EOD) and accuracy (test error) measures on three different datasets (German Credit, COMPAS, and Drug Consumption) with different \u03f5. Retrieved from [1].\nReScience C 8.2 (#22) \u2013 Tafuro et al. 2022 17"}], "title": "[Re] Exacerbating Algorithmic Bias through Fairness Attacks", "year": 2022}