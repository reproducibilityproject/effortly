{"abstractText": "In this study, we present our results and experience during replicating the paper titled \u201dLifting 2D StyleGAN for 3D-Aware Face Generation\u201d [1]. Thiswork proposes amodel, called LiftedGAN, that disentangles the latent space of StyleGAN2 [2] into texture, shape, viewpoint, lighting components and utilizes those components to render novel synthetic images. This approach claims to enable the ability of manipulating viewpoint and lighting components separately without altering other features of the image. We have trained the proposed model in PyTorch [3], and have conducted all experiments presented in the original work. Thereafter, we have written the evaluation code from scratch. Our re-implementation enables us to better compare different models inferring on the same latent vector input. Wewere able to reproducemost of the results presented in the original paper both qualitatively and quantitatively.", "authors": [{"affiliations": [], "name": "Do\u011fa Y\u0131lmaz"}, {"affiliations": [], "name": "Furkan K\u0131nl\u0131"}, {"affiliations": [], "name": "Bar\u0131\u015f \u00d6zcan"}, {"affiliations": [], "name": "Furkan K\u0131ra\u00e7"}, {"affiliations": [], "name": "Koustuv Sinha"}, {"affiliations": [], "name": "Sharath Chandra Raparthy"}], "id": "SP:5be89586742448d53a102dda86b1a698da897835", "references": [{"authors": ["Y. Shi", "D. Aggarwal", "A.K. Jain"], "title": "Lifting 2D StyleGAN for 3D-Aware Face Generation", "year": 2021}, {"authors": ["T. Karras", "S. Laine", "M. Aittala", "J. Hellsten", "J. Lehtinen", "T. Aila"], "title": "Analyzing and Improving the Image Quality of StyleGAN", "year": 2020}, {"authors": ["A. Paszke"], "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library.", "venue": "Advances in Neural Information Processing Systems 32", "year": 2019}, {"authors": ["T. Karras", "S. Laine", "T. Aila"], "title": "A Style-Based Generator Architecture for Generative Adversarial Networks. 2019", "year": 2019}, {"authors": ["Y. Choi", "Y. Uh", "J. Yoo", "J.-W. Ha"], "title": "StarGAN v2: Diverse Image Synthesis for Multiple Domains", "year": 2020}, {"authors": ["Z. Liu", "P. Luo", "X. Wang", "X. Tang"], "title": "Deep Learning Face Attributes in the Wild.", "venue": "Proceedings of International Conference on Computer Vision (ICCV)", "year": 2015}, {"authors": ["Y. Tian", "X. Peng", "L. Zhao", "S. Zhang", "andD"], "title": "N.Metaxas.CR-GAN: Learning Complete Representations forMulti-view Generation", "year": 2018}, {"authors": ["L. Tran", "X. Yin", "X. Liu"], "title": "Disentangled Representation Learning GAN for Pose-Invariant Face Recognition.", "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "year": 2017}, {"authors": ["Y. Hu", "X.Wu", "B. Yu", "R. He", "Z. Sun"], "title": "Pose-Guided Photorealistic Face Rotation.", "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition", "year": 2018}, {"authors": ["Y. Deng", "J. Yang", "D. Chen", "F. Wen", "X. Tong"], "title": "Disentangled and Controllable Face Image Generation via 3D Imitative-Contrastive Learning", "year": 2020}, {"authors": ["T. Nguyen-Phuoc", "C. Li", "L. Theis", "C. Richardt", "Y.-L. Yang"], "title": "HoloGAN: Unsupervised learning of 3D representations from natural images. 2019", "year": 1904}, {"authors": ["M. Heusel", "H. Ramsauer", "T. Unterthiner", "B. Nessler", "S. Hochreiter"], "title": "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium", "year": 2018}, {"authors": ["J. Johnson", "A. Alahi", "L. Fei-Fei"], "title": "Perceptual Losses for Real-Time Style Transfer and Super-Resolution", "year": 2016}, {"authors": ["K. Simonyan", "A. Zisserman"], "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition", "year": 2015}], "sections": [{"text": "R E S C I E N C E C Replication / ML Reproducibility Challenge 2021\n[Re] Lifting 2D StyleGAN for 3D-Aware Face Generation"}, {"heading": "Do\u011fa Y\u0131lmaz1, ID , Furkan K\u0131nl\u01311, ID , Bar\u0131\u015f \u00d6zcan1, ID , and Furkan K\u0131ra\u00e71, ID", "text": "1Video, Vision and Graphics Lab, \u00d6zye\u011fin University, Istanbul, Turkey\nEdited by Koustuv Sinha,\nSharath Chandra Raparthy\nReviewed by Anonymous Reviewers\nReceived 04 February 2022\nPublished 23 May 2022\nDOI 10.5281/zenodo.6574719"}, {"heading": "Reproducibility Summary", "text": "In this study, we present our results and experience during replicating the paper titled \u201dLifting 2D StyleGAN for 3D-Aware Face Generation\u201d [1]. Thiswork proposes amodel, called LiftedGAN, that disentangles the latent space of StyleGAN2 [2] into texture, shape, viewpoint, lighting components and utilizes those components to render novel synthetic images. This approach claims to enable the ability of manipulating viewpoint and lighting components separately without altering other features of the image. We have trained the proposed model in PyTorch [3], and have conducted all experiments presented in the original work. Thereafter, we have written the evaluation code from scratch. Our re-implementation enables us to better compare different models inferring on the same latent vector input. Wewere able to reproducemost of the results presented in the original paper both qualitatively and quantitatively."}, {"heading": "Scope of Reproducibility", "text": "In the scope of this study, we aim to reproduce all of the qualitative and quantitative re\u2010 sults of LiftedGAN, including the ablation study, on FFHQ [4] and AFHQ Cat [5] datasets. Additionally, we further extend the experiments presented in the original work by test\u2010 ing the proposed approach on CelebA [6] dataset."}, {"heading": "Methodology", "text": "Wehave adopted the source code for training from the author\u2019s repository. We havewrit\u2010 ten the evaluation scripts from scratch in PyTorch to test the original and reproduced weights on the same latent vector. Our experiments have been completed on a single Nvidia Quadro RTX 6000 in 1 day for each, and it requires \u223c11GB GPUmemory for train\u2010 ing."}, {"heading": "Results", "text": "We have achieved to reproduce the results qualitatively and quantitatively on a large scale. We also validated the generalization ability of the model by training and testing it on CelebA dataset. Although our experimental results are not identical with the original paper, they are consistent and validates the claims made by the original work.\nCopyright \u00a9 2022 D. Y\u0131lmaz et al., released under a Creative Commons Attribution 4.0 International license. Correspondence should be addressed to Do\u011fa Y\u0131lmaz (doga.yilmaz.11481@ozu.edu.tr) The authors have declared that no competing interests exist. Code is available at https://github.com/yilmazdoga/lifting-2d-stylegan-for-3d-aware-face-generation. \u2013 SWH swh:1:dir:c4289f0be8edafa7d050efc178bd1bc9bed0bbdc. Open peer review is available at https://openreview.net/forum?id=BcNonfQ3RY.\nReScience C 8.2 (#46) \u2013 Y\u0131lmaz et al. 2022 1"}, {"heading": "What was easy", "text": "The paper is well\u2010written. The main components of the LiftedGAN was open\u2010source, and implemented in PyTorch, which facilitated our reproduction study."}, {"heading": "What was difficult", "text": "3D evaluation and reconstruction scripts were not available in the official repository. Also, there were some missing implementation details to reproduce some results in the original work.\nCommunication with original authors Wewere in contact with the authors since the beginning of the challenge. We could not achieve to reproduce 3D evaluation and reconstruction parts, fortunately, the authors swiftly answered our questions regarding the topic.\n1 Introduction\nThe paper [1] proposes a framework that disentangles the latent space of a pre\u2010trained StyleGAN2 [2] for 3D\u2010aware face generation. The previous approaches are trained to generate random faces, thus they do not offer direct manipulation over the semantic attributes such as lighting or pose in the generated image. A number of studies exists that aims to manipulate the semantic attributes of the generated images directly [7, 8, 9, 10, 11]. Although these feature manipulation methods have shown ability to generate faces with high visual quality under assigned poses, it is unclear whether other features such as identity are preserved when we change the pose parameters. In the paper [1], to overcome this problem, a pre\u2010trained StyleGAN2 is distilled into a 3D\u2010aware generator, which outputs the generated image with its viewpoints, light direction and 3D informa\u2010 tion. The framework proposed in the original paper [1], namely LiftedGAN, is composed of five sub\u2010networks that are responsible for light direction, viewpoint, foreground/back\u2010 ground map, depth, and texture components. These sub\u2010networks are than utilized to render a 2D face image. As the main claim of the paper, this method achieves to change the light direction and viewpoint without affecting the other important features such as texture and shape. In this reproducibility report, we studied LiftedGAN for generating and manipulating human and cat faces. During this work, we have implemented the testing loops for running the experiments on the same randomly generated latent vectors. We have also trained both the StyleGAN2 and LiftedGANmodels with different datasets from scratch. Furthermore, we present the results of the original work on different domains and com\u2010 pare the obtained results with the ones reported in the original paper. Finally, we report the important details about certain issues encountered during reproduction.\n2 Scope of reproducibility\nThemain idea of the paper is to train a 3D generative network by distilling the knowledge in StyleGAN2 for building a 3D generator that disentangles the generation process into different 3D modules. Afterwards, those modules are utilized to render a 2D face image. The proposed framework, namely LiftedGAN, claims to provide on\u2010par performance to the state\u2010of\u2010the\u2010art face generationmethods in termsof Fr\u00e9chet InceptionDistance (FID) [12] score while providing the ability to change the viewpoint and light direction. To validate these claims, we try to investigate the following questions:\nReScience C 8.2 (#46) \u2013 Y\u0131lmaz et al. 2022 2\n\u2022 Is the implementation details described in the paper and the provided code suffi\u2010 cient for replicating the quantitative results reported in the paper?\n\u2022 Are the qualitative results visually\u2010plausible?\n\u2022 Could our replication obtain similar qualitative results compared to the reported qualitative results in the original paper?\n\u2022 Could our replication obtain similar FID scores compared to the reported results in the original paper?\n\u2022 How does the architecture perform when trained on other datasets (e.g., CelebA)?\n3 Methodology\nWe have adopted the code for the architecture and the training loop from the official repository of the paper. Due to the nature of both StyleGAN2 and LiftedGAN, the frame\u2010 work samples a random latent vector from the latent space and uses that vector to gener\u2010 ate a new face. This makes comparing the original and reproduced results not possible by using the original code, since the generated face is changed for each trial as we run the original test loop. To overcome this issue, we have written a modified version of the original testing loop that stores the randomly generated latent vector and provides it to different versions of the LiftedGAN model. At this point, we found that the paper is well\u2010written, and contains the details required to reproduce the most of the qualitative and some of the quantitative results. Since the official repository of the paper is publicly available, we mainly focused on reproduc\u2010 ing the original experiments in a controlled manner and extending the experiments on different datasets to further validate the claims made by the original paper. In this section, we introduce the implementation details of LiftedGAN, the points in the paper which were important for reproduction, hyperparameters we used, and our experimental setup.\n3.1 Model descriptions The main idea of LiftedGAN is to train a 3D generative network by leveraging the knowl\u2010 edge in pre\u2010trained StyleGAN2. The StyleGAN2 network is composed of two parts: a multi\u2010layer perceptron (MLP) that maps a latent code z \u2208 Z to a style codew \u2208 W , and a\nReScience C 8.2 (#46) \u2013 Y\u0131lmaz et al. 2022 3\n2D generator G2D that synthesizes a face image from the style code w. LiftedGAN aims to build a 3D generator that disentangles the generation process ofG2D into different 3D modules, including texture, shape, lighting and pose, which are then utilized to render a 2D face image. As shown in the Figure 1, the framework involves two pathways, which are the reconstruction pathway and style manipulation (i.e.. perturbation) pathway.\n3D Generator \u2014 As shown in Figure 1, the 3D generator, denoted as G3D, is composed of five trainable sub\u2010networks: DV , DL, DS, DT , M , a pre\u2010trained StyleGAN2 G2D and a differentiable renderer R. M is used as style manipulation network that transfers a style code W\u0302 to a new style code with a specified lighting and viewpoint. This approach creates w0 = M(w\u0302, L0, V0) thus, G2D(w0) outputs a lighting and viewpoint neutralized face image. The rest of the sub\u2010networks DV , DL, DS, DT are responsible from the viewpoint, lighting, depth and shape representation, respectively. Finally, R is used to output a rendered image Iw = R(A,S, T, V, L) where A is the face image with neutral viewpoint and lighting, S, T , V , L are the depth, shape representation, desired view\u2010 point and desired lighting, respectively.\nLoss Functions \u2014 As mentioned in Section 3.1, the framework has two pathways for face reconstruction and style manipulation. As shown in Figure 1, the reconstruction path\u2010 way uses L1 loss whereas the style manipulation pathway uses the perturbation loss. The overall reconstruction loss function consists of five objective functions, which are reconstruction loss Lrec, photometric flip loss Lflip, perturbation loss Lperturb, identity variance loss, Lidt and albedo map loss LregA . Overall loss function and its each compo\u2010 nent are defined below. Reconstruction loss is defined as following:\nLrec = ||Iw \u2212 I\u0302w||1 + \u03bbpercLperc(Iw, I\u0302w) (1)\nwhere Lperc refers to the perceptual loss [13] using a pre\u2010trained VGG\u201016 network [14], I\u0302w is the proxy image output by StyleGAN2 and Iw is the image rendered byR. Lflip has the same formulation as Lrec except that it uses flipped albedo and shape maps during the rendering. Perturbation loss is defined as following:\nLLVcyc = ||V\u0303 \u2032 \u2212 V \u2032||2 + ||L\u0303\u2032 \u2212 L\u2032||2 (2)\nL (a) perturb = d(I \u2032 w, G2D(w\n\u2032)) + \u03b2 ||w\u2032 \u2212 \u00b5w||2\n2\u03c32w (3)\nL (b) perturb = d(R(A,S, T, V \u2032, L\u2032), I\u0302 \u2032w) + \u03bbLVcycLLVcyc (4)\nLperturb = L (a) perturb + L (a) perturb (5)\nwhere w\u0302 is a randomly sampled style code, w\u2032 is the manipulated style code, I\u0302w\u2032 rep\u2010 resents the proxy image generated by the manipulated style code, V \u2032 and L\u2032 are the randomly sampled viewpoint and lighting vectors, V\u0303 \u2032 = DV (w\u2032) and L\u0303\u2032 = DL(w\u2032). Also, \u00b5w is the empirical mean and \u03c3w is the standard deviation of randomly generated style codes. I \u2032w is the rotated and relighted face image output generated by R(A,S, T, V \u2032, L\u2032). Identity variance loss component is defined as following:\nLidt = ||f(Iw0)\u2212 f(I \u2032w)||2 (6)\nwhere Iw0 is the texture map and f is a pre\u2010trained face recognition network. Albedo map loss component LregA is also defined as following:\nLregA = ||KA||\u2217 (7)\nReScience C 8.2 (#46) \u2013 Y\u0131lmaz et al. 2022 4\nwhere K is the albedo matrix that is composed of filtered and vectorized albedo maps and ||.||\u2217 denotes the nuclear norm. The overall loss function for the 3D generator used in the reconstruction pathway is as following:\nLG3D = \u03bbrecLrec + \u03bbflipLflip + \u03bbperturbLperturb + \u03bbidtLidt + \u03bbregALregA (8)\n3.2 Hyper-parameters The hyper\u2010parameters used in the original work are mostly the objective function co\u2010 efficients, and the default values mentioned in their paper are presented in Table 1. During our additional experiments on CelebA, we have followed the same settings that the authors used for FFHQ. We have also considered the batch size and learning rate as hyper\u2010parameters, and they are set to 8 and 1e\u22124, respectively for all of our experiments.\n3.3 Datasets Following the paper, we have conducted our experiments on two well\u2010known datasets: FFHQ, AFHQ Cat. The original paper uses FFHQ for training the StyleGAN2, and the original LiftedGAN framework uses the generated data from the pre\u2010trained StyleGAN2. Moreover, in the original work, AFHQ Cat is used to validate the performance of the architecture on a different domain. In addition to FFHQ, we have also conducted ad\u2010 ditional experiments on CelebA dataset to further validate the generalization ability of LiftedGAN. The details are provided in Table 1.\nQuadro RTX 6000. The second one has Intel 3770K CPU, 8 GB RAM and 2x Nvidia GTX 1080. StyleGAN2 trainings for our custom datasets have been conducted in our second ma\u2010 chine, and take approximately 2\u20103 days to be completed, whereas LiftedGAN trainings have been conducted on our first machine, and completed in \u223c1 day. The experiments we conducted for reproducing this work do not require any other significant resources, but GPU memory.\n4 Results\nWehave conducted all experiments by following the descriptions given in the paper. We re\u2010implemented the test scripts that enables us to run two different models on a single latent vector. In general, we were able to reproduce the quantitative and qualitative re\u2010 sults on FFHQ and AFHQ Cat datasets. We extend the results of AFHQ Cat presented in the original work by conducting the lighting and viewpoint (i.e. pitch) manipulation. Moreover, we extend the experiments given in the original work by training the Lift\u2010 edGAN from scratch and testing it on CelebA.\n4.1 Results reproducing the original work\nQualitative results \u2014 As shown in Figure 2, we have achieved visually on\u2010par face gener\u2010 ation performance on FFHQ. Although there are slight differences in our results com\u2010 pared to the results presented in the original work (e.g., the absence of glasses in the second column and the first row), they do not reduce the face generation quality and the identical features for all samples are mostly preserved. We provide more face gener\u2010 ation examples for more extensive comparison in our supplementary materials and the reproduction repository. Figure 3 demonstrates the comparison of the viewpoint rotation between the outputs obtained by using the weights given by the authors and the outputs reproduced by our work. At this point, we validate that LiftedGAN achieves to change the viewpoints in the generated images without affecting the other visual features. Moreover, in Figure 4, we show both qualitative results of the original work and our reproduction study on chang\u2010 ing the direction of the light source task on FFHQ dataset. We can state that LiftedGAN also achieves to change the direction of the light source in generated images. In our study, we were able to reproduce these results. In the originalwork, the examples of face generation results between interpolated latent codes are demonstrated. The main claim in the paper is that LiftedGAN can achieve a smooth change between two disparate samples. To validate this claim, we have gener\u2010 ated the face images by using the interpolated latent codes, and observed the effect of the viewpoint rotation strategy, as in the original work. Our reproduced weights can\nReScience C 8.2 (#46) \u2013 Y\u0131lmaz et al. 2022 6\ngenerate similar faces to the ones produced by the original weights with the same view\u2010 point rotations, as presented in Figure 6. Qualitative results of the ablation study for our reproduction are shown in Figure 5. We also provide more visual examples for all these additional experiments in our supple\u2010 mentary materials and the reproduction repository.\nQuantitative results \u2014 In this section, we present our quantitative results of this reproduc\u2010 tion study in Table 3, and compare with the ones reported in the original work. The authors have conducted several ablation studies on FFHQ. Particularly, they remove symmetric reconstruction loss (i.e., wo_flip), perturbation loss (i.e., wo_perturb), identity regularization loss (i.e., wo_idt) and albedo consistency loss (i.e., wo_rega), respectively, to re\u2010train their proposed architecture for further comparison. Our reproduced results have lower FID scores than the ones reported in the paper, as well as all ablation studies. As claimed in the original work, the model cannot produce visually\u2010plausible and logi\u2010 cally reasonable shapes for the generated faces, and this can be observedmore dramati\u2010 cally in our reproduced results. Moreover, we additionally measure the performance of the proposed architecture and its variants on AFHQ, which is not reported in the orig\u2010 inal work. We obtain more similar quantitative results for the reproduction on AFHQ Cat dataset.\n4.2 Results beyond the original work\nExtended experiments on AFHQ \u2014 In the original work, a controlled generation strategy on cat headshas been followed in order to demonstrate that the framework is object\u2010agnostic. However, this experiment is limited, and conducted on only the viewpointmanipulation on yaw axis. We present the visual results of our controlled generation on cat heads in Figure 7 (for the viewpoint manipulation in yaw and pitch axes) and in Figure 8 (for changing the light direction). At this point, we can validate that the framework is able to work well on different objects, not only human faces.\nThe performance on CelebA \u2014 To extend the scope of the experiments in the original work, and validate the generalization ability of the architecture, we have re\u2010trained the frame\u2010 work from scratch on CelebA. The visual results of this experiment can be seen in Figure 9. Themain observations for this experiment are as follows: (1) the overall performance is similar to the one for FFHQ, (2) the outputs for the face generation is visually\u2010plausible, (3) the viewpointmanipulation can be achieved on this dataset, (4) there are some visual artifacts in the outputs for the task of changing the light direction.\nReScience C 8.2 (#46) \u2013 Y\u0131lmaz et al. 2022 8\n5 Discussion\nWe can clearly say that the paper reproduced was well\u2010written. Although there are a few missing implementation details in the paper and a few missing evaluation scripts in the official repository, we were able to reproduce the results reported in the original work on a large scale. Overall, we were able to obtain similar qualitative results when compared to the original work. Our results are visually\u2010plausible. The quantitative re\u2010 sults do not exactly match with the reported results, but eventually not very far from them. In addition to these results, we demonstrate the reproduced results of the view\u2010 point rotation on yaw and pitch axes and changing the light direction tasks, the visual results of the ablation study and the task of generating interpolated and rotated faces. We extend the experiments on AFHQ Cat dataset, and also observe the performance of the proposed methodology on an additional dataset (i.e., CelebA).\n5.1 What was easy The code was open\u2010source, and implemented in PyTorch, hence adopting the training loop and model implementation facilitated our reproduction study. The provided pre\u2010 trained StyleGAN2 weights significantly reduced our required GPU hours for FFHQ ex\u2010 periments.\n5.2 What was difficult Since the 3D evaluation and reconstruction scripts are not available in the official reposi\u2010 tory and not describedwith enough detail in the original paper to reproduce it, we could not achieve to reproduce the results related to 3D reconstruction metric.\nReScience C 8.2 (#46) \u2013 Y\u0131lmaz et al. 2022 9\n5.3 Communication with original authors Wewere in contact with the authors since the beginning of the challenge. We could not succeed to reproduce the 3D reconstruction task, fortunately, they swiftly answered our questions, and provided more information for reproducing the task."}], "title": "[Re] Lifting 2D StyleGAN for 3D-Aware Face Generation", "year": 2022}