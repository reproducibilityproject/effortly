{"abstractText": "Gao et al. [1] propose to leverage policies consisting of a series of data augmentations for preventing the possibility of reconstruction attacks on the training data of gradients. The goal of this study is to: (1) Verify the findings of the authors about the performance of the found policies and the correlation between the reconstruction metric and pro\u2010 vided protection. (2) Explore if the defence generalizes to an attacker that has knowl\u2010 edge about the policy used.", "authors": [{"affiliations": [], "name": "Alfonso Taboada Warmerdam"}, {"affiliations": [], "name": "Lodewijk Loerakker"}, {"affiliations": [], "name": "Lucas Meijer"}, {"affiliations": [], "name": "Ole Nissen"}, {"affiliations": [], "name": "Koustuv Sinha"}, {"affiliations": [], "name": "Sharath Chandra Raparthy"}], "id": "SP:da4858c12b084c975679c1f87f9a141262fd6ae5", "references": [{"authors": ["W. Gao", "S. Guo", "T. Zhang", "H. Qiu", "Y. Wen", "Y. Liu"], "title": "Privacy-preserving collaborative learning with automatic transformation search.", "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition", "year": 2021}, {"authors": ["Q. Yang", "Y. Liu", "T. Chen", "Y. Tong"], "title": "Federated machine learning: Concept and applications.", "venue": "ACM Transactions on Intelligent Systems and Technology (TIST)", "year": 2019}, {"authors": ["S. Guo", "T. Zhang", "X. Xie", "L. Ma", "T. Xiang", "Y. Liu"], "title": "Towards byzantine-resilient learning in decentralized systems.", "year": 2002}, {"authors": ["L. Melis", "C. Song", "E. De Cristofaro", "V. Shmatikov"], "title": "Exploiting unintended feature leakage in collaborative learning.", "venue": "IEEE Symposium on Security and Privacy (SP). IEEE", "year": 2019}, {"authors": ["J. Mellor", "J. Turner", "A. Storkey", "E.J. Crowley"], "title": "Neural architecture search without training.", "venue": "In: International Conference on Machine Learning. PMLR", "year": 2021}, {"authors": ["M. Balunovi\u0107", "D.I. Dimitrov", "R. Staab", "M. Vechev"], "title": "Bayesian Framework for Gradient Leakage.", "year": 2021}, {"authors": ["M. Jaderberg", "K. Simonyan", "A. Zisserman"], "title": "Spatial transformer networks.", "venue": "Advances in neural information processing systems", "year": 2015}, {"authors": ["H. Xiao", "K. Rasul", "R. Vollgraf"], "title": "Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms. cite arxiv:1708.07747Comment: Dataset is freely available at https://github.com/zalandoresearch/fashion-mnist Benchmark is available at http://fashion-mnist.s3website.eu-central-1.amazonaws.com", "year": 2017}, {"authors": ["S. Ruder"], "title": "An overview of gradient descent optimization algorithms.", "venue": "arXiv preprint arXiv:1609.04747", "year": 2016}, {"authors": ["I. Sutskever", "J. Martens", "G. Dahl", "G. Hinton"], "title": "On the importance of initialization and momentum in deep learning.", "venue": "In: International conference on machine learning. PMLR", "year": 2013}, {"authors": ["A. Paszke"], "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library.", "venue": "Neural Information Processing Systems 32", "year": 2019}, {"authors": ["A. Hore", "D. Ziou"], "title": "Image quality metrics: PSNR vs. SSIM.", "venue": "20th international conference on pattern recognition", "year": 2010}], "sections": [{"text": "R E S C I E N C E C Replication / ML Reproducibility Challenge 2021\n[Re] Privacy-preserving collaborative learning with automatic transformation search Alfonso Taboada Warmerdam1, ID , Lodewijk Loerakker1\u201e ID , Lucas Meijer1\u201e ID , and Ole Nissen1\u201e ID 1University of Amsterdam, Amsterdam, Netherlands\nEdited by Koustuv Sinha,\nSharath Chandra Raparthy\nReviewed by Anonymous Reviewers\nReceived 04 February 2022\nPublished 23 May 2022\nDOI 10.5281/zenodo.6574713\n1 Reproducibility Summary"}, {"heading": "Scope of Reproducibility", "text": "Gao et al. [1] propose to leverage policies consisting of a series of data augmentations for preventing the possibility of reconstruction attacks on the training data of gradients. The goal of this study is to: (1) Verify the findings of the authors about the performance of the found policies and the correlation between the reconstruction metric and pro\u2010 vided protection. (2) Explore if the defence generalizes to an attacker that has knowl\u2010 edge about the policy used."}, {"heading": "Methodology", "text": "For the experiments conducted in this research, parts of the code from Gao et al. were refactored to allow for more clear and robust experimentation. Approximately a week of computation time is needed for our experiments on a 1080 Ti GPU."}, {"heading": "Results", "text": "It was possible to verify the results from the original paper within a reasonable margin of error. However, the reproduced results show that the claimed protection does not generalize to an attacker that has knowledge over the augmentations used. Addition\u2010 ally, the results show that the optimal augmentations are often predictable since the policies found by the proposed search algorithm mostly consist of the augmentations that perform best individually."}, {"heading": "What Was Easy", "text": "The design of the search algorithm allowed for easy iterations of experiments since ob\u2010 taining the metrics of a single policy can be done in under a minute on an average GPU. It was helpful that the authors provided the code of their experiments.\nCopyright \u00a9 2022 A.T. Warmerdam et al., released under a Creative Commons Attribution 4.0 International license. Correspondence should be addressed to Alfonso Taboada Warmerdam (avtwarmerdam@gmail.com) The authors have declared that no competing interests exist. Code is available at https://github.com/Linkerbrain/fact-ai-2021-project \u2013 DOI 10.5281/zenodo.6482966. \u2013 SWH swh:1:dir:30aff8e620b5a2f17c4ec653b4b6fe80272fdaff. Open peer review is available at https://openreview.net/forum?id=S9Iepnz7hRY.\nReScience C 8.2 (#44) \u2013 Warmerdam et al. 2022 1"}, {"heading": "What Was difficult", "text": "To obtain the reconstruction score and accuracy of a policy, the architecture needs to be trained for about 10 GPU\u2010hours. Thismakes it difficult to verify howwell the searchmet\u2010 rics correlate with these scores. It also prevented us to test the random policy baseline, as this requires the training to be repeated at least 10 times which requires significant computational power."}, {"heading": "Communication With Original Authors", "text": "An e\u2010mail was sent to the original authors regarding the differences in results. Unfortu\u2010 nately no response has been received so far.\nReScience C 8.2 (#44) \u2013 Warmerdam et al. 2022 2\n2 Introduction\nCollaborative learning is becoming increasingly common. A deep learning model can be trained bymultiple participants without the parties having to share their training set [2, 3, 4]. Instead, they share gradients, given a public model. This allows private data to be used for training non\u2010private networks. However, recent works discovered that the shared gradients may be used to recover sensitive training samples. This development poses a serious threat to collaborative learning. Gao et al.[1] proposes the ATSPrivacy\u2010 Framework as a solution for such reconstruction attacks. The goal of the ATSPrivacy\u2010Framework is to use simple data augmentations, such as translations and changes in contrast, to obfuscate the images and make them more dif\u2010 ficult to reconstruct. The authors show that some of these augmentations significantly increase privacy under their attack model without severely impacting the accuracy. A search algorithm is designed to find a combination of augmentations (referred to as a policy) that work well. The search algorithm aims to find the policy that protects the privacy of training images the most, while still maintaining model accuracy. A privacy score and an accuracy score are introduced as search metrics in order to estimate the re\u2010 construction score of an attacker and the accuracy of the model. The accuracy score is implemented based on techniques from Mellor et al. [5], and the privacy metric is a novel technique. The authors claim that the metrics provide suitable estimations of the model accuracy and reconstruction score, without needing to train the model first. As a response on Gao et al., Balunovi\u0107 et al. [6] have shown that the proposed method is not secure during early stages of training. The goal of this research is to extend the research of Gao et al., and find out whether the method is also insecure in the final stages of training. This will be researched by incorporating knowledge of the policies being used as a defense into the attack model.\n3 Scope of Reproducibility\nThe reproducibility is split into two parts. The aim of the first part is to reproduce the claims fromGao et al., by recreating their experiments. The second part of this research focusses on the expansion of the original framework by performing additional experi\u2010 ments that give insight in how the findings of the authors are able to generalize to more intelligent attackers.\n3.1 Reproduction The first results are generated to reproduce the correlation between the proposed pri\u2010 vacy score as an estimation of the reconstruction score after training. This will be done by recreating the empirical validation mentioned in the paper of Gao et al. This work will not include experiments on researching the accuracy score, as the authors base this decision on previous work and this experiment would require a lot of computing power than available. Additionally, the performance of the policies found by the authors is verified using the search algorithm created by Gao et al. Concretely, the two claims that are reproduced are:\n1. The privacy score proposed in the paper correlates with the reconstruction score of an attack.\n2. The policies selected by the search algorithm reduce the reconstruction score sig\u2010 nificantly while not resulting in a great loss of accuracy.\nReScience C 8.2 (#44) \u2013 Warmerdam et al. 2022 3\n3.2 Additional Insight In the original paper several attackmethods have been tested, which all follow the same strategy but with different optimizers and distance measures. This is further illustrated in Section 4. All the attackmethods do not use any knowledge about possible augmenta\u2010 tions on the data. The goal of this study is to provide insight into what happens when an attacker makes assumptions on what augmentations are used. Additionally, this claim is supported by showing that the policies that are found by the model have very limited diversity, which makes it easy to predict what augmentations are used. This research provides the following insights:\n1. The effectiveness of the most promising policies selected by the paper for protect\u2010 ing the data is greatly reduced when the attacker knows that this policy is being used.\n2. Most policies score worse than no policy on both the accuracy and privacy search metric.\n3. The policies that score better than no policy on the search metrics often consist of the augmentations that scored best individually.\n4 Methodology\n4.1 Model Descriptions Twomodels are used in the framework, the systemandattackmodels. The systemmodel is a standard collaborative learning system where multiple parties train a global model M. The Attack model is considered an independent party in the collaborative learning system. The gradients are shared to all parties in each iteration and the attacker tries to reconstruct private training samples from the shared gradients. System Model Multiple parties train a global model M. All participants own a private dataset D. Let L be the loss function and letW bet themodel parameters. Each iteration a training sample (x, y) is randomly selected by all parties. After randomly selecting the training sample, the loss L(x, y) is calculated using forward propagation and then the gradient\u2207W (x, y) = \u2202L(x,y)\u2202W is calculated using backpropagation. Attack Model Given a gradient \u2207W (x, y) the attacker wants to find a sample and label pair (x\u2032, y\u2032), such that thematching gradient\u2207W (x\u2032, y\u2032) approximates\u2207W . This can be expressed by minimizing the optimization problem shown in Equation 1.\nx\u2217, y\u2217 = argminx\u2032,y\u2032 ||\u2207W (x, y)\u2212\u2207W (x\u2032, y\u2032)|| (1)\nA reconstruction attack is considered successful when x\u2217 is very similar to x. The term ||\u2207W (x, y) \u2212 \u2207W (x\u2032, y\u2032)|| is called the gradient loss. This term corresponds with the L1\u2010norm, but it can also be replaced by the L2\u2010norm or cosine distance. Protection In order to protect against reconstruction attacks the original dataset D is transformed into a new dataset D\u0302. The new dataset is created by applying a set of trans\u2010 formation functions T = t1 \u25e6 t2 \u25e6 ... \u25e6 tn on each sample x \u2208 D, resulting in x\u0302 = T (x). The data owner then uses D\u0302 to calculate the gradients during training and shares them with the other participants. Privacy Score Due to the expensive computation time of the PSNR metric, it is not an efficient method to compare the privacy effect amongst candidate policies. A new pri\u2010 vacy score is developed by the authors, which is intended to reflect the privacy leakage given a transformation policy and a model which is trained only for a few epochs. The privacy score is given by Equation 2. This equation is a numerical integration over K steps which estimates the area under the curve of the gradient similarity during a recon\u2010 struction attack.\nReScience C 8.2 (#44) \u2013 Warmerdam et al. 2022 4\nSpri(T ) \u2248 1 |D|K \u2211 x\u2208D K\u22121\u2211 j=0 GradSim(x\u2032( i K ), T (x)) (2)\nWhere x\u2032(i) = (1 \u2212 i) \u2217 x0 + i \u2217 T (x) and GradSim measures the gradient similarity between two input samples (x1, x2) with the same class y, as given by equation 3.\nGradSim(x1, x2) = < \u2207W (x1, y),\u2207(x2, y) >\n||\u2207W (x1, y)|| \u00b7 ||\u2207W (x2, y)|| (3)\nAccuracy Score Besides the privacy requirement, it is also important to maintain model accuracy. Mellor et al. [5] proposed a technique to explore neural architectures without the need of model training. Based on this work Gao et al. create a technique to search for transformations that maintain model performance. The accuracy score defined by Gao et al. is shown in Equation 4.\nSacc(T ) = 1\nN N\u22121\u2211 i=0 log(\u03c3J,i + \u03f5) + (\u03c3J,i + \u03f5)\u22121 (4)\nWhere \u03f5 is a small positive value used for numerical stability, and \u03c3J,i is the i\u2019th eigen\u2010 value of the correlation matrix of the jacobian J = ( \u2202f\u2202x\u03021 , . . . \u2202f \u2202x\u0302N\n) for a randomly initial\u2010 ized model f and a mini\u2010batch of samples transformed by the target policy T : {x\u0302n}Nn=1. The Search Algorithm The goal of the search algorithm is to identify a policy set by com\u2010 bining qualified methods. Two models should be prepared: (1) the privacy quantifica\u2010 tion model, (2) a model that is randomly initialized without the use of any optimization strategies. This second model is used for accuracy quantification. Cmax policies are randomly sampled. The privacy and accuracy scores of each policy are then calculated. When the accuracy score is lower than a predefined threshold, the policy is rejected. The top\u2010n polices are then selected based on the privacy score from the final policy set. Enhanced Attack Model It can be expected that an attacker has knowledge about the augmentations that are used to protect the data. Based on this assumption, a simple variation on the attack model is explored that allows the attacker to learn a translation along with the reconstruction of an image. This is inspired by the observation that all the successful augmentations selected by the paper make use of translations or crops that create large black areas in the image, but the original reconstruction algorithm is never able to reconstruct these. In order to learn this translation, the attempted solution x\u2217which is being reconstructed by the algorithm is first put through a translation module. This module can be fine\u2010 tuned during the reconstruction process by the attacker in order to find a suitable trans\u2010 lation that reduces the gradient loss. This is done by propagating the gradients through the differentiable translation module and training the parameters tx and ty, denoting the shift of the image on the x and y axes respectively. Amore general affine transforma\u2010 tion can also be learned, but this is of little use for the given augmentations since they don\u2019t shear, scale or rotate the images. The implementation of this translation module is inspired by Spatial Transformer Networks [7] which use a similar mechanism to pre\u2010 process data before it is given to convolutional neural networks. Figure 1 illustrates how the enhanced algorithm works. The parameters tx and ty from the translation module are constrained to a maximum andminimum constant each such that the translation does not become too large. These constraints are set depending on the augmentation policy under attack.\n4.2 Datasets Gao et al. used the CIFAR100 [8] and Fashion MNIST [9] datasets during their research. The CIFAR100 dataset consists of 100 distinct classes, each containing 600 32x32 images.\nReScience C 8.2 (#44) \u2013 Warmerdam et al. 2022 5\nThere are 500 training images and 100 testing images per class. The Fashion MNIST contains 70,000 fashion products which can be assigned to 10 distinct categories. Each of the 10 classes contains 7,000 images. There are 60,000 training images and 10,000 testing images.\n4.3 Hyperparameters The same hyperparameters that were used in the paper are used for this reproduction. The search algorithm is trained using 10% of the available training set for 50 epochs. The full training of a model is done with 100% of the training set for 200 epochs. Both are trained with a batch size of 128 and Stochastic Gradient Descent (SGD) [10] with weight decay and Nesterov momentum [11]. The learning rate is set to 0.1 and decays with a multistep linear scheduler after epochs 75, 125 and 175 with a gamma of 0.1. The parameter \u03f5 in equation 4 is set to 10\u22125. The reconstruction algorithm reconstructs one image at a time in 4800 iterations. The same optimizer is used with the same hyperparameters, but the learning rate decays after iterations \u223c 1800, \u223c 3000 and \u223c 4200. Cosine distance is used as the gradient similarity metric (see Equation 1). The constraints for the enhanced attack algorithm are set such that |tx| < 1.1 and |ty| < 0.2 for the 3-1-7 policy (see Section 4.4), |tx| < 0.4 and |ty| < 0.4 for the 43-18-18 policy and such that |tx| < 1.1 and |ty| < 0.4 for the Hybrid policy. A value of 1 for these constraints corresponds to half the width/height of the image frame.\n4.4 Experimental Setup and Code The codebase for the paper is available on GitHub [12]. This codebase was used in this study as a starting point to reproduce the claims made by Gao et al. The codebase was modified to run the experiments on systems available during this research and to run additional experiments. The models in the framework rely on the PyTorch library [13]. The adapted code used for the experiments in this report can also be found on GitHub [14]. At most 3 functions are drawn from a set of 50 transformations from the data augmen\u2010 tation library in the defence implementation. The functions are denoted as i \u2212 j \u2212 k , where i, j and k are the indexes of the functions from the augmentation library. Note that functions can be applied multiple times within the same concatenation of policies. The following experiments were conducted:\n1. To verify the claimed correlation, 100 random policies were evaluated by perform\u2010 ing a reconstruction attack of 2500 iterations on a ResNet20 DNN on the CIFAR100 dataset.\n2. To verify the claimed results, four models were fully trained using the CIFAR100 dataset and the ResNet20 DNN: with no policy, the policies 3-1-7, 43-18-18 and finally the Hybrid policy, which randomly chooses either the 3-1-7 augmentations or the 43-18-18 augmentations for each image. This was repeated for the F\u2010Mnist\nReScience C 8.2 (#44) \u2013 Warmerdam et al. 2022 6\ndataset with the same architecture, but using the policies 19-15-45 and 2-43-21. Subsequently a reconstruction attack was performed using the same settings as the original authors.\n3. To give insight into the performance of the enhanced attack, the CIFAR100models mentioned above were attacked using the enhanced reconstruction attack.\n4. To give insight into the distribution of policies, 1500 benchmarks were done using the privacy and accuracy score on the CIFAR100 dataset with the ResNet20 archi\u2010 tecture. Additionally, all augmentations listed in the original paper were evalu\u2010 ated individually in the same setting.\nAccuracy is used to measure model performance. Accuracy is defined as the ratio of correct classifications to the total number of classifications. The similarity of a recon\u2010 structed image to the original is measured by the Peak Signal to Noise Ratio (PSNR) [15] of the two, which is measured in decibels and correlates to the logarithm of the mean square differences between the pixels of one image and the other. To measure the over\u2010 all resistance of a model to reconstruction attacks, the average PSNR is taken over 100 reconstructions.\n4.5 Computational Requirements We had access to a single GeForce 1080 Ti GPU from the Lisa Cluster [16], which has a Bronze 3104 (1.7GHz) processor with 12 CPU Cores and 256GM RAM of memory. For the Cifar100 dataset with the ResNet architecture, a complete trainingcycle took 2 hours and evaluating it under the reconstruction attack took 10 hours in order to recon\u2010 struct 100 images. The policy search took about 1 minute per policy.\n5 Results\n5.1 Reproduction Correlation privacy score and reconstruction score. In the original paper the authors show that their proposed privacy score has a positive correlationwith the reconstruction score of the attacker. A comparison between their results and the reproduced results can be found in Figure 2. It can be seen that the results in the figures do not match. When fitting a linear trend between the metrics, there appears to be no correlation in the reproduced results. Possible explanations for this are explained in Section 6.\nThe Performance of Selected Policies Gao et al. claim the policies selected by the search algorithm reduce the PSNR reconstruction score significantly while not resulting in a\nReScience C 8.2 (#44) \u2013 Warmerdam et al. 2022 7\ngreat loss of accuracy. The results presented in the paper along with the results that resulted from the reproduction can be seen in Table 1. Some results from the reproduction differ substantially from the ones presented in the paper, as can be seen from the red entries in Table 1. The biggest difference is the recon\u2010 struction score for the unaugmented policy of the Cifar100. Which differs greatly from the reconstruction score presented in the original paper. However, it is still significantly higher than that of the augmented policies. The model\u2019s accuracies are comparable in most instances. But, the accuracy found in this research on the 3-1-7 policy of the Cifar100 dataset scores almost 13% below the accuracy of the unaugmented policy, compared to 6% in the original paper.\nFigure 3 shows a selection of image reconstructions performed by the attackmodel. Full sets of examples used to calculate the reconstructions scores can be found in Appendix B.\n(a) Unaugmented, Cifar100\n(b) Hybrid, Cifar100\n(c) Unaugmented, F\u2010Mnist\n(d) Hybrid, F\u2010Mnist\nDiversity of policies In the original paper a brief analysis is providedof the privacy scores that the 50 augmentations achieve individually. The reproduced results can be seen in Figure 5. The results closely match the results in the original paper. The 10 best performing augmentations mostly overlap.\nIn the paper the authors state that the best performing augmentations are often selected in the best policies. To get more insight into this claim, 1500 random policies are evalu\u2010 ated in this research. The results can be seen in Figure 6. In this research the evaluation set was generatedwithout a policy. It can be seen thatmost randompolicies score worse than this benchmark. More specifically, no\u2010policy scored 0.32 on the privacy scorewhile random policies scored 0.38 on average. Policies containing at least 2 of the top 10 in\u2010 dividually scoring augmentations do scored slightly better with a score of 0.31. Policies consisting out of three of the top 5 policies scored better on average than 97.3% of all policies tested.\nReScience C 8.2 (#44) \u2013 Warmerdam et al. 2022 9\n6 Discussion\nThe first claim, which states that the privacy score proposed in the paper correlates with the PSNR of a reconstruction attack, cannot be supported by the reproductions, since no correlation between the reconstruction PSNR and the privacy score has been found, as can be seen in figure 2. A possible explanation for this could be that the model was trained with unaugmented data instead of augmented data, as the authors of the paper did not specify what was used to obtain the results. Due to computational constraints, it was not feasible to train the model on the complete dataset for every policy. For that reason the decision was made to evaluate the reconstructions attack for every policy using a model trained on unaugmented data. Further experimentations could be done to see if the pattern changes if this extensive test is done. A second difference between the experiments is that due to the limitations, the attack was only tested on 20 images instead of 100. This could make the results more uncertain, as a smaller sample size is used. However, the second claim, stating that the policies selected by the search algorithm reduce the PSNR reconstruction score significantly while not resulting in a great loss of accuracy, holds true in the reproductions for almost all the selected policies. Nev\u2010 ertheless, the accuracy of the 3\u20101\u20107 policy is significantly lower when compared to the unaugmented policy, as can be seen in Table 1a. There is no obvious explanation for this difference, but could be attributed to the non\u2010deterministic nature of the training process of the model. The results also hint that the protection provided by candidate policies can be circum\u2010 vented by incorporating a translation module into the attack algorithm. In fact, it is possible that the augmented images are easier to reconstruct than the originals in this scenario. This can be seen from the PSNR score of the 48-18-18 policy in Table 2, as this score was higher than that of the unaugmented policy. A reason for this being the case could be the smaller search space for the attacker, as the default algorithm has to reconstruct a whole image consisting ofN \u00d7M \u00d73 individual pixels for aN \u00d7M colour image, but the enhanced attack algorithm, thanks to its parameterization, only needs to reconstruct the non\u2010black pixels and the translation parameters. For the 3-1-7 policy for example, since roughly half the image is shifted outside the frame by the augmen\u2010 tations, the algorithm only needs to learn 12N \u00d7M \u00d7 3 pixel values and the translation parameters tx and ty. The enhanced algorithm still cannot recover information from images that was deleted by the augmentations, such as the regions of the image that are cropped out, or the precise brightness, provided that the image is not augmented differently multiple times\nReScience C 8.2 (#44) \u2013 Warmerdam et al. 2022 10\nwhenused for collaborative training. This could prove useful in the future for protecting the privacy of participants in collaborative learning systems.\n6.1 What Was Easy The design of the search algorithm allowed for easy iterations of experiments since ob\u2010 taining the scores of a single policy can be done in under a minute. This enabled us to test a lot of policies in different scenario\u2019s which gave a lot of insight into the distribu\u2010 tion of the performance of augmentations. Well\u2010performing policies can often be found within an hour.\n6.2 What Was Difficult To obtain the PSNR and accuracy score of a policy, the architecture needs to be trained for about 10 GPU\u2010hours. This makes it difficult to verify how well the search metrics correlate with these scores. It also prevented us to test the random policy baseline, as this requires the training to be repeated at least 10 times which requires significant com\u2010 putational power.\n6.3 Communication With Original Authors An e\u2010mail was sent to the original authors regarding the differing results in the first claim and for the mathematical intuition behind the accuracy score. Unfortunately no response has been received so far."}], "title": "[Re] Privacy-preserving collaborative learning with automatic transformation search", "year": 2022}