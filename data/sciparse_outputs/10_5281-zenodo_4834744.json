{"abstractText": "All the experiments described in the paper were fully re-implemented using NumPy, SciPy and PyTorch. The experiments on synthetic data were run on a CPU, while the deep learning experiments were run using a Nvidia RTX 2080 Ti GPU. Running the experimentationnecessary to gain some insight on someof the network architectures used and reproducing the real-world experiments required over 550 GPU hours.", "authors": [{"affiliations": [], "name": "David Mizrahi"}, {"affiliations": [], "name": "O\u011fuz Kaan Y\u00fcksel"}, {"affiliations": [], "name": "Aiday Marlen Kyzy"}, {"affiliations": [], "name": "Koustuv Sinha"}, {"affiliations": [], "name": "Sasha Luccioni"}], "id": "SP:e854eb97bcf5b10e6a0f01fb2b2655c237b09e3a", "references": [{"authors": ["Y. Bengio", "P. Simard", "P. Frasconi"], "title": "Learning long-term dependencies with gradient descent is difficult.", "venue": "IEEE transactions on neural networks", "year": 1994}, {"authors": ["J. Zhang", "T. He", "S. Sra", "A. Jadbabaie"], "title": "Analysis of Gradient Clipping and Adaptive Scaling with a Relaxed Smoothness Condition.", "year": 1905}, {"authors": ["A.K. Menon", "A.S. Rawat", "S. Kumar", "S. Reddi"], "title": "Can gradient clippingmitigate label noise?", "venue": "In: International Conference on Learning Representations (ICLR)", "year": 2020}, {"authors": ["A. Ekholm", "J. Palmgren"], "title": "A model for a binary response with misclassifications.", "venue": "Proceedings of the international conference on generalised linear models. Springer", "year": 1982}, {"authors": ["A. Menon", "B. Van Rooyen", "C.S. Ong", "B. Williamson"], "title": "Learning from corrupted binary labels via classprobability estimation.", "venue": "In: International Conference on Machine Learning. PMLR", "year": 2015}, {"authors": ["Z. Zhang", "M.R. Sabuncu"], "title": "Generalized Cross Entropy Loss for Training Deep Neural Networks with Noisy Labels.", "venue": "[cs, stat] (Nov", "year": 2018}, {"authors": ["B. van Rooyen", "A.K. Menon", "R.C. Williamson"], "title": "Learning with Symmetric Label Noise: The Importance of Being Unhinged.", "venue": "URL: http://arxiv.org/abs/1505", "year": 2015}, {"authors": ["P.J. Huber"], "title": "Robust Estimation of a Location Parameter.", "venue": "The Annals of Mathematical Statistics", "year": 1964}, {"authors": ["P.M. Long", "R.A. Servedio"], "title": "Random classification noise defeats all convex potential boosters.", "venue": "en. In: Machine Learning", "year": 2010}, {"authors": ["N. Ding"], "title": "Statistical machine learning in the t-exponential family of distributions.", "venue": "PhD thesis. Jan", "year": 2013}, {"authors": ["C.R. Harris"], "title": "Array programming with NumPy.", "venue": "en. In: Nature", "year": 2020}, {"authors": ["P. Virtanen"], "title": "SciPy 1.0: fundamental algorithms for scientific computing in Python.", "venue": "en. In: Nature Methods 17.3 (Mar", "year": 2020}, {"authors": ["O. Yadan"], "title": "Hydra - A framework for elegantly configuring complex applications. 2019", "venue": "URL: https://github. com/facebookresearch/hydra", "year": 2019}, {"authors": ["A. Paszke"], "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library.", "venue": "[cs, stat] (Dec", "year": 2019}, {"authors": ["Y. Lecun", "L. Bottou", "Y. Bengio", "P. Haffner"], "title": "Gradient-based learning applied to document recognition.", "venue": "Proceedings of the IEEE 86.11 (Nov", "year": 1998}, {"authors": ["D.P. Kingma", "J. Ba"], "title": "Adam: A Method for Stochastic Optimization.", "venue": "[cs] (Jan", "year": 2017}, {"authors": ["A. Krizhevsky", "G. Hinton"], "title": "Learning Multiple Layers of Features from Tiny Images.", "venue": "en. In:", "year": 2009}, {"authors": ["S. Zagoruyko", "N. Komodakis. \u201cWide Residual Networks.\u201d en. In"], "title": "Procedings of the British Machine Vision Conference 2016", "venue": "York, UK: British Machine Vision Association, 2016, pp. 87.1\u201387.12. DOI: 10.5244/C.30.87. URL: http://www.bmva.org/bmvc/2016/papers/paper087/index.html", "year": 2020}, {"authors": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "title": "Deep Residual Learning for Image Recognition.", "venue": "[cs] (Dec", "year": 2015}, {"authors": ["K. Liu. kuangliu/pytorch-cifar. original-date"], "title": "2017-01-21T05:43:20Z", "venue": "Dec. 2017. URL: https : / / github . com/ kuangliu/pytorch-cifar", "year": 2020}, {"authors": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei-Fei"], "title": "ImageNet: A Large-Scale Hierarchical Image Database.", "year": 2009}, {"authors": ["T. DeVries", "G.W. Taylor"], "title": "Improved Regularization of Convolutional Neural Networks with Cutout.", "venue": "[cs] (Nov", "year": 2017}, {"authors": ["H. Zhang", "M. Cisse", "Y.N. Dauphin", "D. Lopez-Paz"], "title": "mixup: Beyond Empirical Risk Minimization.", "venue": "[cs, stat] (Apr", "year": 2018}, {"authors": ["J. Li", "R. Socher", "S.C.H. Hoi"], "title": "DivideMix: Learning with Noisy Labels as Semi-supervised Learning.", "venue": "[cs] (Feb", "year": 2020}, {"authors": ["M.R. Zhang", "J. Lucas", "G. Hinton", "J. Ba"], "title": "Lookahead Optimizer: k steps forward, 1 step back.", "venue": "[cs, stat] (Dec", "year": 2019}, {"authors": ["Y.E. Nesterov. \u201cA method for solving the convex programming problem with convergence rate O(1/k^2).\u201d In"], "title": "Dokl", "venue": "Akad. Nauk SSSR 269 (1983), pp. 543\u2013547. URL: https://ci.nii.ac.jp/naid/10029946121/", "year": 2020}, {"authors": ["I. Sutskever", "J. Martens", "G. Dahl", "G. Hinton"], "title": "On the importance of initialization and momentum in deep learning.", "year": 2013}, {"authors": ["P. Micikevicius et al. \u201cMixed Precision Training.\u201d en. In"], "title": "Oct", "venue": "2017). URL: https://arxiv.org/abs/1710.03740v3", "year": 2020}, {"authors": ["T. Akiba", "S. Sano", "T. Yanase", "T. Ohta", "M. Koyama"], "title": "Optuna: A Next-generation Hyperparameter Optimization Framework. 2019", "year": 2021}], "sections": [{"text": "R E S C I E N C E C Replication / ML Reproducibility Challenge 2020\n[Re] Can gradient clipping mitigate label noise?\nDavid Mizrahi1, ID , O\u011fuz Kaan Y\u00fcksel1, ID , and Aiday Marlen Kyzy1, ID 1EPFL, Lausanne, Switzerland\nEdited by Koustuv Sinha, Sasha Luccioni\nReviewed by Anonymous Reviewers\nReceived 29 January 2021\nPublished 27 May 2021\nDOI 10.5281/zenodo.4834744"}, {"heading": "Reproducibility Summary", "text": ""}, {"heading": "Scope of Reproducibility", "text": "The original paper proposes partiallyHuberised losses, whichpossess label noise robustness. The authors claim that there exist label noise scenarios that defeat Huberised but not partially Huberised losses, and that partially Huberised versions of existing losses perform well on real-world datasets subject to symmetric label noise."}, {"heading": "Methodology", "text": "All the experiments described in the paper were fully re-implemented using NumPy, SciPy and PyTorch. The experiments on synthetic data were run on a CPU, while the deep learning experiments were run using a Nvidia RTX 2080 Ti GPU. Running the experimentationnecessary to gain some insight on someof the network architectures used and reproducing the real-world experiments required over 550 GPU hours."}, {"heading": "Results", "text": "Overall, our results mostly support the claims of the original paper. For the synthetic experiments, our results differ when using the exact values described in the paper, although they still support the main claim. After slightly modifying some of the experiment settings, our reproducedfigures are nearly identical to thefigures from the original paper. For the deep learning experiments, our results differ, with some of the baselines reaching a much higher accuracy on MNIST, CIFAR-10 and CIFAR-100. Nonetheless, with the help of an additional experiment, our results support the authors\u02bc claim that partially Huberised losses perform well on real-world datasets subject to label noise."}, {"heading": "What was easy", "text": "The original paper is well written and insightful, whichmade it fairly easy to implement the partially Huberised version of standard losses based on the information given. In addition, recreating the synthetic datasets used in two of the original paper s\u0313 experiments was relatively straightforward.\nCopyright \u00a9 2021 D. Mizrahi, O.K. Y\u00fcksel and A.M. Kyzy, released under a Creative Commons Attribution 4.0 International license. Correspondence should be addressed to David Mizrahi (david.mizrahi@epfl.ch) The authors have declared that no competing interests exist. Code is available at https://github.com/dmizr/phuber. \u2013 SWH swh:1:dir:3363073199859b8bec0c4362e47aa1e786985793. Open peer review is available at https://openreview.net/forum?id=TM_SgwWJA23.\nReScience C 7.2 (#13) \u2013 Mizrahi, Y\u00fcksel and Kyzy 2021 1"}, {"heading": "What was difficult", "text": "Even though the authors were very detailed in their feedback, finding the exact hyperparameters used in the real-world experiments required many iterations of inquiry and experimentation. In addition, the CIFAR-10 and CIFAR-100 experiments can be difficult to reproduce due to the high number of experiment configurations, resulting in many training runs and a relatively high computational cost of over 550 GPU hours.\nCommunication with original authors We contacted the authors onmultiple occasions regarding some of the hyperparameters used in their experiments, to which they promptly replied with very detailed explanations.\nReScience C 7.2 (#13) \u2013 Mizrahi, Y\u00fcksel and Kyzy 2021 2\n1 Introduction\nGradient clipping is a well-established technique in machine learning, usually motivated by its benefits in optimization. For example, clipping is used extensively to remedy the well-known problem of exploding gradients [1], commonly faced when training recurrent neural networks. Intuitively, it ensures that the norm of the gradient behaves well under iterates of optimization. Indeed, Zhang et al. [2] provide a theoretical explanation of the improved convergence speed of gradient clipping over standard gradient descent. In this work, however, we reproduce the paper \u201dCan gradient clipping mitigate label noise?\u201d (referenced as \u201dthe paper\u201d or \u201dthe original paper\u201d) byMenon et al. [3] (referenced as \u201dthe authors\u201d) which focuses on robustness properties of gradient clipping. Informally, clipping caps the influence of any descent direction, whichmight help in the presence of label noise. Startingwith this intuition, the authors studywhether clipping can alleviate the problem of label noise studied in Ekholm and Palmgren [4], Menon et al. [5], and Zhang and Sabuncu [6]. More specifically, they analyze the problem under symmetric label noise with the following simple linear setting: stochastic gradient descent with a linear model in a binary classification task. Before turning our attention to the paper s\u0313 experiments, which are themain focus of this reproducibility work, we state twomain theoretical findings in this linear setup and the resulting novel extension of the cross-entropy loss function: \u2022 Gradient clipping does not provide label noise robustness even in this simple lin-\near setup. Specifically, clipping is linked to using a Huberised loss, which preserves classification-calibration but is not robust to symmetric label noise.\n\u2022 A new clipping variant for composite losses is proposed, where only the contribution from the base loss is considered for clipping. The equivalent partially Huberised loss preserves classification-calibration and is robust to symmetric label noise.\n\u2022 The resulting multi-class generalization of the partially Huberised cross-entropy loss is given in Equation 1. Suppose we have softmax probability estimates p\u03b8(x, y), then the partiallyHuberised softmax cross-entropy loss (PHuber-CE) is defined for \u03c4 > 1 as:\n\u2113\u03b8(x, y) = { \u2212\u03c4 \u00b7 p\u03b8(x, y) + log \u03c4 + 1, if p\u03b8(x, y) \u2264 1\u03c4 \u2212 log p\u03b8(x, y), otherwise.\n(1)\nThen, the authors evaluate their partially Huberised loss in experiments on synthetic data (referenced as \u201dsynthetic experiments\u201d) to demonstrate its behavior under symmetric label noise. They show symmetric label noise scenarios that defeat the logistic loss and the Huberised logistic loss, but not the partially Huberised logistic loss. Moreover, they assess the effectiveness of partial Huberisation on real-world datasets subject to symmetric label noise (referenced as \u201dreal-world experiments\u201d). They empirically verify that partially Huberised versions of existing losses behave well in the presence of symmetric label noise, through deep-learning experiments on theMNIST, CIFAR-10 and CIFAR-100 datasets. We thoroughly reproduce the synthetic and real-world experiments in section 3 and section 4 respectively. Then, we evaluate the experimental results in section 5 and conclude with the assessment of empirical claims in section 6.\n2 Background\nGradient clipping. Consider a supervised learning task with samples (x, y) \u2208 (X \u00d7Y) \u223c D, and a loss function l\u03b8 : X \u00d7Y \u2192 R. For this setting the gradient g(\u03b8) and the clipped gradient g\u0304\u03c4 (\u03b8) are defined as follows:\nReScience C 7.2 (#13) \u2013 Mizrahi, Y\u00fcksel and Kyzy 2021 3\ng(\u03b8) = 1\nN N\u2211 n=1 \u2207l\u03b8(xn, yn) g\u0304\u03c4 (\u03b8) = { \u03c4 g(\u03b8)\u2225g(\u03b8)\u22252 if \u2225g(\u03b8)\u22252 \u2265 \u03c4 g(\u03b8) otherwise.\nLabel noise. In classification under label noise, one has samples from a noisy distribution PD\u0304(x, y) instead of a clean distribution PD(x, y). For example, under symmetric label noise, all instances have a constant probability of their labels being flipped uniformly to any of the other classes. The task remains to minimize risk over the clean distribution D. Some recent loss-based proposals for learning under symmetric label noise are the linear or unhinged loss [7] and the generalized cross-entropy loss [6]. Huberised losses. Huberised and partially Huberised losses, as defined in the paper, are closely related to the Huber loss [8], which is widely employed in robust regression. In the binary classification setting, for a predictor f : X \u2192 R and labels y \u2208 {\u00b11}, these losses are derived from the logistic loss \u03d5(f(x) \u00b7 y) = \u03d5(z) = log(1 + e\u2212z), which can also be written as \u03d5(z) = \u03c6(F (z)), with the base loss \u03c6(u) = \u2212 logu and the link function F (z) = \u03c3(z). The Huberised logistic loss \u03d5\u0304\u03c4 (Equation 2) linearises the entire logistic loss beyond a certain threshold, while the partially Huberised logistic loss \u03d5\u0303\u03c4 (Equation 3) linearises only the base loss but leaves the link function intact.\n\u03d5\u0304\u03c4 (z) = { \u2212\u03c4 \u00b7 z \u2212 log(1\u2212 \u03c4)\u2212 \u03c4 \u00b7 \u03c3\u22121(\u03c4) if z \u2264 \u2212\u03c3\u22121(\u03c4) log (1 + e\u2212z) otherwise. (2)\n\u03d5\u0303\u03c4 (z) =\n{ \u2212\u03c4 \u00b7 \u03c3(z) + log \u03c4 + 1 if z \u2264 \u03c3\u22121 ( 1 \u03c4 ) log (1 + e\u2212z) otherwise. (3)\nThe partially Huberised softmax cross-entropy loss (Equation 1) is obtained by applying that same partial Huberisation to the softmax cross-entropy loss, in which the link function is a softmax instead of a sigmoid. For more information on Huberised losses, we kindly refer to the original paper [3].\n3 Synthetic experiments\nWe now study two synthetic experiments proposed by the authors to show the existence of label noise scenarios that defeatHuberised but not partiallyHuberised losses. Wewill start by discussing the 2D setting proposed in Long and Servedio [9] and then discuss the 1D outliers setting given in Ding [10]. These experiments are fully re-implemented with NumPy [11] and SciPy [12]. Experimental setups including methods and hyperparameters are fully verified according to the original paper and in necessary cases, according to the additional details obtained from the authors. Our experiments are configurable through the Hydra framework [13]. Our code re-implementing both the synthetic and real-world experiments is available at: https://github.com/dmizr/phuber\n3.1 Long and Servedio dataset Long and Servedio [9] consider a set of four positive labeled points: one large margin example (1, 0), one puller example (\u03b3, 5\u03b3) and two penalizer examples (\u03b3,\u2212\u03b3)where 0 < \u03b3 < 16 , in a binary classification task with a linear model without a bias term. The halfspace x1 > 0 correctly classifies all the samples. However, one can show that under symmetric label noise, minimizing over a wide range of convex losses with a suitable \u03b3 will result in a predictor equivalent to a random predictor. The authors build on Long and Servedio [9], and consider amixture of six isotropic Gaussians N (\u00b5i, \u03c32I2), with \u03c3 = 0.01 and \u00b5i \u2208 {\u00b1(1, 0),\u00b1(\u03b3, 5\u03b3),\u00b1(\u03b3,\u2212\u03b3)} \u2282 R2, with \u03b3 = 124 . Mixing weights are 1 4 for the two Gaussians centered around \u00b1(\u03b3,\u2212\u03b3) and 1 8\nfor the rest. An instance (x1, x2) is labeled positive if x1 \u2265 0 and negative otherwise.\nReScience C 7.2 (#13) \u2013 Mizrahi, Y\u00fcksel and Kyzy 2021 4\nN = 1000 random samples are drawn from this distribution, and the label of each sample is flipped with corruption probability \u03c1 < 0.5. Then, a linear classifier is trained using Scipy s\u0313 SLSQP (Sequential Least Squares Programming) optimizer for a maximum of 100 iterations with each of the following losses:\n\u2022 the logistic loss \u2022 the Huberised version of the logistic loss, with \u03c4 = \u03c3(\u22121) \u2248 0.26 \u2022 the partially Huberised version of the logistic loss, with \u03c4 = 1 + e\u22121 \u2248 1.36\nAfter contacting the authors, we found that the above \u03c4 values were used instead of the values provided in the original paper, which were \u03c4 = 1.0 and \u03c4 = 2.0 for the Huberised and the partially Huberised loss respectively. Once trained, the classifier is evaluated on 500 clean test samples. Figure 1a and Figure 1b show our results over 500 independent runs for \u03c1 = 0.45 and \u03c1 = 0.2 respectively. When using \u03c1 = 0.45, as stated in the original paper, we fail to reproduce a figure that exactly matches the authors\u02bc results. However, through experimentation, we found that for a lower level of noise corruption such as \u03c1 = 0.2, we get results that are very similar to the original paper, with the partially Huberised loss always achieving perfect classification, while the logistic and Huberised losses succumb to label noise and perform no better than chance.\n3.2 Outliers dataset The 1D setting fromDing [10] is composed of 10,000 linearly separable inliers: 5000 samples from the unit variance GaussianN (1, 1)with positive label, and 5000 samples from the mirror image N (\u22121, 1) with negative label. In addition, 50 outliers are added: 25 samples fromN (\u2212200, 1) with positive label, and 25 samples fromN (200, 1) with negative label. Assuming a linear model characterized by a scalar \u03b8 \u2208 R, we comparatively evaluate the empirical risk with andwithout outliers. We use the same three losses as in subsection 3.1 but with \u03c4 = 0.1 and \u03c4 = 1.1 for the Huberised and partially Huberised loss respectively. 1 Figure 1c shows our results where dashed and solid curves represent the cases with and without outliers respectively. As in the original paper, the optimal solutions for the logistic and Huberised loss are changed from \u03b8\u2217 = +\u221e to \u03b8\u2217 = 0 with the introduction of outliers, whereas the partially Huberised loss remains intact.\n1In the original paper, the \u03c4 values mistakenly reported as 1.0 and 2.0, along with the values in subsection 3.1. These updated values are obtained from the authors, after informing them \u03c4 = 1.0 for Huberisation is equivalent to keeping base loss intact.\nReScience C 7.2 (#13) \u2013 Mizrahi, Y\u00fcksel and Kyzy 2021 5\n4 Real-world experiments\nWe now consider the deep learning experiments performed on three image classificationdatasets: MNIST, CIFAR-10 andCIFAR-100. These experimentswere fully re-implemented with PyTorch [14], according to the description from the paper and implementation details obtained from the authors after contacting them. Configuration management for these experiments was done with the help of the Hydra framework [13].\n4.1 MNIST\nMethodology \u2014MNIST is a dataset of handwritten digits, consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image associated with a label from 10 distinct classes. The dataset is normalized using the mean and standard deviation from the training set, and no data augmentation is applied. The training labels are then corrupted with symmetric noise at flip probability \u03c1 \u2208 {0.0, 0.2, 0.4, 0.6}. As in the original paper, the same random seed is used to corrupt the training labels across all trials. We use a LeNet-5 [15], with a few modifications in order to reproduce the authors\u02bc settings as accurately as possible. Most notably, the tanh activation layers from the original LeNet are changed toReLU , and the weights are initialized according to a truncated normal distribution with standard deviation \u03c3 = 0.1. This model is trained for 20 epochs using Adam [16] with batch sizeN = 32, and weight decay of 10\u22123. The initial learning rate is set to 0.001, and is lowered following an exponential decay schedule with decay rate 0.1 and decay steps of 10,000. That is, the learning rate at iteration n is set to: \u03b7n = \u03b70 \u00b7 rn/s, with \u03b70 = 0.001, r = 0.1 and s = 104. According to the authors, these hyperparameter values were chosen to obtain a good baseline performance in a setting with no label noise. For each level of label noise corruption, the test set accuracy of 6 different loss functions is compared:\n\u2022 the cross-entropy loss (CE) \u2022 the linear or unhinged loss [7] \u2022 the generalized cross-entropy loss (GCE), with \u03b1 = 0.7 [6] \u2022 the cross-entropy loss, with global gradient clipping applied using a max norm threshold of \u03c4 = 0.1 \u2022 the partially Huberised version of the cross-entropy loss (PHuber-CE), with \u03c4 = 10 \u2022 the partially Huberised version of the generalized cross-entropy loss (PHuber-GCE), with \u03b1 = 0.7 and \u03c4 = 10. The CE loss serves as a baseline, while the linear and GCE losses serve as representative noise-robust losses. The model and hyperparameters used are identical for all losses at all levels of label noise. For each of the real-world experiments and for each of the partially Huberised losses, the authors selected \u03c4 \u2208 {2, 10} so as to maximize accuracy on a validation set, in a setting with flip probability \u03c1 = 0.6.\nComputational requirements \u2014 This LeNetmodelwas trainedwith aNvidia RTX 2080 Ti GPU. Each run took roughly 2 minutes. Fully reproducing the authors\u02bc experiments required training this model 72 times, in order to do 3 trials for each combination of loss function and level of label noise. This resulted in a total training time of around 2 hours.\nResults \u2014 Our results are reported in Table 1, and a comparison with the original paper s\u0313 results can be found in Figure 2. Our reproductionmatches the results from the original paper for both the PHuber-CE and PHuber-GCE losses, although the CE, CE with gradient clipping and linear losses perform considerably better at high levels of label noise than what was reported. As a consequence, the partially Huberised version of these losses do not outperform the base losses at high levels of label noise, contrary to the\nReScience C 7.2 (#13) \u2013 Mizrahi, Y\u00fcksel and Kyzy 2021 6\noriginal paper s\u0313 results. It is of note that in our reproduction, all losses, except for the CE loss with gradient clipping, perform comparably, with a test accuracy higher than 97.5% at all levels of label noise.\n4.2 CIFAR-10 and CIFAR-100\nMethodology \u2014 The CIFAR-10 and CIFAR-100 datasets [17] both consist of a training set of 50,000 examples and a test set of 10,000 examples. Each example is a 32 \u00d7 32 color image, associated with a label from 10 distinct classes for CIFAR-10, and 100 distinct classes for CIFAR-100. Both datasets are normalized using per-channel mean and standard deviation, and the standard data augmentation for these datasets is applied, akin to Zagoruyko and Komodakis [18]. That is, images are zero-padded with 4 pixels on each side to obtain a 40\u00d740 image, and then a random 32\u00d732 crop is extracted andmirrored horizontally with 50% probability. As in the MNIST experiment, the training labels are corrupted with symmetric noise at flip probability \u03c1 \u2208 {0.0, 0.2, 0.4, 0.6}, with identical noise seed across trials. For both of these experiments, we use a ResNet-50 [19], as implemented in Liu [20]. This implementation of the ResNet-50 differs from the one described byHe et al. [19] tomake it more appropriate for classification on small images. The number of filters per layer is identical, but the first layer, originally a 7x7 convolutional layer with stride 2 and padding 3, is changed to a 3x3 convolutional layer with stride 1 and padding 1, and the max-pooling layer that follows is removed. By removing these early downsampling layers, this architecture performs better on CIFAR-10 and CIFAR-100 than the original ResNet-50 2, which was designed for classification on ImageNet [21]. We decided to use such an implementation for several reasons: First, it is used inmany popular papers performing classification on CIFAR with ResNets, such as DeVries and Taylor [22], Zhang et al. [23], Li, Socher, and Hoi [24], and Zhang et al. [25]. Second, using the original ResNet50 yielded poor results, especially on partially Huberised losses. Third, after contacting the authors about their implementation, they confirmed using a ResNet-50 with some of\nReScience C 7.2 (#13) \u2013 Mizrahi, Y\u00fcksel and Kyzy 2021 7\nthe early downsampling layers removed, but could not provide more details as to which layers were specifically changed or removed. For CIFAR-10, this ResNet is trained for 400 epochs using SGDwith Nesterovmomentum 0.1 [26, 27], batch size N = 64, and weight decay of 5\u00d7 10\u22124. 3 The initial learning rate is set to 0.1 and is divided by 10 at the 160th, 300th and 360th epoch. For CIFAR-100, this ResNet is trained for 200 epochs using SGD with Nesterov momentum 0.1, batch size N = 128, and weight decay of 5 \u00d7 10\u22124. 4 The initial learning rate is set to 0.1 and is divided by 5 at the 60th, 120th and 160th epoch. According to the authors, these hyperparameters were partially based on the setting from DeVries and Taylor [22], and were chosen to obtain a good performance with CE in a setting with no label noise. As in the MNIST experiment, the test set accuracy of the CE, CE with gradient clipping, linear, GCE, PHuber-CE and PHuber-GCE losses are compared. The tunable parameters for these losses are identical to the ones used in the MNIST experiment, except for PHuber-CE for CIFAR-10, where \u03c4 = 2. The model and hyperparameters used are identical for all losses at all levels of label noise. We also report an additional experiment, wherewe train amodel onCIFAR-100 using the PHuber-CE loss with \u03c4 = 50. This corresponds to linearizing the base loss at probability threshold 0.02.\nComputational requirements \u2014Weuse aNvidia RTX 2080 Ti GPU to train thesemodels. With full precision training, a run on CIFAR-10 takes approximately 11 hours, while a run on CIFAR-100 takes approximately 4 hours, due to the lower amount of epochs and higher batch size. In order to accelerate the training process, we implement mixed precision training [28], which results in a 2x speed-up with no decrease in accuracy compared to full precision training. Fully reproducing the authors\u02bc experiments required training each model 72 times, resulting in a total training time of around 400 hours for the CIFAR-10 experiments, and around 150 hours for the CIFAR-100 experiments.\nResults \u2014 Our results are reported in Table 2, and a comparison with the original paper s\u0313 results can be found in Figure 3 and Figure 4. On CIFAR-10, our reproduction achieves comparable or better results than the original paper for nearly all configurations, except for the Linear, GCE and PHuber-GCE losses which perform worse for \u03c1 = 0.6. Surprisingly, the CE loss with gradient clipping performs considerably better than what was reported in the presence of label noise, achieving the second-highest accuracy for \u03c1 = 0.6, behind PHuber-CE. Similar to the original paper s\u0313 results, PHuber-CE with \u03c4 = 2 is competitive with CE in the absence of label noise, and achieves very good results under label noise, outperforming all the other losses. Notably, in our reproduction, PHuber-CE outperforms the linear loss for \u03c1 = 0.4, which was not the case in the original paper. OnCIFAR-100, our reproduction achieves better results than the original paper for nearly all configurations. Most notably, the accuracy of the CE, GCE and PHuber-GCE losses are noticeably better at all levels of noise corruption. As in the original paper, PHuberGCE with \u03c4 = 10 achieves the best accuracy out of all losses for \u03c1 = 0.4 and \u03c1 = 0.6, and performs comparably to GCE for \u03c1 = 0.0 and \u03c1 = 0.2. Unlike the paper s\u0313 results, PHuber-CE with \u03c4 = 10 performs quite poorly compared to CE, even in settings with\n2In the ResNet paper, He et al. also propose ResNet architectures suited for CIFAR-10 classification, such as the ResNet-44 and ResNet-56, which have fewer filters per layer compared to the implementations from Liu [20], resulting in faster training at the cost of lower accuracy. These ResNet architectures were not used in our reproduction as the authors specifically mentioned using a ResNet-50.\n3In the original paper, the weight decay is mistakenly reported as 5 \u00d7 10\u22123, and it was not specified that the type of momentum used was Nesterov momentum. These updated hyperparameters were obtained from the authors, after informing them of our difficulty reproducing their experiments with the values from the paper.\n4See previous footnote.\nReScience C 7.2 (#13) \u2013 Mizrahi, Y\u00fcksel and Kyzy 2021 8\nhigh levels of label noise where it should supposedly perform well. However, with our additional experiment using PHuber-CE with \u03c4 = 50, we show that there exist values of \u03c4 for which PHuber-CE performs comparably to CE in the noise-free case, and outperforms CE at high levels of label noise.\nReScience C 7.2 (#13) \u2013 Mizrahi, Y\u00fcksel and Kyzy 2021 9\n5 Discussion\nWe now discuss whether our experimental results support the claims of the paper. For the Long and Servedio experiment, when reusing exactly the parameters described in the paper and in the authors\u02bc clarifications, our results do not perfectly match those from the original paper. After experimenting with some of the parameters, we did find values that produce results nearly identical to those shown in the paper. Nonetheless, these results still support the claim the authors made for the synthetic experiments, namely, that there exist label noise scenarios for both the Long and Servedio [9] setting and the Ding [10] setting which defeat a Huberised but not a partially Huberised loss. While we made a considerable effort to ensure that our implementation matches the paper s\u0313 description, the difference in results could be due to some minor differences in our implementations, or to a difference in the random seeds used to sample from the mixture model and flip the labels. For the MNIST experiment, all losses, except for CE with clipping, perform comparably, achieving very high accuracy for all levels of label noise. This differs from the results of the original paper, where CE and linear losses were affected by label noise. As a result, it is difficult to support or reject any claim made regarding these losses with this experiment. For both the CIFAR-10 and CIFAR-100 experiments, our results differ even after fixing the hyperparameters which were accidentally misreported in the original paper, with our implementation yielding a noticeably higher test accuracy for the CE loss with clipping on CIFAR-10, and the CE, GCE and PHuber-GCE losses on CIFAR-100. This is likely due to the ResNet-50 architecture used, as the generally higher accuracy could be explained if our model happens to have a higher number of parameters than theirs. The deep learning framework used could also lead to different results, as the authors mentioned using TensorFlow while we used PyTorch. Finally, this could also be caused by the random seed used to add label noise, although we did not notice any significant difference in results when changing this seed. For theCIFAR-10 experiment, our reproduction supports the claim that for these specific hyperparameters, partially Huberised losses are competitive with the base loss in the noise-free case and can outperform it under label noise. In addition, this experiment also shows that PHuber-CE can be very effective at mitigating symmetric label noise, as it performs considerably better than the representative noise-robust losses at high levels of label noise. For the CIFAR-100 experiment, our reproduction shows that PHuber-GCE loss with \u03c4 = 10 is competitive with the base loss (GCE) in the noise-free case and can outperform it at high levels of label noise, which supports the aforementioned claim. However, this claim does not hold for the PHuber-CE loss with \u03c4 = 10, which performs worse than CE in all cases. Despite that, we show with our additional experiment that there exists a value of \u03c4 for which the PHuber-CE loss performs comparably to CE in the noise-free case, and improves upon it under label noise. Our additional experiment shows that the value of \u03c4 plays a crucial role in the performance of partially Huberised losses. Both the PHuber-CE and GCE losses interpolate between the linear and the CE loss. PHuber-CE and GCEmimic the linear loss for \u03c4 \u2192 1 and \u03b1 = 1 respectively, while for \u03c4 \u2192 +\u221e and \u03b1 \u2192 0, they mimic the CE loss. As the linear loss fails to train properly in our CIFAR-100 experiment, it is expected to obtain poor results for these losses if the tunable parameter usedmakes them too similar to the linear loss. As PHuber-GCE combines both of these losses, it can also perform poorly in such a scenario. Furthermore, the CE loss with gradient clipping also has a tunable parameter which strongly affects performance, as our reproduction shows that for a max norm \u03c4 = 0.1, CE with clipping can perform significantly better than CE on CIFAR-10, and significantly worse on CIFAR-100. In order to properly compare these losses, it would therefore be of interest to find, for\nReScience C 7.2 (#13) \u2013 Mizrahi, Y\u00fcksel and Kyzy 2021 10\neach level of label noise, the tunable parameter values for which they perform best, by using random search or a hyperparameter tuning framework such as Optuna [29] on a validation set. While such a hyper-parameter search has a high computational cost, it would offer some valuable insights on how well each of these losses performs, and how sensitive they are to changes to their tunable parameters. We leave such exploration for future work.\n6 Conclusion\nIn this work, we fully re-implement the experiments performed in Menon et al. [3]. For the synthetic experiments, our results differ when using the exact values described in the paper, although they still support themain claim, and by slightlymodifying some experiment settings, we obtain results almost identical to those of the original paper. Our results also differ for the deep learning experiments, with some of the baselines performing better than described. Nonetheless, these experiments still support the claim that partiallyHuberised losses performwell on real-world datasets subject to label noise. Our additional experiment also provides further insight on the performance of partially Huberised losses, as it empirically shows that the value of \u03c4 can play an important role in the performance of models trained with these losses. We thus believe it would be of interest to perform further experiments focused on tuning these losses for different levels of label noise, although this would incur a relatively high computational cost."}], "title": "[Re] Can gradient clipping mitigate label noise?", "year": 2021}